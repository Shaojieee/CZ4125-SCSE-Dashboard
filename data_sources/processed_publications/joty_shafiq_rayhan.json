[{"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:FPJr55Dyh1AC", "title": "Align before fuse: Vision and language representation learning with momentum distillation", "authors": ["Junnan Li", "Ramprasaath R Selvaraju", "Akhilesh Deepak Gotmare", "Shafiq Joty", "Caiming Xiong", "Steven Hoi"], "description": "Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR , ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and models are available at https://github. com/salesforce/ALBEF.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [15, 248, 567, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:FAceZFleit8C", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "authors": ["Yue Wang", "Weishi Wang", "Shafiq Joty", "Steven CH Hoi"], "description": "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https: //github.com/salesforce/CodeT5 .", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [5, 160, 401, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:7PzlFSSx8tAC", "title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "authors": ["Pengfei Liu", "Shafiq Joty", "Helen Meng"], "description": "The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task. We propose a general class of discriminative models based on recurrent neural networks (RNNs) and word embeddings that can be successfully applied to such tasks without any taskspecific feature engineering effort. Our experimental results on the task of opinion target identification show that RNNs, without using any hand-crafted features, outperform feature-rich CRF-based models. Our framework is flexible, allows us to incorporate other linguistic features, and achieves results that rival the top performing systems in SemEval-2014.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 23, 50, 58, 61, 75, 78, 52, 45, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:UxriW0iASnsC", "title": "Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models", "authors": ["Jiuxiang Gu", "Jianfei Cai", "Shafiq R Joty", "Li Niu", "Gang Wang"], "description": "Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [9, 61, 85, 90, 97, 62, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:hMsQuOkrut0C", "title": "DeepER--Deep Entity Resolution", "authors": ["Muhammad Ebraheem", "Saravanan Thirumuruganathan", "Shafiq Joty", "Mourad Ouzzani", "Nan Tang"], "description": "Entity resolution (ER) is a key data integration problem. Despite the efforts in 70+ years in all aspects of ER, there is still a high demand for democratizing ER - humans are heavily involved in labeling data, performing feature engineering, tuning parameters, and defining blocking functions. With the recent advances in deep learning, in particular distributed representation of words (a.k.a. word embeddings), we present a novel ER system, called DeepER, that achieves good accuracy, high efficiency, as well as ease-of-use (i.e., much less human efforts). For accuracy, we use sophisticated composition methods, namely uni- and bi-directional recurrent neural networks (RNNs) with long short term memory (LSTM) hidden units, to convert each tuple to a distributed representation (i.e., a vector), which can in turn be used to effectively capture similarities between tuples. We consider both the case where pre-trained word embeddings are available as well the case where they are not; we present ways to learn and tune the distributed representations. For efficiency, we propose a locality sensitive hashing (LSH) based blocking approach that uses distributed representations of tuples; it takes all attributes of a tuple into consideration and produces much smaller blocks, compared with traditional methods that consider only a few attributes. For ease-of-use, DeepER requires much less human labeled data and does not need feature engineering, compared with traditional machine learning based approaches which require handcrafted features, and similarity functions along with their associated thresholds. We evaluate our algorithms on multiple datasets\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [10, 33, 76, 83, 81, 62, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:D03iK_w7-QYC", "title": "Robust classification of crisis-related data on social networks using convolutional neural networks", "authors": ["Dat Nguyen", "Kamela Ali Al Mannai", "Shafiq Joty", "Hassan Sajjad", "Muhammad Imran", "Prasenjit Mitra"], "description": "The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. The scarcity of labeled data, particularly in the early hours of a crisis, delays the learning process. Existing classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for identifying useful tweets during a crisis situation. At the onset of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [12, 20, 42, 66, 71, 59, 33, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:9Nmd_mFXekcC", "title": "Conversational agents in health care: scoping review and conceptual analysis", "authors": ["Lorainne Tudor Car", "Dhakshenya Ardhithy Dhinagaran", "Bhone Myint Kyaw", "Tobias Kowatsch", "Shafiq Joty", "Yin-Leng Theng", "Rifat Atun"], "description": "Background Conversational agents, also known as chatbots, are computer programs designed to simulate human text or verbal conversations. They are increasingly used in a range of fields, including health care. By enabling better accessibility, personalization, and efficiency, conversational agents have the potential to improve patient care. Objective This study aimed to review the current applications, gaps, and challenges in the literature on conversational agents in health care and provide recommendations for their future research, design, and application. Methods We performed a scoping review. A broad literature search was performed in MEDLINE (Medical Literature Analysis and Retrieval System Online; Ovid), EMBASE (Excerpta Medica database; Ovid), PubMed, Scopus, and Cochrane Central with the search terms \u201cconversational agents,\u201d \u201cconversational AI,\u201d \u201cchatbots,\u201d and associated synonyms. We also searched the gray literature using sources such as the OCLC (Online Computer Library Center) WorldCat database and ResearchGate in April 2019. Reference lists of relevant articles were checked for further articles. Screening and data extraction were performed in parallel by 2 reviewers. The included evidence was analyzed narratively by employing the principles of thematic analysis. Results The literature search yielded 47 study reports (45 articles and 2 ongoing clinical trials) that matched the inclusion criteria. The identified conversational agents were largely delivered via smartphone apps (n=23) and used free text only as the main input (n=19) and\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [7, 57, 92, 96, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:rO6llkc54NcC", "title": "Sleep quality prediction from wearable data using deep learning", "authors": ["Aarti Sathyanarayana", "Shafiq Joty", "Luis Fernandez-Luque", "Ferda Ofli", "Jaideep Srivastava", "Ahmed Elmagarmid", "Teresa Arora", "Shahrad Taheri"], "description": "Background: The importance of sleep is paramount to health. Insufficient sleep can reduce physical, emotional, and mental well-being and can lead to a multitude of health complications among people with chronic conditions. Physical activity and sleep are highly interrelated health behaviors. Our physical activity during the day (ie, awake time) influences our quality of sleep, and vice versa. The current popularity of wearables for tracking physical activity and sleep, including actigraphy devices, can foster the development of new advanced data analytics. This can help to develop new electronic health (eHealth) applications and provide more insights into sleep science.Objective: The objective of this study was to evaluate the feasibility of predicting sleep quality (ie, poor or adequate sleep efficiency) given the physical activity wearable data during awake time. In this study, we focused on predicting good or poor sleep efficiency as an indicator of sleep quality.Methods: Actigraphy sensors are wearable medical devices used to study sleep and physical activity patterns. The dataset used in our experiments contained the complete actigraphy data from a subset of 92 adolescents over 1 full week. Physical activity data during awake time was used to create predictive models for sleep quality, in particular, poor or good sleep efficiency. The physical activity data from sleep time was used for the evaluation. We compared the predictive performance of traditional logistic regression with more advanced deep learning methods: multilayer perceptron (MLP), convolutional neural network (CNN), simple Elman-type recurrent neural network (RNN), long short-term\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 9, 32, 39, 41, 46, 40, 32, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:yB1At4FlUx8C", "title": "Gedi: Generative discriminator guided sequence generation", "authors": ["Ben Krause", "Akhilesh Deepak Gotmare", "Bryan McCann", "Nitish Shirish Keskar", "Shafiq Joty", "Richard Socher", "Nazneen Fatema Rajani"], "description": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [35, 98, 83, 6]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:9ZlFYXVOiuMC", "title": "Codra: A novel discriminative framework for rhetorical analysis", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Raymond T Ng"], "description": "Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence structure. In this article, we present CODRA\u2014 a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse.  CODRA comprises a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the\u00a0\u2026", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [4, 21, 21, 19, 39, 31, 33, 23, 14, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:5nxA0vEk-isC", "title": "Combining intra-and multi-sentential rhetorical parsing for document-level discourse analysis", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Raymond Ng", "Yashar Mehdad"], "description": "We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 17, 20, 25, 19, 18, 37, 20, 17, 12, 8, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:D_sINldO8mEC", "title": "ANR: Aspect-based neural recommender", "authors": ["Jin Yao Chin", "Kaiqi Zhao", "Shafiq Joty", "Gao Cong"], "description": "Textual reviews, which are readily available on many e-commerce and review websites such as Amazon and Yelp, serve as an invaluable source of information for recommender systems. However, not all parts of the reviews are equally important, and the same choice of words may reflect a different meaning based on its context. In this paper, we propose a novel end-to-end Aspect-based Neural Recommender (ANR) to perform aspect-based representation learning for both users and items via an attention-based component. Furthermore, we model the multi-faceted process behind how users rate items by estimating the aspect-level user and item importance by adapting the neural co-attention mechanism. Our proposed model concurrently address several shortcomings of existing recommender systems, and a thorough experimental study on 25 benchmark datasets from Amazon and Yelp shows that ANR\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 11, 24, 35, 37, 47, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:wbdj-CoPYUoC", "title": "Domain Adaptation with Adversarial Training and Graph Embeddings", "authors": ["Firoj Alam", "Shafiq Joty", "Muhammad Imran"], "description": "The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 14, 33, 38, 38, 21, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:tkaPQYYpVKoC", "title": "Unpaired image captioning via scene graph alignments", "authors": ["Jiuxiang Gu", "Shafiq Joty", "Jianfei Cai", "Handong Zhao", "Xu Yang", "Gang Wang"], "description": "Most of current image captioning models heavily rely on paired image-caption datasets. However, getting large scale image-caption paired data is labor-intensive and time-consuming. In this paper, we present a scene graph-based approach for unpaired image captioning. Our framework comprises an image scene graph generator, a sentence scene graph generator, a scene graph encoder, and a sentence decoder. Specifically, we first train the scene graph encoder and the sentence decoder on the text modality. To align the scene graphs between images and sentences, we propose an unsupervised feature alignment method that maps the scene graph features from the image to the sentence modality. Experimental results show that our proposed model can generate quite promising results without using any image-caption training pairs, outperforming existing methods by a wide margin.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [4, 22, 39, 45, 34, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:NhqRSupF_l8C", "title": "Using discourse structure improves machine translation evaluation", "authors": ["Francisco Guzm\u00e1n", "Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Preslav Nakov"], "description": "We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment-and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 17, 15, 10, 18, 15, 8, 10, 11, 8, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:BwyfMAYsbu0C", "title": "DAGA: Data augmentation with a generation approach for low-resource tagging tasks", "authors": ["Bosheng Ding", "Linlin Liu", "Lidong Bing", "Canasai Kruengkrai", "Thien Hai Nguyen", "Shafiq Joty", "Luo Si", "Chunyan Miao"], "description": "Data augmentation techniques have been widely used to improve machine learning performance as they enhance the generalization capability of models. In this work, to generate high quality synthetic data for low-resource tagging tasks, we propose a novel augmentation method with language models trained on the linearized labeled sentences. Our method is applicable to both supervised and semi-supervised settings. For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks. For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base. The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 19, 44, 47, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:4fKUyHm3Qg0C", "title": "SegBot: A Generic Neural Text Segmentation Model with Pointer Network", "authors": ["Jing Li", "Aixin Sun", "Shafiq Joty"], "description": "Text segmentation is a fundamental task in natural language processing that comes in two levels of granularity:(i) segmenting a document into a sequence of topical segments (topic segmentation), and (ii) segmenting a sentence into a sequence of elementary discourse units (EDU segmentation). Traditional solutions to the two tasks heavily rely on carefully designed features. The recently proposed neural models do not need manual feature engineering, but they either suffer from sparse boundary tags or they cannot well handle the issue of variable size output vocabulary. We propose a generic end-to-end segmentation model called SEGBOT. SEGBOT uses a bidirectional recurrent neural network to encode input text sequence. The model then uses another recurrent neural network together with a pointer network to select text boundaries in the input sequence. In this way, SEGBOT does not require hand-crafted features. More importantly, our model inherently handles the issue of variable size output vocabulary and the issue of sparse boundary tags. In our experiments, SEGBOT outperforms state-of-the-art models on both topic and EDU segmentation tasks.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 12, 23, 29, 18, 20, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:yD5IFk8b50cC", "title": "A neural local coherence model", "authors": ["Dat Tien Nguyen", "Shafiq Joty"], "description": "We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [4, 11, 15, 19, 18, 12, 15, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:LkGwnXOMwfcC", "title": "A novel discriminative framework for sentence-level discourse analysis", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Raymond Ng"], "description": "We propose a complete probabilistic discriminative framework for performing sentencelevel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin.", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 2, 15, 14, 10, 8, 6, 11, 6, 6, 8, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&citation_for_view=hR249csAAAAJ:fEOibwPWpKIC", "title": "It's Morphin'Time! Combating Linguistic Discrimination with Inflectional Perturbations", "authors": ["Samson Tan", "Shafiq Joty", "Min-Yen Kan", "Richard Socher"], "description": "Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [8, 26, 33, 20, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:XiVPGOgt02cC", "title": "VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions", "authors": ["Qing Li", "Qingyi Tao", "Shafiq Joty", "Jianfei Cai", "Jiebo Luo"], "description": "Most existing works in visual question answering (VQA) are dedicated to improving the accuracy of predicted answers, while disregarding the explanations. We argue that the explanation for an answer is of the same or even more importance compared with the answer itself, since it makes the question and answering process more understandable and traceable. To this end, we propose a new task of VQA-E (VQA with Explanation), where the computational models are required to generate an explanation with the predicted answer. We first construct a new dataset, and then frame the VQA-E problem in a multi-task learning architecture. Our VQA-E dataset is automatically derived from the VQA v2 dataset by intelligently exploiting the available captions. We have conducted a user study to validate the quality of explanations synthesized by our method. We quantitatively show that the additional supervision from explanations can not only produce insightful textual sentences to justify the answers, but also improve the performance of answer prediction. Our model outperforms the state-of-the-art methods by a clear margin on the VQA v2 dataset.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [4, 10, 17, 19, 21, 17, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:bnK-pcrLprsC", "title": "Unpaired Image Captioning by Language Pivoting", "authors": ["Jiuxiang Gu", "Shafiq Joty", "Jianfei Cai", "Wang Gang"], "description": "Image captioning is a multimodal task involving computer vision and natural language processing, where the goal is to learn a mapping from the image to its natural language description. In general, the mapping function is learned from a training set of image-caption pairs. However, for some language, large scale image-caption paired corpus might not be available. We present an approach to this unpaired image captioning problem by language pivoting. Our method can effectively capture the characteristics of an image captioner from the pivot language (Chinese) and align it to the target language (English) using another pivot-target (Chinese-English) parallel corpus. We evaluate our method on two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative comparisons against several baseline approaches demonstrate the effectiveness of our method.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 14, 11, 16, 25, 15, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:ZfRJV9d4-WMC", "title": "Vd-bert: A unified vision and dialog transformer with bert", "authors": ["Yue Wang", "Shafiq Joty", "Michael R Lyu", "Irwin King", "Caiming Xiong", "Steven CH Hoi"], "description": "Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [6, 14, 36, 23, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:Y0pCki6q_DkC", "title": "Unsupervised modeling of dialog acts in asynchronous conversations", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Chin-Yew Lin"], "description": "We present unsupervised approaches to the problem of modeling dialog acts in asynchronous conversations; ie, conversations where participants collaborate with each other at different times. In particular, we investigate a graph-theoretic deterministic framework and two probabilistic conversation models (ie, HMM and HMM+ Mix) for modeling dialog acts in emails and forums. We train and test our conversation models on (a) temporal order and (b) graph-structural order of the datasets. Empirical evaluation suggests (i) the graph-theoretic framework that relies on lexical and structural similarity metrics is not the right model for this task,(ii) conversation models perform better on the graphstructural order than the temporal order of the datasets and (iii) HMM+ Mix is a better conversation model than the simple HMM model.", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 9, 12, 4, 11, 8, 8, 8, 4, 1, 3, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:eq2jaN3J8jMC", "title": "Sentence-level evidence embedding for claim verification with hierarchical attention networks", "authors": ["Jing Ma", "Wei Gao", "Shafiq Joty", "Kam-Fai Wong"], "description": "Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. Our model consists of three main components: 1) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) An output layer predicts the verdict based on the embedded evidence. Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 9, 26, 24, 14, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:AvfA0Oy_GE0C", "title": "Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation", "authors": ["Alexander R Fabbri", "Simeng Han", "Haoyuan Li", "Haoran Li", "Marjan Ghazvininejad", "Shafiq Joty", "Dragomir Radev", "Yashar Mehdad"], "description": "Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ data augmentation via round-trip translation as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [8, 38, 30, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:t6usbXjVLHcC", "title": "Graph Based Semi-supervised Learning with Convolutional Neural Networks to Classify Crisis Related Tweets", "authors": ["Firoj Alam", "Shafiq Joty", "Muhammad Imran"], "description": "During time-critical situations such as natural disasters, rapid classification of data posted on social networks by affected people is useful for humanitarian organizations to gain situ-ational awareness and to plan response efforts. However, the scarcity of labeled data in the early hours of a crisis hinders machine learning tasks thus delays crisis response. In this work, we propose to use an inductive semi-supervised tech-nique to utilize unlabeled data, which is often abundant at the onset of a crisis event, along with fewer labeled data. Specif-ically, we adopt a graph-based deep learning framework to learn an inductive semi-supervised model. We use two real-world crisis datasets from Twitter to evaluate the proposed approach. Our results show significant improvements using unlabeled data as compared to only using labeled data.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 10, 16, 15, 17, 16, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:4JMBOYKVnBMC", "title": "Topic segmentation and labeling in asynchronous conversations", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Raymond T Ng"], "description": "Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 12, 7, 8, 9, 9, 5, 4, 6, 8, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:7T2F9Uy0os0C", "title": "Tree-Structured Attention with Hierarchical Accumulation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Steven CH Hoi", "Richard Socher"], "description": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with \"Hierarchical Accumulation\" to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German translation task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [8, 21, 25, 16, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:ML0RJ9NH7IQC", "title": "Data diversification: A simple strategy for neural machine translation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Kui Wu", "Ai Ti Aw"], "description": "We introduce Data Diversification: a simple but effective strategy to boost neural machine translation (NMT) performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset on which the final NMT model is trained. Our method is applicable to all NMT models. It does not require extra monolingual data like back-translation, nor does it add more computations and parameters like ensembles of models. Our method achieves state-of-the-art BLEU scores of 30.7 and 43.7 in the WMT'14 English-German and English-French translation tasks, respectively. It also substantially improves on 8 other translation tasks: 4 IWSLT tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala). We demonstrate that our method is more effective than knowledge distillation and dual learning, it exhibits strong correlation with ensembles of models, and it trades perplexity off for better BLEU score.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [7, 15, 29, 15, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:YFjsv_pBGBYC", "title": "Convkn at semeval-2016 task 3: Answer and question selection for question answering on arabic and english fora", "authors": ["Alberto Barr\u00f3n-Cedeno", "Daniele Bonadiman", "Giovanni Da San Martino", "Shafiq Joty", "Alessandro Moschitti", "Fahad Al Obaidli", "Salvatore Romeo", "Kateryna Tymoshenko", "Antonio Uva"], "description": "We describe our system, ConvKN, participating to the SemEval-2016 Task 3 \u201cCommunity Question Answering\u201d. The task targeted the reranking of questions and comments in real-life web fora both in English and Arabic. ConvKN combines convolutional tree kernels with convolutional neural networks and additional manually designed features including text similarity and thread specific features. For the first time, we applied tree kernels to syntactic trees of Arabic sentences for a reranking task. Our approaches obtained the second best results in three out of four tasks. The only task we performed averagely is the one where we did not use tree kernels in our classifier.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [9, 12, 20, 13, 8, 4, 2, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:VL0QpB8kHFEC", "title": "A Unified Linear-Time Framework for Sentence-Level Discourse Parsing", "authors": ["Xiang Lin", "Shafiq Joty", "Prathyusha Jwalapuram", "M Saiful Bari"], "description": "We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse tree in a top-down fashion. Both the segmenter and the parser are based on Pointer Networks and operate in linear time. Our segmenter yields an  score of 95.4, and our parser achieves an  score of 81.7 on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and approaching human agreement on both tasks (98.3 and 83.0 ).", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [7, 19, 17, 16, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:JV2RwH3_ST0C", "title": "Qcri: Answer selection for community question answering-experiment for arabic and english", "authors": ["Massimo Nicosia", "Simone Filice", "Alberto Barr\u00f3n-Cedeno", "Iman Saleh", "Hamdy Mubarak", "Wei Gao", "Preslav Nakov", "Giovanni Da San MARTINO", "Alessandro Moschitti", "Kareem Darwish", "Lluis Marquz MARQUZ", "Shafiq Joty", "Walid Magdy MAGDY"], "description": "This paper describes QCRI\u2019s participation in SemEval-2015 Task 3 \u201cAnswer Selection in Community Question Answering\u201d, which targeted real-life Web forums, and was offered in both Arabic and English. We apply a supervised machine learning approach considering a manifold of features including among others word n-grams, text similarity, sentiment analysis, the presence of specific words, and the context of a comment. Our approach was the best performing one in the Arabic subtask and the third best in the two English subtasks", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 24, 9, 9, 11, 4, 3, 2, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:hFOr9nPyWt4C", "title": "Thread-level information for comment classification in community question answering", "authors": ["Alberto Barr\u00f3n-Cedeno", "Simone Filice", "Giovanni Da San Martino", "Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Preslav Nakov", "Alessandro Moschitti"], "description": "Community Question Answering (cQA) is a new application of QA in social contexts (eg, fora). It presents new interesting challenges and research directions, eg, exploiting the dependencies between the different comments of a thread to select the best answer for a given question. In this paper, we explored two ways of modeling such dependencies:(i) by designing specific features looking globally at the thread; and (ii) by applying structure prediction models. We trained and evaluated our models on data from SemEval-2015 Task 3 on Answer Selection in cQA. Our experiments show that:(i) the thread-level features consistently improve the performance for a variety of machine learning models, yielding state-of-the-art results; and (ii) sequential dependencies between the answer labels captured by structured prediction models are not enough to improve the results, indicating that more information is needed in the joint model.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 16, 12, 12, 9, 7, 5, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:Dip1O2bNi0gC", "title": "Dialogue act recognition in synchronous and asynchronous conversations", "authors": ["Maryam Tavafi", "Yashar Mehdad", "Shafiq Joty", "Giuseppe Carenini", "Raymond Ng"], "description": null, "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 7, 7, 3, 4, 15, 7, 9, 1, 3, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:JQOojiI6XY0C", "title": "Zero-Resource Cross-Lingual Named Entity Recognition", "authors": ["M Saiful Bari", "Shafiq Joty", "Prathyusha Jwalapuram"], "description": "Recently, neural methods have achieved state-of-the-art (SOTA) results in Named Entity Recognition (NER) tasks for many languages without the need for manually crafted features. However, these models still require manually annotated training data, which is not available for many languages. In this paper, we propose an unsupervised cross-lingual NER model that can transfer NER knowledge from one language to another in a completely unsupervised way without relying on any bilingual dictionary or parallel data. Our model achieves this through word-level adversarial learning and augmented fine-tuning with parameter sharing and feature augmentation. Experiments on five different languages demonstrate the effectiveness of our approach, outperforming existing models by a good margin and setting a new SOTA for each language pair.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 9, 18, 11, 14, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:foquWX3nUaYC", "title": "MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER", "authors": ["Liu Linlin", "Ding Bosheng", "Bing Lidong", "Joty Shafiq", "Si Luo", "Miao Chunyan"], "description": null, "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [5, 25, 19, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:HoB7MX3m0LUC", "title": "Global thread-level inference for comment classification in community question answering", "authors": ["Shafiq Joty", "Alberto Barr\u00f3n-Cedeno", "Giovanni Da San Martino", "Simone Filice", "Llu\u00eds M\u00e0rquez", "Alessandro Moschitti", "Preslav Nakov"], "description": null, "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 13, 4, 14, 8, 2, 6, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:abG-DnoFyZgC", "title": "Cross-language learning with adversarial neural networks: Application to community question answering", "authors": ["Shafiq Joty", "Preslav Nakov", "Llu\u00eds M\u00e0rquez", "Israa Jaradat"], "description": "We address the problem of cross-language adaptation for question-question similarity reranking in community question answering, with the objective to port a system trained on one input language to another input language given labeled training data for the first language and only unlabeled data for the second language. In particular, we propose to use adversarial training of neural networks to learn high-level features that are discriminative for the main learning task, and at the same time are invariant across the input languages. The evaluation results show sizable improvements for our cross-language adversarial neural network (CLANN) model over a strong non-adversarial system.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 7, 11, 8, 7, 9, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:N5tVd3kTz84C", "title": "A unified neural coherence model", "authors": ["Han Cheol Moon", "Tasnim Mohiuddin", "Shafiq Joty", "Xu Chi"], "description": "Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these models often fail on harder tasks with more realistic application scenarios. In particular, the existing models underperform on tasks that require the model to be sensitive to local contexts such as candidate ranking in conversational dialogue and in machine translation. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 13, 15, 12, 8, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:d1gkVwhDpl0C", "title": "Complex question answering: unsupervised learning approaches and experiments", "authors": ["Yllias Chali", "Shafiq R Joty", "Sadid A Hasan"], "description": "Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (ie lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (ie empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 0, 4, 8, 2, 2, 3, 4, 3, 2, 7, 2, 3, 3, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:5ugPr518TE4C", "title": "Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach", "authors": ["Tasnim Mohiuddin", "Shafiq Joty", "Dat Nguyen"], "description": "We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 11, 8, 13, 6, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:O3NaXMp0MMsC", "title": "Pairwise Neural Machine Translation Evaluation", "authors": ["Francisco Guzm\u00e1n", "Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Preslav Nakov"], "description": null, "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 9, 4, 11, 6, 4, 6, 5, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:IjCSPb-OGe4C", "title": "Improving graph-based random walks for complex question answering using syntactic, shallow semantic and extended string subsequence kernels", "authors": ["Yllias Chali", "Sadid A Hasan", "Shafiq R Joty"], "description": "The task of answering complex questions requires inferencing and synthesizing information from multiple documents that can be seen as a kind of topic-oriented, informative multi-document summarization. In generic summarization the stochastic, graph-based random walk method to compute the relative importance of textual units (i.e. sentences) is proved to be very successful. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. This paper presents the impact of syntactic and semantic information in the graph-based random walk method for answering complex questions. Initially, we apply tree kernel functions to perform the similarity measures between sentences in the random walk framework. Then, we extend our work further to incorporate the Extended String Subsequence Kernel\u00a0\u2026", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 2, 5, 1, 4, 4, 2, 2, 4, 1, 8, 4, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:0KyAp5RtaNEC", "title": "Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading", "authors": ["Yifan Gao", "Chien-Sheng Wu", "Jingjing Li", "Shafiq Joty", "Steven CH Hoi", "Caiming Xiong", "Irwin King", "Michael R Lyu"], "description": "Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose Discern, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision \"yes/no/irrelevant\" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at https://github.com/Yifan-Gao/Discern.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 13, 14, 14, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:4OULZ7Gr8RgC", "title": "Machine translation evaluation with neural networks", "authors": ["Francisco Guzm\u00e1n", "Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Preslav Nakov"], "description": "We present a framework for machine translation evaluation using neural networks in a pairwise setting, where the goal is to select the better translation from a pair of hypotheses, given the reference translation. In this framework, lexical, syntactic and semantic information from the reference and the two hypotheses is embedded into compact distributed vector representations, and fed into a multi-layer neural network that models nonlinear interactions between each of the hypotheses and the reference, as well as between the two hypotheses. We experiment with the benchmark datasets from the WMT Metrics shared task, on which we obtain the best results published so far, with the basic network configuration. We also perform a series of experiments to analyze and understand the contribution of the different components of the network. We evaluate variants and extensions, including fine-tuning of the semantic\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 7, 5, 7, 6, 9, 5, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:1yQoGdGgb4wC", "title": "Using Clinical Notes with Time Series Data for ICU Management", "authors": ["Swaraj Khadanga", "Karan Aggarwal", "Shafiq Joty", "Jaideep Srivastava"], "description": "Monitoring patients in ICU is a challenging and high-cost task. Hence, predicting the condition of patients during their ICU stay can help provide better acute care and plan the hospital's resources. There has been continuous progress in machine learning research for ICU management, and most of this work has focused on using time series signals recorded by ICU instruments. In our work, we show that adding clinical notes as another modality improves the performance of the model for three benchmark tasks: in-hospital mortality prediction, modeling decompensation, and length of stay forecasting that play an important role in ICU management. While the time-series data is measured at regular intervals, doctor notes are charted at irregular times, making it challenging to model them together. We propose a method to model them jointly, achieving considerable improvement across benchmark tasks over baseline time-series model. Our implementation can be found at \\url{https://github.com/kaggarwal/ClinicalNotesICU}.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 6, 10, 15, 11, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:-_dYPAW6P2MC", "title": "An attention-based rumor detection model with tree-structured recursive neural networks", "authors": ["Jing Ma", "Wei Gao", "Shafiq Joty", "Kam-Fai Wong"], "description": "Rumor spread in social media severely jeopardizes the credibility of online content. Thus, automatic debunking of rumors is of great importance to keep social media a healthy environment. While facing a dubious claim, people often dispute its truthfulness sporadically in their posts containing various cues, which can form useful evidence with long-distance dependencies. In this work, we propose to learn discriminative features from microblog posts by following their non-sequential propagation structure and generate more powerful representations for identifying rumors. For modeling non-sequential structure, we first represent the diffusion of microblog posts with propagation trees, which provide valuable clues on how a claim in the original post is transmitted and developed over time. We then present a bottom-up and a top-down tree-structured models based on Recursive Neural Networks (RvNN) for rumor\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 13, 11, 15, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:qUcmZB5y_30C", "title": "DiscoTK: Using discourse structure for machine translation evaluation", "authors": ["Shafiq Joty", "Francisco Guzm\u00e1n", "Llu\u00eds M\u00e0rquez", "Preslav Nakov"], "description": null, "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 12, 6, 8, 2, 4, 2, 4, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:5icHVeHT4IsC", "title": "Impact of physical activity on sleep: A deep learning based exploration", "authors": ["Aarti Sathyanarayana", "Shafiq Joty", "Luis Fernandez-Luque", "Ferda Ofli", "Jaideep Srivastava", "Ahmed Elmagarmid", "Shahrad Taheri", "Teresa Arora"], "description": "The importance of sleep is paramount for maintaining physical, emotional and mental wellbeing. Though the relationship between sleep and physical activity is known to be important, it is not yet fully understood. The explosion in popularity of actigraphy and wearable devices, provides a unique opportunity to understand this relationship. Leveraging this information source requires new tools to be developed to facilitate data-driven research for sleep and activity patient-recommendations. In this paper we explore the use of deep learning to build sleep quality prediction models based on actigraphy data. We first use deep learning as a pure model building device by performing human activity recognition (HAR) on raw sensor data, and using deep learning to build sleep prediction models. We compare the deep learning models with those build using classical approaches, i.e. logistic regression, support vector machines, random forest and adaboost. Secondly, we employ the advantage of deep learning with its ability to handle high dimensional datasets. We explore several deep learning models on the raw wearable sensor output without performing HAR or any other feature extraction. Our results show that using a convolutional neural network on the raw wearables output improves the predictive value of sleep quality from physical activity, by an additional 8% compared to state-of-the-art non-deep learning approaches, which itself shows a 15% improvement over current practice. Moreover, utilizing deep learning on raw data eliminates the need for data pre-processing and simplifies the overall workflow to analyze actigraphy data for sleep and\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 3, 11, 7, 8, 5, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:fbc8zXXH2BUC", "title": "Is GPT-3 a Good Data Annotator?", "authors": ["Bosheng Ding", "Chengwei Qin", "Linlin Liu", "Lidong Bing", "Shafiq Joty", "Boyang Li"], "description": "GPT-3 (Generative Pre-trained Transformer 3) is a large-scale autoregressive language model developed by OpenAI, which has demonstrated impressive few-shot performance on a wide range of natural language processing (NLP) tasks. Hence, an intuitive application is to use it for data annotation. In this paper, we investigate whether GPT-3 can be used as a good data annotator for NLP tasks. Data annotation is the process of labeling data that could be used to train machine learning models. It is a crucial step in the development of NLP systems, as it allows the model to learn the relationship between the input data and the desired output. Given the impressive language capabilities of GPT-3, it is natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [38, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:WZBGuue-350C", "title": "UXLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual NLP", "authors": ["M Saiful Bari", "Tasnim Mohiuddin", "Shafiq Joty"], "description": "Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks. However,  annotated data for every target task in every target language is rare, especially for low-resource languages. We propose XLA, a novel data augmentation framework for self-supervised learning in zero-resource transfer learning scenarios. In particular, XLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language task. At its core, XLA performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we conduct extensive experiments on zero-resource cross-lingual transfer tasks for Named Entity Recognition (NER), Natural Language Inference (NLI) and paraphrase identification on Paraphrase Adversaries from Word Scrambling (PAWS). XLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin. With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to XLA's success.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [9, 13, 10, 6]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:qjMakFHDy7sC", "title": "A SVM-based ensemble approach to multi-document summarization", "authors": ["Yllias Chali", "Sadid A Hasan", "Shafiq R Joty"], "description": "In this paper, we present a Support Vector Machine (SVM) based ensemble approach to combat the extractive multi-document summarization problem. Although SVM can have a good generalization ability, it may experience a performance degradation through wrong classifications. We use a committee of several SVMs, i.e. Cross-Validation Committees (CVC), to form an ensemble of classifiers where the strategy is to improve the performance by correcting errors of one classifier using the accurate output of others. The practicality and effectiveness of this technique is demonstrated using the experimental results.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 1, 4, 5, 4, 3, 0, 1, 1, 4, 6, 5, 2, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:4TOpqqG69KYC", "title": "Improving the performance of the random walk model for answering complex questions", "authors": ["Yllias Chali", "Shafiq Joty"], "description": "We consider the problem of answering complex questions that require inferencing and synthesizing information from multiple documents and can be seen as a kind of topicoriented, informative multi-document summarization. The stochastic, graph-based method for computing the relative importance of textual units (ie sentences) is very successful in generic summarization. In this method, a sentence is encoded as a vector in which each component represents the occurrence frequency (TF* IDF) of a word. However, the major limitation of the TF* IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. In this paper, we study the impact of syntactic and shallow semantic information in the graph-based method for answering complex questions.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 2, 4, 4, 4, 1, 2, 4, 2, 3, 3, 0, 3, 1, 0, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:WJVC3Jt7v1AC", "title": "SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization", "authors": ["Mathieu Ravaut", "Shafiq Joty", "Nancy F Chen"], "description": "Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [13, 24, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:2VqYfGB8ITEC", "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization", "authors": ["Shankar Kanthara", "Rixie Tiffany Ko Leong", "Xiang Lin", "Ahmed Masry", "Megh Thakkar", "Enamul Hoque", "Shafiq Joty"], "description": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [10, 26, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:9pM33mqn1YgC", "title": "Reuse and adaptation for entity resolution through transfer learning", "authors": ["Saravanan Thirumuruganathan", "Shameem A Puthiya Parambath", "Mourad Ouzzani", "Nan Tang", "Shafiq Joty"], "description": "Entity resolution (ER) is one of the fundamental problems in data integration, where machine learning (ML) based classifiers often provide the state-of-the-art results. Considerable human effort goes into feature engineering and training data creation. In this paper, we investigate a new problem: Given a dataset D_T for ER with limited or no training data, is it possible to train a good ML classifier on D_T by reusing and adapting the training data of dataset D_S from same or related domain? Our major contributions include (1) a distributed representation based approach to encode each tuple from diverse datasets into a standard feature space; (2) identification of common scenarios where the reuse of training data can be beneficial; and (3) five algorithms for handling each of the aforementioned scenarios. We have performed comprehensive experiments on 12 datasets from 5 different domains (publications, movies, songs, restaurants, and books). Our experiments show that our algorithms provide significant benefits such as providing superior performance for a fixed training data size.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 10, 9, 10, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:anf4URPfarAC", "title": "LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5", "authors": ["Chengwei Qin", "Shafiq Joty"], "description": "Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we define this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a unified framework for it based on prompt tuning of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addition, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and significantly outperform previous methods in different LFLL settings.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [18, 18, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:Wp0gIr-vW9MC", "title": "Discriminative reranking of discourse parses using tree kernels", "authors": ["Shafiq Joty", "Alessandro Moschitti"], "description": "In this paper, we present a discriminative approach for reranking discourse trees generated by an existing probabilistic discourse parser. The reranker relies on tree kernels (TKs) to capture the global dependencies between discourse units in a tree. In particular, we design new computational structures of discourse trees, which combined with standard TKs, originate novel discourse TKs. The empirical evaluation shows that our reranker can improve the state-of-the-art sentence-level parsing accuracy from 79.77% to 82.15%, a relative error reduction of 11.8%, which in turn pushes the state-of-the-art documentlevel accuracy from 55.8% to 57.3%.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 8, 3, 4, 3, 8, 5, 2, 2, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:TQgYirikUcIC", "title": "How to avoid unwanted pregnancies: Domain adaptation using neural network models", "authors": ["Shafiq Joty", "Hassan Sajjad", "Nadir Durrani", "Kamla Al-Mannai", "Ahmed Abdelali", "Stephan Vogel"], "description": "We present novel models for domain adaptation based on the neural network joint model (NNJM). Our models maximize the cross entropy by regularizing the loss function with respect to in-domain model. Domain adaptation is carried out by assigning higher weight to out-domain sequences that are similar to the in-domain data. In our alternative model we take a more restrictive approach by additionally penalizing sequences similar to the outdomain data. Our models achieve better perplexities than the baseline NNJM models and give improvements of up to 0.5 and 0.6 BLEU points in Arabic-to-English and English-to-German language pairs, on a standard task of translating TED talks.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 9, 2, 3, 4, 2, 7, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:IRz6iEL74y4C", "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning", "authors": ["Ahmed Masry", "Do Xuan Long", "Jia Qing Tan", "Shafiq Joty", "Enamul Hoque"], "description": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 29, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:URolC5Kub84C", "title": "Online Conversation Disentanglement with Pointer Networks", "authors": ["Tao Yu", "Shafiq Joty"], "description": "Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation disentanglement aims to separate intermingled messages into detached conversations. However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability. In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering. We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and proposes a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion. We also introduce a joint-learning objective to better capture contextual information. Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 10, 8, 13, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:hkOj_22Ku90C", "title": "Hierarchical pointer net parsing", "authors": ["Linlin Liu", "Xiang Lin", "Shafiq Joty", "Simeng Han", "Lidong Bing"], "description": "Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 8, 9, 9, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:35r97b3x0nAC", "title": "Mind Your Inflections! Improving NLP for Non-Standard English with Base-Inflection Encoding", "authors": ["Samson Tan", "Shafiq Joty", "Lav R Varshney", "Min-Yen Kan"], "description": "Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols. Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data. Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 10, 16, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:_B80troHkn4C", "title": "Revisiting Adversarial Autoencoder for Unsupervised Word Translation with Cycle Consistency and Improved Training", "authors": ["Tasnim Mohiuddin", "Shafiq Joty"], "description": "Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [5, 8, 6, 10, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:kh2fBNsKQNwC", "title": "SUper Team at SemEval-2016 Task 3: Building a feature-rich system for community question answering", "authors": ["Tsvetomila Mihaylova", "Pepa Gencheva", "Martin Boyanov", "Ivana Yovcheva", "Todor Mihaylov", "Momchil Hardalov", "Yasen Kiprov", "Daniel Balchev", "Ivan Koychev", "Preslav Nakov", "Ivelina Nikolova", "Galia Angelova"], "description": "We present the system we built for participating in SemEval-2016 Task 3 on Community Question Answering. We achieved the best results on subtask C, and strong results on subtasks A and B, by combining a rich set of various types of features: semantic, lexical, metadata, and user-related. The most important group turned out to be the metadata for the question and for the comment, semantic vectors trained on QatarLiving data and similarities between the question and the comment for subtasks A and C, and between the original and the related question for Subtask B.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 28]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:tzM49s52ZIMC", "title": "LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space", "authors": ["Tasnim Mohiuddin", "M Saiful Bari", "Shafiq Joty"], "description": "Most of the successful and predominant methods for bilingual lexicon induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e., approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses nonlinear mapping in the latent space of two independently trained auto-encoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [5, 10, 9, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:epqYDVWIO7EC", "title": "Response Selection for Multi-Party Conversations with Dynamic Topic Tracking", "authors": ["Weishi Wang", "Shafiq Joty", "Steven CH Hoi"], "description": "While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 10, 8, 11, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:a0OBvERweLwC", "title": "Discourse Structure in Machine Translation Evaluation", "authors": ["Shafiq Joty", "Francisco Guzm\u00e1n", "Llu\u00eds M\u00e0rquez", "Preslav Nakov"], "description": "In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment level and at the system level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DiscoTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 7, 3, 8, 7, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:_xSYboBqXhAC", "title": "Cross-language question re-ranking", "authors": ["Giovanni Da San Martino", "Salvatore Romeo", "Alberto Barro\u00f3n-Cede\u00f1o", "Shafiq Joty", "Llu\u00eds Ma\u00e0rquez", "Alessandro Moschitti", "Preslav Nakov"], "description": "We study how to find relevant questions in community forums when the language of the new questions is different from that of the existing questions in the forum. In particular, we explore the Arabic-English language pair. We compare a kernel-based system with a feed-forward neural network in a scenario where a large parallel corpus is available for training a machine translation system, bilingual dictionaries, and cross-language word embeddings. We observe that both approaches degrade the performance of the system when working on the translated text, especially the kernel-based system, which depends heavily on a syntactic kernel. We address this issue using a cross-language tree kernel, which compares the original Arabic tree to the English trees of the related questions. We show that this kernel almost closes the performance gap with respect to the monolingual system. On the neural network side, we\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 9, 6, 6, 2, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:8k81kl-MbHgC", "title": "Towards topic labeling with phrase entailment and aggregation", "authors": ["Yashar Mehdad", "Giuseppe Carenini", "Raymond Ng", "Shafiq Joty"], "description": "We propose a novel framework for topic labeling that assigns the most representative phrases for a given set of sentences covering the same topic. We build an entailment graph over phrases that are extracted from the sentences, and use the entailment relations to identify and select the most relevant phrases. We then aggregate those selected phrases by means of phrase generalization and merging. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 2, 3, 4, 6, 2, 2, 2, 2, 3, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:mB3voiENLucC", "title": "Using joint models or domain adaptation in statistical machine translation", "authors": ["Nadir Durrani", "Hassan Sajjad", "Shafiq Joty", "Ahmed Abdelali", "Stephan Vogel"], "description": "Joint models have recently shown to improve the state-of-the-art in machine translation (MT). We apply EM-based mixture modeling and data selection techniques using two joint models, namely the Operation Sequence Model or OSM\u2014an ngram-based translation and reordering model, and the Neural Network Joint Model or NNJM\u2014a continuous space translation model, to carry out domain adaptation for MT. The diversity of the two models, OSM with inherit reordering information and NNJM with continuous space modeling makes them interesting to be explored for this task. Our contribution in this paper is fusing the existing known techniques (linear interpolation, cross-entropy) with the state-of-the-art MT models (OSM, NNJM). On a standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we observed statistically significant improvements of up to+ 0.9 BLEU points.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 11, 5, 0, 3, 1, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:qxL8FJ1GzNcC", "title": "Exploiting conversation structure in unsupervised topic segmentation for emails", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Gabriel Murray", "Raymond Ng"], "description": "This work concerns automatic topic segmentation of email conversations. We present a corpus of email threads manually annotated with topics, and evaluate annotator reliability. To our knowledge, this is the first such email corpus. We show how the existing topic segmentation models (ie, Lexical Chain Segmenter (LCSeg) and Latent Dirichlet Allocation (LDA)) which are solely based on lexical information, can be applied to emails. By pointing out where these methods fail and what any desired model should consider, we propose two novel extensions of the models that not only use lexical information but also exploit finer level conversation structure in a principled way. Empirical evaluation shows that LCSeg is a better model than LDA for segmenting an email thread into topical clusters and incorporating conversation structure into these models improves the performance significantly.", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 2, 7, 1, 3, 2, 3, 0, 1, 0, 1, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:nrtMV_XWKgEC", "title": "Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning", "authors": ["Xiangxi Shi", "Xu Yang", "Jiuxiang Gu", "Shafiq Joty", "Jianfei Cai"], "description": "Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors, such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the semantic difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from semantic changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets .", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 6, 6, 13, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:RYcK_YlVTxYC", "title": "Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models", "authors": ["Shafiq Joty", "Enamul Hoque"], "description": "This paper addresses the problem of speech act recognition in written asynchronous conversations (eg, fora, emails). We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences. Our models use sentence representations encoded by a long short term memory (LSTM) recurrent neural model. Empirical evaluation shows the effectiveness of our approach over existing ones:(i) LSTMs provide better task-specific representations, and (ii) the global joint model improves over local models.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 8, 8, 2, 0, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:0izLItjtcgwC", "title": "GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems", "authors": ["Bosheng Ding", "Junjie Hu", "Lidong Bing", "Sharifah Mahani Aljunied", "Shafiq Joty", "Luo Si", "Chunyan Miao"], "description": "Much recent progress in task-oriented dialogue (ToD) systems has been driven by available annotation data across multiple domains for training. Over the last few years, there has been a move towards data curation for multilingual ToD systems that are applicable to serve people speaking different languages. However, existing multilingual ToD datasets either have a limited coverage of languages due to the high cost of data curation, or ignore the fact that dialogue entities barely exist in countries speaking these languages. To tackle these limitations, we introduce a novel data curation method that generates GlobalWoZ -- a large-scale multilingual ToD dataset globalized from an English ToD dataset for three unexplored use cases. Our method is based on translating dialogue templates and filling them with local entities in the target-language countries. We release our dataset as well as a set of strong baselines to encourage research on learning multilingual ToD systems for real use cases.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [11, 13, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:ye4kPcJQO24C", "title": "Watch it twice: Video captioning with a refocused video encoder", "authors": ["Xiangxi Shi", "Jianfei Cai", "Shafiq Joty", "Jiuxiang Gu"], "description": "With the rapid growth of video data and the increasing demands of various crossmodal applications such as intelligent video search and assistance towards visually-impaired people, video captioning task has received a lot of attention recently in computer vision and natural language processing fields. The state-of-the-art video captioning methods focus more on encoding the temporal information, while lacking effective ways to remove irrelevant temporal information and also neglecting the spatial details. In particular, the current unidirectional video encoder can be negatively affected by irrelevant temporal information, especially the irrelevant information at the beginning and at the end of a video. In addition, disregarding detailed spatial features may lead to incorrect word choices in decoding. In this paper, we propose a novel recurrent video encoding method and a novel visual spatial feature for the video\u00a0\u2026", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 6, 5, 9, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:LjlpjdlvIbIC", "title": "Discourse analysis and its applications", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Raymond Ng", "Gabriel Murray"], "description": "Discourse processing is a suite of Natural Language Processing (NLP) tasks to uncover linguistic structures from texts at several levels, which can support many downstream applications. This involves identifying the topic structure, the coherence structure, the coreference structure, and the conversation structure for conversational discourse. Taken together, these structures can inform text summarization, machine translation, essay scoring, sentiment analysis, information extraction, question answering, and thread recovery. The tutorial starts with an overview of basic concepts in discourse analysis\u2013monologue vs. conversation, synchronous vs. asynchronous conversation, and key linguistic structures in discourse analysis. We also give an overview of linguistic structures and corresponding discourse analysis tasks that discourse researchers are generally interested in, as well as key applications on which these discourse structures have an impact.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 7, 5, 6, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:kRWSkSYxWN8C", "title": "A structured learning approach with neural conditional random fields for sleep staging", "authors": ["Karan Aggarwal", "Swaraj Khadanga", "Shafiq Joty", "Louis Kazaglis", "Jaideep Srivastava"], "description": "Sleep plays a vital role in human health, both mental and physical. Sleep disorders like sleep apnea are increasing in prevalence, with the rapid increase in factors like obesity. Sleep apnea is most commonly treated with Continuous Positive Air Pressure (CPAP) therapy. Presently, however, there is no mechanism to monitor a patient's progress with CPAP. Accurate detection of sleep stages from CPAP flow signal is crucial for such a mechanism. We propose, for the first time, an automated sleep staging model based only on the flow signal. Deep neural networks have recently shown high accuracy on sleep staging by eliminating handcrafted features. However, these methods focus exclusively on extracting informative features from the input signal, without paying much attention to the dynamics of sleep stages in the output sequence. We propose an end-to-end framework that uses a combination of deep\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 7, 4, 4, 6, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:GnPB-g6toBAC", "title": "Joint Learning with Global Inference for Comment Classification in Community Question Answering", "authors": ["Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Preslav Nakov"], "description": "This paper addresses the problem of comment classification in community Question Answering. Following the state of the art, we approach the task with a global inference process to exploit the information of all comments in the answer-thread in the form of a fully connected graph. Our contribution comprises two novel joint learning models that are on-line and integrate inference within learning. The first one jointly learns two node-and edge-level MaxEnt classifiers with stochastic gradient descent and integrates the inference step with loopy belief propagation. The second model is an instance of fully connected pairwise CRFs (FCCRF). The FCCRF model significantly outperforms all other approaches and yields the best results on the task to date. Crucial elements for its success are the global normalization and an Ising-like edge potential.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [5, 3, 6, 4, 1, 3, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:_OXeSy2IsFwC", "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation", "authors": ["Yixin Liu", "Alexander R Fabbri", "Pengfei Liu", "Yilun Zhao", "Linyong Nan", "Ruilin Han", "Simeng Han", "Shafiq Joty", "Chien-Sheng Wu", "Caiming Xiong", "Dragomir Radev"], "description": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation protocols and benchmarks for summarization either exhibit low inter-annotator agreement or lack the scale needed to draw statistically significant conclusions, and an in-depth analysis of human evaluation is lacking. In this work, we address the shortcomings of existing summarization evaluation along the following axes: 1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which relies on fine-grained semantic units and allows for high inter-annotator agreement. 2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of over 22k summary-level annotations over state-of-the-art systems on three datasets. 3) We compare our ACU protocol with three other human evaluation protocols, underscoring potential confounding factors in evaluation setups. 4) We evaluate existing automatic metrics using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. Furthermore, our findings have important implications for evaluating large language models (LLMs), as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [22, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:VaXvl8Fpj5cC", "title": "RST Parsing from Scratch", "authors": ["Thanh-Tung Nguyen", "Xuan-Phi Nguyen", "Shafiq Joty", "Xiaoli Li"], "description": "We introduce a novel top-down end-to-end formulation of document-level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high-scoring trees. With extensive experiments on the standard English RST discourse treebank, we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. More importantly, it does so without using any handcrafted features, making it faster and easily adaptable to new languages and domains.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [3, 12, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:kuK5TVdYjLIC", "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots", "authors": ["Samson Tan", "Shafiq Joty"], "description": "Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong black-box adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter directly aligns the clean example with its translations before extracting phrases as perturbations. Our phrase-level attack has a success rate of 89.75% against XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme that trains in the same number of steps as the original model and show that it improves model accuracy.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [8, 9, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:ZuybSZzF8UAC", "title": "Evaluating pronominal anaphora in machine translation: An evaluation measure and a test suite", "authors": ["Prathyusha Jwalapuram", "Shafiq Joty", "Irina Temnikova", "Preslav Nakov"], "description": "The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 6, 6, 4, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:lmc2jWPfTJgC", "title": "Reliability Testing for Natural Language Processing Systems", "authors": ["Samson Tan", "Shafiq Joty", "Kathy Baxter", "Araz Taeihagh", "Gregory A Bennett", "Min-Yen Kan"], "description": "Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing -- with an emphasis on interdisciplinary collaboration -- will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [6, 8, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:V3AGJWp-ZtQC", "title": "Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings", "authors": ["Shafiq Joty", "Lluis Marquez", "Preslav Nakov"], "description": "We address jointly two important tasks for Question Answering in community forums: given a new question, (i) find related existing questions, and (ii) find relevant answers to this new question. We further use an auxiliary task to complement the previous two, i.e., (iii) find good answers with respect to the thread question in a question-comment thread. We use deep neural networks (DNNs) to learn meaningful task-specific embeddings, which we then incorporate into a conditional random field (CRF) model for the multitask setting, performing joint learning over a complex graph structure. While DNNs alone achieve competitive results when trained to produce the embeddings, the CRF, which makes use of the embeddings and the dependencies between the tasks, improves the results significantly and consistently across a variety of evaluation metrics, thus showing the complementarity of DNNs and structured learning.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 8, 4, 3, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:_5tno0g5mFcC", "title": "Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation", "authors": ["Chengwei Qin", "Shafiq Joty"], "description": "Existing continual relation learning (CRL) methods rely on plenty of labeled training data for learning a new task, which can be hard to acquire in real scenario as getting large and representative labeled data is often expensive and time-consuming. It is therefore necessary for the model to learn novel relational patterns with very few labeled data while avoiding catastrophic forgetting of previous task knowledge. In this paper, we formulate this challenging yet practical problem as continual few-shot relation learning (CFRL). Based on the finding that learning for new emerging few-shot tasks often results in feature distributions that are incompatible with previous tasks' learned distributions, we propose a novel method based on embedding space regularization and data augmentation. Our method generalizes to new few-shot tasks and avoids catastrophic forgetting of previous tasks by enforcing extra constraints on the relational embeddings and by adding extra {relevant} data in a self-supervised manner. With extensive experiments we demonstrate that our method can significantly outperform previous state-of-the-art methods in CFRL task settings.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [6, 12, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:tuHXwOkdijsC", "title": "Rethinking Coherence Modeling: Synthetic vs. Downstream Tasks", "authors": ["Tasnim Mohiuddin", "Prathyusha Jwalapuram", "Xiang Lin", "Shafiq Joty"], "description": "Although coherence modeling has come a long way in developing novel models, their evaluation on downstream applications for which they are purportedly developed has largely been neglected. With the advancements made by neural approaches in applications such as machine translation (MT), summarization and dialog systems, the need for coherence evaluation of these tasks is now more crucial than ever. However, coherence models are typically evaluated only on synthetic tasks, which may not be representative of their performance in downstream applications. To investigate how representative the synthetic tasks are of downstream use cases, we conduct experiments on benchmarking well-known traditional and neural coherence models on synthetic sentence ordering tasks, and contrast this with their performance on three downstream applications: coherence evaluation for MT and summarization, and next utterance prediction in retrieval-based dialog. Our results demonstrate a weak correlation between the model performances in the synthetic tasks and the downstream applications, {motivating alternate training and evaluation methods for coherence models.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [6, 5, 5, 2]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:vbGhcppDl1QC", "title": "Explicit memory tracker with coarse-to-fine reasoning for conversational machine reading", "authors": ["Yifan Gao", "Chien-Sheng Wu", "Shafiq Joty", "Caiming Xiong", "Richard Socher", "Irwin King", "Michael R Lyu", "Steven CH Hoi"], "description": "The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them. In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision. Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions. On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4. We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows. Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 9, 3, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:WF5omc3nYNoC", "title": "Exploiting conversational features to detect high-quality blog comments", "authors": ["Nicholas FitzGerald", "Giuseppe Carenini", "Gabriel Murray", "Shafiq Joty"], "description": "In this work, we present a method for classifying the quality of blog comments using Linear-Chain Conditional Random Fields (CRFs). This approach is found to yield high accuracy on binary classification of high-quality comments, with conversational features contributing strongly to the accuracy. We also present a new corpus of blog data in conversational form, complete with user-generated quality moderation labels from the science and technology news blog Slashdot.", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 5, 1, 2, 3, 1, 1, 2, 1, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:Tyk-4Ss8FVUC", "title": "Do automatic annotation techniques have any impact on supervised complex question answering?", "authors": ["Yllias Chali", "Sadid A Hasan", "Shafiq Joty"], "description": "In this paper, we analyze the impact of different automatic annotation methods on the performance of supervised approaches to the complex question answering problem (defined in the DUC-2007 main task). Huge amount of annotated or labeled data is a prerequisite for supervised training. The task of labeling can be accomplished either by humans or by computer programs. When humans are employed, the whole process becomes time consuming and expensive. So, in order to produce a large set of labeled data we prefer the automatic annotation strategy. We apply five different automatic annotation techniques to produce labeled data using ROUGE similarity measure, Basic Element (BE) overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel (ESSK). The representative supervised methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (Max-Ent). Evaluation results are presented to show the impact.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 1, 2, 2, 2, 3, 2, 1, 2, 0, 0, 0, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:OR75R8vi5nAC", "title": "Folio: Natural language reasoning with first-order logic", "authors": ["Simeng Han", "Hailey Schoelkopf", "Yilun Zhao", "Zhenting Qi", "Martin Riddell", "Luke Benson", "Lucy Sun", "Ekaterina Zubova", "Yujie Qiao", "Matthew Burtell", "David Peng", "Jonathan Fan", "Yixin Liu", "Brian Wong", "Malcolm Sailor", "Ansong Ni", "Linyong Nan", "Jungo Kasai", "Tao Yu", "Rui Zhang", "Shafiq Joty", "Alexander R Fabbri", "Wojciech Kryscinski", "Xi Victoria Lin", "Caiming Xiong", "Dragomir Radev"], "description": "We present FOLIO, a human-annotated, open-domain, and logically complex and diverse dataset for reasoning in natural language (NL), equipped with first order logic (FOL) annotations. FOLIO consists of 1,435 examples (unique conclusions), each paired with one of 487 sets of premises which serve as rules to be used to deductively reason for the validity of each conclusion. The logical correctness of premises and conclusions is ensured by their parallel FOL annotations, which are automatically verified by our FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO automatically constitute a new NL-FOL translation dataset using FOL as the logical form. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models (BERT, RoBERTa) and few-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). For NL-FOL translation, we experiment with GPT-3 and Codex. Our results show that one of the most capable Large Language Model (LLM) publicly available, GPT-3 davinci, achieves only slightly better than random results with few-shot prompting on a subset of FOLIO, and the model is especially bad at predicting the correct truth values for False and Unknown conclusions. Our dataset and code are available at https://github.com/Yale-LILY/FOLIO.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 14, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:5Ul4iDaHHb8C", "title": "Video captioning with boundary-aware hierarchical language decoding and joint video prediction", "authors": ["Xiangxi Shi", "Jianfei Cai", "Jiuxiang Gu", "Shafiq Joty"], "description": "The explosion of video data on the Internet requires effective and efficient technology to generate captions automatically for people, especially those who are visually impaired. Despite the great progress of video captioning research, particularly in video feature encoding, the language decoder is still largely based on the prevailing recurrent structure such as LSTM, which tends to prefer frequent words that align with the video and do not generalize well to new videos. In this paper, we propose a boundary-aware hierarchical language decoder for video captioning, which consists of a high-level decoder, working as a global (caption-level) language model, and a low-level decoder, working as a local (phrase-level) language model. Most importantly, we introduce a binary gate into the low-level language decoder to detect the phrasal boundaries. Together with other advanced components including a joint video\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 3, 8, 2]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:AXPGKjj_ei8C", "title": "Adversarial Unsupervised Representation Learning for Activity Time-Series", "authors": ["Karan Aggarwal", "Shafiq Joty", "Luis Fernandez-Luque", "Jaideep Srivastava"], "description": "Sufficient physical activity and restful sleep play a major role in the prevention and cure of many chronic conditions. Being able to proactively screen and monitor such chronic conditions would be a big step forward for overall health. The rapid increase in the popularity of wearable devices pro-vides a significant new source, making it possible to track the user\u2019s lifestyle real-time. In this paper, we propose a novel unsupervised representation learning technique called activ-ity2vecthat learns and \u201csummarizes\u201d the discrete-valued ac-tivity time-series. It learns the representations with three com-ponents:(i) the co-occurrence and magnitude of the activ-ity levels in a time-segment,(ii) neighboring context of the time-segment, and (iii) promoting subject-invariance with ad-versarial training. We evaluate our method on four disorder prediction tasks using linear classifiers. Empirical evaluation demonstrates that our proposed method scales and performs better than many strong baselines. The adversarial regime helps improve the generalizability of our representations by promoting subject invariant features. We also show that using the representations at the level of a day works the best since human activity is structured in terms of daily routines.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 3, 3, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:M3ejUd6NZC8C", "title": "Supervised topic segmentation of email conversations", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Gabriel Murray", "Raymond Ng"], "description": "We propose a graph-theoretic supervised topic segmentation model for email conversations which combines (i) lexical knowledge,(ii) conversational features, and (iii) topic features. We compare our results with the existing unsupervised models (ie, LCSeg and LDA), and with their two extensions for email conversations (ie, LCSeg+ FQG and LDA+ FQG) that not only use lexical information but also exploit finer conversation structure. Empirical evaluation shows that our supervised model is the best performer and achieves highest accuracy by combining the three different knowledge sources, where knowledge about the conversation has proved to be the most important indicator for segmenting emails.", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 1, 2, 1, 3, 1, 0, 2, 0, 1, 2, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:Aul-kAQHnToC", "title": "Is gpt-3 a psychopath? evaluating large language models from a psychological perspective", "authors": ["Xingxuan Li", "Yutong Li", "Linlin Liu", "Lidong Bing", "Shafiq Joty"], "description": "Are large language models (LLMs) like GPT-3 psychologically safe? In this work, we design unbiased prompts to evaluate LLMs systematically from a psychological perspective. Firstly, we test the personality traits of three different LLMs with Short Dark Triad (SD-3) and Big Five Inventory (BFI). We find all of them show higher scores on SD-3 than the human average, indicating a relatively darker personality. Furthermore, LLMs like InstructGPT and FLAN-T5, which are fine-tuned with safety metrics, do not necessarily have more positive personalities. They score higher on Machiavellianism and Narcissism than GPT-3. Secondly, we test the LLMs in GPT-3 series on well-being tests to study the impact of fine-tuning with more training data. Interestingly, we observe a continuous increase in well-being scores from GPT-3 to InstructGPT. Following the observations, we show that instruction-finetune FLAN-T5 with positive answers in BFI can effectively improve the model from a psychological perspective. Finally, we call on the community to evaluate and improve LLMs' safety systematically instead of at the sentence level only.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 16, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:Ri6SYOTghG4C", "title": "Weakly supervised neuro-symbolic module networks for numerical reasoning", "authors": ["Amrita Saha", "Shafiq Joty", "Steven CH Hoi"], "description": "Neural Module Networks (NMNs) have been quite successful in incorporating explicit reasoning as learnable modules in various question answering tasks, including the most generic form of numerical reasoning over text in Machine Reading Comprehension (MRC). However, to achieve this, contemporary NMNs need strong supervision in executing the query as a specialized program over reasoning modules and fail to generalize to more open-ended settings without such supervision. Hence we propose Weakly-Supervised Neuro-Symbolic Module Network (WNSMN) trained with answers as the sole supervision for numerical reasoning based MRC. It learns to execute a noisy heuristic program obtained from the dependency parsing of the query, as discrete actions over both neural and symbolic reasoning modules and trains it end-to-end in a reinforcement learning framework with discrete reward from answer matching. On the numerical-answer subset of DROP, WNSMN out-performs NMN by 32% and the reasoning-free language model GenBERT by 8% in exact match accuracy when trained under comparable weak supervised settings. This showcases the effectiveness and generalizability of modular networks that can handle explicit discrete reasoning over noisy programs in an end-to-end manner.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [10, 2, 4]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:tYavs44e6CUC", "title": "Towards Enhancing Database Education: Natural Language Generation Meets Query Execution Plans", "authors": ["Weiguo Wang", "Sourav S Bhowmick", "Hui Li", "Shafiq R Joty", "Siyuan Liu", "Peng Chen"], "description": "The database systems course is offered as part of an undergraduate computer science degree program in many major universities. A key learning goal of learners taking such a course is to understand how sql queries are processed in a rdbms in practice. Since aquery execution plan (qep ) describes the execution steps of a query, learners can acquire the understanding by perusing the qep s generated by a rdbms. Unfortunately, in practice, it is often daunting for a learner to comprehend these qep s containing vendor-specific implementation details, hindering her learning process. In this paper, we present a novel, end-to-end,generic system called lantern that generates a natural language description of a qep to facilitate understanding of the query execution steps. It takes as input an sql query and its qep, and generates a natural language description of the execution strategy deployed by the underlying rdbms\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 9, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:4MWp96NkSFoC", "title": "Self-Supervised Relationship Probing", "authors": ["Jiuxiang Gu", "Jason Kuen", "Shafiq Joty", "Jianfei Cai", "Vlad Morariu", "Handong Zhao", "Tong Sun"], "description": "Structured representations of images that model visual relationships are beneficial for many vision and vision-language applications. However, current human-annotated visual relationship datasets suffer from the long-tailed predicate distribution problem which limits the potential of visual relationship models. In this work, we introduce a self-supervised method that implicitly learns the visual relationships without relying on any ground-truth visual relationship annotations. Our method relies on 1) intra-and inter-modality encodings to respectively model relationships within each modality separately and jointly, and 2) relationship probing, which seeks to discover the graph structure within each modality. By leveraging masked language modeling, contrastive learning, and dependency tree distances for self-supervision, our method learns better object features as well as implicit visual relationships. We verify the effectiveness of our proposed method on various vision-language tasks that benefit from improved visual relationship understanding.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 9, 6, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=20&pagesize=80&citation_for_view=hR249csAAAAJ:uc_IGeMz5qoC", "title": "Speech Transformer with Speaker Aware Persistent Memory.", "authors": ["Yingzhu Zhao", "Chongjia Ni", "Cheung-Chi Leung", "Shafiq R Joty", "Eng Siong Chng", "Bin Ma"], "description": "End-to-end models have been introduced into automatic speech recognition (ASR) successfully and achieved superior performance compared with conventional hybrid systems, especially with the newly proposed transformer model. However, speaker mismatch between training and test data remains a problem, and speaker adaptation for transformer model can be further improved. In this paper, we propose to conduct speaker aware training for ASR in transformer model. Specifically, we propose to embed speaker knowledge through a persistent memory model into speech transformer encoder at utterance level. The speaker information is represented by a number of static speaker i-vectors, which is concatenated to speech utterance at each encoder self-attention layer. Persistent memory is thus formed by carrying speaker information through the depth of encoder. The speaker knowledge is captured from self\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 3, 7, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:dTyEYWd-f8wC", "title": "NEURON: Query execution plan meets natural language processing for augmenting DB education", "authors": ["Siyuan Liu", "Sourav S Bhowmick", "Wanlu Zhang", "Shu Wang", "Wanyi Huang", "Shafiq Joty"], "description": "A core component of a database systems course at the undergraduate level is the design and implementation of the query optimizer in an rdbms. The query optimization process produces aquery execution plan (qep ), which represents an execution strategy for an sql query. Unfortunately, in practice, it is often difficult for a student to comprehend a query execution strategy by perusing its qep, hindering her learning process. In this demonstration, we present a novel system called neuron that facilitates natural language interaction with qep s to enhance its understanding. neuron accepts an sql query (which may include joins, aggregation, nesting, among other things) as input, executes it, and generates a simplified natural language description (both in text and voice form) of the execution strategy deployed by the underlying rdbms. Furthermore, it facilitates understanding of various features related to a qep through\u00a0\u2026", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 1, 3, 6, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:u-x6o8ySG0sC", "title": "Uofl: Word sense disambiguation using lexical cohesion", "authors": ["Yllias Chali", "Shafiq Joty"], "description": "One of the main challenges in the applications (ie: text summarization, question answering, information retrieval, etc.) of Natural Language Processing is to determine which of the several senses of a word is used in a given context. The problem is phrased as \u201cWord Sense Disambiguation (WSD)\u201d in the NLP community. This paper presents the dictionary based disambiguation technique that adopts the assumption of one sense per discourse in the context of SemEval-2007 Task 7:\u201cCoarse-grained English all-words\u201d.", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 2, 0, 2, 1, 0, 0, 3, 0, 0, 2, 0, 0, 1, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:jL-93Qbq4QoC", "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets", "authors": ["Md Tahmid Rahman Laskar", "M Saiful Bari", "Mizanur Rahman", "Md Amran Hossen Bhuiyan", "Shafiq Joty", "Jimmy Xiangji Huang"], "description": "The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [15, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:uLbwQdceFCQC", "title": "Unsupervised word translation with adversarial autoencoder", "authors": ["Tasnim Mohiuddin", "Shafiq Joty"], "description": "Crosslingual word embeddings learned from monolingual embeddings have a crucial                     role in many downstream tasks, ranging from machine translation to transfer                     learning. Adversarial training has shown impressive success in learning                     crosslingual embeddings and the associated word translation task without any                     parallel data by mapping monolingual embeddings to a shared space. However,                     recent work has shown superior performance for non-adversarial methods in more                     challenging language pairs. In this article, we investigate adversarial                     autoencoder for unsupervised word translation and propose two novel extensions                     to it that yield more stable training and improved results. Our method includes                     regularization terms to enforce cycle consistency and input reconstruction, and                     puts the target\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [4, 5, 1, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:HE397vMXCloC", "title": "A factorial deep markov model for unsupervised disentangled representation learning from speech", "authors": ["Sameer Khurana", "Shafiq Rayhan Joty", "Ahmed Ali", "James Glass"], "description": "We present the Factorial Deep Markov Model (FDMM) for representation learning of speech. The FDMM learns disentangled, interpretable and lower dimensional latent representations from speech without supervision. We use a static and dynamic latent variable to exploit the fact that information in a speech signal evolves at different time scales. Latent representations learned by the FDMM outperform a baseline i-vector system on speaker verification and dialect identification while also reducing the error rate of a phone recognition system in a domain mismatch scenario.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 7, 2, 3, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:_axFR9aDTf0C", "title": "Unsupervised cross-lingual image captioning", "authors": ["Jiahui Gao", "Yi Zhou", "Philip LH Yu", "Shafiq Joty", "Jiuxiang Gu"], "description": null, "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 5, 6]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:lvd772isFD0C", "title": "Enhancing multilingual language model with massive multilingual knowledge triples", "authors": ["Linlin Liu", "Xin Li", "Ruidan He", "Lidong Bing", "Shafiq Joty", "Luo Si"], "description": "Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:kz9GbA2Ns4gC", "title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation", "authors": ["Xiang Lin", "Simeng Han", "Shafiq Joty"], "description": "Advanced large-scale neural language models have led to significant success in many language generation tasks. However, the most commonly used training objective, Maximum Likelihood Estimation (MLE), has been shown problematic, where the trained model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad, a modification straight to the gradient of the loss function, to remedy the degeneration issue of the standard MLE objective. By directly maneuvering the gradient information, ScaleGrad makes the model learn to use novel tokens. Empirical results show the effectiveness of our method not only in open-ended generation, but also in directed generation tasks. With the simplicity in architecture, our method can serve as a general training objective that is applicable to most of the neural text generation tasks.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 6, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:_FM0Bhl9EiAC", "title": "Phrase-based attentions", "authors": ["Phi Xuan Nguyen", "Shafiq Joty"], "description": "Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 3, 0, 3, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:b0M2c_1WBrUC", "title": "CON-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec", "authors": ["Tanay Kumar Saha", "Shafiq Joty", "Mohammad Hasan"], "description": "We present a novel approach to learn distributed representation of sentences from unlabeled data by modeling both content and context of a sentence. The content model learns sentence representation by predicting its words. On the other hand, the context model comprises a neighbor prediction component and a regularizer to model distributional and proximity hypotheses, respectively. We propose an online algorithm to train the model components jointly. We evaluate the models in a setup, where contextual information is available. The experimental results on tasks involving classification, clustering, and ranking of sentences show that our model outperforms the best existing models by a wide margin across multiple datasets.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 3, 3, 2, 0, 2, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:2osOgNQ5qMEC", "title": "University of Lethbridge's Participation in TREC 2007 QA Track.", "authors": ["Yllias Chali", "Shafiq R Joty"], "description": "Question Answering (QA) is retrieving answers to natural language questions from a collection of documents rather than retrieving relevant documents containing the keywords of the query which is performed by search engines. What a user usually wants is often a precise answer to a question. For example, given the question \u201cWho won the nobel prize in peace in 2006?\u201d what a user really wants is the answer \u201cDr. Muhammad Yunus\u201d, in stead of reading through lots of documents that contain the words \u201cwin\u201d,\u201cnobel\u201d,\u201cprize\u201d,\u201cpeace\u201d and \u201c2006\u201d etc. This means that question answering systems will possibly be integral to the next generation of search engines. The Text Retrieval Conference, TREC1 QA track is the major large-scale evaluation environment for open-domain question answering systems. The questions in the TREC-2007 QA track are clustered by target, which is the overall theme or topic of the questions. The track has three types of questions: 1. factoid that require only one correct response, 2. list that require a non redundant list of correct responses and 3. other questions that require a non redundant list of facts about the target that has not already been discovered by a previous answer. We took the approach of designing a question answering system that is based on document tagging and question classification. Question classification extracts useful information (ie answer type) from the question about how to answer the question. Document tagging extracts useful information from the documents, which will be used in finding the answer to the question. We used different available tools to tag the documents. Our system classifies the\u00a0\u2026", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 4, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:XoXfffV-tXoC", "title": "Span-Level Emotion Cause Analysis by BERT-based Graph Attention Network", "authors": ["Xiangju Li", "Wei Gao", "Shi Feng", "Wang Daling", "Shafiq Joty"], "description": "We study the task of span-level emotion cause analysis (SECA), which is focused on identifying the specific emotion cause span(s) triggering a certain emotion in the text. Compared to the popular clause-level emotion cause analysis (CECA), it is a finer-grained emotion cause analysis (ECA) task. In this paper, we design a BERT-based graph attention network for emotion cause span(s) identification. The proposed model takes advantage of the structure of BERT to capture the relationship information between emotion and text, and utilizes graph attention network to model the structure information of the text. Our SECA method can be easily used for extracting clause-level emotion causes for CECA as well. Experimental results show that the proposed method consistently outperforms the state-of-the-art ECA methods on benchmark emotion cause dataset.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:eflP2zaiRacC", "title": "Modeling speech acts in asynchronous conversations: A neural-CRF approach", "authors": ["Shafiq Joty", "Tasnim Mohiuddin"], "description": "Participants in an asynchronous conversation (e.g., forum, e-mail) interact with each other at different times, performing certain communicative acts, called speech acts (e.g., question, request). In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network (LSTM-RNN) first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field (CRF) model to capture the conversational dependencies between sentences. The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 1, 2, 5, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:bz8QjSJIRt4C", "title": "Models for capturing temporal smoothness in evolving networks for learning latent representation of nodes", "authors": ["Tanay Kumar Saha", "Thomas Williams", "Mohammad Al Hasan", "Shafiq Joty", "Nicholas K Varberg"], "description": "In a dynamic network, the neighborhood of the vertices evolve across different temporal snapshots of the network. Accurate modeling of this temporal evolution can help solve complex tasks involving real-life social and interaction networks. However, existing models for learning latent representation are inadequate for obtaining the representation vectors of the vertices for different time-stamps of a dynamic network in a meaningful way. In this paper, we propose latent representation learning models for dynamic networks which overcome the above limitation by considering two different kinds of temporal smoothness: (i) retrofitted, and (ii) linear transformation. The retrofitted model tracks the representation vector of a vertex over time, facilitating vertex-based temporal analysis of a network. On the other hand, linear transformation based model provides a smooth transition operator which maps the representation vectors of all vertices from one temporal snapshot to the next (unobserved) snapshot-this facilitates prediction of the state of a network in a future time-stamp. We validate the performance of our proposed models by employing them for solving the temporal link prediction task. Experiments on 9 real-life networks from various domains validate that the proposed models are significantly better than the existing models for predicting the dynamics of an evolving network.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 2, 3, 1, 0, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:cFHS6HbyZ2cC", "title": "CQAVis: Visual text analytics for community question answering", "authors": ["Enamul Hoque", "Shafiq Joty", "Luis Marquez", "Giuseppe Carenini"], "description": "Community question answering (CQA) forums can provide effective means for sharing information and addressing a user's information needs about particular topics. However, many such online forums are not moderated, resulting in many low quality and redundant comments, which makes it very challenging for users to find the appropriate answers to their questions. In this paper, we apply a user-centered design approach to develop a system, CQAVis, which supports users in identifying high quality comments and get their questions answered. Informed by the user's requirements, the system combines both text analytics and interactive visualization techniques together in a synergistic way. Given a new question posed by the user, the text analytic module automatically finds relevant answers by exploring existing related questions and the comments within their threads. Then the visualization module presents the\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 1, 2, 2, 1, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:ZHo1McVdvXMC", "title": "A Deep Fusion Model for Domain Adaptation in Phrase-based MT", "authors": ["Nadir Durrani", "Hassan Sajjad", "Shafiq Joty", "Ahmed Abdelali"], "description": "We present a novel fusion model for domain adaptation in Statistical Machine Translation. Our model is based on the joint source-target neural network Devlin et al., 2014, and is learned by fusing in-and out-domain models. The adaptation is performed by backpropagating errors from the output layer to the word embedding layer of each model, subsequently adjusting parameters of the composite model towards the in-domain data. On the standard tasks of translating English-to-German and Arabic-to-English TED talks, we observed average improvements of+ 0.9 and+ 0.7 BLEU points, respectively over a competition grade phrase-based system. We also demonstrate improvements over existing adaptation methods.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 1, 3, 0, 3, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:QIV2ME_5wuYC", "title": "Selecting sentences for answering complex questions", "authors": ["Yllias Chali", "Shafiq Joty"], "description": "Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented, informative multi-document summarization. In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: kmeans and Expectation Maximization (EM), for computing relative importance of the sentences. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. We extracted different kinds of features (ie lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query. We used a local search technique to learn the weights of the features. For all our methods of generating summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 3, 1, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:eQOLeE2rZwMC", "title": "Unsupervised Approach for Selecting Sentences in Query-based Summarization.", "authors": ["Yllias Chali", "Shafiq R Joty"], "description": "When a user is served with a ranked list of relevant documents by the standard document search engines, his search task is usually not over. He has to go through the entire document contents to judge its relevance and to find the precise piece of information he was looking for. Query-relevant summarization tries to remove the onus on the end-user by providing more condensed and direct access to relevant information. Query-relevant summarization is the task to synthesize a fluent, well-organized summary of the document collection that answers the user questions. We extracted several features of different types (ie lexical, lexical semantic, statistical and cosine similarity) for each of the sentences in the document collection in order to measure its relevancy to the user query. We experimented with two well-known unsupervised statistical machine learning techniques: K-Means and EM algorithms and evaluated their performances. For all these methods of generating summaries, we have shown the effects of different kinds of features.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:uJ-U7cs_P_0C", "title": "Differentiable Window for Dynamic Local Attention", "authors": ["Thanh-Tung Nguyen", "Xuan-Phi Nguyen", "Shafiq Joty", "Xiaoli Li"], "description": "We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions. We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways. We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling. Our experimental results demonstrate consistent and sizable improvements across all tasks.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 3, 4, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:RGFaLdJalmkC", "title": "Advances in focused retrieval: A general review", "authors": ["Shafiq Rayhan Joty", "Sheikh Sadid-Al-Hasan"], "description": "When a user is served with a ranked list of relevant documents by the standard document retrieval systems (i.e. search engines), his search task is usually not over. The next step for him is to look into the documents themselves in search for the precise piece of information he was looking for. This method is time consuming, and a correct answer could easily be missed, by either an incorrect query resulting in missing documents or by careless reading. Focused retrieval tries to remove the onus on the end-user, by providing more direct access to relevant information. Focused retrieval is becoming increasingly important in all areas of information retrieval. In this paper we investigate the various aspects of focused retrieval.", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 1, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:2tRrZ1ZAMYUC", "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework", "authors": ["Ruochen Zhao", "Xingxuan Li", "Shafiq Joty", "Chengwei Qin", "Lidong Bing"], "description": "As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [8, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:vDijr-p_gm4C", "title": "Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses", "authors": ["Prathyusha Jwalapuram", "Shafiq Joty", "Youlin Shen"], "description": "Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En. Code available at .", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 6, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:dfsIfKJdRG4C", "title": "Domain adaptation using neural network joint model", "authors": ["Shafiq Joty", "Nadir Durrani", "Hassan Sajjad", "Ahmed Abdelali"], "description": "We explore neural joint models for the task of domain adaptation in machine translation in two ways: (i)\u00a0we apply state-of-the-art domain adaptation techniques, such as mixture modelling and data selection using the recently proposed Neural Network Joint Model (NNJM) (Devlin et\u00a0al., 2014); (ii)\u00a0we propose two novel approaches to perform adaptation through instance weighting and weight readjustment in the NNJM framework. In our first approach, we propose a pair of models called Neural Domain Adaptation Models (NDAM) that minimizes the cross entropy by regularizing the loss function with respect to in-domain (and optionally to out-domain) model. In the second approach, we present a set of Neural Fusion Models (NFM) that combines the in- and the out-domain models by readjusting their parameters based on the in-domain data.We evaluated our models on the standard task of translating English-to\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 1, 3, 2, 1, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:-f6ydRqryjwC", "title": "Learning to differentiate better from worse translations", "authors": ["Francisco Guzm\u00e1n", "Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Alessandro Moschitti", "Preslav Nakov", "Massimo Nicosia"], "description": "We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically. The evaluation results show that learning in the proposed framework yields better correlation with humans than computing the direct similarity over the same type of structures. Also, we show our structural kernel learning (SKL) can be a general framework for MT evaluation, in which syntactic and semantic information can be naturally incorporated.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 1, 2, 0, 1, 0, 0, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:t7zJ5fGR-2UC", "title": "Cross-model Back-translated Distillation for Unsupervised Machine Translation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Thanh-Tung Nguyen", "Wu Kui", "Ai Ti Aw"], "description": "Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization, language modeling and iterative back-translation, though they may apply them differently. Crucially, iterative back-translation and denoising auto-encoding for language modeling provide data diversity to train the UMT systems. However, the gains from these diversification processes has seemed to plateau. We introduce a novel component to the standard UMT framework called Cross-model Back-translated Distillation (CBD), that is aimed to induce another level of data diversification that existing principles lack. CBD is applicable to all previous UMT approaches. In our experiments, CBD achieves the state of the art in the WMT\u201914 English-French, WMT\u201916 English-German and English-Romanian bilingual unsupervised translation tasks, with 38.2, 30.1, and 36.3 BLEU respectively. It also yields 1.5\u20133.3 BLEU improvements in IWSLT English-French and English-German tasks. Through extensive experimental analyses, we show that CBD is effective because it embraces data diversity while other similar variants do not.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 3, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:j8SEvjWlNXcC", "title": "Efficient Constituency Parsing by Pointing", "authors": ["Thanh-Tung Nguyen", "Xuan-Phi Nguyen", "Shafiq Joty", "Xiaoli Li"], "description": "We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity. Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 2, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:uWQEDVKXjbEC", "title": "Regularized and retrofitted models for learning sentence representation with context", "authors": ["Tanay Kumar Saha", "Shafiq Joty", "Naeemul Hassan", "Mohammad Al Hasan"], "description": "Vector representation of sentences is important for many text processing tasks that involve classifying, clustering, or ranking sentences. For solving these tasks, bag-of-word based representation has been used for a long time. In recent years, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform traditional bag-of-words representations. However, most existing methods belonging to the neural models consider only the content of a sentence, and disregard its relations with other sentences in the context. In this paper, we first characterize two types of contexts depending on their scope and utility. We then propose two approaches to incorporate contextual information into content-based models. We evaluate our sentence representation models in a setup, where context is available to infer sentence vectors. Experimental results demonstrate that our\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 3, 0, 1, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:zYLM7Y9cAGgC", "title": "The University of British Columbia at TAC 2008.", "authors": ["Gabriel Murray", "Shafiq R Joty", "Giuseppe Carenini", "Raymond T Ng"], "description": "In this paper we describe the University of British Columbia\u2019s participation in the Text Analysis Conference 2008. This work represents our first submission to the DUC/TAC series of conferences, and we participated in both the summarization tasks: the main update task as well as the pilot task on summarizing blog opinions. We describe our systems in detail and describe our performance in the context of all submitted systems.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 2, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:yMeIxYmEMEAC", "title": "Towards Summary Candidates Fusion", "authors": ["Mathieu Ravaut", "Shafiq Joty", "Nancy F Chen"], "description": "Sequence-to-sequence deep neural models fine-tuned for abstractive summarization can achieve great performance on datasets with enough human annotations. Yet, it has been shown that they have not reached their full potential, with a wide gap between the top beam search output and the oracle beam. Recently, re-ranking methods have been proposed, to learn to select a better summary candidate. However, such methods are limited by the summary quality aspects captured by the first-stage candidates. To bypass this limitation, we propose a new paradigm in second-stage abstractive summarization called SummaFusion that fuses several summary candidates to produce a novel abstractive second-stage summary. Our method works well on several summarization datasets, improving both the ROUGE scores and qualitative properties of fused summaries. It is especially good when the candidates to fuse are worse, such as in the few-shot setup where we set a new state-of-the-art. We will make our code and checkpoints available at https://github.com/ntunlp/SummaFusion/.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:6ZxmRoH8BuwC", "title": "LANTERN: Boredom-conscious Natural Language Description Generation of Query Execution Plans for Database Education", "authors": ["Peng Chen", "Hui Li", "Sourav S Bhowmick", "Shafiq R Joty", "Weiguo Wang"], "description": "The database systems course in an undergraduate computer science degree program is gaining increasing importance due to the continuous supply of database-related jobs as well as the rise of Data Science. A key learning goal of learners taking such a course is to understand how SQL queries are executed in an RDBMS in practice. An RDBMS typically exposes a query execution plan (QEP) in a visual or textual format, which describes the execution steps for a given query. However, it is often daunting for a learner to comprehend these QEPs containing vendor-specific implementation details. In this demonstration, we present a novel, generic, and portable system called LANTERN that generates a natural language (NL)-based description of the execution strategy chosen by the underlying RDBMS to process a query. It provides a declarative framework called POOL for subject matter experts (SME) to efficiently\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [4, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:ruyezt5ZtCIC", "title": "Span-level Emotion Cause Analysis with Neural Sequence Tagging", "authors": ["Xiangju Li", "Wei Gao", "Shi Feng", "Wang Daling", "Shafiq Joty"], "description": "This paper addresses the task of span-level emotion cause analysis (SECA). It is a finer-grained emotion cause analysis (ECA) task, which aims to identify the specific emotion cause span(s) behind certain emotions in text. In this paper, we formalize SECA as a sequence tagging task for which several variants of neural network-based sequence tagging models to extract specific emotion cause span(s) in the given context. These models combine different types of encoding and decoding approaches. Furthermore, to make our models more \"emotionally sensitive'', we utilize the multi-head attention mechanism to enhance the representation of context. Experimental evaluations conducted on two benchmark datasets demonstrate the effectiveness of the proposed models.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 2, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:NXb4pA-qfm4C", "title": "Cascaded fast and slow models for efficient semantic code search", "authors": ["Akhilesh Deepak Gotmare", "Junnan Li", "Shafiq Joty", "Steven CH Hoi"], "description": "The goal of natural language semantic code search is to retrieve a semantically relevant code snippet from a fixed set of candidates using a natural language query. Existing approaches are neither effective nor efficient enough towards a practical semantic code search system. In this paper, we propose an efficient and accurate semantic code search framework with cascaded fast and slow models, in which a fast transformer encoder model is learned to optimize a scalable index for fast retrieval followed by learning a slow classification-based re-ranking model to improve the performance of the top K results from the fast retrieval. To further reduce the high memory cost of deploying two separate models in practice, we propose to jointly train the fast and slow model based on a single transformer encoder with shared parameters. The proposed cascaded approach is not only efficient and scalable, but also achieves state-of-the-art results with an average mean reciprocal ranking (MRR) score of 0.7795 (across 6 programming languages) as opposed to the previous state-of-the-art result of 0.713 MRR on the CodeSearchNet benchmark.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:zLWjf1WUPmwC", "title": "Cross Attention with Monotonic Alignment for Speech Transformer.", "authors": ["Yingzhu Zhao", "Chongjia Ni", "Cheung-Chi Leung", "Shafiq R Joty", "Eng Siong Chng", "Bin Ma"], "description": "Transformer, a state-of-the-art neural network architecture, has been used successfully for different sequence-to-sequence transformation tasks. This model architecture disperses the attention distribution over entire input to learn long-term dependencies, which is important for some sequence-to-sequence tasks, such as neural machine translation and text summarization. However, automatic speech recognition (ASR) has a characteristic to have monotonic alignment between text output and speech input. Techniques like Connectionist Temporal Classification (CTC), RNN Transducer (RNN-T) and Recurrent Neural Aligner (RNA) build on top of this monotonic alignment and use local encoded speech representations for corresponding token prediction. In this paper, we present an effective cross attention biasing technique in transformer that takes monotonic alignment between text output and speech input into consideration by making use of cross attention weights. Specifically, a Gaussian mask is applied on cross attention weights to limit the input speech context range locally given alignment information. We further introduce a regularizer for alignment regularization. Experiments on LibriSpeech dataset find that our proposed model can obtain improved output-input alignment for ASR, and yields 14.5%-25.0% relative word error rate (WER) reductions.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 2, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:GtLg2Ama23sC", "title": "Can Your Context-Aware MT System Pass the DiP Benchmark Tests?: Evaluation Benchmarks for Discourse Phenomena in Machine Translation", "authors": ["Prathyusha Jwalapuram", "Barbara Rychalska", "Shafiq Joty", "Dominika Basaj"], "description": "Despite increasing instances of machine translation (MT) systems including contextual information, the evidence for translation quality improvement is sparse, especially for discourse phenomena. Popular metrics like BLEU are not expressive or sensitive enough to capture quality improvements or drops that are minor in size but significant in perception. We introduce the first of their kind MT benchmark datasets that aim to track and hail improvements across four main discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. We also introduce evaluation methods for these tasks, and evaluate several baseline MT systems on the curated datasets. Surprisingly, we find that existing context-aware models do not improve discourse-related translations consistently across languages and phenomena.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 3, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:ULOm3_A8WrAC", "title": "Interactive exploration of asynchronous conversations: Applying a user-centered approach to design a visual text analytic system", "authors": ["Enamul Hoque", "Giuseppe Carenini", "Shafiq Joty"], "description": "Exploring an online conversation can be very difficult for a user, especially when it becomes a long complex thread. We follow a human-centered design approach to tightly integrate text mining methods with interactive visualization techniques to support the users in fulfilling their information needs. The resulting visual text analytic system provides multifaceted exploration of asynchronous conversations. We discuss a number of open challenges and possible directions for further improvement including the integration of interactive human feedback in the text mining loop, applying more advanced text analysis methods with visualization techniques, and evaluating the system with real users.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 1, 1, 0, 1, 1, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:4X0JR2_MtJMC", "title": "Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases", "authors": ["Xingxuan Li", "Ruochen Zhao", "Yew Ken Chia", "Bosheng Ding", "Lidong Bing", "Shafiq Joty", "Soujanya Poria"], "description": "We introduce Chain of Knowledge (CoK), a framework that augments large language models with structured knowledge bases to improve factual correctness and reduce hallucination. Compared to previous works which only retrieve unstructured texts, CoK leverages structured knowledge bases which support complex queries and offer more direct factual statements. To assist large language models to effectively query knowledge bases, we propose a query generator model with contrastive instruction-tuning. As the query generator is separate from the frozen large language model, our framework is modular and thus easily adapted to various knowledge sources and models. Experiments show that our framework significantly enhances the factual correctness of large language models on knowledge-intensive tasks.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:k8Z6L05lTy4C", "title": "AUGVIC: Exploiting BiText Vicinity for Low-Resource NMT", "authors": ["Tasnim Mohiuddin", "M Saiful Bari", "Shafiq Joty"], "description": "The success of Neural Machine Translation (NMT) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs, NMT systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train/test) and monolingual data might degrade the performance. To alleviate such issues, we propose AUGVIC, a novel data augmentation framework for low-resource NMT which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with finer level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from AUGVIC with the ones from the extra monolingual data, we achieve further improvements. We show that AUGVIC helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of AUGVIC, we perform an in-depth framework analysis.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [3, 2, 0, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:rmuvC79q63oC", "title": "Hyperedge2vec: Distributed representations for hyperedges", "authors": ["Ankit Sharma", "Shafiq Joty", "Himanshu Kharkwal", "Jaideep Srivastava"], "description": "Data structured in form of overlapping or non-overlapping sets is found in a variety of domains, sometimes explicitly but often subtly. For example, teams, which are of prime importance in social science studies are \\enquote{sets of individuals}; \\enquote{item sets} in pattern mining are sets; and for various types of analysis in language studies a sentence can be considered as a \\enquote{set or bag of words}. Although building models and inference algorithms for structured data has been an important task in the fields of machine learning and statistics, research on \\enquote{set-like} data still remains less explored. Relationships between pairs of elements can be modeled as edges in a graph. However, modeling relationships that involve all members of a set, a hyperedge is a more natural representation for the set. In this work, we focus on the problem of embedding hyperedges in a hypergraph (a network of overlapping sets) to a low dimensional vector space. We propose a probabilistic deep-learning based method as well as a tensor-based algebraic model, both of which capture the hypergraph structure in a principled manner without loosing set-level information. Our central focus is to highlight the connection between hypergraphs (topology), tensors (algebra) and probabilistic models. We present a number of interesting baselines, some of which adapt existing node-level embedding models to the hyperedge-level, as well as sequence based language techniques which are adapted for set structured hypergraph topology. The performance is evaluated with a network of social groups and a network of word phrases. Our experiments show that\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 0, 1, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:3s1wT3WcHBgC", "title": "An Interactive System for Exploring Community Question Answering Forums.", "authors": ["Enamul Hoque", "Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Alberto Barr\u00f3n-Cede\u00f1o", "Alessandro Moschitti", "Giovanni Da San Martino", "Preslav Nakov", "Salvatore Romeo", "Giuseppe Carenini."], "description": "We present an interactive system to provide effective and efficient search capabilities in Community Question Answering (cQA) forums. The system integrates state-of-the-art technology for answer search with a Web-based user interface specifically tailored to support the cQA forum readers. The answer search module automatically finds relevant answers for a new question by exploring related questions and the comments within their threads. The graphical user interface presents the search results and supports the exploration of related information. The system is running live at http://www. qatarliving. com/betasearch/.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 0, 2, 0, 1, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:mVmsd5A6BfQC", "title": "Semantic kernels for semantic parsing", "authors": ["Iman Saleh", "Alessandro Moschitti", "Preslav Nakov", "Llu\u00eds M\u00e0rquez", "Shafiq Joty"], "description": "We present an empirical study on the use of semantic information for Concept Segmentation and Labeling (CSL), which is an important step for semantic parsing. We represent the alternative analyses output by a state-of-the-art CSL parser with tree structures, which we rerank with a classifier trained on two types of semantic tree kernels: one processing structures built with words, concepts and Brown clusters, and another one using semantic similarity among the words composing the structure. The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:4DMP91E08xMC", "title": "A study of using syntactic and semantic structures for concept segmentation and labeling", "authors": ["Iman Saleh", "Scott Cyphers", "Jim Glass", "Shafiq Joty", "Llu\u00eds M\u00e0rquez", "Alessandro Moschitti", "Preslav Nakov"], "description": "This paper presents an empirical study on using syntactic and semantic information for Concept Segmentation and Labeling (CSL), a well-known component in spoken language understanding. Our approach is based on reranking N-best outputs from a state-of-the-art CSL parser. We perform extensive experimentation by comparing different tree-based kernels with a variety of representations of the available linguistic information, including semantic concepts, words, POS tags, shallow and full syntax, and discourse trees. The results show that the structured representation with the semantic concepts yields significant improvement over the base CSL parser, much larger compared to learning with an explicit feature vector representation. We also show that shallow syntax helps improve the results and that discourse relations can be partially beneficial.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:0CzhzZyukY4C", "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond", "authors": ["Philippe Laban", "Wojciech Kry\u015bci\u0144ski", "Divyansh Agarwal", "Alexander R Fabbri", "Caiming Xiong", "Shafiq Joty", "Chien-Sheng Wu"], "description": "With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals that most LLMs fail on more complex formulations of the task and exposes issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8\\% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:KbBQZpvPDL4C", "title": "OpenCQA: Open-ended Question Answering with Charts", "authors": ["Shankar Kantharaj", "Xuan Long Do", "Rixie Tiffany Ko Leong", "Jia Qing Tan", "Enamul Hoque", "Shafiq Joty"], "description": "Charts are very popular to analyze data and convey important insights. People often analyze visualizations to answer open-ended questions that require explanatory answers. Answering such questions are often difficult and time-consuming as it requires a lot of cognitive and perceptual efforts. To address this challenge, we introduce a new task called OpenCQA, where the goal is to answer an open-ended question about a chart with descriptive texts. We present the annotation process and an in-depth analysis of our dataset. We implement and evaluate a set of baselines under three practical settings. In the first setting, a chart and the accompanying article is provided as input to the model. The second setting provides only the relevant paragraph(s) to the chart instead of the entire article, whereas the third setting requires the model to generate an answer solely based on the chart. Our analysis of the results show that the top performing models generally produce fluent and coherent text while they struggle to perform complex logical and arithmetic reasoning.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:IUKN3-7HHlwC", "title": "Designing, Developing, Evaluating, and Implementing a Smartphone-Delivered, Rule-Based Conversational Agent (DISCOVER): Development of a Conceptual Framework", "authors": ["Dhakshenya Ardhithy Dhinagaran", "Laura Martinengo", "Moon-Ho Ringo Ho", "Shafiq Joty", "Tobias Kowatsch", "Rifat Atun", "Lorainne Tudor Car"], "description": "Background Conversational agents (CAs), also known as chatbots, are computer programs that simulate human conversations by using predetermined rule-based responses or artificial intelligence algorithms. They are increasingly used in health care, particularly via smartphones. There is, at present, no conceptual framework guiding the development of smartphone-based, rule-based CAs in health care. To fill this gap, we propose structured and tailored guidance for their design, development, evaluation, and implementation. Objective The aim of this study was to develop a conceptual framework for the design, evaluation, and implementation of smartphone-delivered, rule-based, goal-oriented, and text-based CAs for health care. Methods We followed the approach by Jabareen, which was based on the grounded theory method, to develop this conceptual framework. We performed 2 literature reviews focusing on health care CAs and conceptual frameworks for the development of mobile health interventions. We identified, named, categorized, integrated, and synthesized the information retrieved from the literature reviews to develop the conceptual framework. We then applied this framework by developing a CA and testing it in a feasibility study. Results The Designing, Developing, Evaluating, and Implementing a Smartphone-Delivered, Rule-Based Conversational Agent (DISCOVER) conceptual framework includes 8 iterative steps grouped into 3 stages, as follows: design, comprising defining the goal, creating an identity, assembling the team, and selecting the\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:a9-T7VOCCH8C", "title": "Effective Fine-Tuning Methods for Cross-lingual Adaptation", "authors": ["Tao Yu", "Shafiq Joty"], "description": "Large scale multilingual pre-trained language models have shown promising results in zero-and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as a complementary to multilingual language modeling using the unlabeled data in the target language. We also propose an adaption method based on contrastive learning to better capture the semantic relationship in the parallel data, when a few translation pairs are available. To show our method\u2019s effectiveness, we conduct extensive experiments on cross-lingual inference and review classification tasks across various languages. We report significant gains compared to directly fine-tuning multilingual pre-trained models and other semi-supervised alternatives.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:LI9QrySNdTsC", "title": "Preventing early endpointing for online automatic speech recognition", "authors": ["Yingzhu Zhao", "Chongjia Ni", "Cheung-Chi Leung", "Shafiq Joty", "Eng Siong Chng", "Bin Ma"], "description": "With the recent development of end-to-end models in speech recognition, there have been more interests in adapting these models for online speech recognition. However, using end-to-end models for online speech recognition is known to suffer from an early endpointing problem, which brings in many deletion errors. In this paper, we propose to address the early endpointing problem from the gradient perspective. Specifically, we leverage on the recently proposed ScaleGrad technique, which was proposed to mitigate the text degeneration issue. Different from ScaleGrad, we adapt it to discourage the early generation of the end-of-sentence () token. A scaling term is added to directly maneuver the gradient of the training loss to encourage the model to learn to keep generating non- tokens. Compared with previous approaches such as voice-activity-detection and end-of-query detection, the proposed method does\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [2, 0, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:PELIpwtuRlgC", "title": "Adaptation of Hierarchical Structured Models for Speech Act Recognition in Asynchronous Conversation", "authors": ["Tasnim Mohiuddin", "Thanh-tung Nguyen", "Shafiq (equal contributions) Joty"], "description": "We address the problem of speech act recognition (SAR) in asynchronous conversations (forums, emails). Unlike synchronous conversations (e.g., meetings, phone), asynchronous domains lack large labeled datasets to train an effective SAR model. In this paper, we propose methods to effectively leverage abundant unlabeled conversational data and the available labeled data from synchronous domains. We carry out our research in three main steps. First, we introduce a neural architecture based on hierarchical LSTMs and conditional random fields (CRF) for SAR, and show that our method outperforms existing methods when trained on in-domain data only. Second, we improve our initial SAR models by semi-supervised learning in the form of pretrained word embeddings learned from a large unlabeled conversational corpus. Finally, we employ adversarial training to improve the results further by leveraging the labeled data from synchronous domains and by explicitly modeling the distributional shift in two domains.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 3, 0, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:tS2w5q8j5-wC", "title": "Thread Reconstruction in Conversational Data using Neural Coherence", "authors": ["Dat Tien Nguyen", "Shafiq Joty", "Basma El Amel Boussaha", "Maarten de Rijke"], "description": "Discussion forums are an important source of information. They are often used to answer specific questions a user might have and to discover more about a topic of interest. Discussions in these forums may evolve in intricate ways, making it difficult for users to follow the flow of ideas. We propose a novel approach for automatically identifying the underlying thread structure of a forum discussion. Our approach is based on a neural model that computes coherence scores of possible reconstructions and then selects the highest scoring, i.e., the most coherent one. Preliminary experiments demonstrate promising results outperforming a number of strong baseline methods.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 0, 0, 0, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:W7OEmFMy1HYC", "title": "Exploiting syntactic and shallow semantic kernels to improve random walks for complex question answering", "authors": ["Yllias Chali", "Shafiq R Joty"], "description": "We consider the problem of answering complex questions that require inferencing and synthesizing information from multiple documents and can be seen as a kind of topic-oriented, informative multi-document summarization. The stochastic, graph-based method for computing the relative importance of textual units (i.e. sentences) is very successful in generic summarization. In this method, a sentence is encoded as a vector in which each component represents the occurrence frequency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. In this paper, we study the impact of syntactic and shallow semantic information in the graph-based method for answering complex questions. Experimental results show the effectiveness of the syntactic and shallow semantic\u00a0\u2026", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 3, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:BJbdYPG6LGMC", "title": "Retrieving multimodal information for augmented generation: A survey", "authors": ["Ruochen Zhao", "Hailin Chen", "Weishi Wang", "Fangkai Jiao", "Xuan Long Do", "Chengwei Qin", "Bosheng Ding", "Xiaobao Guo", "Minzhi Li", "Xingxuan Li", "Shafiq Joty"], "description": "In this survey, we review methods that retrieve multimodal knowledge to assist and augment generative models. This group of works focuses on retrieving grounding contexts from external sources, including images, codes, tables, graphs, and audio. As multimodal learning and generative AI have become more and more impactful, such retrieval augmentation offers a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. We provide an in-depth review of retrieval-augmented generation in different modalities and discuss potential future directions. As this is an emerging field, we continue to add new papers and methods.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:wMgC3FpKEyYC", "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning", "authors": ["Fan Yin", "Jesse Vig", "Philippe Laban", "Shafiq Joty", "Caiming Xiong", "Chien-Sheng Jason Wu"], "description": "Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60\\% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a meta-tuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:mlAyqtXpCwEC", "title": "Data Selection Curriculum for Neural Machine Translation", "authors": ["Tasnim Mohiuddin", "Philipp Koehn", "Vishrav Chaudhary", "James Cross", "Shruti Bhosale", "Shafiq Joty"], "description": "Neural Machine Translation (NMT) models are typically trained on heterogeneous data that are concatenated and randomly shuffled. However, not all of the training data are equally useful to the model. Curriculum training aims to present the data to the NMT models in a meaningful order. In this work, we introduce a two-stage curriculum training framework for NMT where we fine-tune a base NMT model on subsets of data, selected by both deterministic scoring using pre-trained methods and online scoring that considers prediction scores of the emerging NMT model. Through comprehensive experiments on six language pairs comprising low- and high-resource languages from WMT'21, we have shown that our curriculum strategies consistently demonstrate better quality (up to +2.2 BLEU improvement) and faster convergence (approximately 50% fewer updates).", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [2, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:a3BOlSfXSfwC", "title": "Refining Low-Resource Unsupervised Translation by Language Disentanglement of Multilingual Translation Model", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Kui Wu", "Ai Ti Aw"], "description": "Numerous recent work on unsupervised machine translation (UMT) implies that competent unsupervised translations of low-resource and unrelated languages, such as Nepali or Sinhala, are only possible if the model is trained in a massive multilingual environment, where these low-resource languages are mixed with high-resource counterparts. Nonetheless, while the high-resource languages greatly help kick-start the target low-resource translation tasks, the language discrepancy between them may hinder their further improvement. In this work, we propose a simple refinement procedure to separate languages from a pre-trained multilingual UMT model for it to focus on only the target low-resource task. Our method achieves the state of the art in the fully unsupervised translation tasks of English to Nepali, Sinhala, Gujarati, Latvian, Estonian and Kazakh, with BLEU score gains of 3.5, 3.5, 3.3, 4.1, 4.2, and 3.3, respectively. Our codebase is available at https://github. com/nxphi47/refineunsupmultilingual_mt", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:P7Ujq4OLJYoC", "title": "Sleepqa: A health coaching dataset on sleep for extractive question answering", "authors": ["Iva Bojic", "Qi Chwen Ong", "Megh Thakkar", "Esha Kamran", "Irving Yu Le Shua", "Jaime Rei Ern Pang", "Jessica Chen", "Vaaruni Nayak", "Shafiq Joty", "Josip Car"], "description": "Question Answering (QA) systems can support health coaches in facilitating clients\u2019 lifestyle behavior changes (eg, in adopting healthy sleep habits). In this paper, we design a domain-specific QA pipeline for sleep coaching. To this end, we release SleepQA, a dataset created from 7,005 passages comprising 4,250 training examples with single annotations and 750 examples with 5-way annotations. We fine-tuned different domain-specific BERT models on our dataset and perform extensive automatic and human evaluation of the resulting end-to-end QA pipeline. Comparisons of our pipeline with baseline show improvements in domain-specific natural language processing on real-world questions. We hope that this dataset will lead to wider research interest in this important health domain.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:fFSKOagxvKUC", "title": "Improving Conversational Recommender System via Contextual and Time-Aware Modeling with Less Domain-Specific Knowledge", "authors": ["Lingzhi Wang", "Shafiq Joty", "Wei Gao", "Xingshan Zeng", "Kam-Fai Wong"], "description": "Conversational Recommender Systems (CRS) has become an emerging research topic seeking to perform recommendations through interactive conversations, which generally consist of generation and recommendation modules. Prior work on CRS tends to incorporate more external and domain-specific knowledge like item reviews to enhance performance. Despite the fact that the collection and annotation of the external domain-specific information needs much human effort and degenerates the generalizability, too much extra knowledge introduces more difficulty to balance among them. Therefore, we propose to fully discover and extract internal knowledge from the context. We capture both entity-level and contextual-level representations to jointly model user preferences for the recommendation, where a time-aware attention is designed to emphasize the recently appeared items in entity-level representations. We further use the pre-trained BART to initialize the generation module to alleviate the data scarcity and enhance the context modeling. In addition to conducting experiments on a popular dataset (ReDial), we also include a multi-domain dataset (OpenDialKG) to show the effectiveness of our model. Experiments on both datasets show that our model achieves better performance on most evaluation metrics with less external knowledge and generalizes well to other domains. Additional analyses on the recommendation and generation tasks demonstrate the effectiveness of our model in different scenarios.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 3, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:sNmaIFBj_lkC", "title": "Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation", "authors": ["Xuan-Phi Nguyen", "Hongyu Gong", "Yun Tang", "Changhan Wang", "Philipp Koehn", "Shafiq Joty"], "description": "Modern unsupervised machine translation systems mostly train their models by generating synthetic parallel training data from large unlabeled monolingual corpora of different languages through various means, such as iterative back-translation. However, there may exist small amount of actual parallel data hidden in the sea of unlabeled data, which has not been exploited. We develop a new fine-tuning objective, called Language-Agnostic Constraint for SwAV loss, or LAgSwAV, which enables a pre-trained model to extract such pseudo-parallel data from the monolingual corpora in a fully unsupervised manner. We then propose an effective strategy to utilize the obtained synthetic data to augment unsupervised machine translation. Our method achieves the state of the art in the WMT'14 English-French, WMT'16 German-English and English-Romanian bilingual unsupervised translation tasks, with 40.2, 36.8, 37.0 BLEU, respectively. We also achieve substantial improvements in the FLoRes low-resource English-Nepali and English-Sinhala unsupervised tasks with 5.3 and 5.4 BLEU, respectively.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [2, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:HtEfBTGE9r8C", "title": "A Unified Speaker Adaptation Approach for ASR", "authors": ["Yingzhu Zhao", "Chongjia Ni", "Cheung-Chi Leung", "Shafiq Joty", "Eng Siong Chng", "Bin Ma"], "description": "Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.58% relative WER reduction, and surpasses the finetuning method by up to relative 2.54%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53% with only a few epochs of training.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:i2xiXl-TujoC", "title": "A Conditional Splitting Framework for Efficient Constituency Parsing", "authors": ["Thanh-Tung Nguyen", "Xuan-Phi Nguyen", "Shafiq Joty", "Xiaoli Li"], "description": "We introduce a generic seq2seq parsing framework that casts constituency parsing problems (syntactic and discourse parsing) into a series of conditional splitting decisions. Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding, which is linear in number of nodes. The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without relying on expensive structured inference. Crucially, for discourse analysis we show that in our formulation, discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring segmentation as a pre-requisite. Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours. In discourse parsing, our method outperforms SoTA by a good margin.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:VOx2b1Wkg3QC", "title": "NLP for conversations: Sentiment, summarization, and group dynamics", "authors": ["Gabriel Murray", "Giuseppe Carenini", "Shafiq Joty"], "description": "The primary goal of this tutorial is for attendees to learn about recent work applying NLP to spoken and written conversations, with a focus on computational models for three related topics: conversational structure, summarization and sentiment detection, and group dynamics. We provide examples of specific NLP tasks within those three areas, how they relate to one another, their applications, and how we evaluate task performance.We will begin by discussing motivations and applications of applying NLP methods to conversations, including downstream applications that could benefit. Attendees will hear about the challenges of working with noisy data, and examples of datasets of spoken and/or written conversations. The first part of the tutorial covers conversational structures, the basic building blocks for working with conversational data. Participants will learn about computational methods for uncovering thread and topic structures of a conversation, detecting dialogue acts and adjacency pairs, identifying participant roles (where relevant), and how to treat disfluencies. We will cover methods for both synchronous (eg, meeting, phone) and asynchronous (eg, forum, email) conversations. In the second part of the tutorial, we will focus on sentiment analysis and summarization. Attendees will learn about the related, overlapping tasks of detecting sentiment, subjectivity, and opinions. We will cover unsupervised and supervised approaches, as well as multimodal sentiment detection. Participants will learn about intrinsic vs. extrinsic evaluation of sentiment analysis methods for conversations. For summarization, we will cover core topics, such as the\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 2, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:UebtZRa9Y70C", "title": "Finding Topics in Emails: Is LDA enough?", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Gabriel Murray", "Raymond Ng"], "description": "Our research addresses the task of finding topics at the sentence level in email conversations. As an asynchronous collaborative application, email has its own characteristics which differ from written monologues (eg, text books, news articles) or spoken dialogs (eg, meetings). Hence, the generative topic models like Latent Dirichlet Allocation (LDA) and its variations, which are successful in finding topics in monologue or dialog, may not be successful by themselves in asynchronous written conversations like emails. However, an effective combination of LDA with other important features can give us the desired results. We first point out the specific characteristics of emails that we need to consider in order to find the inherent topics discussed in an email conversation. Then we demonstrate why the generative topic models by themselves may not be adequate for this task. We propose a novel graph-theoretic framework to solve the problem. Crucial to our proposed approach is that it captures the discriminative email features and integrates the strengths of the supervised approach with the unsupervised technique considering LDA yet as one of the important factors.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:_FxGoFyzp5QC", "title": "Answering complex questions using query-focused summarization technique", "authors": ["Yllias Chali", "Shafiq R Joty"], "description": "Unlike simple questions, complex questions cannot be answered by simply extracting named entities. These questions require inferencing and synthesizing information from multiple documents that can be seen as a kind of topic-oriented, informative multi-document summarization. In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: k-means and Expectation Maximization (EM), for computing relative importance of the sentences. The feature set includes different kinds of features: lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic. A gradient descent local search technique is used to learn the optimal weights of the features. The effects of the different features are also shown for all the methods of generating summaries.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:4fGpz3EwCPoC", "title": "Answer extraction for simple and complex questions", "authors": ["Shafiq Rayhan Joty"], "description": "When a user is served with a ranked list of relevant documents by the standard document search engines, his search task is usually not over. He has to go through the entire document contents to find the precise piece of information he was looking for. Question answering, which is the retrieving of answers to natural language questions from a document collection, tries to remove the onus on the end-user by providing direct access to relevant information. This thesis is concerned with open-domain question answering. We have considered both simple and complex questions. Simple questions (i.e. factoid and list) are easier to answer than questions that have complex information needs and require inferencing and synthesizing information from multiple documents. Our question answering system for simple questions is based on question classification and document tagging. Question classification extracts useful information (i.e. answer type) about how to answer the question and document tagging extracts useful information from the documents, which is used in finding the answer to the question. For complex questions, we experimented with both empirical and machine learning approaches. We extracted several features of different types (i.e. lexical, lexical semantic, syntactic and semantic) for each of the sentences in the document collection in order to measure its relevancy to the user query. One hill climbing local search strategy is used to fine-tune the feature-weights. We also experimented with two unsupervised machine learning techniques: k-means and Expectation Maximization (EM) algorithms and evaluated their performance. For all\u00a0\u2026", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:4xDN1ZYqzskC", "title": "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages", "authors": ["Philippe Laban", "Jesse Vig", "Wojciech Kryscinski", "Shafiq Joty", "Caiming Xiong", "Chien-Sheng Wu"], "description": "Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits - such as adding relevant background information or reordering content - may require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWiPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWiPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.6, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWiPE-trained models generate more complex edits while reducing unwanted edits.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:5MTHONV0fEkC", "title": "SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning", "authors": ["M Saiful Bari", "Aston Zhang", "Shuai Zheng", "Xingjian Shi", "Yi Zhu", "Shafiq Joty", "Mu Li"], "description": "Pre-trained large language models can efficiently interpolate human-written prompts in a natural way. Multitask prompted learning can help generalization through a diverse set of tasks at once, thus enhancing the potential for more effective downstream fine-tuning. To perform efficient multitask-inference in the same batch, parameter-efficient fine-tuning methods such as prompt tuning have been proposed. However, the existing prompt tuning methods may lack generalization. We propose SPT, a semi-parametric prompt tuning method for multitask prompted learning. The novel component of SPT is a memory bank from where memory prompts are retrieved based on discrete prompts. Extensive experiments, such as (i) fine-tuning a full language model with SPT on 31 different tasks from 8 different domains and evaluating zero-shot generalization on 9 heldout datasets under 5 NLP task categories and (ii) pretraining\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:3htObqc8RwsC", "title": "CoHS-CQG: Context and History Selection for Conversational Question Generation", "authors": ["Xuan Long Do", "Bowei Zou", "Liangming Pan", "Nancy F Chen", "Shafiq Joty", "Ai Ti Aw"], "description": "Conversational question generation (CQG) serves as a vital task for machines to assist humans, such as interactive reading comprehension, through conversations. Compared to traditional single-turn question generation (SQG), CQG is more challenging in the sense that the generated question is required not only to be meaningful, but also to align with the occurred conversation history. While previous studies mainly focus on how to model the flow and alignment of the conversation, there has been no thorough study to date on which parts of the context and history are necessary for the model. We argue that shortening the context and history is crucial as it can help the model to optimise more on the conversational alignment property. To this end, we propose CoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the context and history of the input. In particular, CoHS selects contiguous sentences and history turns according to their relevance scores by a top-p strategy. Our model achieves state-of-the-art performances on CoQA in both the answer-aware and answer-unaware settings.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:OcBU2YAGkTUC", "title": "GradMask: Gradient-Guided Token Masking for Textual Adversarial Example Detection", "authors": ["Han Cheol Moon", "Shafiq Joty", "Xu Chi"], "description": "We present GradMask, a simple adversarial example detection scheme for natural language processing (NLP) models. It uses gradient signals to detect adversarially perturbed tokens in an input sequence and occludes such tokens by a masking process. GradMask provides several advantages over existing methods including improved detection performance and an interpretation of its decision with a only moderate computational cost. Its approximated inference cost is no more than a single forward- and back-propagation through the target model without requiring any additional detection module. Extensive evaluation on widely adopted NLP benchmark datasets demonstrates the efficiency and effectiveness of GradMask. Code and models are available at https://github.com/Han8931/grad_mask_detection", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:LO7wyVUgiFcC", "title": "Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling", "authors": ["Prathyusha Jwalapuram", "Shafiq Joty", "Xiang Lin"], "description": "Given the claims of improved text generation quality across various pre-trained neural models, we consider the coherence evaluation of machine generated text to be one of the principal applications of coherence models that needs to be investigated. Prior work in neural coherence modeling has primarily focused on devising new architectures for solving the permuted document task. We instead use a basic model architecture and show significant improvements over state of the art within the same training regime. We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup, and enhance the model further through automatic hard negative mining coupled with a large global negative queue encoded by a momentum encoder. We show empirically that increasing the density of negative samples improves the basic model, and using a global negative queue further improves and stabilizes the model while training with hard negative samples. We evaluate the coherence model on task-independent test sets that resemble real-world applications and show significant improvements in coherence evaluations of downstream tasks.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [2, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:0N-VGjzr574C", "title": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings", "authors": ["Linlin Liu", "Thien Hai Nguyen", "Shafiq Joty", "Lidong Bing", "Luo Si"], "description": "Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual tasks. However, most existing approaches to learn CLWE including the ones with contextual embeddings are sense agnostic. In this work, we propose a novel framework to align contextual embeddings at the sense level by leveraging cross-lingual signal from bilingual dictionaries only. We operationalize our framework by first proposing a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate significant performance improvement for word sense disambiguation tasks. We then propose a sense alignment objective on top of the sense-aware cross entropy loss for cross-lingual model pretraining, and pretrain cross-lingual models for several language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best baseline results, our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks, respectively.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:86PQX7AUzd4C", "title": "Co-morbidity exploration on wearables activity data using unsupervised pre-training and multi-task learning", "authors": ["Karan Aggarwal", "Shafiq Joty", "Luis F Luque", "Jaideep Srivastava"], "description": "Physical activity and sleep play a major role in the prevention and management of many chronic conditions. It is not a trivial task to understand their impact on chronic conditions. Currently, data from electronic health records (EHRs), sleep lab studies, and activity/sleep logs are used. The rapid increase in the popularity of wearable health devices provides a significant new data source, making it possible to track the user's lifestyle real-time through web interfaces, both to consumer as well as their healthcare provider, potentially. However, at present there is a gap between lifestyle data (e.g., sleep, physical activity) and clinical outcomes normally captured in EHRs. This is a critical barrier for the use of this new source of signal for healthcare decision making. Applying deep learning to wearables data provides a new opportunity to overcome this barrier. To address the problem of the unavailability of clinical data from a major fraction of subjects and unrepresentative subject populations, we propose a novel unsupervised (task-agnostic) time-series representation learning technique called act2vec. act2vec learns useful features by taking into account the co-occurrence of activity levels along with periodicity of human activity patterns. The learned representations are then exploited to boost the performance of disorder-specific supervised learning models. Furthermore, since many disorders are often related to each other, a phenomenon referred to as co-morbidity, we use a multi-task learning framework for exploiting the shared structure of disorder inducing life-style choices partially captured in the wearables data. Empirical evaluation using actigraphy\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 1, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:gKiMpY-AVTkC", "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?", "authors": ["Chengwei Qin", "Shafiq Joty", "Qian Li", "Ruochen Zhao"], "description": "Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:uDGL6kOW6j0C", "title": "Building Extractive Question Answering System to Support Human-AI Health Coaching Model for Sleep Domain", "authors": ["Iva Bojic", "Qi Chwen Ong", "Shafiq Joty", "Josip Car"], "description": "Non-communicable diseases (NCDs) are a leading cause of global deaths, necessitating a focus on primary prevention and lifestyle behavior change. Health coaching, coupled with Question Answering (QA) systems, has the potential to transform preventive healthcare. This paper presents a human-Artificial Intelligence (AI) health coaching model incorporating a domain-specific extractive QA system. A sleep-focused dataset, SleepQA, was manually assembled and used to fine-tune domain-specific BERT models. The QA system was evaluated using automatic and human methods. A data-centric framework enhanced the system's performance by improving passage retrieval and question reformulation. Although the system did not outperform the baseline in automatic evaluation, it excelled in the human evaluation of real-world questions. Integration into a Human-AI health coaching model was tested in a pilot Randomized Controlled Trial (RCT).", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:6yz0xqPARnAC", "title": "LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models", "authors": ["Fangkai Jiao", "Zhiyang Teng", "Shafiq Joty", "Bosheng Ding", "Aixin Sun", "Zhengyuan Liu", "Nancy F Chen"], "description": "Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results on two challenging logical reasoning benchmarks demonstrate the effectiveness of LogicLLM. Besides, we conduct extensive ablation studies to analyze the key factors in designing logic-oriented proxy tasks.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:tH6gc1N1XXoC", "title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval", "authors": ["Mohammad Abdullah Matin Khan", "M Saiful Bari", "Xuan Long Do", "Weishi Wang", "Md Rizwan Parvez", "Shafiq Joty"], "description": "The ability to solve problems is a hallmark of intelligence and has been an enduring goal in AI. AI systems that can create programs as solutions to problems or assist developers in writing programs can increase productivity and make programming more accessible. Recently, pre-trained large language models have shown impressive abilities in generating new codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap rather than actual execution whereas semantic similarity (or equivalence) of two code segments depends only on their ``execution similarity'', i.e., being able to get the same output for a given input.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:hCrLmN-GePgC", "title": "Remember what you have drawn: Semantic image manipulation with memory", "authors": ["Xiangxi Shi", "Zhonghua Wu", "Guosheng Lin", "Jianfei Cai", "Shafiq Joty"], "description": "Image manipulation with natural language, which aims to manipulate images with the guidance of language descriptions, has been a challenging problem in the fields of computer vision and natural language processing (NLP). Currently, a number of efforts have been made for this task, but their performances are still distant away from generating realistic and text-conformed manipulated images. Therefore, in this paper, we propose a memory-based Image Manipulation Network (MIM-Net), where a set of memories learned from images is introduced to synthesize the texture information with the guidance of the textual description. We propose a two-stage network with an additional reconstruction stage to learn the latent memories efficiently. To avoid the unnecessary background changes, we propose a Target Localization Unit (TLU) to focus on the manipulation of the region mentioned by the text. Moreover, to learn a robust memory, we further propose a novel randomized memory training loss. Experiments on the four popular datasets show the better performance of our method compared to the existing ones.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:Mojj43d5GZwC", "title": "Discourse processing and its applications in text mining", "authors": ["Shafiq Joty", "Giuseppe Carenini", "Raymond T Ng", "Gabriel Murray"], "description": "Zero-knowledge (ZK) proofs have become a central building block for a variety of modern security protocols. Modern ZK constructions, such as the Groth-Sahai proof system, offer novel types of cryptographic flexibility: a participant is able to re-randomize existing ZK proofs to achieve, for instance, message unlink ability in anonymity protocols, she can hide public parts of a ZK proof statement to meet her specific privacy requirements, and she can logically compose ZK proofs in order to construct new proof statements. ZK proof systems that permit these transformations are called malleable. However, since these transformations are accessible also to the adversary, analyzing the security of these protocols requires one to cope with a much more comprehensive attacker model--a challenge that automated protocol analysis thus far has not been capable of dealing with. In this work, we introduce the first symbolic\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 2, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:0EnyYjriUFMC", "title": "Detecting informative blog comments using tree structured conditional random fields", "authors": ["Wei Jin", "Shafiq Joty", "Giuseppe Carenini", "Raymond Ng"], "description": "The Internet provides a variety of ways for people to easily share, socialize, and interact with each other. One of the most popular platforms is the online blog. This causes a vast amount of new text data in the form of blog comments and opinions about news, events and products being generated everyday. However, not all comments are informative. Informative or high quality comments have great impact on the readers\u2019 opinions about the original post content, such as the quality of the product discussed in the post, or the interpretation of a political event. Therefore, developing an efficient and effective mechanism to detect the most informative comments is highly desirable. For this purpose, sites like Slashdot, where users volunteer to rate comments based on their informativeness, can be a great resource to build such automated system using supervised machine learning techniques.Our research concerns building an automatic comment classification system leveraging this freely available valuable resources. Specifically, we discuss how informative comments in blogs can be detected using Conditional Random Fields (CRFs)[6]. Blog conversations typically have a tree-like structure in which an initial post is followed by comments, and each comment can be followed by other comments. In this work, we propose to use Tree-structured Conditional Random Fields (TCRFs) to capture the dependencies in a tree-like conversational structure. This is in contrast with previous work [1] in which results produced by linear-chain CRF models had to be aggregated heuristically. As an additional contribution, we present a new blog corpus consisting of\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:YsMSGLbcyi4C", "title": "University of Lethbridge\u2019s participation in DUC-2007 main task", "authors": ["Yllias Chali", "Shafiq R Joty"], "description": "This paper presents the summarization technique implemented by the University of Lethbridge summarizer in order to generate summaries of maximum 250 words from multiple documents. We describe our system for query-focused summarization based on an enhanced, feature-based framework.", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:PVgj2kMGcgYC", "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning", "authors": ["Ahmed Masry", "Parsa Kavehzadeh", "Xuan Long Do", "Enamul Hoque", "Shafiq Joty"], "description": "Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-level tasks to extract the visual elements (e.g., bars, lines) and data from charts, and (ii) high-level tasks to acquire chart understanding and reasoning skills. We find that pretraining the model on a large corpus with chart-specific low- and high-level tasks followed by finetuning on three down-streaming tasks results in state-of-the-art performance on three downstream tasks.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:artPoR2Yc-kC", "title": "Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation", "authors": ["Yixin Liu", "Alexander R Fabbri", "Yilun Zhao", "Pengfei Liu", "Shafiq Joty", "Chien-Sheng Wu", "Caiming Xiong", "Dragomir Radev"], "description": "Interpretability and efficiency are two important considerations for the adoption of neural automatic metrics. In this work, we develop strong-performing automatic metrics for reference-based summarization evaluation, based on a two-stage evaluation pipeline that first extracts basic information units from one text sequence and then checks the extracted units in another sequence. The metrics we developed include two-stage metrics that can provide high interpretability at both the fine-grained unit level and summary level, and one-stage metrics that achieve a balance between efficiency and interoperability. We make the developed tools publicly available through a Python package and GitHub.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:jgBuDB5drN8C", "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles", "authors": ["Kung-Hsiang Huang", "Philippe Laban", "Alexander R Fabbri", "Prafulla Kumar Choubey", "Shafiq Joty", "Caiming Xiong", "Chien-Sheng Wu"], "description": "Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, to our knowledge, the summarization of diverse information dispersed across multiple articles about an event has not been previously investigated. The latter imposes a different set of challenges for a summarization model. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Moreover, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of the summaries, as well as their correlation with human assessments. We applied our findings to study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover less than 40% of the diverse information on average.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:ALROH1vI_8AC", "title": "Xgen-7b technical report", "authors": ["Erik Nijkamp", "Tian Xie", "Hiroaki Hayashi", "Bo Pang", "Congying Xia", "Chen Xing", "Jesse Vig", "Semih Yavuz", "Philippe Laban", "Ben Krause", "Senthil Purushwalkam", "Tong Niu", "Wojciech Kry\u015bci\u0144ski", "Lidiya Murakhovs' ka", "Prafulla Kumar Choubey", "Alex Fabbri", "Ye Liu", "Rui Meng", "Lifu Tu", "Meghana Bhat", "Chien-Sheng Wu", "Silvio Savarese", "Yingbo Zhou", "Shafiq Joty", "Caiming Xiong"], "description": "Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence open-source LLMs.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:sJsF-0ZLhtgC", "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications", "authors": ["Han Cheol Moon", "Shafiq Joty", "Ruochen Zhao", "Megh Thakkar", "Xu Chi"], "description": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:AHdEip9mkN0C", "title": "Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation", "authors": ["Xuan Long Do", "Bowei Zou", "Shafiq Joty", "Anh Tai Tran", "Liangming Pan", "Nancy F Chen", "Ai Ti Aw"], "description": "Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answer-unaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question to be generated (boolean/span-based) implicitly. Modeling the question type explicitly is crucial as the answer, which hints the models to generate a boolean or span-based question, is unavailable. To this end, we present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a sentence is selected as the rationale from a semantic graph that we construct, and extract the answer span from it. For the how-to-ask stage, a classifier determines the target answer type of the question via two explicit control signals before generating and filtering. In addition, we propose Conv-Distinct, a novel evaluation metric for CQG, to evaluate the diversity of the generated conversation from a context. Compared with the existing answer-unaware CQG models, the proposed SG-CQG achieves state-of-the-art\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:otzGkya1bYkC", "title": "Contrastive Learning with Generated Representations for Inductive Knowledge Graph Embedding", "authors": ["Qian Li", "Shafiq Joty", "Daling Wang", "Shi Feng", "Yifei Zhang", "Chengwei Qin"], "description": "With the evolution of Knowledge Graphs (KGs), new entities emerge which are not seen before. Representation learning of KGs in such an inductive setting aims to capture and transfer the structural patterns from existing entities to new entities. However, the performance of existing methods in inductive KGs are limited by sparsity and implicit transfer. In this paper, we propose VMCL, a Contrastive Learning (CL) framework with graph guided Variational autoencoder on Meta-KGs in the inductive setting. We first propose representation generation to capture the encoded and generated representations of entities, where the generated variations can densify representations with complementary features. Then, we design two CL objectives that work across entities and meta-KGs to simulate the transfer mode. With extensive experiments we demonstrate that our proposed VMCL can significantly outperform previous state-of-the-art baselines.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:buQ7SEKw-1sC", "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts", "authors": ["Xuan-Phi Nguyen", "Sharifah Mahani Aljunied", "Shafiq Joty", "Lidong Bing"], "description": "Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, in low-resource languages, obtaining such hand-picked exemplars can still be challenging, where unsupervised techniques may be necessary. Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs' ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages. When evaluated on zero-shot multilingual summarization, our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is also favored by GPT-4.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:NyGDZy8z5eUC", "title": "Unlocking Temporal Question Answering for Large Language Models Using Code Execution", "authors": ["Xingxuan Li", "Liying Cheng", "Qingyu Tan", "Hwee Tou Ng", "Shafiq Joty", "Lidong Bing"], "description": "Large language models (LLMs) have made significant progress in natural language processing (NLP), and are utilized extensively in various applications. Recent works, such as chain-of-thought (CoT), have shown that intermediate reasoning steps can improve the performance of LLMs for complex reasoning tasks, such as math problems and symbolic question-answering tasks. However, we notice the challenge that LLMs face when it comes to temporal reasoning. Our preliminary experiments show that generating intermediate reasoning steps does not always boost the performance of complex temporal question-answering tasks. Therefore, we propose a novel framework that combines the extraction capability of LLMs and the logical reasoning capability of a Python solver to tackle this issue. Extensive experiments and analysis demonstrate the effectiveness of our framework in handling intricate time-bound reasoning tasks.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:F1b5ZUV5XREC", "title": "Explaining Language Models' Predictions with High-Impact Concepts", "authors": ["Ruochen Zhao", "Shafiq Joty", "Yongjie Wang", "Tan Wang"], "description": "The emergence of large-scale pretrained language models has posed unprecedented challenges in deriving explanations of why the model has made some predictions. Stemmed from the compositional nature of languages, spurious correlations have further undermined the trustworthiness of NLP systems, leading to unreliable model explanations that are merely correlated with the output predictions. To encourage fairness and transparency, there exists an urgent demand for reliable explanations that allow users to consistently understand the model's behavior. In this work, we propose a complete framework for extending concept-based interpretability methods to NLP. Specifically, we propose a post-hoc interpretability method for extracting predictive high-level features (concepts) from the pretrained model's hidden layer activations. We optimize for features whose existence causes the output predictions to change substantially, \\ie generates a high impact. Moreover, we devise several evaluation metrics that can be universally applied. Extensive experiments on real and synthetic tasks demonstrate that our method achieves superior results on {predictive impact}, usability, and faithfulness compared to the baselines.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:Ak0FvsSvgGUC", "title": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning", "authors": ["Lifu Tu", "Jin Qu", "Semih Yavuz", "Shafiq Joty", "Wenhao Liu", "Caiming Xiong", "Yingbo Zhou"], "description": "Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent classification. Our results demonstrate the strong and efficient modeling ability of NLI-based classifiers and the large cross-lingual transfer improvements achieved by our aligned prompts, particularly in few-shot settings.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:4vMrXwiscB8C", "title": "Content-filtering AI systems\u2013limitations, challenges and regulatory approaches", "authors": ["Althaf Marsoof", "Andr\u00e9s Luco", "Harry Tan", "Shafiq Joty"], "description": "Online service providers, and even governments, have increasingly relied on Artificial Intelligence (\u2018AI\u2019) to regulate content on the internet. In some jurisdictions, the law has incentivised, if not obligated, service providers to adopt measures to detect, track, and remove objectionable content such as terrorist propaganda. Consequently, service providers are being pushed to use AI to moderate online content. However, content-filtering AI systems are subject to limitations that affect their accuracy and transparency. These limitations open the possibility for legitimate content to be removed and objectionable content to remain online. Such an outcome could endanger human well-being and the exercise of our human rights. In view of these challenges, we argue that the design and use of content-filtering AI systems should be regulated. AI ethics principles such as transparency, explainability, fairness, and human-centricity\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:HeT0ZceujKMC", "title": "Alleviating Sparsity of Open Knowledge Graphs with Ternary Contrastive Learning", "authors": ["Qian Li", "Shafiq Joty", "Daling Wang", "Shi Feng", "Yifei Zhang"], "description": "Sparsity of formal knowledge and roughness of non-ontological construction make sparsity problem particularly prominent in Open Knowledge Graphs (OpenKGs). Due to sparse links, learning effective representation for few-shot entities becomes difficult. We hypothesize that by introducing negative samples, a contrastive learning (CL) formulation could be beneficial in such scenarios. However, existing CL methods model KG triplets as binary objects of entities ignoring the relation-guided ternary propagation patterns and they are too generic, i.e., they ignore zero-shot, few-shot and synonymity problems that appear in OpenKGs. To address this, we propose TernaryCL, a CL framework based on ternary propagation patterns among head, relation and tail. TernaryCL designs Contrastive Entity and Contrastive Relation to mine ternary discriminative features with both negative entities and relations, introduces Contrastive Self to help zero- and few-shot entities learn discriminative features, Contrastive Synonym to model synonymous entities, and Contrastive Fusion to aggregate graph features from multiple paths. Extensive experiments on benchmarks demonstrate the superiority of TernaryCL over state-of-the-art models.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:OP4eGU-M3BUC", "title": "Learning Label Modular Prompts for Text Classification in the Wild", "authors": ["Hailin Chen", "Amrita Saha", "Shafiq Joty", "Steven CH Hoi"], "description": "Machine learning models usually assume i.i.d data during training and testing, but data and tasks in real world often change over time. To emulate the transient nature of real world, we propose a challenging but practical task: text classification in-the-wild, which introduces different non-stationary training/testing stages. Decomposing a complex task into modular components can enable robust generalisation under such non-stationary environment. However, current modular approaches in NLP do not take advantage of recent advances in parameter efficient tuning of pretrained language models. To close this gap, we propose MODULARPROMPT, a label-modular prompt tuning framework for text classification tasks. In MODULARPROMPT, the input prompt consists of a sequence of soft label prompts, each encoding modular knowledge related to the corresponding class label. In two of most formidable settings, MODULARPROMPT outperforms relevant baselines by a large margin demonstrating strong generalisation ability. We also conduct comprehensive analysis to validate whether the learned prompts satisfy properties of a modular representation.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:yqoGN6RLRZoC", "title": "BotSIM: An End-to-End Bot Simulation Framework for Commercial Task-Oriented Dialog Systems", "authors": ["Guangsen Wang", "Samson Tan", "Shafiq Joty", "Gang Wu", "Jimmy Au", "Steven Hoi"], "description": "We present BotSIM, a data-efficient end-to-end Bot SIMulation toolkit for commercial text-based task-oriented dialog (TOD) systems. BotSIM consists of three major components: 1) a Generator that can infer semantic-level dialog acts and entities from bot definitions and generate user queries via model-based paraphrasing; 2) an agenda-based dialog user Simulator (ABUS) to simulate conversations with the dialog agents; 3) a Remediator to analyze the simulated conversations, visualize the bot health reports and provide actionable remediation suggestions for bot troubleshooting and improvement. We demonstrate BotSIM's effectiveness in end-to-end evaluation, remediation and multi-intent dialog generation via case studies on two commercial bot platforms. BotSIM's \"generation-simulation-remediation\" paradigm accelerates the end-to-end bot evaluation and iteration process by: 1) reducing manual test cases creation efforts; 2) enabling a holistic gauge of the bot in terms of NLU and end-to-end performance via extensive dialog simulation; 3) improving the bot troubleshooting process with actionable suggestions. A demo of our system can be found at https://tinyurl.com/mryu74cd and a demo video at https://youtu.be/qLi5iSoly30.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:ipzZ9siozwsC", "title": "Universal Speech Transformer.", "authors": ["Yingzhu Zhao", "Chongjia Ni", "Cheung-Chi Leung", "Shafiq R Joty", "Eng Siong Chng", "Bin Ma"], "description": "Transformer model has made great progress in speech recognition. However, compared with models with iterative computation, transformer model has fixed encoder and decoder depth, thus losing the recurrent inductive bias. Besides, finding the optimal number of layers involves trial-and-error attempts. In this paper, the universal speech transformer is proposed, which to the best of our knowledge, is the first work to use universal transformer for speech recognition. It generalizes the speech transformer with dynamic numbers of encoder/decoder layers, which can relieve the burden of tuning depth related hyperparameters. Universal transformer adds the depth and positional embeddings repeatedly for each layer, which dilutes the acoustic information carried by hidden representation, and it also performs a partial update of hidden vectors between layers, which is less efficient especially on the very deep models. For better use of universal transformer, we modify its processing framework by removing the depth embedding and only adding the positional embedding once at transformer encoder frontend. Furthermore, to update the hidden vectors efficiently, especially on the very deep models, we adopt a full update. Experiments on LibriSpeech, Switchboard and AISHELL-1 datasets show that our model outperforms a baseline by 3.88%-13.7%, and surpasses other model with less computation cost.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:edDO8Oi4QzsC", "title": "Resurrecting Submodularity for Neural Text Generation", "authors": ["Simeng Han", "Xiang Lin", "Shafiq Joty"], "description": "Submodularity is desirable for a variety of objectives in content selection where the current neural encoder-decoder framework is inadequate. However, it has so far not been explored in the neural encoder-decoder system for text generation. In this work, we define diminishing attentions with submodular functions and in turn, prove the submodularity of the effective neural coverage. The greedy algorithm approximating the solution to the submodular maximization problem is not suited to attention score optimization in auto-regressive generation. Therefore instead of following how submodular function has been widely used, we propose a simplified yet principled solution. The resulting attention module offers an architecturally simple and empirically effective method to improve the coverage of neural text generation. We run experiments on three directed text generation tasks with different levels of recovering rate, across two modalities, three different neural model architectures and two training strategy variations. The results and analyses demonstrate that our method generalizes well across these settings, produces texts of good quality and outperforms state-of-the-art baselines.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:kzcrU_BdoSEC", "title": "Addressing Community Question Answering in English and Arabic", "authors": ["Giovanni Da San Martino", "Alberto Barr\u00f3n-Cede\u00f1o", "Salvatore Romeo", "Alessandro Moschitti", "Shafiq Joty", "Fahad A Al Obaidli", "Kateryna Tymoshenko", "Antonio Uva", "Daniele Bonadiman"], "description": "This paper studies the impact of different types of features applied to learning to re-rank questions in community Question Answering. We tested our models on two datasets released in SemEval-2016 Task 3 on \u201cCommunity Question Answering\u201d. Task 3 targeted real-life Web fora both in English and Arabic. Our models include bag-of-words features (BoW), syntactic tree kernels (TKs), rank features, embeddings, and machine translation evaluation features. To the best of our knowledge, structural kernels have barely been applied to the question reranking task, where they have to model paraphrase relations. In the case of the English question re-ranking task, we compare our learning to rank (L2R) algorithms against a strong baseline given by the Google-generated ranking (GR). The results show that (i) the shallow structures used in our TKs are robust enough to noisy data and (ii) improving GR is possible, but effective BoW features and TKs along with an accurate model of GR features in the used L2R algorithm are required. In the case of the Arabic question re-ranking task, for the first time we applied tree kernels on syntactic trees of Arabic sentences. Our approaches to both tasks obtained the second best results on SemEval-2016 subtasks B on English and D on Arabic.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:KlAtU1dfN6UC", "title": "Supervised Approaches to Complex Question Answering", "authors": ["Yllias Chali", "Sadid A Hasan", "Shafiq R Joty"], "description": "Unlike simple questions, complex questions cannot be answered easily as they often require inferencing and synthesizing information from multiple documents. Hence, this task is accomplished by the query-focused multi-document summarization systems. In this paper, we consider the problem definition given at the DUC-2007 main task and experiment with different supervised learning techniques to confront the complex question answering problem. As representative supervised methods, we use Support Vector Machines (SVM), Hidden Markov Models (HMM), Conditional Random Fields (CRF), and MaxEnt (Maximum Entropy). We also experiment with an ensemble based approach combining the individual decisions of these classifiers. We use DUC-2006 data to train our systems, whereas 25 topics of DUC-2007 data set are used as test data. The evaluation results reveal the effectiveness of these approaches in the problem domain.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:_Qo2XoVZTnwC", "title": "UofL at TAC 2008 Update Summarization and Question Answering.", "authors": ["Yllias Chali", "Sadid A Hasan", "Shafiq R Joty"], "description": "In this paper, we describe our update summarization and question answering (QA) systems participated in the TAC 2008 competition. We submitted three runs for the update summarization task using unsupervised and supervised techniques. On the other hand, the question answering system is built on our previous system participated in TREC 2007 QA track with different approach followed for the squishy list type questions. We submitted a single run for the QA task. This paper also presents the preliminary evaluation results of our systems.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:YohjEiUPhakC", "title": "Automatic Annotation Techniques for Supervised and Semi-supervised Query-focused Summarization", "authors": ["Shafiq R Joty"], "description": "In this paper, we study one semi-supervised and several supervised methods for extractive query-focused multi-document summarization. Traditional approaches to multidocument summarization are either unsupervised or supervised. The unsupervised approaches use heuristic rules to select the most important sentences, which are hard to generalize. On the other hand, huge amount of annotated data is a prerequisite for supervised training, the availability of which is very rare for a very new research problem like query-focused summarization. However, the availability of the abstract summaries from different evaluation framework allows us to experiment with the semi-supervised approach and the sentence alignment methods to annotate the document sentences automatically. We employ five different automatic annotation techniques to build the extracts from the human abstracts. We use TF* IDF1 based cosine similarity, Extended String Subsequence Kernel (ESSK), Basic Element (BE) overlap, Syntactic Similarity, and Semantic Similarity measures as the annotation methods. Based on these annotations, we experiment with: a) two supervised multi-class classifiers; Support Vector Machines (SVM) and Logistic Regression (LR), b) three regression models; SVM, Bagging and Gaussian Processes (GP), and c) one sequence labeler; Conditional Random Fields (CRF). Our initial results of SVM classifier based on a very small subset of DUC-2006 and DUC-2007 data show the effectiveness of our approaches.", "publication_year": null, "citations_by_year": {"year": ["unknown"], "num_citations": [1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:BzfGm06jWhQC", "title": "Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation", "authors": ["Chengwei Qin", "Shafiq Joty", "Chen Chen"], "description": "Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the learning of the current task and replayed tasks. With extensive experiments, we demonstrate that DMEA can consistently outperform existing methods in different LSG settings.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=100&pagesize=100&citation_for_view=hR249csAAAAJ:43bX7VzcjpAC", "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models", "authors": ["Ansong Ni", "Pengcheng Yin", "Yilun Zhao", "Martin Riddell", "Troy Feng", "Rui Shen", "Stephen Yin", "Ye Liu", "Semih Yavuz", "Caiming Xiong", "Shafiq Joty", "Yingbo Zhou", "Dragomir Radev", "Arman Cohan"], "description": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluations of the output programs. This enables us to identify and analyze the typical failure modes across various tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We also release the evaluation framework and all model outputs, hoping to lay the groundwork for further future research in this domain.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:ubry08Y2EpUC", "title": "RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair", "authors": ["Weishi Wang", "Yue Wang", "Shafiq Joty", "Steven CH Hoi"], "description": "Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:zCSUwVk65WsC", "title": "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue", "authors": ["David Schlangen", "Svetlana Stoyanchev", "Shafiq Joty", "Ond\u0159ej Du\u0161ek", "Casey Kennington", "Malihe Alikhani"], "description": "We are glad to pen the first few words for the proceedings of SIGDIAL 2023, the 24rd Annual Meeting of the Special Interest Group on Discourse and Dialogue. The SIGDIAL conference is a premier publication venue for research in discourse and dialogue. This year the conference is organized together with the conference on International Natural Language Generation (INLG). The format is hybrid with most participants and presenters in-person. Zoom was used for remote presentations and Discord was used as a communication platform for both remote and local participants.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:lgwcVrK6X84C", "title": "Exploring the Integration Strategies of Retriever and Large Language Models", "authors": ["Ye Liu", "Semih Yavuz", "Rui Meng", "Meghana Moorthy", "Shafiq Joty", "Caiming Xiong", "Yingbo Zhou"], "description": "The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating \"unknown\" outputs, even when the correct document is among the top-k retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, we provide insightful observations on how to effectively leverage retrieved passages to enhance the answer generation capability of LLMs.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:ZzlSgRqYykMC", "title": "Unsupervised Summarization Re-ranking", "authors": ["Mathieu Ravaut", "Shafiq Joty", "Nancy Chen"], "description": "With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models whereas only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the pre-trained unsupervised PEGASUS by 4.37% to 7.27% relative mean ROUGE across four widely-adopted summarization benchmarks, and achieves relative gains of 7.51% (up to 23.73%) averaged over 30 transfer setups.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:r_AWSJRzSzQC", "title": "Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations", "authors": ["Linlin Liu", "Xingxuan Li", "Megh Thakkar", "Xin Li", "Lidong Bing", "Shafiq Joty", "Luo Si"], "description": "Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:48xauSegjOkC", "title": "PromptSum: Parameter-Efficient Controllable Abstractive Summarization", "authors": ["Mathieu Ravaut", "Hailin Chen", "Ruochen Zhao", "Chengwei Qin", "Shafiq Joty", "Nancy Chen"], "description": "Prompt tuning (PT), a parameter-efficient technique that only tunes the additional prompt embeddings while keeping the backbone pre-trained language model (PLM) frozen, has shown promising results in language understanding tasks, especially in low-resource scenarios. However, effective prompt design methods suitable for generation tasks such as summarization are still lacking. At the same time, summarization guided through instructions (discrete prompts) can achieve a desirable double objective of high quality and controllability in summary generation. Towards a goal of strong summarization performance under the triple conditions of parameter-efficiency, data-efficiency, and controllability, we introduce PromptSum, a method combining PT with a multi-task objective and discrete entity prompts for abstractive summarization. Our model achieves competitive ROUGE results on popular abstractive summarization benchmarks coupled with a strong level of controllability through entities, all while only tuning several orders of magnitude less parameters.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:CaZNVDsoPx4C", "title": "Systems and methods for an end-to-end evaluation and testing framework for task-oriented dialog systems", "authors": null, "description": "Embodiments provide a software framework for evaluating and troubleshooting real-world task-oriented bot systems. Specifically, the evaluation framework includes a generator that infers dialog acts and entities from bot definitions and generates test cases for the system via model-based paraphrasing. The framework may also include a simulator for task-oriented dialog user simulation that supports both regression testing and end-to-end evaluation. The framework may also include a remediator to analyze and visualize the simulation results, remedy some of the identified issues, and provide actionable suggestions for improving the task-oriented dialog system.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:Bg7qf7VwUHIC", "title": "A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets", "authors": ["Iva Bojic", "Josef Halim", "Verena Suharman", "Sreeja Tar", "Qi Chwen Ong", "Duy Phung", "Mathieu Ravaut", "Shafiq Joty", "Josip Car"], "description": "Low-quality data can cause downstream problems in high-stakes applications. Data-centric approach emphasizes on improving dataset quality to enhance model performance. High-quality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation. Thus, it is vital to ensure high-quality domain-specific training data. In this paper, we propose a framework for enhancing the data quality of original datasets. We applied the proposed framework to four biomedical datasets and showed relative improvement of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when using back translation to enhance the original dataset quality.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:e_rmSamDkqQC", "title": "Systems and methods for explicit memory tracker with coarse-to-fine reasoning in conversational machine reading", "authors": null, "description": "Embodiments described herein provide systems and methods for an Explicit Memory Tracker (EMT) that tracks each rule sentence to perform decision making and to generate follow-up clarifying questions. Specifically, the EMT first segments the regulation text into several rule sentences and allocates the segmented rule sentences into memory modules, and then feeds information regarding the user scenario and dialogue history into the EMT sequentially to update each memory module separately. At each dialogue turn, the EMT makes a decision among based on current memory status of the memory modules whether further clarification is needed to come up with an answer to a user question. The EMT determines that further clarification is needed by identifying an underspecified rule sentence span by modulating token-level span distributions with sentence-level selection scores. The EMT extracts the\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:TIZ-Mc8IlK0C", "title": "Systems and methods for a transformer network with tree-based attention for natural language processing", "authors": null, "description": "Embodiments described herein provide an attention-based tree encoding mechanism. Specifically, the attention layer receives as input the pre-parsed constituency tree of a sentence and the lower-layer representations of all nodes. The attention layer then performs upward accumulation to encode the tree structure from leaves to the root in a bottom-up fashion. Afterwards, weighted aggregation is used to compute the final representations of non-terminal nodes.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:nVrZBo8bIpAC", "title": "Dynamic Scheduled Sampling with Imitation Loss for Neural Text Generation", "authors": ["Xiang Lin", "Prathyusha Jwalapuram", "Shafiq Joty"], "description": "State-of-the-art neural text generation models are typically trained to maximize the likelihood of each token in the ground-truth sequence conditioned on the previous target tokens. However, during inference, the model needs to make a prediction conditioned on the tokens generated by itself. This train-test discrepancy is referred to as exposure bias. Scheduled sampling is a curriculum learning strategy that gradually exposes the model to its own predictions during training to mitigate this bias. Most of the proposed approaches design a scheduler based on training steps, which generally requires careful tuning depending on the training setup. In this work, we introduce Dynamic Scheduled Sampling with Imitation Loss (DySI), which maintains the schedule based solely on the training time accuracy, while enhancing the curriculum learning by introducing an imitation loss, which attempts to make the behavior of the decoder indistinguishable from the behavior of a teacher-forced decoder. DySI is universally applicable across training setups with minimal tuning. Extensive experiments and analysis show that DySI not only achieves notable improvements on standard machine translation benchmarks, but also significantly improves the robustness of other text generation models.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:8xutWZnSdmoC", "title": "Systems and methods for code understanding and generation", "authors": null, "description": "Embodiments described herein a code generation and understanding model that builds on a Transformer-based encoder-decoder framework. The code generation and understanding model is configured to derive generic representations for programming language (PL) and natural language (NL) in code domain via pre-training on unlabeled code corpus, and then to benefit many code-related downstream tasks with fine-tuning. Apart from the denoising sequence-to-sequence objectives widely adopted for pre-training on natural language, identifier tagging and prediction pre-training objective is adopted to enable the model to better leverage the crucial token type information from PL, which specifically are the identifiers assigned by developers.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:LhH-TYMQEocC", "title": "Systems and methods for semantic code search", "authors": null, "description": "Embodiments described herein provides a contrastive learning framework that leverages hard negative examples, that are mined globally from the entire training corpus for a given query to improve the quality of code and natural language representations. Specifically, similar examples from the training corpus are extracted and used as hard negatives in an online manner during training while keeping the minibatch construction random.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:GFxP56DSvIMC", "title": "PromptSum: Planning with Mixed Prompts for Parameter-Efficient Controllable Abstractive Summarization", "authors": ["Mathieu Ravaut", "Hailin Chen", "Ruochen Zhao", "Chengwei Qin", "Shafiq Joty", "Nancy F Chen"], "description": "Prompt tuning (PT), a technique that only tunes the additional prompt embeddings while keeping the backbone pretrained language model frozen, has shown promising results in language understanding tasks, especially in low-resource scenarios. However, there lacks better prompt design methods for generation tasks such as summarization. At the same time, summarization guided through instructions (discrete prompts) can achieve a desirable double objective of higher quality and controllability in summary generation. Towards a triple goal of data-efficiency, parameter-efficiency and controllability, we introduce PromptSum, a method combining PT with a multi-task objective and discrete entity prompts for abstractive summarization. Our model achieves state-of-the-art results on several popular few-shot benchmarks as well as a strong level of controllability through entities, all while only tuning several orders of magnitude less parameters.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:Ehil0879vHcC", "title": "Randomized Smoothing with Masked Inference for Adversarially Robust NLP Systems", "authors": ["Han Cheol Moon", "Shafiq Joty", "Ruochen Zhao", "Megh Thakkar", "Xu Chi"], "description": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://anonymous.4open.science/r/RSMI.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:nZcligLrVowC", "title": "Towards Causal Concepts for Explaining Language Models", "authors": ["Ruochen Zhao", "Shafiq Joty", "Yongjie Wang", "Prathyusha Jwalapuram"], "description": "The emergence of large-scale pretrained language models has posed unprecedented challenges in deriving explanations of why the model has made some predictions. Stemmed from the compositional nature of languages, spurious correlations have further undermined the trustworthiness of NLP systems. Thus, there exists an urgent demand for causal explanations to encourage fairness and transparency. To derive more causal, usable, and faithful explanations, we propose a complete framework for interpreting language models by deriving causal concepts. Specifically, we propose a post-hoc method that derives both high-level concepts and surface-level local explanations from hidden layer activations. To ensure causality, we optimize for a causal loss that maximizes the Average Treatment Effect (ATE), where we intervene on the concept-level as an innovative substitute to the traditional counterfactual interventions. Moreover, we devise several causality evaluation metrics for explanations that can be universally applied. Extensive experiments on real and synthetic tasks demonstrate that our method achieves superior results on causality, usability, and faithfulness compared to the baselines. Our codebase is available at \\url{https://anonymous.4open.science/r/CausalConcept}.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:dBIO0h50nwkC", "title": "Systems and methods for code-mixing adversarial training", "authors": null, "description": "Embodiments described herein provide adversarial attacks targeting the cross-lingual generalization ability of massive multilingual representations, demonstrating their effectiveness on multilingual models for natural language inference and question answering. An efficient adversarial training scheme can thus be implemented with the adversarial attacks, which takes the same number of steps as standard supervised training and show that it encourages language-invariance in representations, thereby improving both clean and robust accuracy.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:OTTXONDVkokC", "title": "Systems and methods for numerical reasoning by a partially supervised numeric reasoning module network", "authors": null, "description": "Embodiments described herein provide systems and methods for a partially supervised training model for questioning answering tasks. Specifically, the partially supervised training model may include two modules\u2014a query parsing module and a program execution module. The query parsing module parses queries into a grogram, and the program execution module execute the program to reach an answer through explicit reasoning and partial supervision. In this way, the partially supervised training model can be trained with answers as supervision, obviating the need for supervision by gold program operations and gold query-span attention at each step of the program.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:S16KYo8Pm5AC", "title": "Systems and methods for generating natural language processing training samples with inflectional perturbations", "authors": null, "description": "Embodiments described herein provide systems and methods for generating an adversarial sample with inflectional perturbations for training a natural language processing (NLP) system. A natural language sentence is received at an inflection perturbation module. Tokens are generated from the natural language sentence. For each token that has a part of speech that is a verb, adjective, or an adverb, an inflected form is determined. An adversarial sample of the natural language sentence is generated by detokenizing inflected forms of the tokens. The NLP system is trained using the adversarial sample.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:umqufdRvDiIC", "title": "Using clinical notes for icu management", "authors": null, "description": "A method can be implemented at one or more computing machines. The method can include receiving, using a server, time-series data corresponding to monitoring instrumentation in a medical care facility. The time-series data corresponds to a selected care recipient. The time-series data is stored in one or more data storage units. The time-series data includes data correlated with a plurality of regular time intervals. The method includes receiving, using a server, aperiodic data corresponding to clinical notes collected in the medical care facility and corresponding to the selected care recipient. The aperiodic data is stored in one or more data storage units. The aperiodic data includes a time stamp. The method includes generating, using a deep neural network and the time-series data and using a convolutional neural network (CNN) and the aperiodic data, a plurality of computer-generated data corresponding to\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:b1wdh0AR-JQC", "title": "Addressing the Vulnerability of NMT in Input Perturbations", "authors": ["Weiwen Xu", "Ai Ti Aw", "Yang Ding", "Kui Wu", "Shafiq Joty"], "description": "Neural Machine Translation (NMT) has achieved significant breakthrough in performance but is known to suffer vulnerability to input perturbations. As real input noise is difficult to predict during training, robustness is a big issue for system deployment. In this paper, we improve the robustness of NMT models by reducing the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach. CER trains the model to resist noise in two steps: (1) perturbation step that breaks the naturalness of input sequence with made-up words; (2) reconstruction step that defends the noise propagation by generating better and more robust contextual representation. Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks demonstrate robustness improvement on both news and social media text. Further fine-tuning experiments on social media text show our approach can converge at a higher position and provide a better adaptation.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:KUbvn5osdkgC", "title": "Computer architecture for identifying sleep stages", "authors": null, "description": "A computing machine receives sensor data representing airflow or air pressure. The computing machine determines, using an artificial neural network, a current sleep stage corresponding to the sensor data. The current sleep stage is one of: wake, rapid eye movement (REM), light sleep, and deep sleep. The artificial neural network comprises a convolutional neural network (CNN), a recurrent neural network (RNN), and a conditional random field (CRF). The computing machine provides an output representing the current sleep stage.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:EYYDruWGBe4C", "title": "Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing", "authors": ["Nafise Sadat Moosavi", "Angela Fan", "Vered Shwartz", "Goran Glava\u0161", "Shafiq Joty", "Alex Wang", "Thomas Wolf"], "description": "It is our great pleasure to welcome you to the first edition of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing.The Natural Language Processing community has, in recent years, demonstrated a notable focus on improving scores on standard benchmarks and taking the lead on community-wide leaderboards such as (Super) GLUE, SentEval or XTREME. While this led to improvements in benchmark performance of (predominantly neural) models, it also resulted in a worrysome increase in model complexity and the amount of computational resources required for training and using the current state-of-the-art models. Moreover, recent research efforts often fail to identify sources of performance gains in models and to justify model complexity beyond benchmark performance.", "publication_year": 2020, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:5qfkUJPXOUwC", "title": "DiP Benchmark Tests: Evaluation Benchmarks for Discourse Phenomena in MT", "authors": ["Prathyusha Jwalapuram", "Barbara Rychalska", "Shafiq Joty", "Dominika Basaj"], "description": "Despite increasing instances of machine translation (MT) systems including extrasentential context information, the evidence for translation quality improvement is sparse, especially for discourse phenomena. Popular metrics like BLEU are not expressive or sensitive enough to capture quality improvements or drops that are minor in size but significant in perception. We introduce the first of their kind MT benchmark testsets that aim to track and hail improvements across four main discourse phenomena: anaphora, lexical consistency, coherence and readability, and discourse connective translation. We also introduce evaluation methods for these tasks, and evaluate several competitive baseline MT systems on the curated datasets. Surprisingly, we find that the complex context-aware models that we test do not improve discourse-related translations consistently across languages and phenomena. Our evaluation benchmark is available as a leaderboard at <dipbenchmark1.github.io>.", "publication_year": 2020, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:ClCfbGk0d_YC", "title": "Enhancing Attention with Explicit Phrasal Alignments", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Thanh-Tung Nguyen"], "description": "The attention mechanism is an indispensable component of any state-of-the-art neural machine translation system. However, existing attention methods are often token-based and ignore the importance of phrasal alignments, which are the backbone of phrase-based statistical machine translation. We propose a novel phrase-based attention method to model n-grams of tokens as the basic attention entities, and design multi-headed phrasal attentions within the Transformer architecture to perform token-to-token and token-to-phrase mappings. Our approach yields improvements in English-German, English-Russian and English-French translation tasks on the standard WMT'14 test set. Furthermore, our phrasal attention method shows improvements on the one-billion-word language modeling benchmark.", "publication_year": 2019, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:HIFyuExEbWQC", "title": "Discourse analysis of asynchronous conversations", "authors": ["Shafiq Rayhan Joty"], "description": "A well-written text is not merely a sequence of independent and isolated sentences, but instead a sequence of structured and related sentences. It addresses a particular topic, often covering multiple subtopics, and is organized in a coherent way that enables the reader to process the information. Discourse analysis seeks to uncover such underlying structures, which can support many applications including text summarization and information extraction. This thesis focuses on building novel computational models of different discourse analysis tasks in asynchronous conversations; ie, conversations where participants communicate with each other at different times (eg, emails, blogs). Effective processing of these conversations can be of great strategic value for both organizations and individuals. We propose novel computational models for topic segmentation and labeling, rhetorical parsing and dialog act recognition in asynchronous conversation. Our approaches rely on two related computational methodologies: graph theory and probabilistic graphical models. The topic segmentation and labeling models find the high-level discourse structure; ie, the global topical structure of an asynchronous conversation. Our graph-based approach extends state-of-the-art methods by integrating a fine-grained conversational structure with other conversational features. On the other hand, the rhetorical parser captures the coherence structure, a finer discourse structure, by identifying coherence relations between the discourse units within each comment of the conversation. Our parser applies an optimal parsing algorithm to probabilities inferred from a\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:1lhNe0rCu4AC", "title": "Hierarchical Evaluation Framework: Best Practices for Human Evaluation", "authors": ["Iva Bojic", "Jessica Chen", "Si Yuan Chang", "Qi Chwen Ong", "Shafiq Joty", "Josip Car"], "description": "Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system\u2019s performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate both components rather than solely focusing on outputs. In future work, we will investigate the potential time-saving benefits of our proposed framework for evaluators assessing NLP systems.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=hR249csAAAAJ&cstart=200&pagesize=100&citation_for_view=hR249csAAAAJ:LgRImbQfgY4C", "title": "Appendix for Data Diversification: A Simple Strategy For Neural Machine Translation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Wu Kui", "Ai Ti Aw"], "description": "We continue to differentiate our method from other existing works. First, Shen et al.[12] also seek to generate diverse set of translations using mixture of experts, not to improve translation quality like ours. In this method, multiple experts are tied into a single NMT model to be trained to generate diverse translations through EM optimization. It does not employ data augmentation, neither forward nor backward translations. Our method does not train multiple peer models with EM training either.Second, iterative back-translation [4] employs back-translation to augment the data in multiple rounds. In each round, a forward (or backward) model takes turn to play the \u201cback-translation\u201d role to train the backward (or forward) model. The role is switched in the next round. In our approach, both directions are involved in each round and multiple models to the achieve ensembling effect.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}}]