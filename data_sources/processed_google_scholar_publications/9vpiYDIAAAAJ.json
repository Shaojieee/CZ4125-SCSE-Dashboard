[{"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:2osOgNQ5qMEC", "title": "Cloudradar: A real-time side-channel attack detection system in clouds", "authors": ["Tianwei Zhang", "Yinqian Zhang", "Ruby B Lee"], "description": "We present CloudRadar, a system to detect, and hence mitigate, cache-based side-channel attacks in multi-tenant cloud systems. CloudRadar operates by correlating two events: first, it exploits signature-based detection to identify when the protected virtual machine (VM) executes a cryptographic application; at the same time, it uses anomaly-based detection techniques to monitor the co-located VMs to identify abnormal cache behaviors that are typical during cache-based side-channel attacks. We show that correlation in the occurrence of these two events offer strong evidence of side-channel attacks. Compared to other work on side-channel defenses, CloudRadar has the following advantages: first, CloudRadar focuses on the root causes of cache-based side-channel attacks and hence is hard to evade using metamorphic attack code, while maintaining a low false positive rate. Second, CloudRadar is\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 15, 27, 47, 57, 41, 42, 32, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:Zph67rFs4hoC", "title": "Model inversion attacks against collaborative inference", "authors": ["Zecheng He", "Tianwei Zhang", "Ruby B Lee"], "description": "The prevalence of deep learning has drawn attention to the privacy protection of sensitive data. Various privacy threats have been presented, where an adversary can steal model owners' private data. Meanwhile, countermeasures have also been introduced to achieve privacy-preserving deep learning. However, most studies only focused on data privacy during training, and ignored privacy during inference. In this paper, we devise a new set of attacks to compromise the inference data privacy in collaborative deep learning systems. Specifically, when a deep neural network and the corresponding inference task are split and distributed to different participants, one malicious participant can accurately recover an arbitrary input fed into this system, even if he has no access to other participants' data or computations, or to prediction APIs to query this system. We evaluate our attacks under different settings, models and\u00a0\u2026", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 16, 50, 57, 73, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:IWHjjKOFINEC", "title": "Adversarial attacks against network intrusion detection in IoT systems", "authors": ["Han Qiu", "Tian Dong", "Tianwei Zhang", "Jialiang Lu", "Gerard Memmi", "Meikang Qiu"], "description": "Deep learning (DL) has gained popularity in network intrusion detection, due to its strong capability of recognizing subtle differences between normal and malicious network activities. Although a variety of methods have been designed to leverage DL models for security protection, whether these systems are vulnerable to adversarial examples (AEs) is unknown. In this article, we design a novel adversarial attack against DL-based network intrusion detection systems (NIDSs) in the Internet-of-Things environment, with only black-box accesses to the DL model in such NIDS. We introduce two techniques: 1) model extraction is adopted to replicate the black-box model with a small amount of training data and 2) a saliency map is then used to disclose the impact of each packet attribute on the detection results, and the most critical features. This enables us to efficiently generate AEs using conventional methods. With\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 30, 110, 51, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:YsMSGLbcyi4C", "title": "Machine learning based DDoS attack detection from source side in cloud", "authors": ["Zecheng He", "Tianwei Zhang", "Ruby B Lee"], "description": "Denial of service (DOS) attacks are a serious threat to network security. These attacks are often sourced from virtual machines in the cloud, rather than from the attacker's own machine, to achieve anonymity and higher network bandwidth. Past research focused on analyzing traffic on the destination (victim's) side with predefined thresholds. These approaches have significant disadvantages. They are only passive defenses after the attack, they cannot use the outbound statistical features of attacks, and it is hard to trace back to the attacker with these approaches. In this paper, we propose a DOS attack detection system on the source side in the cloud, based on machine learning techniques. This system leverages statistical information from both the cloud server's hypervisor and the virtual machines, to prevent network packages from being sent out to the outside network. We evaluate nine machine learning algorithms\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 10, 20, 24, 50, 37, 31, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:ZeXyd9-uunAC", "title": "Deepsweep: An evaluation framework for mitigating DNN backdoor attacks using data augmentation", "authors": ["Han Qiu", "Yi Zeng", "Shangwei Guo", "Tianwei Zhang", "Meikang Qiu", "Bhavani Thuraisingham"], "description": "Public resources and services (e.g., datasets, training platforms, pre-trained models) have been widely adopted to ease the development of Deep Learning-based applications. However, if the third-party providers are untrusted, they can inject poisoned samples into the datasets or embed backdoors in those models. Such an integrity breach can cause severe consequences, especially in safety- and security-critical applications. Various backdoor attack techniques have been proposed for higher effectiveness and stealthiness. Unfortunately, existing defense solutions are not practical to thwart those attacks in a comprehensive way. In this paper, we investigate the effectiveness of data augmentation techniques in mitigating backdoor attacks and enhancing DL models' robustness. An evaluation framework is introduced to achieve this goal. Specifically, we consider a unified defense solution, which (1) adopts a data\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [4, 49, 65, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:YOwf2qJgpHMC", "title": "Stealthy and efficient adversarial attacks against deep reinforcement learning", "authors": ["Jianwen Sun", "Tianwei Zhang", "Xiaofei Xie", "Lei Ma", "Yan Zheng", "Kangjie Chen", "Yang Liu"], "description": "Adversarial attacks against conventional Deep Learning (DL) systems and algorithms have been widely studied, and various defenses were proposed. However, the possibility and feasibility of such attacks against Deep Reinforcement Learning (DRL) are less explored. As DRL has achieved great success in various complex tasks, designing effective adversarial attacks is an indispensable prerequisite towards building robust DRL algorithms. In this paper, we introduce two novel adversarial attack techniques to stealthily and efficiently attack the DRL agents. These two techniques enable an adversary to inject adversarial samples in a minimal set of critical moments while causing the most severe damage to the agent. The first technique is the critical point attack: the adversary builds a model to predict the future environmental states and agent's actions, assesses the damage of each possible attack strategy, and selects the optimal one. The second technique is the antagonist attack: the adversary automatically learns a domain-agnostic model to discover the critical moments of attacking the agent in an episode. Experimental results demonstrate the effectiveness of our techniques. Specifically, to successfully attack the DRL agent, our critical point technique only requires 1 (TORCS) or 2 (Atari Pong and Breakout) steps, and the antagonist technique needs fewer than 5 steps (4 Mujoco tasks), which are significant improvements over state-of-the-art methods.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [12, 22, 31, 25, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:8k81kl-MbHgC", "title": "Privacy-preserving machine learning through data obfuscation", "authors": ["Tianwei Zhang", "Zecheng He", "Ruby B Lee"], "description": "As machine learning becomes a practice and commodity, numerous cloud-based services and frameworks are provided to help customers develop and deploy machine learning applications. While it is prevalent to outsource model training and serving tasks in the cloud, it is important to protect the privacy of sensitive samples in the training dataset and prevent information leakage to untrusted third parties. Past work have shown that a malicious machine learning service provider or end user can easily extract critical information about the training samples, from the model parameters or even just model outputs. In this paper, we propose a novel and generic methodology to preserve the privacy of training data in machine learning applications. Specifically we introduce an obfuscate function and apply it to the training data before feeding them to the model training task. This function adds random noise to existing samples, or augments the dataset with new samples. By doing so sensitive information about the properties of individual samples, or statistical properties of a group of samples, is hidden. Meanwhile the model trained from the obfuscated dataset can still achieve high accuracy. With this approach, the customers can safely disclose the data or models to third-party providers or end users without the need to worry about data privacy. Our experiments show that this approach can effective defeat four existing types of machine learning privacy attacks at negligible accuracy cost.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 8, 17, 22, 21, 12, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:kNdYIx-mwKoC", "title": "Sensitive-sample fingerprinting of deep neural networks", "authors": ["Zecheng He", "Tianwei Zhang", "Ruby Lee"], "description": "Numerous cloud-based services are provided to help customers develop and deploy deep learning applications. When a customer deploys a deep learning model in the cloud and serves it to end-users, it is important to be able to verify that the deployed model has not been tampered with. In this paper, we propose a novel and practical methodology to verify the integrity of remote deep learning models, with only black-box access to the target models. Specifically, we define Sensitive-Sample fingerprints, which are a small set of human unnoticeable transformed inputs that make the model outputs sensitive to the model's parameters. Even small model changes can be clearly reflected in the model outputs. Experimental results on different types of model integrity attacks show that we proposed approach is both effective and efficient. It can detect model integrity breaches with high accuracy (> 99.95%) and guaranteed zero false positives on all evaluated attacks. Meanwhile, it only requires up to 103X fewer model inferences, compared with non-sensitive samples.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [5, 7, 15, 22, 22, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:u5HHmVD_uO8C", "title": "New models of cache architectures characterizing information leakage from cache side channels", "authors": ["Tianwei Zhang", "Ruby B Lee"], "description": "Side-channel attacks try to breach confidentiality and retrieve critical secrets through the side channels. Cache memories are a potential source of information leakage through side-channel attacks, many of which have been proposed. Meanwhile, different cache architectures have also been proposed to defend against these attacks. However, there are currently no means for comparing and evaluating the effectiveness of different defense solutions against these attacks. In this paper, we propose a novel method to evaluate a system's vulnerability to side-channel attacks. We establish side-channel leakage models based on the non-interference property. Then we define how the security aspects of a cache architecture can be modeled as a finite-state machine (FSM) with state transitions that cause interference. We use mutual information to quantitatively reveal potential side-channel leakage of the architectures, and\u00a0\u2026", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 7, 10, 6, 9, 2, 4, 10, 10, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:d1gkVwhDpl0C", "title": "CloudMonatt: An architecture for security health monitoring and attestation of virtual machines in cloud computing", "authors": ["Tianwei Zhang", "Ruby B Lee"], "description": "Cloud customers need guarantees regarding the security of their virtual machines (VMs), operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated by the customer not knowing where his VM is executing, and on the semantic gap between what the customer wants to know versus what can be measured in the cloud. We present an architecture for monitoring a VM's security health, with the ability to attest this to the customer in an unforgeable manner. We show a concrete implementation of property-based attestation and a full prototype based on the OpenStack open source cloud software.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 6, 13, 10, 6, 7, 7, 4, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:bFI3QPDXJZMC", "title": "Characterization and prediction of deep learning workloads in large-scale gpu datacenters", "authors": ["Qinghao Hu", "Peng Sun", "Shengen Yan", "Yonggang Wen", "Tianwei Zhang"], "description": "Modern GPU datacenters are critical for delivering Deep Learning (DL) models and services in both the research community and industry. When operating a datacenter, optimization of resource scheduling and management can bring significant financial benefits. Achieving this goal requires a deep understanding of the job features and user behaviors. We present a comprehensive study about the characteristics of DL jobs and resource management. First, we perform a large-scale analysis of real-world job traces from SenseTime. We uncover some interesting conclusions from the perspectives of clusters, jobs and users, which can facilitate the cluster system designs. Second, we introduce a general-purpose framework, which manages resources based on historical data. As case studies, we design (1) a Quasi-Shortest-Service-First scheduling service, which can minimize the cluster-wide average job completion\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [2, 25, 26, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:HoB7MX3m0LUC", "title": "A survey of microarchitectural side-channel vulnerabilities, attacks, and defenses in cryptography", "authors": ["Xiaoxuan Lou", "Tianwei Zhang", "Jun Jiang", "Yinqian Zhang"], "description": "Side-channel attacks have become a severe threat to the confidentiality of computer applications and systems. One popular type of such attacks is the microarchitectural attack, where the adversary exploits the hardware features to break the protection enforced by the operating system and steal the secrets from the program. In this article, we systematize microarchitectural side channels with a focus on attacks and defenses in cryptographic applications. We make three contributions. (1) We survey past research literature to categorize microarchitectural side-channel attacks. Since these are hardware attacks targeting software, we summarize the vulnerable implementations in software, as well as flawed designs in hardware. (2) We identify common strategies to mitigate microarchitectural attacks, from the application, OS, and hardware levels. (3) We conduct a large-scale evaluation on popular cryptographic\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [2, 23, 26, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:QIV2ME_5wuYC", "title": "Attacking and protecting data privacy in edge\u2013cloud collaborative inference systems", "authors": ["Zecheng He", "Tianwei Zhang", "Ruby B Lee"], "description": "Benefiting from the advance of deep learning (DL) technology, Internet-of-Things (IoT) devices and systems are becoming more intelligent and multifunctional. They are expected to run various DL inference tasks with high efficiency and performance. This requirement is challenged by the mismatch between the limited computing capability of edge devices and large-scale deep neural networks. Edge-cloud collaborative systems are then introduced to mitigate this conflict, enabling resource-constrained IoT devices to host arbitrary DL applications. However, the introduction of third-party clouds can bring potential privacy issues to edge computing. In this article, we conduct a systematic study about the opportunities of attacking and protecting the privacy of edge-cloud collaborative systems. Our contributions are twofold: 1) we first devise a set of new attacks for an untrusted cloud to recover arbitrary inputs fed into the\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 9, 20, 24, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:5nxA0vEk-isC", "title": "Icorating: A deep-learning system for scam ico identification", "authors": ["Shuqing Bian", "Zhenpeng Deng", "Fei Li", "Will Monroe", "Peng Shi", "Zijun Sun", "Wei Wu", "Sikuang Wang", "William Yang Wang", "Arianna Yuan", "Tianwei Zhang", "Jiwei Li"], "description": "Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP, NEO) have been rapidly gaining ground in use, value, and understanding among the public, bringing astonishing profits to investors. Unlike other money and banking systems, most digital tokens do not require central authorities. Being decentralized poses significant challenges for credit rating. Most ICOs are currently not subject to government regulations, which makes a reliable credit rating system for ICO projects necessary and urgent. In this paper, we introduce IcoRating, the first learning--based cryptocurrency rating system. We exploit natural-language processing techniques to analyze various aspects of 2,251 digital currencies to date, such as white paper content, founding teams, Github repositories, websites, etc. Supervised learning models are used to correlate the life span and the price change of cryptocurrencies with these features. For the best setting, the proposed system is able to identify scam ICO projects with 0.83 precision. We hope this work will help investors identify scam ICOs and attract more efforts in automatically evaluating and analyzing ICO projects.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [4, 12, 10, 7, 12, 6, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:u-x6o8ySG0sC", "title": "Side channel vulnerability metrics: the promise and the pitfalls", "authors": ["Tianwei Zhang", "Fangfei Liu", "Si Chen", "Ruby B Lee"], "description": "Side-channels enable attackers to break a cipher by exploiting observable information from the cipher program's execution to infer its secret key. While some defenses have been proposed to protect information leakage due to certain side channels, the effectiveness of these defenses have mostly been given only qualitative analysis by their authors. It is desirable to have a general quantitative method and metric to evaluate a system's vulnerability to side-channel attacks. In this paper, we define the features of a good side-channel leakage metric. We review a recently proposed metric called the Side-channel Vulnerability Factor (SVF) and discuss its merits and issues. We suggest the CSV metric, which tries to show how to overcome some of the shortcomings of the SVF metric, without completely changing its character. We use software cache side-channel attacks and defenses as an example to compare the metrics\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 7, 2, 6, 1, 3, 7, 6, 5, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:NhqRSupF_l8C", "title": "Badpre: Task-agnostic backdoor attacks to pre-trained nlp foundation models", "authors": ["Kangjie Chen", "Yuxian Meng", "Xiaofei Sun", "Shangwei Guo", "Tianwei Zhang", "Jiwei Li", "Chun Fan"], "description": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \\Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 12, 32, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:zYLM7Y9cAGgC", "title": "Dos attacks on your memory in cloud", "authors": ["Tianwei Zhang", "Yinqian Zhang", "Ruby B Lee"], "description": "In cloud computing, network Denial of Service (DoS) attacks are well studied and defenses have been implemented, but severe DoS attacks on a victim's working memory by a single hostile VM are not well understood. Memory DoS attacks are Denial of Service (or Degradation of Service) attacks caused by contention for hardware memory resources on a cloud server. Despite the strong memory isolation techniques for virtual machines (VMs) enforced by the software virtualization layer in cloud servers, the underlying hardware memory layers are still shared by the VMs and can be exploited by a clever attacker in a hostile VM co-located on the same server as the victim VM, denying the victim the working memory he needs. We first show quantitatively the severity of contention on different memory resources. We then show that a malicious cloud customer can mount low-cost attacks to cause severe performance\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [7, 4, 10, 6, 9, 6, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:aqlVkmm33-oC", "title": "Toward secure and efficient deep learning inference in dependable IoT systems", "authors": ["Han Qiu", "Qinkai Zheng", "Tianwei Zhang", "Meikang Qiu", "Gerard Memmi", "Jialiang Lu"], "description": "The rapid development of deep learning (DL) enables resource-constrained systems and devices [e.g., Internet of Things (IoT)] to perform sophisticated artificial intelligence (AI) applications. However, AI models, such as deep neural networks (DNNs), are known to be vulnerable to adversarial examples (AEs). Past works on defending against AEs require heavy computations in the model training or inference processes, making them impractical to be applied in IoT systems. In this article, we propose a novel method, Super-IoT, to enhance the security and efficiency of AI applications in distributed IoT systems. Specifically, Super-IoT utilizes a pixel drop operation to eliminate adversarial perturbations from the input and reduce network transmission throughput. Then, it adopts a sparse signal recovery method to reconstruct the dropped pixels and wavelet-based denoising method to reduce the artificial noise. Super-IoT\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [5, 12, 14, 10, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:dfsIfKJdRG4C", "title": "Fast nearest neighbor machine translation", "authors": ["Yuxian Meng", "Xiaoya Li", "Xiayu Zheng", "Fei Wu", "Xiaofei Sun", "Tianwei Zhang", "Jiwei Li"], "description": "Though nearest neighbor Machine Translation (NN-MT) \\citep{khandelwal2020nearest} has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. NN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast NN-MT to address this issue. Fast NN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast NN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast NN-MT is two-orders faster than NN-MT, and is only two times slower than the standard NMT model. Fast NN-MT enables the practical use of NN-MT systems in real-world MT applications. The code is available at \\url{https://github.com/ShannonAI/fast-knn-nmt}", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [3, 14, 21, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&citation_for_view=9vpiYDIAAAAJ:M3ejUd6NZC8C", "title": "Stealing deep reinforcement learning models for fun and profit", "authors": ["Kangjie Chen", "Shangwei Guo", "Tianwei Zhang", "Xiaofei Xie", "Yang Liu"], "description": "This paper presents the first model extraction attack against Deep Reinforcement Learning (DRL), which enables an external adversary to precisely recover a black-box DRL model only from its interaction with the environment. Model extraction attacks against supervised Deep Learning models have been widely studied. However, those techniques cannot be applied to the reinforcement learning scenario due to DRL models' high complexity, stochasticity and limited observable information. We propose a novel methodology to overcome the above challenges. The key insight of our approach is that the process of DRL model extraction is equivalent to imitation learning, a well-established solution to learn sequential decision-making policies. Based on this observation, our methodology first builds a classifier to reveal the training algorithm family of the targeted black-box DRL model only based on its predicted actions\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [7, 14, 10, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:7PzlFSSx8tAC", "title": "Privacy-preserving collaborative learning with automatic transformation search", "authors": ["Wei Gao", "Shangwei Guo", "Tianwei Zhang", "Han Qiu", "Yonggang Wen", "Yang Liu"], "description": "Collaborative learning has gained great popularity due to its benefit of data privacy protection: participants can jointly train a Deep Learning model without sharing their training sets. However, recent works discovered that an adversary can fully recover the sensitive training samples from the shared gradients. Such reconstruction attacks pose severe threats to collaborative learning. Hence, effective mitigation solutions are urgently desired. In this paper, we propose to leverage data augmentation to defeat reconstruction attacks: by preprocessing sensitive images with carefully-selected transformation policies, it becomes infeasible for the adversary to extract any useful information from the corresponding gradients. We design a novel search method to automatically discover qualified policies. We adopt two new metrics to quantify the impacts of transformations on data privacy and model usability, which can significantly accelerate the search speed. Comprehensive evaluations demonstrate that the policies discovered by our method can defeat existing reconstruction attacks in collaborative learning, with high efficiency and negligible impact on the model performance.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [6, 11, 12, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:_xSYboBqXhAC", "title": "Fine-tuning is not enough: A simple yet effective watermark removal attack for DNN models", "authors": ["Shangwei Guo", "Tianwei Zhang", "Han Qiu", "Yi Zeng", "Tao Xiang", "Yang Liu"], "description": "Watermarking has become the tendency in protecting the intellectual property of DNN models. Recent works, from the adversary's perspective, attempted to subvert watermarking mechanisms by designing watermark removal attacks. However, these attacks mainly adopted sophisticated fine-tuning techniques, which have certain fatal drawbacks or unrealistic assumptions. In this paper, we propose a novel watermark removal attack from a different perspective. Instead of just fine-tuning the watermarked models, we design a simple yet powerful transformation algorithm by combining imperceptible pattern embedding and spatial-level transformations, which can effectively and blindly destroy the memorization of watermarked models to the watermark samples. We also introduce a lightweight fine-tuning strategy to preserve the model performance. Our solution requires much less resource or knowledge about the watermarking scheme than prior works. Extensive experimental results indicate that our attack can bypass state-of-the-art watermarking solutions with very high success rates. Based on our attack, we propose watermark augmentation techniques to enhance the robustness of existing watermarks.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 12, 15, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:ipzZ9siozwsC", "title": "Jailbreaking chatgpt via prompt engineering: An empirical study", "authors": ["Yi Liu", "Gelei Deng", "Zhengzi Xu", "Yuekang Li", "Yaowen Zheng", "Ying Zhang", "Lida Zhao", "Tianwei Zhang", "Yang Liu"], "description": "Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [25, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:WbkHhVStYXYC", "title": "Triggerless backdoor attack for NLP tasks with clean labels", "authors": ["Leilei Gan", "Jiwei Li", "Tianwei Zhang", "Xiaoya Li", "Yuxian Meng", "Fei Wu", "Yi Yang", "Shangwei Guo", "Chun Fan"], "description": "Backdoor attacks pose a new threat to NLP models. A standard strategy to construct poisoned data in backdoor attacks is to insert triggers (e.g., rare words) into selected sentences and alter the original label to a target label. This strategy comes with a severe flaw of being easily detected from both the trigger and the label perspectives: the trigger injected, which is usually a rare word, leads to an abnormal natural language expression, and thus can be easily detected by a defense model; the changed target label leads the example to be mistakenly labeled and thus can be easily detected by manual inspections. To deal with this issue, in this paper, we propose a new strategy to perform textual backdoor attacks which do not require an external trigger, and the poisoned samples are correctly labeled. The core idea of the proposed strategy is to construct clean-labeled examples, whose labels are correct but can lead to test label changes when fused with the training set. To generate poisoned clean-labeled examples, we propose a sentence generation model based on the genetic algorithm to cater to the non-differentiable characteristic of text data. Extensive experiments demonstrate that the proposed attacking strategy is not only effective, but more importantly, hard to defend due to its triggerless and clean-labeled nature. Our work marks the first step towards developing triggerless attacking strategies in NLP.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 10, 18, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:UxriW0iASnsC", "title": "Interpreting deep learning models in natural language processing: A review", "authors": ["Xiaofei Sun", "Diyi Yang", "Xiaoya Li", "Tianwei Zhang", "Yuxian Meng", "Han Qiu", "Guoyin Wang", "Eduard Hovy", "Jiwei Li"], "description": "Neural network models have achieved state-of-the-art performances in a wide range of natural language processing (NLP) tasks. However, a long-standing criticism against neural network models is the lack of interpretability, which not only reduces the reliability of neural NLP systems but also limits the scope of their applications in areas where interpretability is essential (e.g., health care applications). In response, the increasing interest in interpreting neural NLP models has spurred a diverse array of interpretation methods over recent years. In this survey, we provide a comprehensive review of various interpretation methods for neural models in NLP. We first stretch out a high-level taxonomy for interpretation methods in NLP, i.e., training-based approaches, test-based approaches, and hybrid approaches. Next, we describe sub-categories in each category in detail, e.g., influence-function based methods, KNN-based methods, attention-based models, saliency-based methods, perturbation-based methods, etc. We point out deficiencies of current methods and suggest some avenues for future research.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [2, 12, 14, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:9yKSN-GCB0IC", "title": "A framework for realizing security on demand in cloud computing", "authors": ["Pramod Jamkhedkar", "Jakub Szefer", "Diego Perez-Botero", "Tianwei Zhang", "Gina Triolo", "Ruby B Lee"], "description": "In this paper we present our vision for Security on Demand in cloud computing: a system where cloud providers can offer customized security for customers' code and data throughout the term of contract. Security on demand enables security-focussed competitive service differentiation and pricing, based on a threat model that matches the customer's security requirements for the virtual machine he is leasing. It also enables a cloud provider to bring in new secure servers to the data center, and derive revenue from these servers, while still using existing servers. We show a framework where customers' security requests can be expressed and enforced by leveraging the capabilities of servers with different security architectures.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 5, 5, 6, 2, 3, 4, 1, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:W7OEmFMy1HYC", "title": "System and method for security health monitoring and attestation of virtual machines in cloud computing systems", "authors": null, "description": "A system for security health monitoring and attestation of virtual machines in cloud computing systems is provided. The system includes a cloud server having a virtual machine and a hypervisor. The cloud server collects security measurement information and signs and hashes the security measurement information using a cryptography engine. The system also includes an attestation server for receiving the hashed security measurement information from the cloud server. The attestation server also verifies the signature and hash values, and interprets the security measurement information. The attestation server generates an attestation report based on the verification and interpretation of the security measurement information.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [5, 4, 10, 0, 2]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:MXK_kJrjxJIC", "title": "Verideep: Verifying integrity of deep neural networks through sensitive-sample fingerprinting", "authors": ["Zecheng He", "Tianwei Zhang", "Ruby B Lee"], "description": "Deep learning has become popular, and numerous cloud-based services are provided to help customers develop and deploy deep learning applications. Meanwhile, various attack techniques have also been discovered to stealthily compromise the model's integrity. When a cloud customer deploys a deep learning model in the cloud and serves it to end-users, it is important for him to be able to verify that the deployed model has not been tampered with, and the model's integrity is protected. We propose a new low-cost and self-served methodology for customers to verify that the model deployed in the cloud is intact, while having only black-box access (e.g., via APIs) to the deployed model. Customers can detect arbitrary changes to their deep learning models. Specifically, we define \\texttt{Sensitive-Sample} fingerprints, which are a small set of transformed inputs that make the model outputs sensitive to the model's parameters. Even small weight changes can be clearly reflected in the model outputs, and observed by the customer. Our experiments on different types of model integrity attacks show that we can detect model integrity breaches with high accuracy (99\\%) and low overhead (10 black-box model accesses).", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 2, 9, 4, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:UeHWp8X0CEIC", "title": "Memory dos attacks in multi-tenant clouds: Severity and mitigation", "authors": ["Tianwei Zhang", "Yinqian Zhang", "Ruby B Lee"], "description": "In cloud computing, network Denial of Service (DoS) attacks are well studied and defenses have been implemented, but severe DoS attacks on a victim's working memory by a single hostile VM are not well understood. Memory DoS attacks are Denial of Service (or Degradation of Service) attacks caused by contention for hardware memory resources on a cloud server. Despite the strong memory isolation techniques for virtual machines (VMs) enforced by the software virtualization layer in cloud servers, the underlying hardware memory layers are still shared by the VMs and can be exploited by a clever attacker in a hostile VM co-located on the same server as the victim VM, denying the victim the working memory he needs. We first show quantitatively the severity of contention on different memory resources. We then show that a malicious cloud customer can mount low-cost attacks to cause severe performance degradation for a Hadoop distributed application, and 38X delay in response time for an E-commerce website in the Amazon EC2 cloud. Then, we design an effective, new defense against these memory DoS attacks, using a statistical metric to detect their existence and execution throttling to mitigate the attack damage. We achieve this by a novel re-purposing of existing hardware performance counters and duty cycle modulation for security, rather than for improving performance or power consumption. We implement a full prototype on the OpenStack cloud system. Our evaluations show that this defense system can effectively defeat memory DoS attacks with negligible performance overhead.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 4, 5, 3, 4, 5, 2, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:VLnqNzywnoUC", "title": "Gpt-ner: Named entity recognition via large language models", "authors": ["Shuhe Wang", "Xiaofei Sun", "Xiaoya Li", "Rongbin Ouyang", "Fei Wu", "Tianwei Zhang", "Jiwei Li", "Guoyin Wang"], "description": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text \"Columbus is a city\" is transformed to generate the text sequence \"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [17, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:zA6iFVUQeVQC", "title": "Novel denial-of-service attacks against cloud-based multi-robot systems", "authors": ["Yuan Xu", "Gelei Deng", "Tianwei Zhang", "Han Qiu", "Yungang Bao"], "description": "The development of robotics technology is accelerated by the strong support from cloud computing. Massive computation resources and services from the cloud make modern multi-robot systems more efficient and powerful. However, the introduction of cloud servers to multi-robot systems can also incur potential Denial-of-Service (DoS) threats, where an adversary can utilize the shared cloud resources to degrade or bring down the robot systems. In this paper, we conduct a comprehensive study about this security issue. By analyzing different attack vectors in cloud-robotic platforms, we propose three new DoS attacks, which manipulate the network resources, micro-architecture resources, and function parameters respectively. We conduct extensive evaluations and case studies to demonstrate the feasibility and severity of our techniques. We alert the robotics community to these catastrophic attacks on the safety\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [2, 12, 10, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:KxtntwgDAa4C", "title": "Gnn-lm: Language modeling based on global contexts via gnn", "authors": ["Yuxian Meng", "Shi Zong", "Xiaoya Li", "Xiaofei Sun", "Tianwei Zhang", "Fei Wu", "Jiwei Li"], "description": "Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \\footnote{The code can be found at https://github.com/ShannonAI/GNN-LM", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 12, 10, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:_kc_bZDykSQC", "title": "Towards byzantine-resilient learning in decentralized systems", "authors": ["Shangwei Guo", "Tianwei Zhang", "Xiaofei Xie", "Lei Ma", "Tao Xiang", "Yang Liu"], "description": "With the proliferation of IoT and edge computing, decentralized learning is becoming more promising. When designing a distributed learning system, one major challenge to consider is Byzantine Fault Tolerance (BFT). Past works have researched Byzantine-resilient solutions for centralized distributed learning. However, there are currently no satisfactory solutions with strong efficiency and security in decentralized systems. In this paper, we propose a novel algorithm, Mozi, to achieve BFT in decentralized learning systems. Specifically, Mozi provides a uniform Byzantine-resilient aggregation rule for benign nodes to select the useful parameter updates and filter out the malicious ones in each training iteration. It guarantees that each benign node in a decentralized system can train a correct model under very strong Byzantine attacks with an arbitrary number of faulty nodes. We perform the theoretical analysis to prove\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [6, 6, 7, 3, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:4DMP91E08xMC", "title": "A unified framework for analyzing and detecting malicious examples of dnn models", "authors": ["Kaidi Jin", "Tianwei Zhang", "Chao Shen", "Yufei Chen", "Ming Fan", "Chenhao Lin", "Ting Liu"], "description": null, "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 8, 5, 9, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:Tyk-4Ss8FVUC", "title": "Monitoring and attestation of virtual machine security health in cloud computing", "authors": ["Tianwei Zhang", "Ruby B Lee"], "description": "Cloud customers need assurances regarding the security of their virtual machines (VMs) operating within an infrastructure-as-a-service cloud system. This is complicated by the customer not knowing where the VM is executing and by the semantic gap between what the customer wants to know versus what can be measured in the cloud. In this article, the authors present an architecture for monitoring a VM's security health. Their architecture can communicate this to the customer in an unforgeable manner. The authors show a concrete implementation of property-based attestation and a full prototype based on the OpenStack open source cloud software.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 4, 7, 3, 2, 4, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:W5xh706n7nkC", "title": "Deepsweep: An evaluation framework for mitigating dnn backdoor attacks using data augmentation", "authors": ["Yi Zeng", "Han Qiu", "Shangwei Guo", "Tianwei Zhang", "Meikang Qiu", "Bhavani Thuraisingham"], "description": "Public resources and services (eg, datasets, training platforms, pre-trained models) have been widely adopted to ease the development of Deep Learning-based applications. However, if the third-party providers are untrusted, they can inject poisoned samples into the datasets or embed backdoors in those models. Such an integrity breach can cause severe consequences, especially in safety-and security-critical applications. Various backdoor attack techniques have been proposed for higher effectiveness and stealthiness. Unfortunately, existing defense solutions are not practical to thwart those attacks in a comprehensive way. In this paper, we investigate the effectiveness of data augmentation techniques in mitigating backdoor attacks and enhancing DL models' robustness. An evaluation framework is introduced to achieve this goal. Specifically, we consider a unified defense solution, which (1) adopts a data\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 10, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:qjMakFHDy7sC", "title": "Secure cache modeling for measuring side-channel leakage", "authors": ["Tianwei Zhang", "Ruby B Lee"], "description": "Side-channel attacks try to break a system\u2019s confidentiality using physical information emitted from the targeted system. Such information is leaked out through cache side channels, which can exist in many parts of the system. Cache memories are a potential source of information leakage through side-channel attacks, many of which have been proposed. Meanwhile, different cache architectures have also been proposed to defend against these attacks. Thus it is necessary to evaluate the effectiveness of the proposed defense approaches.In this paper, we propose two methods to evaluate a system\u2019s vulnerability to cache side-channel attacks. First, we run actual attack programs and recover the cipher keys to directly show if the target system is attackable through such side-channel attacks. We also provide a new key-vote metric to quantify the system\u2019s vulnerability to the attack. The actual attack is accurate, but is slow and cipher specific. Hence, we propose a second method based on new models of cache architectures and their information leakage potential. We define a novel Interference Matrix to evaluate a system\u2019s vulnerability to entire categories of cache side-channel attacks, rather than to a specific attack. These models can give more comprehensive conclusions on a system\u2019s vulnerability to side channel attacks. Finally we check whether the two methods give consistent results.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 1, 3, 1, 1, 2, 7, 0, 2, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:uc_IGeMz5qoC", "title": "Prompt Injection attack against LLM-integrated Applications", "authors": ["Yi Liu", "Gelei Deng", "Yuekang Li", "Kailong Wang", "Tianwei Zhang", "Yepang Liu", "Haoyu Wang", "Yan Zheng", "Yang Liu"], "description": "Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [17, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:l7t_Zn2s7bgC", "title": "Efficient, private and robust federated learning", "authors": ["Meng Hao", "Hongwei Li", "Guowen Xu", "Hanxiao Chen", "Tianwei Zhang"], "description": "Federated learning (FL) has demonstrated tremendous success in various mission-critical large-scale scenarios. However, such promising distributed learning paradigm is still vulnerable to privacy inference and byzantine attacks. The former aims to infer the privacy of target participants involved in training, while the latter focuses on destroying the integrity of the constructed model. To mitigate the above two issues, a few works recently explored unified solutions by utilizing generic secure computation techniques and common byzantine-robust aggregation rules, but there are two major limitations: 1) they suffer from impracticality due to efficiency bottlenecks, and 2) they are still vulnerable to various types of attacks because of model incomprehensiveness.  To approach the above problems, in this paper, we present SecureFL, an efficient, private and byzantine-robust FL framework. SecureFL follows the state-of-the\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 6, 15, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:pyW8ca7W8N0C", "title": "Byzantine-resilient decentralized stochastic gradient descent", "authors": ["Shangwei Guo", "Tianwei Zhang", "Han Yu", "Xiaofei Xie", "Lei Ma", "Tao Xiang", "Yang Liu"], "description": "Decentralized learning has gained great popularity to improve learning efficiency and preserve data privacy. Each computing node makes equal contribution to collaboratively learn a Deep Learning model. The elimination of centralized Parameter Servers (PS) can effectively address many issues such as privacy, performance bottleneck and single-point-failure. However, how to achieve Byzantine Fault Tolerance in decentralized learning systems is rarely explored, although this problem has been extensively studied in centralized systems. In this paper, we present an in-depth study towards the Byzantine resilience of decentralized learning systems with two contributions. First, from the adversarial perspective, we theoretically illustrate that Byzantine attacks are more dangerous and feasible in decentralized learning systems: even one malicious participant can arbitrarily alter the models of other participants by\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 12, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:4TOpqqG69KYC", "title": "Mitigating advanced adversarial attacks with more advanced gradient obfuscation techniques", "authors": ["Han Qiu", "Yi Zeng", "Qinkai Zheng", "Tianwei Zhang", "Meikang Qiu", "Gerard Memmi"], "description": "Deep Neural Networks (DNNs) are well-known to be vulnerable to Adversarial Examples (AEs). A large amount of efforts have been spent to launch and heat the arms race between the attackers and defenders. Recently, advanced gradient-based attack techniques were proposed (e.g., BPDA and EOT), which have defeated a considerable number of existing defense methods. Up to today, there are still no satisfactory solutions that can effectively and efficiently defend against those attacks. In this paper, we make a steady step towards mitigating those advanced gradient-based attacks with two major contributions. First, we perform an in-depth analysis about the root causes of those attacks, and propose four properties that can break the fundamental assumptions of those attacks. Second, we identify a set of operations that can meet those properties. By integrating these operations, we design two preprocessing functions that can invalidate these powerful attacks. Extensive evaluations indicate that our solutions can effectively mitigate all existing standard and advanced attack techniques, and beat 11 state-of-the-art defense solutions published in top-tier conferences over the past 2 years. The defender can employ our solutions to constrain the attack success rate below 7% for the strongest attacks even the adversary has spent dozens of GPU hours.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 6, 9, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:3fE2CSJIrl8C", "title": "Analyzing cache side channels using deep neural networks", "authors": ["Tianwei Zhang", "Yinqian Zhang", "Ruby B Lee"], "description": "Cache side-channel attacks aim to breach the confidentiality of a computer system and extract sensitive secrets through CPU caches. In the past years, different types of side-channel attacks targeting a variety of cache architectures have been demonstrated. Meanwhile, different defense methods and systems have also been designed to mitigate these attacks. However, quantitatively evaluating the effectiveness of these attacks and defenses has been challenging. We propose a generic approach to evaluating cache side-channel attacks and defenses. Specifically, our method builds a deep neural network with its inputs as the adversary's observed information, and its outputs as the victim's execution traces. By training the neural network, the relationship between the inputs and outputs can be automatically discovered. As a result, the prediction accuracy of the neural network can serve as a metric to quantify how\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 3, 2, 7, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:pqnbT2bcN3wC", "title": "Temporal watermarks for deep reinforcement learning models", "authors": ["Kangjie Chen", "Shangwei Guo", "Tianwei Zhang", "Shuxin Li", "Yang Liu"], "description": "Watermarking has become a popular and attractive technique to protect the Intellectual Property (IP) of Deep Learning (DL) models. However, very few studies explore the possibility of watermarking Deep Reinforcement Learning (DRL) models. Common approaches in the DL context embed backdoors into the protected model and use special samples to verify the model ownership. These solutions are easy to be detected, and can potentially affect the performance and behaviors of the target model. Such limitations make existing solutions less applicable to safety-and security-critical tasks and scenarios, where DRL has been widely used. In this work, we propose a novel watermarking scheme for DRL protection. Instead of using spatial watermarks as in DL models, we introduce temporal watermarks, which can reduce the potential impact and damage to the target model, while achieving ownership verification with high fidelity. Specifically,(1) we design a new damage metric to select sequential states for watermark generation;(2) we introduce a new reward function to efficiently alter the model\u2019s behaviors for watermark embedding;(3) we propose to utilize a predefined probability density function of actions over the watermark states as the verification evidence. Our method is general and can be applied to various DRL tasks with either deterministic or stochastic reinforcement learning algorithms. Extensive experimental results show that it can effectively preserve the functionality of DRL models and exhibit significant robustness against common model modifications, eg, fine-tuning and model compression.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [5, 7, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:D_sINldO8mEC", "title": "Threats to pre-trained language models: Survey and taxonomy", "authors": ["Shangwei Guo", "Chunlong Xie", "Jiwei Li", "Lingjuan Lyu", "Tianwei Zhang"], "description": "Pre-trained language models (PTLMs) have achieved great success and remarkable performance over a wide range of natural language processing (NLP) tasks. However, there are also growing concerns regarding the potential security issues in the adoption of PTLMs. In this survey, we comprehensively systematize recently discovered threats to PTLM systems and applications. We perform our attack characterization from three interesting perspectives. (1) We show threats can occur at different stages of the PTLM pipeline raised by different malicious entities. (2) We identify two types of model transferability (landscape, portrait) that facilitate attacks. (3) Based on the attack goals, we summarize four categories of attacks (backdoor, evasion, data privacy and model privacy). We also discuss some open problems and research directions. We believe our survey and taxonomy will inspire future studies towards secure and privacy-preserving PTLMs.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 13, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:AXPGKjj_ei8C", "title": "A practical fog-based privacy-preserving online car-hailing service system", "authors": ["Jianfei Sun", "Guowen Xu", "Tianwei Zhang", "Mamoun Alazab", "Robert H Deng"], "description": "Aiming for minimizing passengers\u2019 waiting time and vehicles\u2019 vacancy rate, online car-hailing service systems with fog computing has been deployed in various scenarios. In this paper, we focus on addressing the security and privacy issues in such a promising system by customizing a new cryptographic primitive to provide the following security guarantees: (1) private, fine-grained and bilateral order matching between passengers and drivers; (2) authenticity verification of passengers\u2019 orders in the form of ciphertext, and (3) temporal assurance of passengers\u2019 ciphertext orders. To the best of our knowledge, no previous system has been designed to meet all three requirements. Existing cryptographic primitives (including forward/puncturable encryption (FE/PE) and attribute based matchmaking encryption (AB-ME)) may be leveraged to partially address some of challenges, but there lacks a comprehensive solution\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [5, 11, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:q3oQSFYPqjQC", "title": "Secure data sharing with flexible cross-domain authorization in autonomous vehicle systems", "authors": ["Jianfei Sun", "Guowen Xu", "Tianwei Zhang", "Xiaochun Cheng", "Xingshuo Han", "Mingjian Tang"], "description": "As an increasingly prevalent technology in intelligent autonomous transportation systems, autonomous vehicle platoon has been indicated the ability to significantly reduce fuel consumption as well as heighten highway safety and throughput. However, existing efforts rarely focus on protecting data confidentiality and authenticity in autonomous vehicle platoons. How to ensure secure and high-fidelity platoon-level communication is still in its infancy. This paper makes the first attempt for efficient and secure communication across autonomous vehicle platoons. Specifically, we present PDSM-FC, the first privacy-preserving data share mechanism with flexible cross-domain authorization over distinctive platoons. The key insight of PDSM-FC is the design of a new ciphertext conversion technique, which allows a ciphertext to be easily converted into another type of ciphertext, facilitating efficient access by all entities\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [5, 10, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:SeFeTyx0c_EC", "title": "An efficient preprocessing-based approach to mitigate advanced adversarial attacks", "authors": ["Han Qiu", "Yi Zeng", "Qinkai Zheng", "Shangwei Guo", "Tianwei Zhang", "Hewu Li"], "description": "Deep Neural Networks are well-known to be vulnerable to Adversarial Examples. Recently, advanced gradient-based attacks were proposed (e.g., BPDA and EOT), which can significantly increase the difficulty and complexity of designing effective defenses. In this paper, we present a study towards the opportunity of mitigating those powerful attacks with only pre-processing operations. We make the following two contributions. First, we perform an in-depth analysis of those attacks and summarize three fundamental properties that a good defense solution should have. Second, we design a lightweight preprocessing function with these properties and the capability of preserving the model's usability and robustness against these threats. Extensive evaluations indicate that our solutions can effectively mitigate all existing standard and advanced attack techniques, and beat 11 state-of-the-art defense solutions\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [4, 6, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:OU6Ihb5iCvQC", "title": "Chronus: A novel deadline-aware scheduler for deep learning training jobs", "authors": ["Wei Gao", "Zhisheng Ye", "Peng Sun", "Yonggang Wen", "Tianwei Zhang"], "description": "Modern GPU clusters support Deep Learning training (DLT) jobs in a distributed manner. Job scheduling is the key to improve the training performance, resource utilization and fairness across users. Different training jobs may require various objectives and demands in terms of completion time. How to efficiently satisfy all these requirements is not extensively studied. We present Chronus, an end-to-end scheduling system to provide deadline guarantee for SLO jobs and maximize the performance of best-effort jobs. Chronus is designed based on the unique features of DLT jobs. (1) It leverages the intra-job predictability of DLT processes to efficiently profile jobs and estimate their runtime speed with dynamic resource scaling. (2) It takes advantages of the DLT preemption feature to select jobs with a lease-based training scheme. (3) It considers the placement sensitivity of DLT jobs to allocate resources with new\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 9, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:Y0pCki6q_DkC", "title": "Host-based dos attacks and defense in the cloud", "authors": ["Tianwei Zhang", "Ruby B Lee"], "description": "We explore host-based DoS attacks, which exploit the shared computing resources in a multi-tenant cloud server to compromise the server's resource availability. We first present a set of attack techniques targeting different types of resources. We show such attacks can significantly affect the performance of co-located VMs, as well as the cloud provider's management services. Then we propose an attack strategy to compromise the availability of the entire datacenter. We show how power-aware optimization techniques can help the attacker achieve his goal faster, with low cost. We design an effective general-purpose method to defeat memory, network and disk DoS attacks. We use a statistical method to detect changes in the usage of different resources. Once an attack happens, we use resource throttling techniques to identify and thwart the malicious VMs. Our evaluation shows that this defense method can\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 3, 1, 1, 4, 3, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:u_35RYKgDlwC", "title": "Defending against backdoor attacks in natural language generation", "authors": ["Xiaofei Sun", "Xiaoya Li", "Yuxian Meng", "Xiang Ao", "Lingjuan Lyu", "Jiwei Li", "Tianwei Zhang"], "description": "The frustratingly fragile nature of neural network models make current natural language generation (NLG) systems prone to backdoor attacks and generate malicious sequences that could be sexist or offensive. Unfortunately, little effort has been invested to how backdoor attacks can affect current NLG models and how to defend against these attacks. In this work, by giving a formal definition of backdoor attack and defense, we investigate this problem on two important NLG tasks, machine translation and dialog generation. Tailored to the inherent nature of NLG models (eg, producing a sequence of coherent words given contexts), we design defending strategies against attacks. We find that testing the backward probability of generating sources given targets yields effective defense performance against all different types of attacks, and is able to handle the one-to-many issue in many NLG tasks such as dialog generation. We hope that this work can raise the awareness of backdoor risks concealed in deep NLG systems and inspire more future work (both attack and defense) towards this direction.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [6, 5]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:ye4kPcJQO24C", "title": "Iron: Private inference on transformers", "authors": ["Meng Hao", "Hongwei Li", "Hanxiao Chen", "Pengzhi Xing", "Guowen Xu", "Tianwei Zhang"], "description": "We initiate the study of private inference on Transformer-based models in the client-server setting, where clients have private inputs and servers hold proprietary models. Our main contribution is to provide several new secure protocols for matrix multiplication and complex non-linear functions like Softmax, GELU activations, and LayerNorm, which are critical components of Transformers. Specifically, we first propose a customized homomorphic encryption-based protocol for matrix multiplication that crucially relies on a novel compact packing technique. This design achieves  less communication ( is the number of rows of the output matrix) over the most efficient work. Second, we design efficient protocols for three non-linear functions via integrating advanced underlying protocols and specialized optimizations. Compared to the state-of-the-art protocols, our recipes reduce about half of the communication and computation overhead. Furthermore, all protocols are numerically precise, which preserve the model accuracy of plaintext. These techniques together allow us to implement\\Name, an efficient Transformer-based private inference framework. Experiments conducted on several real-world datasets and models demonstrate that\\Name achieves  less communication and  less runtime compared to the prior art.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [1, 12, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:D03iK_w7-QYC", "title": "Topology-aware differential privacy for decentralized image classification", "authors": ["Shangwei Guo", "Tianwei Zhang", "Guowen Xu", "Han Yu", "Tao Xiang", "Yang Liu"], "description": "Image classification is a fundamental artificial intelligence task that labels images into one of some predefined classes. However, training complex image classification models requires a large amount of computation resources and data in order to reach state-of-the-art performance. This demand drives the growth of distributed deep learning, where multiple agents cooperatively train global models with their individual datasets. Among such learning systems, decentralized learning is particularly attractive, as it can improve the efficiency and fault tolerance by eliminating the centralized parameter server, which could be the single point of failure or performance bottleneck. Although the agents do not need to disclose their training image samples, they exchange parameters with each other at each iteration, which can put them at the risk of data privacy leakage. Past works demonstrated the possibility of recovering\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [3, 4, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:LkGwnXOMwfcC", "title": "Design, implementation and verification of cloud architecture for monitoring a virtual machine's security health", "authors": ["Tianwei Zhang", "Ruby B Lee"], "description": "Cloud customers need assurances regarding the security of their virtual machines (VMs), operating within an Infrastructure as a Service (IaaS) cloud system. This is complicated by the customer not knowing where his VM is executing, and on the semantic gap between what the customer wants to know versus what can be measured in the cloud. We present CloudMonatt, an architecture for monitoring a VM's security health. We show a full prototype based on the OpenStack open source cloud software. We also verify CloudMonatt to show that there are no security vulnerabilities that could allow an attacker to subvert its protection. As such, we conduct a systematic security verification of CloudMonatt. We model and verify the network protocols within the distributed system, as well as interactions of hardware/software modules inside the cloud server. Our results show that CloudMonatt is capable of delivering this\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 4, 2, 5, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:XD-gHx7UXLsC", "title": "Text Classification via Large Language Models", "authors": ["Xiaofei Sun", "Xiaoya Li", "Jiwei Li", "Fei Wu", "Shangwei Guo", "Tianwei Zhang", "Guoyin Wang"], "description": "Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning. In this paper, we introduce \\textbf{C}lue \\textbf{A}nd \\textbf{R}easoning \\textbf{P}rompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generalization ability and the task-specific evidence provided by the full labeled dataset. Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP delivers impressive abilities on low-resource and domain-adaptation setups. Specifically, Specifically, using 16 examples per class, CARP achieves comparable performances to supervised models with 1,024 examples per class.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [11, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:bnK-pcrLprsC", "title": "Benchmarking and analyzing 3d human pose and shape estimation beyond algorithms", "authors": ["Hui En Pang", "Zhongang Cai", "Lei Yang", "Tianwei Zhang", "Ziwei Liu"], "description": "3D human pose and shape estimation (aka``human mesh recovery'') has achieved substantial progress. Researchers mainly focus on the development of novel algorithms, while less attention has been paid to other critical factors involved. This could lead to less optimal baselines, hindering the fair and faithful evaluations of newly designed methodologies. To address this problem, this work presents the\\textit {first} comprehensive benchmarking study from three under-explored perspectives beyond algorithms.\\emph {1) Datasets.} An analysis on 31 datasets reveals the distinct impacts of data samples: datasets featuring critical attributes (\\emph {ie} diverse poses, shapes, camera characteristics, backbone features) are more effective. Strategical selection and combination of high-quality datasets can yield a significant boost to the model performance.\\emph {2) Backbones.} Experiments with 10 backbones, ranging from CNNs to transformers, show the knowledge learnt from a proximity task is readily transferable to human mesh recovery.\\emph {3) Training strategies.} Proper augmentation techniques and loss designs are crucial. With the above findings, we achieve a PA-MPJPE of 47.3 (mm) on the 3DPW test set with a relatively simple model. More importantly, we provide strong baselines for fair comparisons of algorithms, and recommendations for building effective training configurations in the future. Codebase is available at\\url {https://github. com/smplbody/hmr-benchmarks}.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 13, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:1sJd4Hv_s6UC", "title": "Share your data carefree: An efficient, scalable and privacy-preserving data sharing service in cloud computing", "authors": ["Jianfei Sun", "Guowen Xu", "Tianwei Zhang", "Hu Xiong", "Hongwei Li", "Robert Deng"], "description": "Benefiting from the powerful computing and storage capabilities of cloud services, data sharing in the cloud has been permeated across various applications including social networks, e-health and crowdsourcing transportation system. Intuitively, outsourcing data to untrusted cloud commonly raises concerns about data privacy breaches. To combat this, one approach is exploiting Broadcast Based Searchable Encryption (BBSE) for secure data sharing. Nevertheless, the latest proposed BBSE is still defective in either security or efficiency. In this article, we propose ESPD, an Efficient, Scalable and Privacy-preserving Data sharing framework over encrypted cloud dataset. Different from previous works, ESPD supports sharing target data to multiple users with distinct secret keys, and keeps a constant ciphertext length with the changes of the amount of system users. This feature significantly improves search efficiency\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 7, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:EUQCXRtRnyEC", "title": "An investigation of byzantine threats in multi-robot systems", "authors": ["Gelei Deng", "Yuan Zhou", "Yuan Xu", "Tianwei Zhang", "Yang Liu"], "description": "Multi-Robot Systems (MRSs) show significant advantages to deal with complex tasks efficiently. However, the system complexity inevitably enlarges the attack surface and adds difficulty in guaranteeing the security and safety of MRSs. In this paper, we present an in-depth investigation about the Byzantine threats in MRSs, where some robot is untrusted. We design a practical methodology to identify potential Byzantine risks in a given MRS workload built from the Robot Operating System (ROS). It consists of three novel steps (requirement specification using signal temporal logic, attack surface determination via data-flow analysis, attack identification using requirement-driven fuzzing) to thoroughly assess MRS workloads. We use this fuzzing method to inspect five typical MRS workloads from past works and the ROS platform, and identify three novel kinds of attacks that can be launched with five attack strategies. We\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [2, 4, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:L8Ckcad2t8MC", "title": "Fencebox: A platform for defeating adversarial examples with data augmentation techniques", "authors": ["Han Qiu", "Yi Zeng", "Tianwei Zhang", "Yong Jiang", "Meikang Qiu"], "description": "It is extensively studied that Deep Neural Networks (DNNs) are vulnerable to Adversarial Examples (AEs). With more and more advanced adversarial attack methods have been developed, a quantity of corresponding defense solutions were designed to enhance the robustness of DNN models. It has become a popularity to leverage data augmentation techniques to preprocess input samples before inference to remove adversarial perturbations. By obfuscating the gradients of DNN models, these approaches can defeat a considerable number of conventional attacks. Unfortunately, advanced gradient-based attack techniques (e.g., BPDA and EOT) were introduced to invalidate these preprocessing effects. In this paper, we present FenceBox, a comprehensive framework to defeat various kinds of adversarial attacks. FenceBox is equipped with 15 data augmentation methods from three different categories. We comprehensively evaluated that these methods can effectively mitigate various adversarial attacks. FenceBox also provides APIs for users to easily deploy the defense over their models in different modes: they can either select an arbitrary preprocessing method, or a combination of functions for a better robustness guarantee, even under advanced adversarial attacks. We open-source FenceBox, and expect it can be used as a standard toolkit to facilitate the research of adversarial attacks and defenses.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 5, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:mVmsd5A6BfQC", "title": "The hidden vulnerability of watermarking for deep neural networks", "authors": ["Shangwei Guo", "Tianwei Zhang", "Han Qiu", "Yi Zeng", "Tao Xiang", "Yang Liu"], "description": null, "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 6, 5, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:J-pR_7NvFogC", "title": "Deep learning workload scheduling in gpu datacenters: Taxonomy, challenges and vision", "authors": ["Wei Gao", "Qinghao Hu", "Zhisheng Ye", "Peng Sun", "Xiaolin Wang", "Yingwei Luo", "Tianwei Zhang", "Yonggang Wen"], "description": "Deep learning (DL) shows its prosperity in a wide variety of fields. The development of a DL model is a time-consuming and resource-intensive procedure. Hence, dedicated GPU accelerators have been collectively constructed into a GPU datacenter. An efficient scheduler design for such GPU datacenter is crucially important to reduce the operational cost and improve resource utilization. However, traditional approaches designed for big data or high performance computing workloads can not support DL workloads to fully utilize the GPU resources. Recently, substantial schedulers are proposed to tailor for DL workloads in GPU datacenters. This paper surveys existing research efforts for both training and inference workloads. We primarily present how existing schedulers facilitate the respective workloads from the scheduling objectives and resource consumption features. Finally, we prospect several promising future research directions. More detailed summary with the surveyed paper and code links can be found at our project website: https://github.com/S-Lab-SystemGroup/Awesome-DL-Scheduling-Papers", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 8, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:cFHS6HbyZ2cC", "title": "Sentence similarity based on contexts", "authors": ["Xiaofei Sun", "Yuxian Meng", "Xiang Ao", "Fei Wu", "Tianwei Zhang", "Jiwei Li", "Chun Fan"], "description": "Existing methods to measure sentence similarity are faced with two challenges: (1) labeled datasets are usually limited in size, making them insufficient to train supervised neural models; and (2) there is a training-test gap for unsupervised language modeling (LM) based models to compute semantic scores between sentences, since sentence-level semantics are not explicitly modeled at training. This results in inferior performances in this task. In this work, we propose a new framework to address these two issues. The proposed framework is based on the core idea that the meaning of a sentence should be defined by its contexts, and that sentence similarity can be measured by comparing the probabilities of generating two sentences given the same context. The proposed framework is able to generate high-quality, large-scale dataset with semantic similarity scores between two sentences in an unsupervised\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 7, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:mvPsJ3kp5DgC", "title": "NN-NER: Named Entity Recognition with Nearest Neighbor Search", "authors": ["Shuhe Wang", "Xiaoya Li", "Yuxian Meng", "Tianwei Zhang", "Rongbin Ouyang", "Jiwei Li", "Guoyin Wang"], "description": "Inspired by recent advances in retrieval augmented methods in NLP~\\citep{khandelwal2019generalization,khandelwal2020nearest,meng2021gnn}, in this paper, we introduce a  nearest neighbor NER (NN-NER) framework, which augments the distribution of entity labels by assigning  nearest neighbors retrieved from the training set. This strategy makes the model more capable of handling long-tail cases, along with better few-shot learning abilities. NN-NER requires no additional operation during the training phase, and by interpolating  nearest neighbors search into the vanilla NER model, NN-NER consistently outperforms its vanilla counterparts: we achieve a new state-of-the-art F1-score of 72.03 (+1.25) on the Chinese Weibo dataset and improved results on a variety of widely used NER benchmarks. Additionally, we show that NN-NER can achieve comparable results to the vanilla NER model with 40\\% less amount of training data. Code available at \\url{https://github.com/ShannonAI/KNN-NER}.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 8, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:qUcmZB5y_30C", "title": "Local black-box adversarial attacks: A query efficient approach", "authors": ["Tao Xiang", "Hangcheng Liu", "Shangwei Guo", "Tianwei Zhang", "Xiaofeng Liao"], "description": "Adversarial attacks have threatened the application of deep neural networks in security-sensitive scenarios. Most existing black-box attacks fool the target model by interacting with it many times and producing global perturbations. However, global perturbations change the smooth and insignificant background, which not only makes the perturbation more easily be perceived but also increases the query overhead. In this paper, we propose a novel framework to perturb the discriminative areas of clean examples only within limited queries in black-box attacks. Our framework is constructed based on two types of transferability. The first one is the transferability of model interpretations. Based on this property, we identify the discriminative areas of a given clean example easily for local perturbations. The second is the transferability of adversarial examples. It helps us to produce a local pre-perturbation for improving query efficiency. After identifying the discriminative areas and pre-perturbing, we generate the final adversarial examples from the pre-perturbed example by querying the targeted model with two kinds of black-box attack techniques, i.e., gradient estimation and random search. We conduct extensive experiments to show that our framework can significantly improve the query efficiency during black-box perturbing with a high attack success rate. Experimental results show that our attacks outperform state-of-the-art black-box attacks under various system settings.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [3, 5, 2, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:vbGhcppDl1QC", "title": "Instruction tuning for large language models: A survey", "authors": ["Shengyu Zhang", "Linfeng Dong", "Xiaoya Li", "Sen Zhang", "Xiaofei Sun", "Shuhe Wang", "Jiwei Li", "Runyi Hu", "Tianwei Zhang", "Fei Wu", "Guoyin Wang"], "description": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:tOudhMTPpwUC", "title": "Faster nearest neighbor machine translation", "authors": ["Shuhe Wang", "Jiwei Li", "Yuxian Meng", "Rongbin Ouyang", "Guoyin Wang", "Xiaoya Li", "Tianwei Zhang", "Shi Zong"], "description": "NN based neural machine translation (NN-MT) has achieved state-of-the-art results in a variety of MT tasks. One significant shortcoming of NN-MT lies in its inefficiency in identifying the  nearest neighbors of the query representation from the entire datastore, which is prohibitively time-intensive when the datastore size is large. In this work, we propose \\textbf{Faster NN-MT} to address this issue. The core idea of Faster NN-MT is to use a hierarchical clustering strategy to approximate the distance between the query and a data point in the datastore, which is decomposed into two parts: the distance between the query and the center of the cluster that the data point belongs to, and the distance between the data point and the cluster center. We propose practical ways to compute these two parts in a significantly faster manner. Through extensive experiments on different MT benchmarks, we show that \\textbf{Faster NN-MT} is faster than Fast NN-MT \\citep{meng2021fast} and only slightly (1.2 times) slower than its vanilla counterpart while preserving model performance as NN-MT. Faster NN-MT enables the deployment of NN-MT models on real-world MT services.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:geHnlv5EZngC", "title": "Systematic testing of autonomous driving systems using map topology-based scenario classification", "authors": ["Yun Tang", "Yuan Zhou", "Tianwei Zhang", "Fenghua Wu", "Yang Liu", "Gang Wang"], "description": "Autonomous Driving Systems (ADSs), which replace humans to drive vehicles, are complex software systems deployed in autonomous vehicles (AVs). Since the execution of ADSs highly relies on maps, it is essential to perform global map-based testing for ADSs to guarantee their correctness and AVs\u2019 safety in different situations. Existing methods focus more on specific scenarios rather than global testing throughout the map. Testing on a global map is challenging since the complex lane connections in a map can generate enormous scenarios. In this work, we propose ATLAS, an approach to ADSs\u2019 collision avoidance testing using map topology-based scenario classification. The core insight of ATLAS is to generate diverse testing scenarios by classifying junction lanes according to their topology-based interaction patterns. First, ATLAS divides the junction lanes into different classes such that an ADS can execute\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:JoZmwDi-zQgC", "title": "Physical backdoor attacks to lane detection systems in autonomous driving", "authors": ["Xingshuo Han", "Guowen Xu", "Yuan Zhou", "Xuehuan Yang", "Jiwei Li", "Tianwei Zhang"], "description": "Modern autonomous vehicles adopt state-of-the-art DNN models to interpret the sensor data and perceive the environment. However, DNN models are vulnerable to different types of adversarial attacks, which pose significant risks to the security and safety of the vehicles and passengers. One prominent threat is the backdoor attack, where the adversary can compromise the DNN model by poisoning the training samples. Although lots of effort has been devoted to the investigation of the backdoor attack to conventional computer vision tasks, its practicality and applicability to the autonomous driving scenario is rarely explored, especially in the physical world. In this paper, we target the lane detection system, which is an indispensable module for many autonomous driving tasks, e.g., navigation, lane switching. We design and realize thefirst physical backdoor attacks to such system. Our attacks are comprehensively\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [1, 8, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:fPk4N6BV_jEC", "title": "Modeling text-visual mutual dependency for multi-modal dialog generation", "authors": ["Shuhe Wang", "Yuxian Meng", "Xiaofei Sun", "Fei Wu", "Rongbin Ouyang", "Rui Yan", "Tianwei Zhang", "Jiwei Li"], "description": "Multi-modal dialog modeling is of growing interest. In this work, we propose frameworks to resolve a specific case of multi-modal dialog generation that better mimics multi-modal dialog generation in the real world, where each dialog turn is associated with the visual context in which it takes place. Specifically, we propose to model the mutual dependency between text-visual features, where the model not only needs to learn the probability of generating the next dialog utterance given preceding dialog utterances and visual contexts, but also the probability of predicting the visual features in which a dialog utterance takes place, leading the generated dialog utterance specific to the visual context. We observe significant performance boosts over vanilla models when the mutual dependency between text and visual features is modeled. Code is available at https://github.com/ShannonAI/OpenViDial.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:hC7cP41nSMkC", "title": "When NAS meets watermarking: ownership verification of DNN models via cache side channels", "authors": ["Lou Xiaoxuan", "Guo Shangwei", "Zhang Tianwei", "Liu Yang"], "description": "We present a novel watermarking scheme to verify the ownership of DNN models. Existing solutions embedded watermarks into the model parameters, which were proven to be removable and detectable by an adversary to invalidate the protection. In contrast, we propose to implant watermarks into the model architectures. We design new algorithms based on Neural Architecture Search (NAS) to generate watermarked architectures, which are unique enough to represent the ownership, while maintaining high model usability. We further leverage cache side channels to extract and verify watermarks from the black-box models at inference. Theoretical analysis and extensive evaluations show our scheme has negligible impact on the model performance, and exhibits strong robustness against various model transformations.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [4, 3, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:8AbLer7MMksC", "title": "Watermarking pre-trained encoders in contrastive learning", "authors": ["Yutong Wu", "Han Qiu", "Tianwei Zhang", "Jiwei Li", "Meikang Qiu"], "description": "Contrastive learning has become a popular technique to pre-train image encoders, which could be used to build various downstream classification models in an efficient way. This process requires a large amount of data and computation resources. Hence, the pre-trained encoders are an important intellectual property that needs to be carefully protected. It is challenging to migrate existing watermarking techniques from the classification tasks to the contrastive learning scenario, as the owner of the encoder lacks the knowledge of the downstream tasks which will be developed from the encoder in the future. We propose the first watermarking methodology for the pre-trained encoders. We introduce a task-agnostic loss function to effectively embed into the encoder a backdoor as the watermark. This backdoor can still exist in any downstream models transferred from the encoder. Extensive evaluations over different\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [2, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:Z5m8FVwuT1cC", "title": "Jailbreaker: Automated jailbreak across multiple large language model chatbots", "authors": ["Gelei Deng", "Yi Liu", "Yuekang Li", "Kailong Wang", "Ying Zhang", "Zefeng Li", "Haoyu Wang", "Tianwei Zhang", "Yang Liu"], "description": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) services due to their exceptional proficiency in understanding and generating human-like text. LLM chatbots, in particular, have seen widespread adoption, transforming human-machine interactions. However, these LLM chatbots are susceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to elicit inappropriate or sensitive responses, contravening service policies. Despite existing attempts to mitigate such threats, our research reveals a substantial gap in our understanding of these vulnerabilities, largely due to the undisclosed defensive measures implemented by LLM service providers. In this paper, we present Jailbreaker, a comprehensive framework that offers an in-depth understanding of jailbreak attacks and countermeasures. Our work makes a dual contribution. First, we propose an innovative methodology inspired by time-based SQL injection techniques to reverse-engineer the defensive strategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat. This time-sensitive approach uncovers intricate details about these services' defenses, facilitating a proof-of-concept attack that successfully bypasses their mechanisms. Second, we introduce an automatic generation method for jailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of automated jailbreak generation across various commercial LLM chatbots. Our method achieves a promising average success rate of 21.58%, significantly outperforming the effectiveness of existing techniques. We have responsibly disclosed our findings to the concerned service\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:V3AGJWp-ZtQC", "title": "SoK: Rethinking sensor spoofing attacks against robotic vehicles from a systematic view", "authors": ["Yuan Xu", "Xingshuo Han", "Gelei Deng", "Jiwei Li", "Yang Liu", "Tianwei Zhang"], "description": "Robotic Vehicles (RVs) have gained great popularity over the past few years. Meanwhile, they are also demonstrated to be vulnerable to sensor spoofing attacks. Although a wealth of research works have presented various attacks, some key questions remain unanswered: are these existing works complete enough to cover all the sensor spoofing threats? If not, how many attacks are not explored, and how difficult is it to realize them?This paper answers the above questions by comprehensively systematizing the knowledge of sensor spoofing attacks against RVs. Our contributions are threefold. (1) We identify seven common attack paths in an RV system pipeline. We categorize and assess existing spoofing attacks from the perspectives of spoofer property, operation, victim characteristic and attack goal. Based on this systematization, we identify 4 interesting insights about spoofing attack designs. (2) We propose a\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [5, 2]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:1yQoGdGgb4wC", "title": "Verifiable, fair and privacy-preserving broadcast authorization for flexible data sharing in clouds", "authors": ["Jianfei Sun", "Guowen Xu", "Tianwei Zhang", "Xuehuan Yang", "Mamoun Alazab", "Robert H Deng"], "description": "The cloud-based data sharing technology with cryptographic primitives enables data owners to outsource data into paradigms and privately share information with arbitrary recipients without geographic barriers. However, we argue that most of existing efforts for outsourced data sharing are either inefficient, inflexible, or incompletely secure due to the following problems: (1) lack of efficient strategies for dynamically designating target ciphertexts to multiple recipients; (2) how to hide the identity of the recipient and (3) how to verify the correctness of outsourced ciphertext transformation without any denial. To the best of our knowledge, no previous work has thoroughly explored the above three issues, motivating us to design such an efficient and comprehensively secure outsourced data sharing mechanism. We design VF-PPBA, the first Verifiable, Fair and Privacy-preserving Broadcast Authorization framework for\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:olpn-zPbct0C", "title": "Hercules: Boosting the performance of privacy-preserving federated learning", "authors": ["Guowen Xu", "Xingshuo Han", "Shengmin Xu", "Tianwei Zhang", "Hongwei Li", "Xinyi Huang", "Robert H Deng"], "description": "In this paper, we address the problem of privacy-preserving federated neural network training with    users. We present Hercules,, an efficient and high-precision training framework that can tolerate collusion of up to    users. Hercules follows the POSEIDON framework proposed by Sav  et al.  (NDSS'21), but makes a qualitative leap in performance with the following contributions: (i) we design a novel parallel homomorphic computation method for matrix operations, which enables fast Single Instruction and Multiple Data (SIMD) operations over ciphertexts. For the multiplication of two    dimensional matrices, our method reduces the computation complexity from    to   . This greatly improves the training efficiency of the neural network since the ciphertext computation is dominated by the convolution operations; (ii) we present an efficient approximation on the sign function based on the composite\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [1, 6, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:NJ774b8OgUMC", "title": "Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only", "authors": ["Kangjie Chen", "Xiaoxuan Lou", "Guowen Xu", "Jiwei Li", "Tianwei Zhang"], "description": "Multi-label models have been widely used in various applications including image annotation and object detection. The fly in the ointment is its inherent vulnerability to backdoor attacks due to the adoption of deep learning techniques. However, all existing backdoor attacks exclusively require to modify training inputs (e.g., images), which may be impractical in real-world applications. In this paper, we aim to break this wall and propose the first clean-image backdoor attack, which only poisons the training labels without touching the training samples. Our key insight is that in a multi-label learning task, the adversary can just manipulate the annotations of training samples consisting of a specific set of classes to activate the backdoor. We design a novel trigger exploration method to find convert and effective triggers to enhance the attack performance. We also propose three target label selection strategies to achieve different goals. Experimental results indicate that our clean-image backdoor can achieve a 98% attack success rate while preserving the model's functionality on the benign inputs. Besides, the proposed clean-image backdoor can evade existing state-of-the-art defenses.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 7, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:P5F9QuxV20EC", "title": "Folden: -Fold Ensemble for Out-Of-Distribution Detection", "authors": ["Xiaoya Li", "Jiwei Li", "Xiaofei Sun", "Chun Fan", "Tianwei Zhang", "Fei Wu", "Yuxian Meng", "Jun Zhang"], "description": "Out-of-Distribution (OOD) detection is an important problem in natural language processing (NLP). In this work, we propose a simple yet effective framework Folden, which mimics the behaviors of OOD detection during training without the use of any external data. For a task with  training labels, Folden induces  sub-models, each of which is trained on a subset with  categories with the left category masked unknown to the sub-model. Exposing an unknown label to the sub-model during training, the model is encouraged to learn to equally attribute the probability to the seen  labels for the unknown label, enabling this framework to simultaneously resolve in- and out-distribution examples in a natural way via OOD simulations. Taking text classification as an archetype, we develop benchmarks for OOD detection using existing text classification datasets. By conducting comprehensive comparisons and analyses on the developed benchmarks, we demonstrate the superiority of Folden against current methods in terms of improving OOD detection performances while maintaining improved in-domain classification accuracy. The code and datasets can be found at: \\url{https://github.com/ShannonAI/kfolden-ood-detection}.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:WqliGbK-hY8C", "title": "On the (In) Security of Secure ROS2", "authors": ["Gelei Deng", "Guowen Xu", "Yuan Zhou", "Tianwei Zhang", "Yang Liu"], "description": "Robot Operating System (ROS) has been the mainstream platform for research and development of robotic applications. This platform is well-known for lacking security features and efficiency for distributed robotic computations. To address these issues, ROS2 is recently developed by utilizing the Data Distribution Service (DDS) to provide security support. Integrated with DDS, ROS2 is expected to establish the basis for trustworthy robotic ecosystems.   In this paper, we systematically study the security of the current ROS2 implementation from three perspectives. By abstracting the key functions from the ROS2 native implementation, we first formally describe the ROS2 system communication workflow and model it using a concurrent modeling language. Second, we verify the model with some key security properties through a model checker, and successfully identify four security vulnerabilities in ROS2's native\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:5awf1xo2G04C", "title": "Improving adversarial robustness of 3d point cloud classification models", "authors": ["Guanlin Li", "Guowen Xu", "Han Qiu", "Ruan He", "Jiwei Li", "Tianwei Zhang"], "description": "3D point cloud classification models based on deep neural networks were proven to be vulnerable to adversarial examples, with a quantity of novel attack techniques proposed by researchers recently. It is of paramount importance to preserve the robustness of 3D models under adversarial environments, considering their broad application in safety- and security-critical tasks. Unfortunately, existing defenses are not general enough to satisfactorily mitigate all types of attacks. In this paper, we design two innovative methodologies to improve the adversarial robustness of 3D point cloud classification models. (1) We introduce CCN, a novel point cloud architecture which can smooth and disrupt the adversarial perturbations. (2) We propose AMS, a novel data augmentation strategy to adaptively balance the model usability and robustness. Extensive evaluations indicate the integration of the two techniques provides\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [1, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:eflP2zaiRacC", "title": "Clean-annotation backdoor attack against lane detection systems in the wild", "authors": ["Xingshuo Han", "Guowen Xu", "Yuan Zhou", "Xuehuan Yang", "Jiwei Li", "Tianwei Zhang"], "description": null, "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [2, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:B3FOqHPlNUQC", "title": "Robust and privacy-preserving collaborative learning: A comprehensive survey", "authors": ["Shangwei Guo", "Xu Zhang", "Fei Yang", "Tianwei Zhang", "Yan Gan", "Tao Xiang", "Yang Liu"], "description": "With the rapid demand of data and computational resources in deep learning systems, a growing number of algorithms to utilize collaborative machine learning techniques, for example, federated learning, to train a shared deep model across multiple participants. It could effectively take advantage of the resources of each participant and obtain a more powerful learning system. However, integrity and privacy threats in such systems have greatly obstructed the applications of collaborative learning. And a large amount of works have been proposed to maintain the model integrity and mitigate the privacy leakage of training data during the training phase for different collaborative learning systems. Compared with existing surveys that mainly focus on one specific collaborative learning system, this survey aims to provide a systematic and comprehensive review of security and privacy researches in collaborative learning. Our survey first provides the system overview of collaborative learning, followed by a brief introduction of integrity and privacy threats. In an organized way, we then detail the existing integrity and privacy attacks as well as their defenses. We also list some open problems in this area and opensource the related papers on GitHub: https://github.com/csl-cqu/awesome-secure-collebrative-learning-papers.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:Tiz5es2fbqcC", "title": "Protecting your NLG models with semantic and robust watermarks", "authors": ["Tao Xiang", "Chunlong Xie", "Shangwei Guo", "Jiwei Li", "Tianwei Zhang"], "description": "Natural language generation (NLG) applications have gained great popularity due to the powerful deep learning techniques and large training corpus. The deployed NLG models may be stolen or used without authorization, while watermarking has become a useful tool to protect Intellectual Property (IP) of deep models. However, existing watermarking technologies using backdoors are easily detected or harmful for NLG applications. In this paper, we propose a semantic and robust watermarking scheme for NLG models that utilize unharmful phrase pairs as watermarks for IP protection. The watermarks give NLG models personal preference for some special phrase combinations. Specifically, we generate watermarks by following a semantic combination pattern and systematically augment the watermark corpus to enhance the robustness. Then, we embed these watermarks into an NLG model without misleading its original attention mechanism. We conduct extensive experiments and the results demonstrate the effectiveness, robustness, and undetectability of the proposed scheme.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:b0M2c_1WBrUC", "title": "Analysis and mitigation of function interaction risks in robot apps", "authors": ["Yuan Xu", "Tianwei Zhang", "Yungang Bao"], "description": "Robot apps are becoming more automated, complex and diverse. An app usually consists of many functions, interacting with each other and the environment. This allows robots to conduct various tasks. However, it also opens a new door for cyber attacks: adversaries can leverage these interactions to threaten the safety of robot operations. Unfortunately, this issue is rarely explored in past works.  We present the first systematic investigation about the function interactions in common robot apps. First, we disclose the potential risks and damages caused by malicious interactions. By investigating the relationships among different functions, we identify and categorize three types of interaction risks. Second, we propose RTron, a novel system to detect and mitigate these risks and protect the operations of robot apps. We introduce security policies for each type of risks, and design coordination nodes to enforce the\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:p__nRnzSRKYC", "title": "Pentestgpt: An llm-empowered automatic penetration testing tool", "authors": ["Gelei Deng", "Yi Liu", "V\u00edctor Mayoral-Vilches", "Peng Liu", "Yuekang Li", "Yuan Xu", "Tianwei Zhang", "Yang Liu", "Martin Pinzger", "Stefan Rass"], "description": "Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this research, we evaluate the performance of LLMs on real-world penetration testing tasks using a robust benchmark created from test machines with platforms. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining an integrated understanding of the overall testing scenario. In response to these insights, we introduce PentestGPT, an LLM-empowered automatic penetration testing tool that leverages the abundant domain knowledge inherent in LLMs. PentestGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PentestGPT not only outperforms LLMs with a task-completion increase of 228.6\\% compared to the \\gptthree model among the benchmark targets but also proves effective in tackling real-world penetration testing challenges. Having been open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:zLWjf1WUPmwC", "title": "Multi-target Backdoor Attacks for Code Pre-trained Models", "authors": ["Yanzhou Li", "Shangqing Liu", "Kangjie Chen", "Xiaofei Xie", "Tianwei Zhang", "Yang Liu"], "description": "Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multi-target attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experiments demonstrate that our approach can effectively and stealthily attack code-related downstream tasks.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:_Ybze24A_UAC", "title": "Secure decentralized image classification with multiparty homomorphic encryption", "authors": ["Guowen Xu", "Guanlin Li", "Shangwei Guo", "Tianwei Zhang", "Hongwei Li"], "description": "Decentralized image classification plays a key role in various scenarios due to its attractive properties, including tolerating high network latency and less prone to single-point failures. Unfortunately, training such a decentralized image classification model is more vulnerable to data privacy leaks compared to other distributed training frameworks. Existing efforts exclusively use differential privacy as the cornerstone to alleviate the threat to data privacy. However, differential privacy is implemented at the expense of accuracy, which goes against our motivation for designing an image classification model without loss of accuracy. To address this problem, we propose D 2 -MHE, the  first  secure and efficient decentralized training framework with lossless precision. Inspired by the latest developments in the homomorphic encryption technology, we design a multiparty version of Brakerski-Fan-Vercauteren (BFV), one of the\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:wbdj-CoPYUoC", "title": "Ownership verification of dnn architectures via hardware cache side channels", "authors": ["Xiaoxuan Lou", "Shangwei Guo", "Jiwei Li", "Tianwei Zhang"], "description": "Deep Neural Networks (DNN) are gaining higher commercial values in computer vision applications, e.g., image classification, video analytics, etc. This calls for urgent demands of the intellectual property (IP) protection of DNN models. In this paper, we present a novel watermarking scheme to achieve the ownership verification of DNN architectures. Existing works all embedded watermarks into the model parameters while treating the architecture as public property. These solutions were proven to be vulnerable by an adversary to detect or remove the watermarks. In contrast, we claim the model architectures as an important IP for model owners, and propose to implant watermarks into the architectures. We design new algorithms based on Neural Architecture Search (NAS) to generate watermarked architectures, which are unique enough to represent the ownership, while maintaining high model usability. Such\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 5, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:08ZZubdj9fEC", "title": "Astraea: A fair deep learning scheduler for multi-tenant gpu clusters", "authors": ["Zhisheng Ye", "Peng Sun", "Wei Gao", "Tianwei Zhang", "Xiaolin Wang", "Shengen Yan", "Yingwei Luo"], "description": "Modern GPU clusters are designed to support distributed Deep Learning jobs from multiple tenants concurrently. Each tenant may have varied and dynamic resource demands. Unfortunately, existing GPU schedulers fail to thoroughly consider the fairness among the tenants and jobs, which can result in unbalanced resource allocation and unfair user experience. In this article, we present an efficient solution to provide strong fairness while maintaining high scheduling effectiveness in multi-tenant GPU clusters. First, we introduce a novel Long-Term GPU-time Fairness metric, which can comprehensively evaluate the fairness at both the tenant and job levels, based on both the temporal and spatial impacts of resource allocation. Second, we design a new and practical GPU scheduler,  Astraea , to enforce the desired fairness among tenants and jobs. Large-scale evaluations show that  Astraea  can improve tenant\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:roLk4NBRz8UC", "title": "Detection and mitigation of security threats in cloud computing", "authors": ["Tianwei Zhang"], "description": "Infrastructure-as-a-Service (IaaS) clouds provide computation and storage services to enterprises and individuals with increased elasticity and low cost. Cloud customers rent resources in the form of virtual machines (VMs). However, these VMs may face various security threats.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 1, 2, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:eQOLeE2rZwMC", "title": "Self-heating effects in gate-all-around silicon nanowire MOSFETs: Modeling and analysis", "authors": ["Xin Huang", "Tianwei Zhang", "Rusheng Wang", "Changze Liu", "Yuchao Liu", "Ru Huang"], "description": "In this paper, an electro-thermal model is proposed for the first time to accurately investigate the self-heating effects in gate-all-around (GAA) silicon nanowire MOSFETs (SNWTs) for thermal-aware design optimization. The model is derived based on the equivalent thermal network method, in which the impacts of gate length dependence, nanowire diameter dependence and surface roughness on the nanowire channel thermal conductivity as well as the influence of unique GAA structure features on the heat dissipation are taken into account. The proposed model agrees well with the experimental results of SNWTs. Based on the model, the impacts of structure parameters on the current driving capabilities and heat dissipation of SNWTs are discussed. The developed electro-thermal model can be further applied to the thermal-aware design of SNWT-based circuits.", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:uLbwQdceFCQC", "title": "PriVDT: An Efficient Two-Party Cryptographic Framework for Vertical Decision Trees", "authors": ["Hanxiao Chen", "Hongwei Li", "Yingzhe Wang", "Meng Hao", "Guowen Xu", "Tianwei Zhang"], "description": "Privacy-preserving decision trees (DTs) in vertical federated learning are one of the most effective tools to facilitate various privacy-critical applications in reality. However, the main bottleneck of current solutions is their huge overhead, mainly due to the adoption of communication-heavy bit decomposition to realize complex non-linear operations, such as comparison and division. In this paper, we present  PriVDT , an efficient two-party framework for private vertical DT training and inference in the offline/online paradigm. Specifically, we customize several cryptographic building blocks based on an advanced primitive, Function Secret Sharing (FSS). First, we construct an optimized comparison protocol to improve the efficiency via reducing the invocation of FSS evaluations. Second, we devise an efficient and privacy-enhanced division protocol without revealing the range of divisors, which utilizes the above comparison\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:1qzjygNMrQYC", "title": "ESB-FL: Efficient and Secure Blockchain-Based Federated Learning with Fair Payment", "authors": ["Biwen Chen", "Honghong Zeng", "Tao Xiang", "Shangwei Guo", "Tianwei Zhang", "Yang Liu"], "description": "Federated learning is a technique that enables multiple parties to collaboratively train a model without sharing raw private data, and it is ideal for smart healthcare. However, it raises new privacy concerns due to the risk of privacy-sensitive medical data leakage. It is not until recently that the privacy-preserving FL (PPFL) has been introduced as a solution to ensure the privacy of training processes. Unfortunately, most existing PPFL schemes are highly dependent on complex cryptographic mechanisms or fail to guarantee the accuracy of training models. Besides, there has been little research on the fairness of the payment procedure in the PPFL with incentive mechanisms. To address the above concerns, we first construct an efficient non-interactive designated decryptor function encryption (NDD-FE) scheme to protect the privacy of training data while maintaining high communication performance. We then propose\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 4, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:LPZeul_q3PIC", "title": "ADS-lead: Lifelong anomaly detection in autonomous driving systems", "authors": ["Xingshuo Han", "Yuan Zhou", "Kangjie Chen", "Han Qiu", "Meikang Qiu", "Yang Liu", "Tianwei Zhang"], "description": "Autonomous Vehicles (AVs) are closely connected in the Cooperative Intelligent Transportation System (C-ITS). They are equipped with various sensors and controlled by Autonomous Driving Systems (ADSs) to provide high-level autonomy. The vehicles exchange different types of real-time data with each other, which can help reduce traffic accidents and congestion, and improve the efficiency of transportation systems. However, when interacting with the environment, AVs suffer from a broad attack surface, and the sensory data are susceptible to anomalies caused by faults, sensor malfunctions, or attacks, which may jeopardize traffic safety and result in serious accidents. In this paper, we propose  ADS-Lead , an efficient collaborative anomaly detection methodology to protect the lane-following mechanism of ADSs.  ADS-Lead  is equipped with a novel transformer-based one-class classification model to identify\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [3, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:abG-DnoFyZgC", "title": "Fingerprinting multi-exit deep neural network models via inference time", "authors": ["Tian Dong", "Han Qiu", "Tianwei Zhang", "Jiwei Li", "Hewu Li", "Jialiang Lu"], "description": "Transforming large deep neural network (DNN) models into the multi-exit architectures can overcome the overthinking issue and distribute a large DNN model on resource-constrained scenarios (e.g. IoT frontend devices and backend servers) for inference and transmission efficiency. Nevertheless, intellectual property (IP) protection for the multi-exit models in the wild is still an unsolved challenge. Previous efforts to verify DNN model ownership mainly rely on querying the model with specific samples and checking the responses, e.g., DNN watermarking and fingerprinting. However, they are vulnerable to adversarial settings such as adversarial training and are not suitable for the IP verification for multi-exit DNN models. In this paper, we propose a novel approach to fingerprint multi-exit models via inference time rather than inference predictions. Specifically, we design an effective method to generate a set of fingerprint samples to craft the inference process with a unique and robust inference time cost as the evidence for model ownership. We conduct extensive experiments to prove the uniqueness and robustness of our method on three structures (ResNet-56, VGG-16, and MobileNet) and three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial settings.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:IjCSPb-OGe4C", "title": "Security verification of hardware-enabled attestation protocols", "authors": ["Tianwei Zhang", "Jakub Szefer", "Ruby B Lee"], "description": "Hardware-software security architectures can significantly improve the security provided to computer users. However, we are lacking a security verification methodology that can provide design-time verification of the security properties provided by such architectures. While verification of an entire hardware-software security architecture is very difficult today, this paper proposes a methodology for verifying essential aspects of the architecture. We use attestation protocols proposed by different hardware security architectures as examples of such essential aspects. Attestation is an important and interesting new requirement for having trust in a remote computer, e.g., in a cloud computing scenario. We use a finite-state model checker to model the system and the attackers, and check the security of the protocols against attacks. We provide new actionable heuristics for designing invariants that are validated by the model\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:XiVPGOgt02cC", "title": "Simc 2.0: Improved secure ml inference against malicious clients", "authors": ["Guowen Xu", "Xingshuo Han", "Tianwei Zhang", "Shengmin Xu", "Jianting Ning", "Xinyi Huang", "Hongwei Li", "Robert H Deng"], "description": "In this paper, we study the problem of secure ML inference against a malicious client and a semi-trusted server such that the client only learns the inference output while the server learns nothing. This problem is first formulated by Lehmkuhl  et al.  with a solution (MUSE, Usenix Security'21), whose performance is then substantially improved by Chandran  et al. 's work (SIMC, USENIX Security'22). However, there still exists a nontrivial gap in these efforts towards practicality, giving the challenges of overhead reduction and secure inference acceleration in an all-round way. Based on this, we propose SIMC 2.0, which complies with the underlying structure of SIMC, but significantly optimizes both the linear and non-linear layers of the model. Specifically, (1) we design a new coding method for parallel homomorphic computation between matrices and vectors. (2) We reduce the size of the garbled circuit (GC) (used to calculate non\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:evX43VCCuoAC", "title": "Backdoor Attacks with Input-unique Triggers in NLP", "authors": ["Xukun Zhou", "Jiwei Li", "Tianwei Zhang", "Lingjuan Lyu", "Muqiao Yang", "Jun He"], "description": "Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related triggers by continuing writing the input with a language model like GPT2. The generated sentence is used as the backdoor trigger. This strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. Experimental results show that the IDBA attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. In addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:ILKRHgRFtOwC", "title": "Boosting Distributed Full-graph GNN Training with Asynchronous One-bit Communication", "authors": ["Meng Zhang", "Qinghao Hu", "Peng Sun", "Yonggang Wen", "Tianwei Zhang"], "description": "Training Graph Neural Networks (GNNs) on large graphs is challenging due to the conflict between the high memory demand and limited GPU memory. Recently, distributed full-graph GNN training has been widely adopted to tackle this problem. However, the substantial inter-GPU communication overhead can cause severe throughput degradation. Existing communication compression techniques mainly focus on traditional DNN training, whose bottleneck lies in synchronizing gradients and parameters. We find they do not work well in distributed GNN training as the barrier is the layer-wise communication of features during the forward pass & feature gradients during the backward pass. To this end, we propose an efficient distributed GNN training framework Sylvie, which employs one-bit quantization technique in GNNs and further pipelines the curtailed communication with computation to enormously shrink the overhead while maintaining the model quality. In detail, Sylvie provides a lightweight Low-bit Module to quantize the sent data and dequantize the received data back to full precision values in each layer. Additionally, we propose a Bounded Staleness Adaptor to control the introduced staleness to achieve further performance enhancement. We conduct theoretical convergence analysis and extensive experiments on various models & datasets to demonstrate Sylvie can considerably boost the training throughput by up to 28.1x.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=20&pagesize=80&citation_for_view=9vpiYDIAAAAJ:7T2F9Uy0os0C", "title": "Cooperative collision avoidance in multirobot systems using fuzzy rules and velocity obstacles", "authors": ["Wenbing Tang", "Yuan Zhou", "Tianwei Zhang", "Yang Liu", "Jing Liu", "Zuohua Ding"], "description": "Collision avoidance is critical in multirobot systems. Most of the current methods for collision avoidance either require high computation costs (e.g., velocity obstacles and mathematical optimization) or cannot always provide safety guarantees (e.g., learning-based methods). Moreover, they cannot deal with uncertain sensing data and linguistic requirements (e.g., the speed of a robot should not be large when it is near to other robots). Hence, to guarantee real-time collision avoidance and deal with linguistic requirements, a distributed and hybrid motion planning method, named Fuzzy-VO, is proposed for multirobot systems. It contains two basic components: fuzzy rules, which can deal with linguistic requirements and compute motion efficiently, and velocity obstacles (VOs), which can generate collision-free motion effectively. The Fuzzy-VO applies an intruder selection method to mitigate the exponential increase of\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:9Nmd_mFXekcC", "title": "Color Backdoor: A Robust Poisoning Attack in Color Space", "authors": ["Wenbo Jiang", "Hongwei Li", "Guowen Xu", "Tianwei Zhang"], "description": "Backdoor attacks against neural networks have been intensively investigated, where the adversary compromises the integrity of the victim model, causing it to make wrong predictions for inference samples containing a specific trigger. To make the trigger more imperceptible and human-unnoticeable, a variety of stealthy backdoor attacks have been proposed, some works employ imperceptible perturbations as the backdoor triggers, which restrict the pixel differences of the triggered image and clean image. Some works use special image styles (eg, reflection, Instagram filter) as the backdoor triggers. However, these attacks sacrifice the robustness, and can be easily defeated by common preprocessing-based defenses. This paper presents a novel color backdoor attack, which can exhibit robustness and stealthiness at the same time. The key insight of our attack is to apply a uniform color space shift for all pixels as the trigger. This global feature is robust to image transformation operations and the triggered samples maintain natural-looking. To find the optimal trigger, we first define naturalness restrictions through the metrics of PSNR, SSIM and LPIPS. Then we employ the Particle Swarm Optimization (PSO) algorithm to search for the optimal trigger that can achieve high attack effectiveness and robustness while satisfying the restrictions. Extensive experiments demonstrate the superiority of PSO and the robustness of color backdoor against different mainstream backdoor defenses.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:hkOj_22Ku90C", "title": "Gnn-sl: Sequence labeling based on nearest examples via gnn", "authors": ["Shuhe Wang", "Yuxian Meng", "Rongbin Ouyang", "Jiwei Li", "Tianwei Zhang", "Lingjuan Lyu", "Guoyin Wang"], "description": "To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging examples benefit the model prediction, we construct a heterogeneous graph, and leverage graph neural networks (GNNs) to transfer information between the retrieved tagging examples and the input word sequence. The augmented node which aggregates information from neighbors is used to do prediction. This strategy enables the model to directly acquire similar tagging examples and improves the general quality of predictions. We conduct a variety of experiments on three typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2) on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the CWS task, and results comparable to SOTA performances on NER datasets, and POS datasets.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:t6usbXjVLHcC", "title": "Privacy-preserving decentralized deep learning with multiparty homomorphic encryption", "authors": ["Guowen Xu", "Guanlin Li", "Shangwei Guo", "Tianwei Zhang", "Hongwei Li"], "description": "Decentralized deep learning plays a key role in collaborative model training due to its attractive properties, including tolerating high network latency and less prone to single-point failures. Unfortunately, such a training mode is more vulnerable to data privacy leaks compared to other distributed training frameworks. Existing efforts exclusively use differential privacy as the cornerstone to alleviate the data privacy threat. However, it is still not clear whether differential privacy can provide a satisfactory utility-privacy trade-off for model training, due to its inherent contradictions. To address this problem, we propose D-MHE, the first secure and efficient decentralized training framework with lossless precision. Inspired by the latest developments in the homomorphic encryption technology, we design a multiparty version of Brakerski-Fan-Vercauteren (BFV), one of the most advanced cryptosystems, and use it to implement private gradient updates of users'local models. D-MHE can reduce the communication complexity of general Secure Multiparty Computation (MPC) tasks from quadratic to linear in the number of users, making it very suitable and scalable for large-scale decentralized learning systems. Moreover, D-MHE provides strict semantic security protection even if the majority of users are dishonest with collusion. We conduct extensive experiments on MNIST and CIFAR-10 datasets to demonstrate the superiority of D-MHE in terms of model accuracy, computation and communication cost compared with existing schemes.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 3, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:Fu2w8maKXqMC", "title": "Alleviating robust overfitting of adversarial training with consistency regularization", "authors": ["Shudong Zhang", "Haichang Gao", "Tianwei Zhang", "Yunyi Zhou", "Zihui Wu"], "description": "Adversarial training (AT) has proven to be one of the most effective ways to defend Deep Neural Networks (DNNs) against adversarial attacks. However, the phenomenon of robust overfitting, i.e., the robustness will drop sharply at a certain stage, always exists during AT. It is of great importance to decrease this robust generalization gap in order to obtain a robust model. In this paper, we present an in-depth study towards the robust overfitting from a new angle. We observe that consistency regularization, a popular technique in semi-supervised learning, has a similar goal as AT and can be used to alleviate robust overfitting. We empirically validate this observation, and find a majority of prior solutions have implicit connections to consistency regularization. Motivated by this, we introduce a new AT solution, which integrates the consistency regularization and Mean Teacher (MT) strategy into AT. Specifically, we introduce a teacher model, coming from the average weights of the student models over the training steps. Then we design a consistency loss function to make the prediction distribution of the student models over adversarial examples consistent with that of the teacher model over clean samples. Experiments show that our proposed method can effectively alleviate robust overfitting and improve the robustness of DNN models against common adversarial attacks.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [1, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:uWQEDVKXjbEC", "title": "A general framework for defending against backdoor attacks via influence graph", "authors": ["Xiaofei Sun", "Jiwei Li", "Xiaoya Li", "Ziyao Wang", "Tianwei Zhang", "Han Qiu", "Fei Wu", "Chun Fan"], "description": "In this work, we propose a new and general framework to defend against backdoor attacks, inspired by the fact that attack triggers usually follow a \\textsc{specific} type of attacking pattern, and therefore, poisoned training examples have greater impacts on each other during training. We introduce the notion of the {\\it influence graph}, which consists of nodes and edges respectively representative of individual training points and associated pair-wise influences. The influence between a pair of training points represents the impact of removing one training point on the prediction of another, approximated by the influence function \\citep{koh2017understanding}. Malicious training points are extracted by finding the maximum average sub-graph subject to a particular size. Extensive experiments on computer vision and natural language processing tasks demonstrate the effectiveness and generality of the proposed framework.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:4OULZ7Gr8RgC", "title": "Parameter estimation for the SEIR model using recurrent nets", "authors": ["Chun Fan", "Yuxian Meng", "Xiaofei Sun", "Fei Wu", "Tianwei Zhang", "Jiwei Li"], "description": "The standard way to estimate the parameters  (e.g., the transmission rate ) of an SEIR model is to use grid search, where simulations are performed on each set of parameters, and the parameter set leading to the least  distance between predicted number of infections and observed infections is selected. This brute-force strategy is not only time consuming, as simulations are slow when the population is large, but also inaccurate, since it is impossible to enumerate all parameter combinations. To address these issues, in this paper, we propose to transform the non-differentiable problem of finding optimal  to a differentiable one, where we first train a recurrent net to fit a small number of simulation data. Next, based on this recurrent net that is able to generalize SEIR simulations, we are able to transform the objective to a differentiable one with respect to , and straightforwardly obtain its optimal value. The proposed strategy is both time efficient as it only relies on a small number of SEIR simulations, and accurate as we are able to find the optimal  based on the differentiable objective. On two COVID-19 datasets, we observe that the proposed strategy leads to significantly better parameter estimations with a smaller number of simulations.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:Wp0gIr-vW9MC", "title": "Generating Adversarial Examples with Controllable Non-transferability", "authors": ["Renzhi Wang", "Tianwei Zhang", "Xiaofei Xie", "Lei Ma", "Cong Tian", "Felix Juefei-Xu", "Yang Liu"], "description": "Adversarial attacks against Deep Neural Networks have been widely studied. One significant feature that makes such attacks particularly powerful is transferability, where the adversarial examples generated from one model can be effective against other similar models as well. A large number of works have been done to increase the transferability. However, how to decrease the transferability and craft malicious samples only for specific target models are not explored yet. In this paper, we design novel attack methodologies to generate adversarial examples with controllable non-transferability. With these methods, an adversary can efficiently produce precise adversarial examples to attack a set of target models he desires, while keeping benign to other models. The first method is Reversed Loss Function Ensemble, where the adversary can craft qualified examples from the gradients of a reversed loss function. This approach is effective for the white-box and gray-box settings. The second method is Transferability Classification: the adversary trains a transferability-aware classifier from the perturbations of adversarial examples. This classifier further provides the guidance for the generation of non-transferable adversarial examples. This approach can be applied to the black-box scenario. Evaluation results demonstrate the effectiveness and efficiency of our proposed methods. This work opens up a new route for generating adversarial examples with new features and applications.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:j8SEvjWlNXcC", "title": "A survey on cybersecurity attacks and defenses for unmanned aerial systems", "authors": ["Zhaoxuan Wang", "Yang Li", "Shihao Wu", "Yuan Zhou", "Libin Yang", "Yuan Xu", "Tianwei Zhang", "Quan Pan"], "description": "In recent years, unmanned aerial systems (UAS) have been widely used in both military and civilian fields. However, their open-source software and protocols have made their security vulnerable, resulting in a growing number of cybersecurity issues. This paper provides a comprehensive review of UAS cybersecurity research, with a focus on attack and defense technologies. Regarding UAS being a system that integrates software and hardware and can work independently with complex tasks, this paper analyzes the UAS architecture and classifies security threats into four categories: communication network security, software security, payload security, and intelligent security. Additionally, it provides an overview of existing threat assessment methods. This paper also highlights representative research progress in UAS cyberattacks and defense technologies in the four identified categories. Finally, this paper\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 1]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:ZfRJV9d4-WMC", "title": "Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks", "authors": ["Jialai Wang", "Ziyuan Zhang", "Meiqi Wang", "Han Qiu", "Tianwei Zhang", "Qi Li", "Zongpeng Li", "Tao Wei", "Chao Zhang"], "description": "Bit-flip attacks (BFAs) have attracted substantial attention recently, in which an adversary could tamper with a small number of model parameter bits to break the integrity of DNNs. To mitigate such threats, a batch of defense methods are proposed, focusing on the untargeted scenarios. Unfortunately, they either require extra trustworthy applications or make models more vulnerable to targeted BFAs. Countermeasures against targeted BFAs, stealthier and more purposeful by nature, are far from well established. In this work, we propose Aegis, a novel defense method to mitigate targeted BFAs. The core observation is that existing targeted attacks focus on flipping critical bits in certain important layers. Thus, we design a dynamic-exit mechanism to attach extra internal classifiers (ICs) to hidden layers. This mechanism enables input samples to early-exit from different layers, which effectively upsets the adversary\u2019s attack plans. Moreover, the dynamic-exit mechanism randomly selects ICs for predictions during each inference to significantly increase the attack cost for the adaptive attacks where all defense mechanisms are transparent to the adversary. We further propose a robustness training strategy to adapt ICs to the attack scenarios by simulating BFAs during the IC training phase, to increase model robustness. Extensive evaluations over four well-known datasets and two popular DNN structures reveal that Aegis could effectively mitigate different state-of-the-art targeted attacks, reducing attack success rate by 5-10\u00d7, significantly outperforming existing defense methods. We open source the code of Aegis 1.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:BwyfMAYsbu0C", "title": "One-bit flip is all you need: When bit-flip attack meets model training", "authors": ["Jianshuo Dong", "Han Qiu", "Yiming Li", "Tianwei Zhang", "Yuanjie Li", "Zeqi Lai", "Chao Zhang", "Shu-Tao Xia"], "description": "Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by flipping only one critical bit on average in the deployment stage. Moreover, our attack still poses a significant threat even when defenses are employed. The codes for reproducing main experiments are available at https://github. com/jianshuod/TBA.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:eMMeJKvmdy0C", "title": "Incremental learning, incremental backdoor threats", "authors": ["Wenbo Jiang", "Tianwei Zhang", "Han Qiu", "Hongwei Li", "Guowen Xu"], "description": "Class incremental learning from a pre-trained DNN model is gaining lots of popularity. Unfortunately, the pre-trained model also introduces a new attack vector, which enables an adversary to inject a backdoor into it and further compromise the downstream models learned from it. Prior works proposed backdoor attacks against the pre-trained models in the transfer learning scenario. However, they become less effective when the adversary does not have the knowledge of the downstream tasks or new data, which is more practical and considered in this paper. To this end, we design the first latent backdoor attacks against incremental learning. We propose two novel techniques, which can effectively and stealthily embed a backdoor into the pre-trained model. Such backdoor can only be activated when the pre-trained model is extended to a downstream model with incremental learning. It has a very high attack\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:WA5NYHcadZ8C", "title": "Primo: Practical {Learning-Augmented} Systems with Interpretable Models", "authors": ["Qinghao Hu", "Harsha Nori", "Peng Sun", "Yonggang Wen", "Tianwei Zhang"], "description": "While machine learning has demonstrated remarkable performance in various computer systems, some substantial flaws can prohibit its deployment in practice, including opaque decision processes, poor generalization and robustness, as well as exorbitant training and inference overhead. Motivated by these deficiencies, we introduce Primo, a unified framework for developers to design practical learning-augmented systems. Specifically,(1) Primo provides two interpretable models (PrAM and PrDT), as well as a Distill Engine, to support different system scenarios and deployment requirements.(2) It adopts Bayes Optimization to automatically identify the optimal model pruning strategy and hyperparameter configuration.(3) It also implements two tools, Monotonic Constraint and Counterfactual Explanation, to achieve transparent debugging and guided model adjustment. Primo can be applied to different types of learning-augmented systems. Evaluations on three state-of-the-art systems show that Primo can provide clear model interpretations, better system performance, and lower deployment costs.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:sSrBHYA8nusC", "title": "DefQ: Defensive Quantization Against Inference Slow-Down Attack for Edge Computing", "authors": ["Han Qiu", "Tianwei Zhang", "Tianzhu Zhang", "Hongyu Li", "Meikang Qiu"], "description": "The novel multiexit deep neural network (DNN) architectures provide a new optimization solution for efficient model inference in edge systems. Inference of most samples can be completed within the first few layers on an edge device without the need to transmit them to a remote server. This can significantly increase the inference speed and system throughput, which is particularly beneficial to the resource-constrained scenarios. Unfortunately, researchers proposed an inference slow-down attack against this technique, where an external adversary can add imperceptible perturbations on clean samples to invalidate the multiexit mechanism. In this article, we propose a defensive quantization ( DefQ ) method as the first defense against the inference slow-down attack. It is designed to be lightweight and can be easily implemented in off-the-shelf camera sensors. Particularly,  DefQ  introduces a novel quantization\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:VOx2b1Wkg3QC", "title": "NASPY: Automated extraction of automated machine learning models", "authors": ["Xiaoxuan Lou", "Shangwei Guo", "Jiwei Li", "Yaoxin Wu", "Tianwei Zhang"], "description": "We present NASPY, an end-to-end adversarial framework to extract the networkarchitecture of deep learning models from Neural Architecture Search (NAS). Existing works about model extraction attacks mainly focus on conventional DNN models with very simple operations, or require heavy manual analysis with lots of domain knowledge.  In contrast, NASPY introduces seq2seq models to automatically identify novel and complicated operations (e.g., separable convolution,dilated convolution) from hardware side-channel sequences. We design two models (RNN-CTC and transformer), which can achieve only 3.2% and 11.3% error rates for operation prediction.  We further present methods to recover the model hyper-parameters and topology from the operation sequence .  With these techniques, NASPY is able to extract the complete NAS model architecture with high fidelity and automation, which are rarely analyzed before.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:f2IySw72cVMC", "title": "A unified anomaly detection methodology for lane-following of autonomous driving systems", "authors": ["Xingshuo Han", "Kangjie Chen", "Yuan Zhou", "Meikang Qiu", "Chun Fan", "Yang Liu", "Tianwei Zhang"], "description": "Autonomous Vehicles (AVs) are equipped with various sensors and controlled by Autonomous Driving Systems (ADSs) to provide high-level autonomy. When interacting with the environment, AVs suffer from a broad attack surface, and the sensory data are susceptible to anomalies caused by faults, sensor malfunctions, or attacks, which may jeopardize traffic safety and result in serious accidents. Most of the current works focus on anomaly detection of specific attacks, such as GPS spoofing or traffic sign attacks. There are no works on scenario-aware anomaly detection for ADSs. In this paper, focusing on the lane-following scenario, we introduce a novel transformer-based one-class classification model to identify time series anomalies and adversarial image examples. It can detect GPS spoofing, traffic sign recognition and lane detection attacks with high efficiency and accuracy. We further design a Swin\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:p2g8aNsByqUC", "title": "A Novel Verifiable Fingerprinting Scheme for Generative Adversarial Networks", "authors": ["Guanlin Li", "Guowen Xu", "Han Qiu", "Shangwei Guo", "Run Wang", "Jiwei Li", "Tianwei Zhang"], "description": "This paper presents a novel fingerprinting scheme for the Intellectual Property (IP) protection of Generative Adversarial Networks (GANs). Prior solutions for classification models adopt adversarial examples as the fingerprints, which can raise stealthiness and robustness problems when they are applied to the GAN models. Our scheme constructs a composite deep learning model from the target GAN and a classifier. Then we generate stealthy fingerprint samples from this composite model, and register them to the classifier for effective ownership verification. This scheme inspires three concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct extensive experiments to show that our solutions outperform existing strategies in terms of stealthiness, functionality-preserving and unremovability.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:rO6llkc54NcC", "title": "PEEL: A Provable Removal Attack on Deep Hiding", "authors": ["Tao Xiang", "Hangcheng Liu", "Shangwei Guo", "Tianwei Zhang"], "description": "Deep hiding, embedding images into another using deep neural networks, has shown its great power in increasing the message capacity and robustness. In this paper, we conduct an in-depth study of state-of-the-art deep hiding schemes and analyze their hidden vulnerabilities. Then, according to our observations and analysis, we propose a novel ProvablE rEmovaL attack (PEEL) using image inpainting to remove secret images from containers without any prior knowledge about the deep hiding scheme. We also propose a systemic methodology to improve the efficiency and image quality of PEEL by carefully designing a removal strategy and fully utilizing the visual information of containers. Extensive evaluations show our attacks can completely remove secret images and has negligible impact on the quality of containers.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:g5m5HwL7SMYC", "title": "Towards practical cloud offloading for low-cost ground vehicle workloads", "authors": ["Yuan Xu", "Tianwei Zhang", "Jimin Han", "Sa Wang", "Yungang Bao"], "description": "Low-cost Ground Vehicles (LGVs) have been widely adopted to conduct various tasks in our daily life. However, the limited on-board battery capacity and computation resources prevent LGVs from taking more complex and intelligent workloads. A promising approach is to offload the computation from local LGVs to remote servers. However, current cloud-robotic research and platforms are still at a very early stage. Compared to other systems and devices, optimizing LGV workload offloading faces more challenges, such as the uncertainty of environments and the mobility feature of devices.In this paper, we explore the opportunities of optimizing cloud offloading of LGV workloads from the perspectives of performance, energy efficiency and network robustness. We first build an analytical model to reveal the computation role and impact of each function in LGV workloads. Then we propose several optimization\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:vDijr-p_gm4C", "title": "YI ZENG", "authors": ["Yi Zeng", "Si Chen", "Won Park", "Z Morley Mao", "Jin Ming", "Ruoxi Jia", "Meikang Qiu", "Tianhao Wang", "Ming Jin", "Han Qiu", "Shangwei Guo", "Tianwei Zhang", "Bhavani Thuraisingham", "Tao Xiang", "Yang Liu", "Gerard Memmi", "Qinkai Zheng", "Huaxi Gu", "Wenting Wei", "Yantao Guo", "Zihao Qi", "Wencheng Chen", "Yanzhe Huang", "Dan Zhu", "Zhihao Xue", "Jian Xiong", "Meiqin Liu"], "description": "YI ZENG Page 1 YI ZENG (858)-952-2135 yizeng@vt.edu Google Scholar Github LinkedIn \nWebpage EDUCATION Virginia Polytechnic Institute and State University May. 2021 - Present \nDoctor of Philosophy in Computer Engineering Major GPA: 4.000/4.0 University of California - \nSan Diego Aug. 2019 - Mar. 2021 Master of Science in Machine Learning and Data Science \nMajor GPA: 3.855/4.0 Xidian University Sep. 2015 - Jun. 2019 Bachelor of Engineering in \nElectrical and Information Engineering Major GPA: 3.810/4.0 HONORS & AWARDS \u2022 Best Paper \nAward, 20th International Conf. on Alg.o & Archit. for Parallel Processing (ICA3PP), 2020; \u2022 Best \nDegree Paper Award, Xidian University, Top 2%, 2019; \u2022 Outstanding Academic Scholarship, \nXidian University, Top 10%, year 2015, 2016, 2017, 2018; SELECTED PUBLICATIONS \n& MANUSCRIPTS (i) Adversarial Unlearning of Backdoors via Implicit Hypergradient Yi \u2026", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 0, 0, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:4MWp96NkSFoC", "title": "Erase and Repair: An Efficient Box-Free Removal Attack on High-Capacity Deep Hiding", "authors": ["Hangcheng Liu", "Tao Xiang", "Shangwei Guo", "Han Li", "Tianwei Zhang", "Xiaofeng Liao"], "description": "Deep hiding, embedding images with others using deep neural networks, has demonstrated impressive efficacy in increasing the message capacity and robustness of secret sharing. In this paper, we challenge the robustness of existing deep hiding schemes by preventing the recovery of secret images, building on our in-depth study of state-of-the-art deep hiding schemes and their vulnerabilities. Leveraging our analysis, we first propose a simple box-free removal attack on deep hiding that does not require any prior knowledge of the deep hiding schemes. To improve the removal performance on the deep hiding schemes that may be enhanced by adversarial training, we further design a more powerful removal attack, efficient box-free removal attack (EBRA), which employs image inpainting techniques to remove secret images from container images. In addition, to ensure the effectiveness of our attack and\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:URolC5Kub84C", "title": "Privacy-Aware and Security-Enhanced Efficient Matchmaking Encryption", "authors": ["Jianfei Sun", "Guowen Xu", "Tianwei Zhang", "Xuehuan Yang", "Mamoun Alazab", "Robert H Deng"], "description": "Data sharing technologies enable users to outsource data and privately share information with arbitrary recipients without geographic barriers. However, existing efforts for secure data sharing are either inflexible, insufficiently-secure or inefficient. In this paper, we invent PS-ME, the first Privacy-aware and Security-enhanced efficient Matchmaking Encryption (ME) for flexible data sharing. To be more specific, we first formulate an identity-based broadcast matchmaking encryption (IB-BME) for one-to-many data sharing, which enables both participants to specify respective access policies to the encrypted data, such that the data can be revealed by multiple recipients in the case that both access policies are satisfied. In IB-BME, a general matchmaking transformation solution realizing one-to-many sharing is initialized. We also formulate the PS-ME with the general matchmaking transformation solution of IB-BME as the\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:EkHepimYqZsC", "title": "GuardHFL: Privacy Guardian for Heterogeneous Federated Learning", "authors": ["Hanxiao Chen", "Meng Hao", "Hongwei Li", "Kangjie Chen", "Guowen Xu", "Tianwei Zhang", "Xilin Zhang"], "description": "Heterogeneous federated learning (HFL) enables clients with different computation and communication capabilities to collaboratively train their own customized models via a query-response paradigm on auxiliary datasets. However, such a paradigm raises serious privacy concerns due to the leakage of highly sensitive query samples and response predictions. We put forth GuardHFL, the first-of-its-kind efficient and privacy-preserving HFL framework. GuardHFL is equipped with a novel HFL-friendly secure querying scheme built on lightweight secret sharing and symmetric-key techniques. The core of GuardHFL is two customized multiplication and comparison protocols, which substantially boost the execution efficiency. Extensive evaluations demonstrate that GuardHFL significantly outperforms the alternative instantiations based on existing state-of-the-art techniques in both runtime and communication cost.", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:JQOojiI6XY0C", "title": "Mind your heart: Stealthy backdoor attack on dynamic deep neural network in edge computing", "authors": ["Tian Dong", "Ziyuan Zhang", "Han Qiu", "Tianwei Zhang", "Hewu Li", "Terry Wang"], "description": "Transforming off-the-shelf deep neural network (DNN) models into dynamic multi-exit architectures can achieve inference and transmission efficiency by fragmenting and distributing a large DNN model in edge computing scenarios (e.g., edge devices and cloud servers). In this paper, we propose a novel backdoor attack specifically on the dynamic multi-exit DNN models. Particularly, we inject a backdoor by poisoning one DNN model\u2019s shallow hidden layers targeting not this vanilla DNN model but only its dynamically deployed multi-exit architectures. Our backdoored vanilla model behaves normally on performance and cannot be activated even with the correct trigger. However, the backdoor will be activated when the victims acquire this model and transform it into a dynamic multi-exit architecture at their deployment. We conduct extensive experiments to prove the effectiveness of our attack on three structures\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:nrtMV_XWKgEC", "title": "PriFR: Privacy-preserving Large-scale File Retrieval System via Blockchain for Encrypted Cloud Data", "authors": ["Hao Ren", "Guowen Xu", "Han Qi", "Tianwei Zhang"], "description": "As a fundamental and commonly used service, file retrieval has been extensively studied by information retrieval, cryptography, and big data communities. In this paper, we consider the problem of privacy-preserving file retrieval. A new framework named PriFR is proposed by integrating the blockchain and cloud computing infrastructures. The large-scale original files are encrypted and outsourced to the public cloud server. The encrypted retrieval indexes are stored on the full nodes in the blockchain to support traceable and unforgeable retrieval services. This design embraces the benefits brought by both cloud computing and blockchain. For the first time, PriFR decomposes the file retrieval problem into the numerical query and keyword search on the file metadata. In doing so, each file can be characterized more precisely than the traditional keyword search based schemes. In addition to functionality, PriFR only\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:fEOibwPWpKIC", "title": "Automatic Transformation Search Against Deep Leakage from Gradients", "authors": ["Wei Gao", "Xu Zhang", "Shangwei Guo", "Tianwei Zhang", "Tao Xiang", "Han Qiu", "Yonggang Wen", "Yang Liu"], "description": "Collaborative learning has gained great popularity due to its benefit of data privacy protection: participants can jointly train a Deep Learning model without sharing their training sets. However, recent works discovered that an adversary can fully recover the sensitive training samples from the shared gradients. Such reconstruction attacks pose severe threats to collaborative learning. Hence, effective mitigation solutions are urgently desired. In this paper, we systematically analyze existing reconstruction attacks and propose to leverage data augmentation to defeat these attacks: by preprocessing sensitive images with carefully-selected transformation policies, it becomes infeasible for the adversary to extract training samples from the corresponding gradients. We first design two new metrics to quantify the impacts of transformations on data privacy and model usability. With the two metrics, we design a novel search\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:2KloaMYe4IUC", "title": "A Comprehensive Defense Framework Against Model Extraction Attacks", "authors": ["Wenbo Jiang", "Hongwei Li", "Guowen Xu", "Tianwei Zhang", "Rongxing Lu"], "description": "As a promising service, Machine Learning as a Service (MLaaS) provides personalized inference functions for clients through paid APIs. Nevertheless, it is vulnerable to model extraction attacks, in which an attacker can extract a functionally-equivalent model by repeatedly querying the APIs with crafted samples. While numerous works have been proposed to defend against model extraction attacks, existing efforts are accompanied by limitations and low comprehensiveness. In this paper, we propose AMAO, a comprehensive defense framework against model extraction attacks. Specifically, AMAO consists of four interlinked successive phases: adversarial training is first exploited to weaken the effectiveness of model extraction attacks. Then, malicious query detection is used to detect malicious queries and mark malicious users. After that, we develop a label-flipping poisoning attack to instruct the adaptive query\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:dQ2og3OwTAUC", "title": "Physical Black-box Adversarial Attacks through Transformations", "authors": ["Wenbo Jiang", "Hongwei Li", "Guowen Xu", "Tianwei Zhang", "Rongxing Lu"], "description": "Deep learning has shown impressive performance in numerous applications. However, recent studies have found that deep learning models are vulnerable to adversarial attacks, where the attacker adds imperceptible perturbations into benign samples to induce misclassifications. Adversarial attacks in the digital domain focus on constructing imperceptible perturbations. However, they are always less effective in the physical world because the perturbations may be destroyed when captured by the camera. Most physical adversarial attacks require adding invisible adversarial features (e.g., a sticker or a laser) to the target object, which may be noticed by human eyes. In this work, we propose to employ image transformation to generate more natural adversarial samples in the physical world. Concretely, we propose two attack algorithms to satisfy different attack goals:  Efficient-AATR  employs a greedy strategy to\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:BrmTIyaxlBUC", "title": "Online adaptation for autonomous unmanned systems driven by requirements satisfaction model", "authors": ["Yixing Luo", "Yuan Zhou", "Haiyan Zhao", "Zhi Jin", "Tianwei Zhang", "Yang Liu", "Danny Barthaud", "Yijun Yu"], "description": "Autonomous unmanned systems (AUSs) emerge to replace human operators for achieving better safety, efficiency, and effectiveness in harsh and difficult missions. They usually run in a highly open and dynamic operating environment, in which some unexpected situations may occur, leading to violations of predefined requirements. In order to maintain stable performance, the AUS control software needs to predict in advance whether the requirements will be violated and then make adaptations to maximize requirements satisfaction. We propose \\documentclass[12pt]{minimal}     \\usepackage{amsmath}     \\usepackage{wasysym}     \\usepackage{amsfonts}     \\usepackage{amssymb}     \\usepackage{amsbsy}     \\usepackage{mathrsfs}     \\usepackage{upgreek}     \\setlength{\\oddsidemargin}{-69pt}     \\begin{document}$$\\mathtt {Captain}$$\\end{document}, a model-driven and control-based online adaptation approach\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:Y5dfb0dijaUC", "title": "Knowledge Science, Engineering and Management: 15th International Conference, KSEM 2022, Singapore, August 6\u20138, 2022, Proceedings, Part II", "authors": ["Gerard Memmi", "Baijian Yang", "Linghe Kong", "Tianwei Zhang", "Meikang Qiu"], "description": "The three-volume sets constitute the refereed proceedings of the 15th International Conference on Knowledge Science, Engineering and Management, KSEM 2022, held in Singapore, during August 6\u20138, 2022. The 169 full papers presented in these proceedings were carefully reviewed and selected from 498 submissions. The papers are organized in the following topical sections: Volume I: Knowledge Science with Learning and AI (KSLA) Volume II: Knowledge Engineering Research and Applications (KERA) Volume III: Knowledge Management with Optimization and Security (KMOS)", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:yD5IFk8b50cC", "title": "Analysis on action tracking reports of covid-19 informs control strategies and vaccine delivery in post-pandemic era", "authors": ["Xiaofei Sun", "Tianjia Guan", "Tao Xue", "Chun Fan", "Meng Yang", "Yuxian Meng", "Tianwei Zhang", "Bahabaike Jiangtulu", "Fei Wu", "Jiwei Li"], "description": "Understanding the spread of SARS-CoV-2 provides important insights for control policies such as social-distancing interventions and vaccine delivery in the post-pandemic era. In this work, we take the advantage of action tracking reports of confirmed COVID-19 patients, which contain the mobility trajectory of patients. We analyzed reports of patients from April 2020 to January 2021 in China, a country where the residents are well-prepared for the \u201cnew normal\u201d world following COVID-19 spread. We developed natural language processing (NLP) tools to transform the unstructured text of action-tracking reports to a structured network of social contacts. An epidemiology model was built on top of the network. Our analysis provides important insights for the development of control policies. Under the \u201cnew normal\u201d conditions, we find that restaurants, locations less protected by mask-wearing, have a greater risk than any other location categories, including locations where people are present at higher densities (e.g., flight). We find that discouraging railway transports is crucial to avoid another wave of breakout during the Chunyun season (a period of travel in China with extremely high traffic load around the Chinese New Year). By formalizing the challenge of finding the optimal vaccine delivery among various different population groups as an optimization problem, our analysis helps to maximize the efficiency of vaccine delivery under the general situation of vaccine supply shortage. We are able to reduce the numbers of infections and deaths by 7.4% and 10.5% respectively with vaccine supply for only 1% of the population. Furthermore, with 10\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [1, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:ZHo1McVdvXMC", "title": "Resisting adversarial examples via wavelet extension and denoising", "authors": ["Qinkai Zheng", "Han Qiu", "Tianwei Zhang", "Gerard Memmi", "Meikang Qiu", "Jialiang Lu"], "description": "It is well known that Deep Neural Networks are vulnerable to adversarial examples. An adversary can inject carefully-crafted perturbations on clean input to manipulate the model output. In this paper, we propose a novel method, WED (Wavelet Extension and Denoising), to better resist adversarial examples. Specifically, WED adopts a wavelet transform to extend the input dimension with the image structures and basic elements. This can add significant difficulty for the adversary to calculate effective perturbations. WED further utilizes wavelet denoising to reduce the impact of adversarial perturbations on the model performance. Evaluations show that WED can resist 7 common adversarial attacks under both black-box and white-box scenarios. It outperforms two state-of-the-art wavelet-based approaches for both model accuracy and defense effectiveness.", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:hqOjcs7Dif8C", "title": "Practical and Scalable Security Verification of Secure Architectures", "authors": ["Jakub Szefer", "Tianwei Zhang", "Ruby B Lee"], "description": "We present a new and practical framework for security verification of secure architectures. Specifically, we break the verification task into external verification and internal verification. External verification considers the external protocols, i.e. interactions between users, compute servers, network entities, etc. Meanwhile, internal verification considers the interactions between hardware and software components within each server. This verification framework is general-purpose and can be applied to a stand-alone server, or a large-scale distributed system. We evaluate our verification method on the CloudMonatt and HyperWall architectures as examples.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:ufrVoPGSRksC", "title": "CloudShelter: Protecting Virtual Machines' Memory Resource Availability in Clouds", "authors": ["Tianwei Zhang", "Yuan Xu", "Yungang Bao", "Ruby B Lee"], "description": "We present CloudShelter, an architecture to protect virtual machines' memory availability from undesired resource contention on the cloud servers. We introduce a new micro-architectural metric: Memory Round Trip Time, to quantify VMs' memory QoS. Using this metric, (1) CloudShelter defines new QoS options for customers when launching VMs. These options can guarantee VMs' memory QoS at different levels even when they face intensive contention with co-located VMs; (2) CloudShelter periodically monitors VMs' memory QoS at runtime: once QoS violations against customers' demands are detected, CloudShelter places this VM into an isolated environment to eliminate contention. CloudShelter can reduce 30.1% performance interference from LLC/DRAM contention and 81.6% interference from bus contention1.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 1, 0, 0]}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:tuHXwOkdijsC", "title": "Backdooring Multimodal Learning", "authors": ["Xingshuo Han", "Yutong Wu", "Qingjie Zhang", "Yuan Zhou", "Yuan Xu", "Han Qiu", "Guowen Xu", "Tianwei Zhang"], "description": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, which poison the training set to alter the model prediction over samples with a specific trigger. While existing efforts mainly focus on unimodal scenarios, modern AI systems usually employ multiple modalities to improve the model performance, making multimodal backdoor attacks more practical but structurally more complex due to inherent modality interactions, multiple attack surfaces, unbalanced modality contributions, etc. These factors affect the effectiveness of backdooring multimodal learning significantly but have not been fully investigated yet. To bridge this gap, we present the first data and computation efficient backdoor attacks towards multimodal learning. Our solution consists of two innovations. First, we propose a novel backdoor gradient-based score (BAGS), which can accurately quantify the contribution of each data sample to the\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:kuK5TVdYjLIC", "title": "Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models", "authors": ["Renyang Liu", "Wei Zhou", "Tianwei Zhang", "Kangjie Chen", "Jun Zhao", "Kwok-Yan Lam"], "description": "Existing black-box attacks have demonstrated promising potential in creating adversarial examples (AE) to deceive deep learning models. Most of these attacks need to handle a vast optimization space and require a large number of queries, hence exhibiting limited practical impacts in real-world scenarios. In this paper, we propose a novel black-box attack strategy, Conditional Diffusion Model Attack (CDMA), to improve the query efficiency of generating AEs under query-limited situations. The key insight of CDMA is to formulate the task of AE synthesis as a distribution transformation problem, i.e., benign examples and their corresponding AEs can be regarded as coming from two distinctive distributions and can transform from each other with a particular converter. Unlike the conventional \\textit{query-and-optimization} approach, we generate eligible AEs with direct conditional transform using the aforementioned data converter, which can significantly reduce the number of queries needed. CDMA adopts the conditional Denoising Diffusion Probabilistic Model as the converter, which can learn the transformation from clean samples to AEs, and ensure the smooth development of perturbed noise resistant to various defense strategies. We demonstrate the effectiveness and efficiency of CDMA by comparing it with nine state-of-the-art black-box attacks across three benchmark datasets. On average, CDMA can reduce the query count to a handful of times; in most cases, the query count is only ONE. We also show that CDMA can obtain  attack success rate for untarget attacks over all datasets and targeted attack over CIFAR-10 with the noise budget\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:b1wdh0AR-JQC", "title": "Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content", "authors": ["Guanlin Li", "Yifei Chen", "Jie Zhang", "Jiwei Li", "Shangwei Guo", "Tianwei Zhang"], "description": "Artificial Intelligence Generated Content (AIGC) is gaining great popularity in social media, with many commercial services available. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images, fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content). Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely without the regulation of the service provider. (2) Watermark forge: the adversary can create illegal content with forged watermarks from another user, causing the service provider to make wrong attributions. We propose WMaGi, a unified framework to achieve both attacks in a holistic way. The key idea is to leverage a pre-trained diffusion model for content processing, and a generative adversarial network for watermark removing or forging. We evaluate WMaGi on different datasets and embedding setups. The results prove that it can achieve high success rates while maintaining the quality of the generated content. Compared with existing diffusion model-based attacks, WMaGi is 5,05011,000 faster.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:hMsQuOkrut0C", "title": "Differentially Private Federated Learning with an Adaptive Noise Mechanism", "authors": ["Rui Xue", "Kaiping Xue", "Bin Zhu", "Xinyi Luo", "Tianwei Zhang", "Qibin Sun", "Jun Lu"], "description": "Federated Learning (FL) enables multiple distributed clients to collaboratively train a model with owned datasets. To avoid the potential privacy threat in FL, researchers propose the DP-FL strategy, which utilizes differential privacy (DP) to add elaborate noise to the exchanged parameters to hide privacy information. DP-FL guarantees the privacy of FL at the cost of model performance degradation. To balance the trade-off between model accuracy and security, we propose a differentially private federated learning scheme with an adaptive noise mechanism. This is challenging, as the distributed nature of FL makes it difficult to appropriately estimate sensitivity, where sensitivity is a concept in DP that determines the scale of noise. To resolve this, we design a generic method for sensitivity estimates based on local and global historical information. We also provide instances on four commonly used optimizers to verify\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:gsN89kCJA0AC", "title": "Catch You Everything Everywhere: Guarding Textual Inversion via Concept Watermarking", "authors": ["Weitao Feng", "Jiyan He", "Jie Zhang", "Tianwei Zhang", "Wenbo Zhou", "Weiming Zhang", "Nenghai Yu"], "description": "AIGC (AI-Generated Content) has achieved tremendous success in many applications such as text-to-image tasks, where the model can generate high-quality images with diverse prompts, namely, different descriptions in natural languages. More surprisingly, the emerging personalization techniques even succeed in describing unseen concepts with only a few personal images as references, and there have been some commercial platforms for sharing the valuable personalized concept. However, such an advanced technique also introduces a severe threat, where malicious users can misuse the target concept to generate highly-realistic illegal images. Therefore, it becomes necessary for the platform to trace malicious users and hold them accountable. In this paper, we focus on guarding the most popular lightweight personalization model, ie, Textual Inversion (TI). To achieve it, we propose the novel concept watermarking, where watermark information is embedded into the target concept and then extracted from generated images based on the watermarked concept. Specifically, we jointly train a watermark encoder and a watermark decoder with the sampler in the loop. It shows great resilience to different diffusion sampling processes possibly chosen by malicious users, meanwhile preserving utility for normal use. In practice, the concept owner can upload his concept with different watermarks (ie, serial numbers) to the platform, and the platform allocates different users with different serial numbers for subsequent tracing and forensics.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:EYYDruWGBe4C", "title": "Backdooring Textual Inversion for Concept Censorship", "authors": ["Jie Zhang", "Florian Kerschbaum", "Tianwei Zhang"], "description": "Recent years have witnessed success in AIGC (AI Generated Content). People can make use of a pre-trained diffusion model to generate images of high quality or freely modify existing pictures with only prompts in nature language. More excitingly, the emerging personalization techniques make it feasible to create specific-desired images with only a few images as references. However, this induces severe threats if such advanced techniques are misused by malicious users, such as spreading fake news or defaming individual reputations. Thus, it is necessary to regulate personalization models (i.e., concept censorship) for their development and advancement. In this paper, we focus on the personalization technique dubbed Textual Inversion (TI), which is becoming prevailing for its lightweight nature and excellent performance. TI crafts the word embedding that contains detailed information about a specific object. Users can easily download the word embedding from public websites like Civitai and add it to their own stable diffusion model without fine-tuning for personalization. To achieve the concept censorship of a TI model, we propose leveraging the backdoor technique for good by injecting backdoors into the Textual Inversion embeddings. Briefly, we select some sensitive words as triggers during the training of TI, which will be censored for normal use. In the subsequent generation stage, if the triggers are combined with personalized embeddings as final prompts, the model will output a pre-defined target image rather than images including the desired malicious concept. To demonstrate the effectiveness of our approach, we conduct\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:ML0RJ9NH7IQC", "title": "Mercury: An Automated Remote Side-channel Attack to Nvidia Deep Learning Accelerator", "authors": ["Xiaobei Yan", "Xiaoxuan Lou", "Guowen Xu", "Han Qiu", "Shangwei Guo", "Chip Hong Chang", "Tianwei Zhang"], "description": "DNN accelerators have been widely deployed in many scenarios to speed up the inference process and reduce the energy consumption. One big concern about the usage of the accelerators is the confidentiality of the deployed models: model inference execution on the accelerators could leak side-channel information, which enables an adversary to preciously recover the model details. Such model extraction attacks can not only compromise the intellectual property of DNN models, but also facilitate some adversarial attacks. Although previous works have demonstrated a number of side-channel techniques to extract models from DNN accelerators, they are not practical for two reasons. (1) They only target simplified accelerator implementations, which have limited practicality in the real world. (2) They require heavy human analysis and domain knowledge. To overcome these limitations, this paper presents Mercury, the first automated remote side-channel attack against the off-the-shelf Nvidia DNN accelerator. The key insight of Mercury is to model the side-channel extraction process as a sequence-to-sequence problem. The adversary can leverage a time-to-digital converter (TDC) to remotely collect the power trace of the target model's inference. Then he uses a learning model to automatically recover the architecture details of the victim model from the power trace without any prior knowledge. The adversary can further use the attention mechanism to localize the leakage points that contribute most to the attack. Evaluation results indicate that Mercury can keep the error rate of model extraction below 1%.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:BUYA1_V_uYcC", "title": "Adversarial Training Over Long-Tailed Distribution", "authors": ["Guanlin Li", "Guowen Xu", "Tianwei Zhang"], "description": "In this paper, we study adversarial training on datasets that obey the long-tailed distribution, which is practical but rarely explored in previous works. Compared with conventional adversarial training on balanced datasets, this process falls into the dilemma of generating uneven adversarial examples (AEs) and an unbalanced feature embedding space, causing the resulting model to exhibit low robustness and accuracy on tail data. To combat that, we propose a new adversarial training framework -- Re-balancing Adversarial Training (REAT). This framework consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs; (2) a carefully constructed penalty function to force a satisfactory feature space. Evaluation results on different datasets and model structures prove that REAT can effectively enhance the model's robustness and preserve the model's clean accuracy. The code can be found in https://github.com/GuanlinLee/REAT.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:AvfA0Oy_GE0C", "title": "Omnipotent Adversarial Training for Unknown Label-noisy and Imbalanced Datasets", "authors": ["Guanlin Li", "Kangjie Chen", "Yuan Xu", "Han Qiu", "Tianwei Zhang"], "description": "Adversarial training is an important topic in robust deep learning, but the community lacks attention to its practical usage. In this paper, we aim to resolve a real-world application challenge, i.e., training a model on an imbalanced and noisy dataset to achieve high clean accuracy and robustness, with our proposed Omnipotent Adversarial Training (OAT). Our strategy consists of two innovative methodologies to address the label noise and data imbalance in the training set. We first introduce an oracle into the adversarial training process to help the model learn a correct data-label conditional distribution. This carefully-designed oracle can provide correct label annotations for adversarial training. We further propose logits adjustment adversarial training to overcome the data imbalance challenge, which can help the model learn a Bayes-optimal distribution. Our comprehensive evaluation results show that OAT outperforms other baselines by more than 20% clean accuracy improvement and 10% robust accuracy improvement under the complex combinations of data imbalance and label noise scenarios. The code can be found in https://github.com/GuanlinLee/OAT.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:L7CI7m0gUJcC", "title": "TPU as Cryptographic Accelerator", "authors": ["Rabimba Karanjai", "Sangwon Shin", "Xinxin Fan", "Lin Chen", "Tianwei Zhang", "Taeweon Suh", "Weidong Shi", "Lei Xu"], "description": "Polynomials defined on specific rings are heavily involved in various cryptographic schemes, and the corresponding operations are usually the computation bottleneck of the whole scheme. We propose to utilize TPU, an emerging hardware designed for AI applications, to speed up polynomial operations and convert TPU to a cryptographic accelerator. We also conduct preliminary evaluation and discuss the limitations of current work and future plan.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:SdhP9T11ey4C", "title": "VerifyML: Obliviously Checking Model Fairness Resilient to Malicious Model Holder", "authors": ["Guowen Xu", "Xingshuo Han", "Gelei Deng", "Tianwei Zhang", "Shengmin Xu", "Jianting Ning", "Anjia Yang", "Hongwei Li"], "description": "In this paper, we present  VerifyML , the first secure inference framework to check the fairness degree of a given Machine learning (ML) model.  VerifyML  is generic and is immune to any obstruction by the malicious model holder during the verification process. We rely on secure two-party computation (2\u00a0PC) technology to implement  VerifyML , and carefully customize a series of optimization methods to boost its performance for both linear and nonlinear layer execution. Specifically, (1)  VerifyML  allows the vast majority of overhead to be performed offline, thus meeting the low latency requirements for online inference. (2) To speed up offline preparation, we first design novel homomorphic parallel computing techniques to accelerate the authenticated Beaver's triple (including matrix- vector and convolution triples) generation procedure. It achieves up to    computation speedup and gains at least    less communication\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:0KyAp5RtaNEC", "title": "Extracting Cloud-based Model with Prior Knowledge", "authors": ["Shiqian Zhao", "Kangjie Chen", "Meng Hao", "Jian Zhang", "Guowen Xu", "Hongwei Li", "Tianwei Zhang"], "description": "Machine Learning-as-a-Service, a pay-as-you-go business pattern, is widely accepted by third-party users and developers. However, the open inference APIs may be utilized by malicious customers to conduct model extraction attacks, i.e., attackers can replicate a cloud-based black-box model merely via querying malicious examples. Existing model extraction attacks mainly depend on the posterior knowledge (i.e., predictions of query samples) from Oracle. Thus, they either require high query overhead to simulate the decision boundary, or suffer from generalization errors and overfitting problems due to query budget limitations. To mitigate it, this work proposes an efficient model extraction attack based on prior knowledge for the first time. The insight is that prior knowledge of unlabeled proxy datasets is conducive to the search for the decision boundary (e.g., informative samples). Specifically, we leverage self-supervised learning including autoencoder and contrastive learning to pre-compile the prior knowledge of the proxy dataset into the feature extractor of the substitute model. Then we adopt entropy to measure and sample the most informative examples to query the target model. Our design leverages both prior and posterior knowledge to extract the model and thus eliminates generalizability errors and overfitting problems. We conduct extensive experiments on open APIs like Traffic Recognition, Flower Recognition, Moderation Recognition, and NSFW Recognition from real-world platforms, Azure and Clarifai. The experimental results demonstrate the effectiveness and efficiency of our attack. For example, our attack achieves 95.1% fidelity\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:-_dYPAW6P2MC", "title": "FastSecNet: An Efficient Cryptographic Framework for Private Neural Network Inference", "authors": ["Meng Hao", "Hongwei Li", "Hanxiao Chen", "Pengzhi Xing", "Tianwei Zhang"], "description": "Private neural network inference has demonstrated great importance in various privacy-critical scenarios. However, the primary challenge remaining in prior works is that the evaluation on encrypted data levies prohibitively high runtime and communication overhead. In this work, we present  FastSecNet , an efficient two-party cryptographic framework for private inference in the dealer-based pre-processing setting. Specifically, 1)  FastSecNet  provides an efficient ReLU protocol for the evalution of non-linear layers, which is built up on a recent advanced cryptographic primitive, function secret sharing (FSS). The core of this construction are an optimized ReLU representation and a customized FSS-based ReLU protocol. 2) For linear layer evaluation, we first propose an efficient PRG-based pre-processing protocol based on the fact that one of the inputs is uniformly random in the offline phase. Then, the online phase only\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:tzM49s52ZIMC", "title": "Introduction to the Special Section on Energy-efficient and Secure Computing for Artificial Intelligence and Beyond", "authors": ["Meikang Qiu", "Ke Xu", "Cheng Zhang", "Tianwei Zhang"], "description": "In recent years, Artificial Intelligence (AI) has become a key component for building smart Internetof-Things (IoT) and network infrastructures. With the development of AI computing algorithms and methods, nowadays, AI computing has evolved as a computational-hungry and data-hungry process. This has caused substantial electricity consumption with a large amount of financial and environmental costs (eg, greenhouse gas emission). From a larger perspective, the global data centers had consumed 3% of global electricity consumption, ranking 11th in the list of country electricity consumption.It is necessary to reduce carbon emission over the next decade to deter escalating rates of the natural disaster. There are two aspects that we would like to pay attention to in this special issue. First, we welcome research works have attempted to reduce the energy consumption of AI computing such as DNN training and\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:tKAzc9rXhukC", "title": "Smaller Is Bigger: Rethinking the Embedding Rate of Deep Hiding", "authors": ["Han Li", "Hangcheng Liu", "Shangwei Guo", "Mingliang Zhou", "Ning Wang", "Tao Xiang", "Tianwei Zhang"], "description": "Deep hiding, concealing secret information using Deep Neural Networks (DNNs), can significantly increase the embedding rate and improve the efficiency of secret sharing. Existing works mainly force on designing DNNs with higher embedding rates or fancy functionalities. In this paper, we want to answer some fundamental questions: how to increase and what determines the embedding rate of deep hiding. To this end, we first propose a novel Local Deep Hiding (LDH) scheme that significantly increases the embedding rate by hiding large secret images into small local regions of cover images. Our scheme consists of three DNNs: hiding, locating, and revealing. We use the hiding network to convert a secret image in a small imperceptible compact secret code that is embedded into a random local region of a cover image. The locating network assists the revealing process by identifying the position of secret codes in the stego image, while the revealing network recovers all full-size secret images from these identified local regions. Our LDH achieves an extremely high embedding rate, i.e.,  bpp and exhibits superior robustness to common image distortions. We also conduct comprehensive experiments to evaluate our scheme under various system settings. We further quantitatively analyze the trade-off between the embedding rate and image quality with different image restoration algorithms.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:kzcrU_BdoSEC", "title": "Lucid: A Non-intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs", "authors": ["Qinghao Hu", "Meng Zhang", "Peng Sun", "Yonggang Wen", "Tianwei Zhang"], "description": "While recent deep learning workload schedulers exhibit excellent performance, it is arduous to deploy them in practice due to some substantial defects, including inflexible intrusive manner, exorbitant integration and maintenance cost, limited scalability, as well as opaque decision processes. Motivated by these issues, we design and implement Lucid, a non-intrusive deep learning workload scheduler based on interpretable models. It consists of three innovative modules. First, a two-dimensional optimized profiler is introduced for efficient job metric collection and timely debugging job feedback. Second, Lucid utilizes an indolent packing strategy to circumvent interference. Third, Lucid orchestrates resources based on estimated job priority values and sharing scores to achieve efficient scheduling. Additionally, Lucid promotes model performance maintenance and system transparent adjustment via a well-designed\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:KUbvn5osdkgC", "title": "Deep Multitask Learning with Progressive Parameter Sharing", "authors": ["Haosen Shi", "Shen Ren", "Tianwei Zhang", "Sinno Jialin Pan"], "description": "We propose a novel progressive parameter-sharing strategy (MPPS) in this paper for effectively training multitask learning models on diverse computer vision tasks simultaneously. Specifically, we propose to parameterize distributions for different tasks to control the sharings, based on the concept of Exclusive Capacity that we introduce. A scheduling mechanism following the concept of curriculum learning is also designed to progressively change the sharing strategy to increase the level of sharing during the learning process. We further propose a novel loss function to regularize the optimization of network parameters as well as the sharing probabilities of each neuron for each task. Our approach can be combined with many state-of-the-art multitask learning solutions to achieve better joint task performance. Comprehensive experiments show that it has competitive performance on three challenging datasets (Multi-CIFAR100, NYUv2, and Cityscapes) using various convolution neural network architectures.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:g3aElNc5_aQC", "title": "Computation and Data Efficient Backdoor Attacks", "authors": ["Yutong Wu", "Xingshuo Han", "Han Qiu", "Tianwei Zhang"], "description": "Backdoor attacks against deep learning have been widely studied. Various attack techniques have been proposed for different domains and paradigms, eg, image, point cloud, natural language processing, transfer learning, etc. These works normally adopt the data poisoning strategy to embed the backdoor. They randomly select samples from the benign training set for poisoning, without considering the distinct contribution of each sample to the backdoor effectiveness, making the attack less optimal. A recent work (IJCAI-22) proposed to use the forgetting score to measure the importance of each poisoned sample and then filter out redundant data for effective backdoor training. However, this method is empirically designed without theoretical proofing. It is also very time-consuming as it needs to go through almost all the training stages for data selection. To address such limitations, we propose a novel confidence-based scoring methodology, which can efficiently measure the contribution of each poisoning sample based on the distance posteriors. We further introduce a greedy search algorithm to find the most informative samples for backdoor injection more promptly. Experimental evaluations on both 2D image and 3D point cloud classification tasks show that our approach can achieve comparable performance or even surpass the forgetting score-based searching method while requiring only several extra epochs' computation of a standard training process.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:uWiczbcajpAC", "title": "Hydro:{Surrogate-Based} Hyperparameter Tuning Service in Datacenters", "authors": ["Qinghao Hu", "Zhisheng Ye", "Meng Zhang", "Qiaoling Chen", "Peng Sun", "Yonggang Wen", "Tianwei Zhang"], "description": "Hyperparameter tuning is an essential step in deep learning model development that provides better model performance at the cost of substantial resources. While existing systems can improve tuning efficiency, they still fail to handle large models with billions of parameters and efficiently leverage cluster resources. Motivated by these deficiencies, we introduce Hydro, a surrogate-based hyperparameter tuning service that optimizes tuning workloads in both the job-level and cluster-level granularities. Specifically, it consists of two key components:(1) Hydro Tuner automatically generates and optimizes surrogate models via scaling, parametrization and fusion;(2) Hydro Coordinator improves tuning efficiency and cluster-wide resource utilization by adaptively leveraging ephemeral and heterogeneous resources. Our comprehensive experiments on two tuning algorithms across six models show that Hydro Tuner can dramatically reduce tuning makespan by up to 78.5 x compared with Ray Tune and no reduction in tuning quality. Hydro's source code is publicly available at https://github. com/S-Lab-System-Group/Hydro.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:yB1At4FlUx8C", "title": "Mitigating Query-based Neural Network Fingerprinting via Data Augmentation", "authors": ["Meiqi Wang", "Han Qiu", "Tianwei Zhang", "Meikang Qiu", "Bhavani Thuraisingham"], "description": "Protecting the intellectual property (IP) of deep neural network (DNN) models becomes essential and urgent with the rapidly increasing cost of DNN training. Fingerprinting is one promising IP protection method that queries suspicious models with specific fingerprint samples to infer and verify IP by comparing the predictions with pre-defined labels. Based on utilizing unique features of target models, various DNN fingerprinting methods are proposed to effectively verify the IP of models remotely with a meager false-positive ratio. In this paper, we propose a novel attack to mitigate query-based fingerprinting methods based on data augmentation methods. We propose a randomized transformation on input samples to significantly mislead the fingerprint samples\u2019 prediction and compromise the IP verification. Then, our attack can keep the model utility with an acceptable accuracy drop in the data-free scenario (i.e\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:uJ-U7cs_P_0C", "title": "{NAUTILUS}: Automated {RESTful}{API} Vulnerability Detection", "authors": ["Gelei Deng", "Zhiyi Zhang", "Yuekang Li", "Yi Liu", "Tianwei Zhang", "Yang Liu", "Guo Yu", "Dongjin Wang"], "description": "RESTful APIs have become arguably the most prevalent endpoint for accessing web services. Blackbox vulnerability scanners are a popular choice for detecting vulnerabilities in web services automatically. Unfortunately, they suffer from a number of limitations in RESTful API testing. Particularly, existing tools cannot effectively obtain the relations between API operations, and they lack the awareness of the correct sequence of API operations during testing. These drawbacks hinder the tools from requesting the API operations properly to detect potential vulnerabilities.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:PR6Y55bgFSsC", "title": "Efficiency Boosting of Secure Cross-platform Recommender Systems over Sparse Data", "authors": ["Hao Ren", "Guowen Xu", "Tianwei Zhang", "Jianting Ning", "Xinyi Huang", "Hongwei Li", "Rongxing Lu"], "description": "Fueled by its successful commercialization, the recommender system (RS) has gained widespread attention. However, as the training data fed into the RS models are often highly sensitive, it ultimately leads to severe privacy concerns, especially when data are shared among different platforms. In this paper, we follow the tune of existing works to investigate the problem of secure sparse matrix multiplication for cross-platform RSs. Two fundamental while critical issues are addressed: preserving the training data privacy and breaking the data silo problem. Specifically, we propose two concrete constructions with significantly boosted efficiency. They are designed for the sparse location insensitive case and location sensitive case, respectively. State-of-the-art cryptography building blocks including homomorphic encryption (HE) and private information retrieval (PIR) are fused into our protocols with non-trivial optimizations. As a result, our schemes can enjoy the HE acceleration technique without privacy trade-offs. We give formal security proofs for the proposed schemes and conduct extensive experiments on both real and large-scale simulated datasets. Compared with state-of-the-art works, our two schemes compress the running time roughly by 10* and 2.8*. They also attain up to 15* and 2.3* communication reduction without accuracy loss.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:VL0QpB8kHFEC", "title": "A Benchmark of Long-tailed Instance Segmentation with Noisy Labels (Short Version)", "authors": ["Guanlin Li", "Guowen Xu", "Tianwei Zhang"], "description": "In this paper, we consider the instance segmentation task on a long-tailed dataset, which contains label noise, i.e., some of the annotations are incorrect. There are two main reasons making this case realistic. First, datasets collected from real world usually obey a long-tailed distribution. Second, for instance segmentation datasets, as there are many instances in one image and some of them are tiny, it is easier to introduce noise into the annotations. Specifically, we propose a new dataset, which is a large vocabulary long-tailed dataset containing label noise for instance segmentation. Furthermore, we evaluate previous proposed instance segmentation algorithms on this dataset. The results indicate that the noise in the training dataset will hamper the model in learning rare categories and decrease the overall performance, and inspire us to explore more effective approaches to address this practical challenge. The code and dataset are available in https://github.com/GuanlinLee/Noisy-LVIS.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:LjlpjdlvIbIC", "title": "Titan: a scheduler for foundation model fine-tuning workloads", "authors": ["Wei Gao", "Peng Sun", "Yonggang Wen", "Tianwei Zhang"], "description": "The recent breakthrough of foundation model (FM) research raises a new trend to acquire efficient DL models by fine-tuning FMs with low-resource datasets. Current GPU clusters are mainly established to develop DL models by training from scratch. How to tailor a GPU cluster scheduler for FM fine-tuning workloads is still not explored. We propose Titan, a scheduler to improve the efficiency of FM fine-tuning workloads based on their three distinct features. (1) It takes full advantage of the fixed model structure to estimate the job duration accurately and configure the fine-tuning workload efficiently. (2) The multi-task adaptivity of FMs enables multiple fine-tuning workloads to share the same model parameters, which can significantly reduce the GPU resource consumption. (3) It considers the pipeline parallelism of FM fine-tuning workloads and concurrently executes the parameter transmission and gradient\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:UHK10RUVsp4C", "title": "Tear Up the Bubble Boom: Lessons Learned From a Deep Learning Research and Development Cluster", "authors": ["Zehua Yang", "Zhisheng Ye", "Tianhao Fu", "Jing Luo", "Xiong Wei", "Yingwei Luo", "Xiaolin Wang", "Zhenlin Wang", "Tianwei Zhang"], "description": "With the proliferation of deep learning, there exists a strong need to efficiently operate GPU clusters for deep learning production in giant AI companies, as well as for research and development (R&D) in small-sized research institutes and universities. Existing works have performed thorough trace analysis on large-scale production-level clusters in giant companies, which discloses the characteristics of deep learning production jobs and motivates the design of scheduling frameworks. However, R&D clusters significantly differ from production-level clusters in both job properties and user behaviors, calling for a different scheduling mechanism. In this paper, we present a detailed workload characterization of an R&D cluster, CloudBrain-I, in a research institute, Peng Cheng Laboratory. After analyzing the fine-grained resource utilization, we discover a severe problem for R&D clusters, resource underutilization, which\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:eq2jaN3J8jMC", "title": "New Secure Sparse Inner Product with Applications to Machine Learning", "authors": ["Guowen Xu", "Shengmin Xu", "Jianting Ning", "Tianwei Zhang", "Xinyi Huang", "Hongwei Li", "Rongxing Lu"], "description": "Sparse inner product (SIP) has the attractive property of overhead being dominated by the intersection of inputs between parties, independent of the actual input size. It has intriguing prospects, especially for boosting machine learning on large-scale data, which is tangled with sparse data. In this paper, we investigate privacy-preserving SIP problems that have rarely been explored before. Specifically, we propose two concrete constructs, one requiring offline linear communication which can be amortized across queries, while the other has sublinear overhead but relies on the more computationally expensive tool. Our approach exploits state-of-the-art cryptography tools including garbled Bloom filters (GBF) and Private Information Retrieval (PIR) as the cornerstone, but carefully fuses them to obtain non-trivial overhead reductions. We provide formal security analysis of the proposed constructs and implement them into representative machine learning algorithms including k-nearest neighbors, naive Bayes classification and logistic regression. Compared to the existing efforts, our method achieves - speedup in runtime and up to  reduction in communication.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:PELIpwtuRlgC", "title": "A Formal Methodology for Verifying Side-Channel Vulnerabilities in Cache Architectures", "authors": ["Ke Jiang", "Tianwei Zhang", "David San\u00e1n", "Yongwang Zhao", "Yang Liu"], "description": "Security-aware CPU caches have been designed to mitigate side-channel attacks and prevent information leakage. How to validate the effectiveness of these designs remains an unsolved problem. Prior works assess the security of architectures empirically without a formal guarantee, making the evaluation results less convincing. In this paper, we propose a comprehensive methodology based on formal methods for security verification of cache architectures. Specifically, we design an entropy-based noninterference reasoning framework with two unwinding conditions to assess the information leakage of the cache designs. The reasoning framework quantifies the dependency relationships by the mutual information between the distributions of input and output of side channels. Given a cache design, we formalize its behavior specification along with the cache layouts into an abstract state machine, to instantiate the\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:_B80troHkn4C", "title": "Saving the Limping: Fault-tolerant Quadruped Locomotion via Reinforcement Learning", "authors": ["Dikai Liu", "Tianwei Zhang", "Jianxiong Yin", "Simon See"], "description": "Quadruped locomotion now has acquired the skill to traverse or even sprint on uneven terrains in remote uncontrolled environment. However, surviving in the wild requires not only the maneuverability, but also the ability to handle unexpected hardware failures. We present the first deep reinforcement learning based methodology to train fault-tolerant controllers, which can bring an injured quadruped back home safely and speedily. We adopt the teacher-student framework to train the controller with close-to-reality joint-locking failure in the simulation, which can be zero-shot transferred to the physical robot without any fine-tuning. Extensive simulation and real-world experiments demonstrate that our fault-tolerant controller can efficiently lead a quadruped stably when it faces joint failure during locomotion.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:_Re3VWB3Y0AC", "title": "Extracting Robust Models with Uncertain Examples", "authors": ["Guanlin Li", "Guowen Xu", "Shangwei Guo", "Han Qiu", "Jiwei Li", "Tianwei Zhang"], "description": "Model extraction attacks are proven to be a severe privacy threat to Machine Learning as a Service (MLaaS). A variety of techniques have been designed to steal a remote machine learning model with high accuracy and fidelity. However, how to extract a robust model with similar resilience against adversarial attacks is never investigated. This paper presents the first study toward this goal. We first analyze those existing extraction solutions either fail to maintain the model accuracy or model robustness or lead to the robust overfitting issue. Then we propose Boundary Entropy Searching Thief (BEST), a novel model extraction attack to achieve both accuracy and robustness extraction under restricted attack budgets. BEST generates a new kind of uncertain examples for querying and reconstructing the victim model. These samples have uniform confidence scores across different classes, which can perfectly balance the trade-off between model accuracy and robustness. Extensive experiments demonstrate that BEST outperforms existing attack methods over different datasets and model architectures under limited data. It can also effectively invalidate state-of-the-art extraction defenses.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:ZuybSZzF8UAC", "title": "Towards Automatic Generation of Advanced Shift Networks", "authors": ["Xiaoxuan Lou", "Guowen Xu", "Kangjie Chen", "Guanlin Li", "Jiwei Li", "Tianwei Zhang"], "description": "Multiplication-less neural networks significantly reduce the time and energy cost on the hardware platform, as the compute-intensive multiplications are replaced with lightweight bit-shift operations. However, existing bit-shift networks are all directly transferred from state-of-the-art convolutional neural networks (CNNs), which lead to non-negligible accuracy drop or even failure of model convergence. To combat this, we propose AutoShiftNet, the first framework tailoring Neural Architecture Search (NAS) to substantially reduce the accuracy gap between bit-shift neural networks and their real-valued counterparts. Specifically, we pioneer dragging NAS into a shift-oriented search space and endow it with the robust topology-related search strategy and custom regularization and stabilization. As a result, our AutoShiftNet breaks through the incompatibility of traditional NAS methods for bit-shift neural networks and achieves more desirable performance in terms of accuracy and convergence. Extensive experiments demonstrate that AutoShiftNet sets a new state-of-the-art for bit-shift neural networks, where the accuracy increases (1.69\u223c8.07)% on CIFAR10, (5.71\u223c18.09)% on CIFAR100 and > 4.36% on ImageNet, especially when many conventional CNNs fail to converge on ImageNet with bit-shift weights.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:N5tVd3kTz84C", "title": "Re-balancing Adversarial Training Over Unbalanced Datasets", "authors": ["Guanlin Li", "Guowen Xu", "Tianwei Zhang"], "description": "In this paper, we study adversarial training on datasets that obey the long-tailed distribution, which is practical but rarely explored by previous works. Compared with conventional adversarial training on the balanced dataset, this process falls into the dilemma of generating uneven adversarial examples (AEs) and an unbalanced feature embedding space, causing the resulting model to exhibit low robustness and accuracy on tail data. To combat that, we propose a new adversarial training framework -- Re-balancing Adversarial Training (REAT). This framework consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs; (2) a carefully constructed penalty function to force a satisfactory feature space. Evaluation results on different datasets and model structures prove that REAT can enhance the model's robustness and preserve the model's clean accuracy.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:HE397vMXCloC", "title": "A Secure Fingerprinting Framework for Distributed Image Classification", "authors": ["Guowen Xu", "Xingshuo Han", "Anguo Zhang", "Tianwei Zhang"], "description": "The deep learning (DL) technology has been widely used for image classification in many scenarios, e.g., face recognition and suspect tracking. Such a highly commercialized application has given rise to intellectual property protection of its DL model. To combat that, the mainstream method is to embed a unique watermark into the target model during the training process. However, existing efforts focus on detecting copyright infringement for a given model, while rarely consider the problem of traitors tracking. Moreover, the watermark embedding process can incur privacy issues for the training data in a distributed manner. In this paper, we propose SECUREMARK-DL, a novel fingerprinting framework to address the above two problems in a distributed learning environment. It embeds a unique fingerprint into the target model for each customer, which can be extracted and verified from any suspicious model once a dispute arises. In addition, it adopts a new privacy partitioning technique in the training process to protect the training data privacy. Extensive experiments demonstrate the robustness of SECUREMARK-DL against various attacks, and its high classification accuracy (> 95%) even if a long-bit (304-bit) fingerprint is embedded into an input image.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:kRWSkSYxWN8C", "title": "ShiftNAS: Towards Automatic Generation of Advanced Mulitplication-Less Neural Networks", "authors": ["Xiaoxuan Lou", "Guowen Xu", "Kangjie Chen", "Guanlin Li", "Jiwei Li", "Tianwei Zhang"], "description": "Multiplication-less neural networks significantly reduce the time and energy cost on the hardware platform, as the compute-intensive multiplications are replaced with lightweight bit-shift operations. However, existing bit-shift networks are all directly transferred from state-of-the-art convolutional neural networks (CNNs), which lead to non-negligible accuracy drop or even failure of model convergence. To combat this, we propose ShiftNAS, the first framework tailoring Neural Architecture Search (NAS) to substantially reduce the accuracy gap between bit-shift neural networks and their real-valued counterparts. Specifically, we pioneer dragging NAS into a shift-oriented search space and endow it with the robust topology-related search strategy and custom regularization and stabilization. As a result, our ShiftNAS breaks through the incompatibility of traditional NAS methods for bit-shift neural networks and achieves more desirable performance in terms of accuracy and convergence. Extensive experiments demonstrate that ShiftNAS sets a new state-of-the-art for bit-shift neural networks, where the accuracy increases (1.69-8.07)% on CIFAR10, (5.71-18.09)% on CIFAR100 and (4.36-67.07)% on ImageNet, especially when many conventional CNNs fail to converge on ImageNet with bit-shift weights.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:5ugPr518TE4C", "title": "Generating Adversarial Examples for Robust Deception against Image Transfer and Reloading", "authors": ["Chuan Zhou", "Duohe Ma", "Tianwei Zhang", "Liming Wang"], "description": "Adversarial examples play an irreplaceable role in evaluating DNNs' security and robustness. It's essential to understanding the adversarial examples' effectiveness to utilize them for model improvement. In this paper, we explore the impact of input transformation on adversarial examples. First, we discover a new phenomenon. The process of RELOAD or TRANSFER may deactivate adversarial examples' malicious functionality. The reason is that these processes would reduce pixel precision, which counters the perturbation added by the adversary. We validate this finding on different mainstream adversarial algorithms. Second, we propose a novel Confidence Iteration method, which can generate more robust adversarial examples. The key idea is to set the confidence threshold and add the pixel loss caused by image reloading or transferring into the calculation. We integrate our solution with existing adversarial\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:fQNAKQ3IYiAC", "title": "Practical and Scalable Security Verification of Secure Architectures", "authors": ["Tianwei Zhang", "Jakub Szefer", "Ruby B Lee"], "description": "We present a new and practical framework for security verification of secure architectures. Specifically, we break the verification task into external verification and internal verification. External verification considers the external protocols, i.e. interactions between users, compute servers, network entities, etc. Meanwhile, internal verification considers the interactions between hardware and software components within each server. This verification framework is general-purpose and can be applied to a stand-alone server, or a large-scale distributed system. We evaluate our verification method on the CloudMonatt and HyperWall architectures as examples.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:eJXPG6dFmWUC", "title": "Practical and Private Heterogeneous Federated Learning", "authors": ["Hanxiao Chen", "Meng Hao", "Hongwei Li", "Guangxiao Niu", "Guowen Xu", "Huawei Wang", "Yuan Zhang", "Tianwei Zhang"], "description": "Heterogeneous federated learning (HFL) enables clients with different computation/communication capabilities to collaboratively train their own customized models, in which the knowledge of models is shared via clients' predictions on a public dataset. However, there are two major limitations: 1) The assumption of public datasets may be unrealistic for data-critical scenarios such as Healthcare and Finance. 2) HFL is vulnerable to various privacy violations since the samples and predictions are completely exposed to adversaries. In this work, we develop PrivHFL, a general and practical framework for privacy-preserving HFL. We bypass the limitations of public datasets by designing a simple yet effective dataset expansion method. The main insight is that expanded data could provide good coverage of natural distributions, which is conducive to the sharing of model knowledge. To further tackle the privacy issue, we exploit the lightweight additive secret sharing technique to construct a series of tailored cryptographic protocols for key building blocks such as secure prediction. Our protocols implement ciphertext operations through simple vectorized computations, which are friendly with GPUs and can be processed by highly-optimized CUDA kernels. Extensive evaluations demonstrate that PrivHFL outperforms prior art up to two orders of magnitude in efficiency and realizes significant accuracy gains on top of the stand-alone method.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:K3LRdlH-MEoC", "title": "A Novel Watermarking Framework for Ownership Verification of DNN Architectures", "authors": ["Xiaoxuan Lou", "Shangwei Guo", "Tianwei Zhang", "Jiwei Li", "Yinqian Zhang", "Yang Liu"], "description": "We present a novel watermarking scheme to achieve the intellectual property (IP) protection and ownership verification of DNN architectures. Existing works all embedded watermarks into the model parameters while treating the architecture as public property. These solutions were proven to be vulnerable by an adversary to detect or remove the watermarks. In contrast, we are the first to claim model architectures as an important IP for model owners, and propose to implant watermarks into the architectures. We design new algorithms based on Neural Architecture Search (NAS) to generate watermarked architectures, which are unique enough to represent the ownership, while maintaining high model usability. Such watermarks can be extracted via side-channel-based model extraction techniques with high fidelity. Extensive evaluations show our scheme has negligible impact on the model performance, and exhibits strong robustness against various model transformations and adaptive attacks.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:XiSMed-E-HIC", "title": "Towards Robust Point Cloud Models with Context-Consistency Network and Adaptive Augmentation", "authors": ["Guanlin Li", "Guowen Xu", "Han Qiu", "Ruan He", "Jiwei Li", "Tianwei Zhang"], "description": "3D point cloud models based on deep neural networks were proven to be vulnerable to adversarial examples, with a quantity of novel attack techniques proposed by researchers recently. It is of paramount importance to preserve the robustness of 3D models under adversarial environments, considering their broad application in safety- and security-critical tasks. Unfortunately, defenses for 3D models are much less studied compared to 2D image models. In this paper, we reason about the vulnerability of 3D models based on the mutual information theory.  Furthermore, we design an effective defense methodology, consisting of two innovations. (1) We introduce CCDGN, a novel 3D DNN architecture which includes robust and light-weight modules to alleviate adversarial examples. (2) We propose AA-AMS a novel data augmentation strategy to adaptively balance the model usability and robustness. Extensive evaluations indicate the integration of the two techniques provides much more robustness than existing defense solutions for 3D models.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:a0OBvERweLwC", "title": "A Stealthy and Robust Fingerprinting Scheme for Generative Models", "authors": ["Li Guanlin", "Guo Shangwei", "Wang Run", "Xu Guowen", "Zhang Tianwei"], "description": "This paper presents a novel fingerprinting methodology for the Intellectual Property protection of generative models. Prior solutions for discriminative models usually adopt adversarial examples as the fingerprints, which give anomalous inference behaviors and prediction results. Hence, these methods are not stealthy and can be easily recognized by the adversary. Our approach leverages the invisible backdoor technique to overcome the above limitation. Specifically, we design verification samples, whose model outputs look normal but can trigger a backdoor classifier to make abnormal predictions. We propose a new backdoor embedding approach with Unique-Triplet Loss and fine-grained categorization to enhance the effectiveness of our fingerprints. Extensive evaluations show that this solution can outperform other strategies with higher robustness, uniqueness and stealthiness for various GAN models.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:M05iB0D1s5AC", "title": "Risk Analysis and Policy Enforcement of Function Interactions in Robot Apps", "authors": ["Yuan Xu", "Tianwei Zhang", "Yungang Bao"], "description": "Robot apps are becoming more automated, complex and diverse. An app usually consists of many functions, interacting with each other and the environment. This allows robots to conduct various tasks. However, it also opens a new door for cyber attacks: adversaries can leverage these interactions to threaten the safety of robot operations. Unfortunately, this issue is rarely explored in past works. We present the first systematic investigation about the function interactions in common robot apps. First, we disclose the potential risks and damages caused by malicious interactions. We introduce a comprehensive graph to model the function interactions in robot apps by analyzing 3,100 packages from the Robot Operating System (ROS) platform. From this graph, we identify and categorize three types of interaction risks. Second, we propose RTron, a novel system to detect and mitigate these risks and protect the operations of robot apps. We introduce security policies for each type of risks, and design coordination nodes to enforce the policies and regulate the interactions. We conduct extensive experiments on 110 robot apps from the ROS platform and two complex apps (Baidu Apollo and Autoware) widely adopted in industry. Evaluation results indicated RTron can correctly identify and mitigate all potential risks with negligible performance cost. To validate the practicality of the risks and solutions, we implement and evaluate RTron on a physical UGV (Turtlebot) with real-word apps and environments.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:3s1wT3WcHBgC", "title": "System and Method for Security Health Monitoring And Attestation Of Virtual Machines In Cloud Computing Systems", "authors": null, "description": "A system for security health monitoring and attestation of virtual machines in cloud computing systems is provided. The system includes a cloud server having a virtual machine and a hypervisor. The cloud server collects security measurement information and signs and hashes the security measurement information using a cryptography engine. The system also includes an attestation server for receiving the hashed security measurement information from the cloud server. The attestation server also verifies the signature and hash values, and interprets the security measurement information. The attestation server generates an attestation report based on the verification and interpretation of the security measurement information.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:TIZ-Mc8IlK0C", "title": "Analysis on Action Tracking Reports of COVID-19 Informs Control Strategies and Vaccine Delivery in Post-Pandemic Era (preprint)", "authors": ["Xiaofei Sun", "Tianjia Guan", "Tao Xue", "Chun Fan", "Meng Yang", "Yuxian Meng", "Tianwei Zhang", "Bahabaike Jiangtulu", "Fei Wu", "Jiwei Li"], "description": "Understanding the spread of SARS-CoV-2 provides important insights for control policies such as social-distancing interventions and vaccine delivery in the post-pandemic era. In this work, we take the advantage of action tracking reports of confirmed COVID-19 patients, which contain details regarding the mobility trajectory of a patient, along with the people with whom the patient has interacted, the timing of diagnosis, and personal information (eg, age and sex). We analyzed reports of 4,410 patients from April 2020 to February 2021 in China, a country where the residents are well-prepared for the\" new normal\" world following COVID-19 spread. We developed natural language processing (NLP) tools to transform the unstructured text of action-tracking reports to a structured network of social contacts. A SEIR model was built on top of the network, and was able to capture important aspects regarding coronavirus transmissions such as location category, age, sex and socioeconomic status. Our analysis provides important insights for the development of control policies. Under the\" new normal\" conditions, we find that restaurants, locations less protected by mask-wearing, have a greater risk than any other location categories, including locations where people are present at higher densities (eg, flight). We find that discouraging railway transports is crucial to avoid another wave of breakout during the Chunyun season (a period of travel in China with extremely high traffic load around the Chinese New Year). By formalizing the challenge of finding the optimal vaccine delivery among various different population groups (eg, sex, age and socioeconomic\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:tYavs44e6CUC", "title": "Parameter Estimation for the SEIR Model Using Recurrent Nets (preprint)", "authors": ["Chun Fan", "Yuxian Meng", "Xiaofei Sun", "Fei Wu", "Tianwei Zhang", "Jiwei Li"], "description": "The standard way to estimate the parameters (eg, the transmission rate ) of an SEIR model is to use grid search, where simulations are performed on each set of parameters, and the parameter set leading to the least  distance between predicted number of infections and observed infections is selected. This brute-force strategy is not only time consuming, as simulations are slow when the population is large, but also inaccurate, since it is impossible to enumerate all parameter combinations. To address these issues, in this paper, we propose to transform the non-differentiable problem of finding optimal  to a differentiable one, where we first train a recurrent net to fit a small number of simulation data. Next, based on this recurrent net that is able to generalize SEIR simulations, we are able to transform the objective to a differentiable one with respect to , and straightforwardly obtain its optimal value. The proposed strategy is both time efficient as it only relies on a small number of SEIR simulations, and accurate as we are able to find the optimal  based on the differentiable objective. On two COVID-19 datasets, we observe that the proposed strategy leads to significantly better parameter estimations with a smaller number of simulations.", "publication_year": 2021, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:dhFuZR0502QC", "title": "Position Paper: Consider Hardware-enhanced Defenses for Rootkit Attacks", "authors": ["Guangyuan Hu", "Tianwei Zhang", "Ruby B Lee"], "description": "Rootkits are malware that attempt to compromise the system\u2019s functionalities while hiding their existence. Various rootkits have been proposed as well as different software defenses, but only very few hardware defenses. We position hardware-enhanced rootkit defenses as an interesting research opportunity for computer architects, especially as many new hardware defenses for speculative execution attacks are being actively considered. We first describe different techniques used by rootkits and their prime targets in the operating system. We then try to shed insights on what the main challenges are in providing a rootkit defense, and how these may be overcome. We show how a hypervisor-based defense can be implemented, and provide a full prototype implementation in an open-source cloud computing platform, OpenStack. We evaluate the performance overhead of different defense mechanisms. Finally, we\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:9ZlFYXVOiuMC", "title": "A Software Stack for Composable Cloud Robotics System", "authors": ["Yuan Xu", "Tianwei Zhang", "Sa Wang", "Yungang Bao"], "description": "Modern cloud robotic applications face new challenges in managing today\u2019s highly distributed and heterogeneous environment. For example, the application programmers must make numerous systematical decisions between the local robot and the cloud server, such as computation deployment, data sharing and function integration. In this paper, we propose RobotCenter, a composable cloud robotics operating system for developing and deploying robotics applications. RobotCenter provides three key functionalities: runtime management, data management and programming abstraction. With these functionalities, RobotCenter enables application programmers to easily develop powerful and diverse robotics applications. Meanwhile, it can efficiently execute these applications with high performance and low energy consumption. We implement a prototype of the design above and use an example of AGV/UAV\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:ULOm3_A8WrAC", "title": "Revisiting and Evaluating Software Side-channel Vulnerabilities and Countermeasures in Cryptographic Applications", "authors": ["Tianwei Zhang", "Jun Jiang", "Yinqian Zhang"], "description": "We systematize software side-channel attacks with a focus on vulnerabilities and countermeasures in the cryptographic implementations. Particularly, we survey past research literature to categorize vulnerable implementations, and identify common strategies to eliminate them. We then evaluate popular libraries and applications, quantitatively measuring and comparing the vulnerability severity, response time and coverage. Based on these characterizations and evaluations, we offer some insights for side-channel researchers, cryptographic software developers and users. We hope our study can inspire the side-channel research community to discover new vulnerabilities, and more importantly, to fortify applications against them.", "publication_year": 2019, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:KlAtU1dfN6UC", "title": "An Architecture for Security Health Monitoring and Attestation in Cloud Computing", "authors": ["Ruby Lee", "Tianwei Zhang"], "description": "An Architecture for Security Health Monitoring and Attestation in Cloud Computing \u2014 Princeton \nUniversity Skip to main navigation Skip to search Skip to main content Princeton University \nHome Princeton University Logo Help & FAQ Home Profiles Research units Facilities Projects \nResearch output Search by expertise, name or affiliation An Architecture for Security Health \nMonitoring and Attestation in Cloud Computing Ruby Lee (Inventor), Tianwei Zhang (Inventor) \nPrinceton University Research output: Innovation Overview Fingerprint Original language \nEnglish State Published - Apr 7 2016 Keywords computers/software cyber security data \nsecurity Other files and links Project page Fingerprint Dive into the research topics of 'An \nArchitecture for Security Health Monitoring and Attestation in Cloud Computing'. Together they \nform a unique fingerprint. Cloud computing Engineering & Materials Science 100% Health \u2026", "publication_year": 2016, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:WF5omc3nYNoC", "title": "Heat dissipation structure of chip", "authors": null, "description": "A heat dissipation structure of a chip in the field of microelectronics is provided. The heat dissipation structure includes a P-type superlattice layer and an N-type superlattice layer formed over an upper surface of the chip by oxidation isolation. The P-type superlattice and the N-type superlattice are isolated by silicon oxide. Through a contact hole the P-type superlattice is electrically connected to a metal layer that is applied with a low potential in the chip, and a metal layer to be connected with an external power source is formed over the P-type superlattice. Through a contact hole the N-type superlattice is electrically connected to a metal layer that is applied with a high-potential power source in the chip, and a metal layer to be connected with an external power source is formed over the N-type superlattice. The potential of the external power source connected with the P-type superlattice is lower than that of the\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:9vf0nzSNQJEC", "title": "Supplementary Materials of Improving Adversarial Robustness of 3D Point Cloud Classification Models", "authors": ["Guanlin Li", "Guowen Xu", "Han Qiu", "Ruan He", "Jiwei Li", "Tianwei Zhang"], "description": "Theorem 1 Let f be a function that maps a point cloud to the feature space, and Q be the distribution of clean point clouds. S is sampled from Q. Qk (S, \u03f5) is the distribution of noisy point clouds, in which each element Sk is perturbed from S with an additional noise \u03f5, and the difference of numbers of points between S and Sk is smaller than a constant k, ie,\u2212 k\u2264| Sk|\u2212| S|\u2264 k. Then for every S\u223c Q and Sk\u223c Qk (S, \u03f5), the mutual information I (Sk, f (S)) has a lower bound, which is negatively correlated with the k-measurement Mk (f, S, Sk).", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vpiYDIAAAAJ&cstart=100&pagesize=100&citation_for_view=9vpiYDIAAAAJ:UebtZRa9Y70C", "title": "Security Verification of Secure Processor Architectures and Systems", "authors": ["Jakub Szefer", "Tianwei Zhang", "Ruby B Lee"], "description": "In the last decade, a number of secure processor architectures have been proposed in academia, and now some are available in consumer products, such as Intel\u2019s SGX or AMD\u2019s SEV. However, most, if not all, of the designs are not thoroughly security verified, bringing into question the security of these architectures, and systems built around them. To address this issue, in this paper we present a new and practical framework for security verification. Specifically, we break the verification task into external verification and internal verification. External verification considers the external protocols, ie interactions between management servers, compute servers, trusted third parties, and the customers. Meanwhile, internal verification considers the interactions between hardware and software components within each server. This verification framework is general-purpose and can be applied to a stand-alone server, a distributed system or a large-scale cloud system. We evaluate our verification method on the CloudMonatt [1] and Hyper-Wall [2] architectures, using both ProVerif and Murphi verification tools, as examples.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}}]