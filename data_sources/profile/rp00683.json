{"full_name": "Lin Weisi", "email": "wslin@ntu.edu.sg", "google_scholar": "https://scholar.google.com/citations?user=D_S41X4AAAAJ&hl=en&oi=ao", "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp00683", "designation": "Associate Chair (Research), School of Computer Science and Engineering", "image_path": "./profile_img/lin_weisi.jpg", "biography": "Weisi Lin graduated from Sun Yat-Sen (Zhongshan) University, China with B.Sc in Electronics and M.Sc in Digital Signal Processing, respectively, and from King\u2019s College, London University, UK with Ph.D in Computer Vision. He taught and researched in Sun Yat-Sen University, Shantou University (China), Bath University (UK), National University of Singapore, Institute of Microelectronics (Singapore), and Institute for Infocomm Research (Singapore). He has been the project leader of 10+ successfully delivered projects in digital multimedia technology development. He also served as the Lab Head, Visual Processing, and then the Acting Department Manager, Media Processing, in Institute for Infocomm Research.  He holds 16 patents, wrote 9 book chapters, edited 3 books,  authored 2 books, published over 400 refereed papers in international journals and conferences, and made more than ten contributions to international standardization. He believes that good theory is practical so has kept a balance of academic research and industrial deployment throughout his working life.", "orcid": "https://orcid.org/0000-0001-9866-1947", "other_websites": ["https://personal.ntu.edu.sg/wslin"], "bachelor_degree": null, "masters": null, "phd": null, "name": "Lin Weisi", "id": "rp00683", "publications": {"Publication Year": ["2011", "2011", "2017", "2005", "2015", "2012", "2014", "2018", "2016", "2015", "2015", "2012", "2011", "2016", "2015", "2012", "2013", "2010", "2017", "2005", "2005", "2014", "2017", "2015", "2005", "2013", "2016", "2016", "2012", "2017", "2003", "2010", "2006", "2014", "2014", "2012", "2014", "2017", "2011", "2011", "2021", "2013", "2016", "2006", "2019", "2008", "2012", "2016", "2004", "2017", "2013", "2017", "2016", "2008", "2014", "2008", "2020", "2017", "2016", "2005", "2018", "2019", "2018", "2011", "2021", "2015", "2017", "2016", "2012", "2016", "2015", "2019", "2016", "2014", "2018", "2017", "2019", "2011", "2016", "2013", "2017", "2005", "2008", "2016", "2013", "Unknown", "2016", "2016", "2020", "2012", "2010", "2016", "2015", "2008", "2015", "2019", "2016", "2008", "2016", "2013", "2011", "2003", "2016", "2012", "2007", "2016", "2016", "2011", "2013", "2021", "2013", "2012", "2021", "2015", "2013", "2008", "2019", "2014", "2009", "2010", "2005", "2015", "2016", "2014", "2016", "2015", "2013", "2012", "2003", "2019", "2017", "2004", "2021", "2017", "2015", "2011", "2017", "2015", "2014", "2011", "2015", "2015", "2015", "2015", "2008", "2014", "2011", "2016", "2010", "2005", "2016", "2015", "2016", "2016", "2015", "2011", "2011", "2017", "2016", "2012", "2011", "2011", "2005", "2010", "2006", "2019", "2018", "2015", "2015", "2017", "2016", "2016", "2015", "2006", "2022", "2022", "2016", "2007", "2019", "2017", "2014", "2009", "2006", "2018", "2017", "2015", "2004", "2004", "2021", "2017", "2022", "2017", "2013", "2015", "2006", "2015", "2011", "2019", "2017", "2016", "2010", "2016", "2022", "2017", "2016", "2014", "2012", "2010", "2008", "2005", "2021", "2021", "2012", "2021", "2019", "2015", "2014", "2009", "2002", "2016", "2015", "2015", "2006", "2005", "2017", "2015", "2014", "2012", "2011", "2006", "2006", "2006", "2005", "1993", "2022", "2019", "2018", "2005", "2004", "2020", "2019", "2018", "2016", "2015", "2015", "2003", "2021", "2019", "2019", "2011", "2007", "2004", "2017", "2015", "2012", "2020", "2019", "2017", "2017", "2016", "2015", "2014", "2012", "2007", "2017", "2016", "2016", "2016", "2015", "2011", "2011", "2003", "2016", "2016", "2015", "2014", "2011", "2021", "2019", "2017", "2017", "2017", "2016", "2022", "2021", "2021", "2019", "2018", "2018", "2016", "2016", "2015", "2014", "1997", "2018", "2016", "2013", "2007", "2021", "2019", "2019", "2018", "2014", "2013", "2012", "2005", "2022", "2022", "2021", "2017", "2016", "2012", "2012", "2003", "2017", "2017", "2016", "2015", "2012", "2023", "2020", "2016", "2015", "2014", "2012", "2022", "2016", "2014", "2011", "2005", "2003", "2023", "2021", "2019", "2016", "2015", "2012", "2010", "2007", "2003", "2023", "2023", "2022", "2022", "2015", "2015", "2015", "2013", "2008", "2004", "2023", "2022", "2022", "2020", "2017", "2017", "2017", "2016", "2016", "2014", "2011", "2011", "2010", "2010", "2010", "2004", "2003", "2023", "2019", "2018", "2017", "2016", "2015", "2015", "2015", "2012", "2011", "2003", "2022", "2022", "2019", "2015", "2015", "2014", "2013", "2013", "2010", "2008", "2008", "2003", "2022", "2021", "2021", "2014", "2012", "2012", "2012", "2011", "2009", "2007", "2005", "2003", "2003", "1997", "2022", "2021", "2021", "2019", "2016", "2015", "2015", "2014", "2013", "2012", "2011", "2009", "2004", "2003", "1998", "2022", "2021", "2017", "2016", "2015", "2015", "2014", "2014", "2013", "2011", "2010", "2010", "2009", "2007", "2005", "1997", "2023", "2023", "2022", "2021", "2017", "2017", "2016", "2015", "2015", "2014", "2014", "2014", "2013", "2013", "2013", "2012", "2010", "2008", "2007", "2004", "2001", "2001", "2023", "2023", "2022", "2022", "2022", "2021", "2021", "2019", "2018", "2018", "2017", "2015", "2015", "2015", "2011", "2011", "2009", "2008", "2007", "2006", "2006", "2004", "2004", "2003", "1999", "2023", "2023", "2021", "2019", "2018", "2018", "2017", "2017", "2016", "2016", "2015", "2015", "2015", "2015", "2013", "2013", "2013", "2013", "2012", "2012", "2012", "2011", "2011", "2011", "2006", "2003", "2003", "2023", "2023", "2023", "2023", "2022", "2022", "2019", "2019", "2019", "2018", "2017", "2016", "2016", "2016", "2016", "2015", "2015", "2013", "2012", "2011", "2011", "2010", "2008", "2008", "2008", "2007", "2005", "2005", "2003", "2002", "2001", "2001", "1999", "1997", "2023", "2023", "2022", "2022", "2022", "2021", "2021", "2021", "2020", "2020", "2020", "2019", "2018", "2016", "2015", "2015", "2015", "2015", "2015", "2015", "2015", "2014", "2014", "2013", "2013", "2013", "2013", "2012", "2012", "2012", "2011", "2010", "2009", "2008", "2007", "2006", "2005", "2004", "2004", "2001", "1992", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "2022", "2022", "2022", "2021", "2021", "2020", "2020", "2020", "2019", "2019", "2019", "2017", "2017", "2017", "2017", "2017", "2016", "2015", "2015", "2015", "2015", "2014", "2014", "2014", "2014", "2014", "2014", "2014", "2013", "2013", "2013", "2013", "2013", "2013", "2013", "2013", "2013", "2012", "2012", "2011", "2011", "2010", "2009", "2008", "2007", "2007", "2004", "1997", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "Title": ["Perceptual visual quality metrics: A survey", "Image quality assessment based on gradient similarity", "Learning a no-reference quality assessment model of enhanced images with big data", "Just noticeable distortion model and its applications in video coding", "The analysis of image contrast: From quality assessment to automatic enhancement", "Saliency detection in the compressed domain for adaptive image retargeting", "No-reference quality assessment of contrast-distorted images based on natural scene statistics", "Review of visual saliency detection with comprehensive information", "No-reference quality metric of contrast-distorted images based on information maximization", "No-reference image sharpness assessment in autoregressive parameter space", "A patch-structure representation method for quality assessment of contrast changed images", "Perceptual quality metric with internal generative mechanism", "A psychovisual quality metric in free-energy principle", "Saliency-guided quality assessment of screen content images", "No-reference image blur assessment based on discrete orthogonal moments", "A saliency detection model using low-level features based on wavelet transform", "A video saliency detection model in compressed domain", "Just noticeable difference for images with decomposition model for separating edge and textured regions", "No-reference quality assessment of screen content pictures", "Modeling visual attention's modulatory aftereffects on visual sensitivity and quality evaluation", "Motion-compensated residue preprocessing in video coding based on just-noticeable-distortion profile", "Video saliency incorporating spatiotemporal cues and uncertainty weighting", "A fast reliable image quality predictor by fusing micro-and macro-structures", "Perceptual quality assessment of screen content images", "Improved estimation for just-noticeable visual distortion", "Perceptual full-reference quality assessment of stereoscopic images by considering binocular visual characteristics", "No-reference quality assessment for multiply-distorted images in gradient domain", "Blind quality assessment of tone-mapped images via analysis of information, naturalness, and structure", "Image quality assessment using multi-method fusion", "Optimizing multistage discriminative dictionaries for blind image quality assessment", "A no-reference quality metric for measuring image blur", "Objective image quality assessment based on support vector regression", "Estimating just-noticeable distortion for video", "Saliency detection for stereoscopic images", "Mulsemedia: State of the art, perspectives, and challenges", "Additive white Gaussian noise level estimation in SVD domain for images", "Saliency-based defect detection in industrial images by using phase spectrum", "Unified blind quality assessment of compressed natural, graphic, and screen content images", "Bottom-up saliency detection model based on human visual sensitivity and amplitude spectrum", "SVD-based quality metric for image and video using machine learning", "Hierarchical alternate interaction network for RGB-D salient object detection", "Reduced-reference image quality assessment with visual information fidelity", "Blind image quality assessment using statistical structural and luminance features", "Adaptive downsampling to improve image compression at low bit rates", "Deep dual-channel neural network for image-based smoke detection", "Cross-dimensional perceptual quality assessment for low bit-rate videos", "Image retargeting quality assessment: A study of subjective scores and objective metrics", "Analysis of distortion distribution for pooling in image quality prediction", "A locally adaptive algorithm for measuring blocking artifacts in images and videos", "Enhanced just noticeable difference model for images with pattern complexity", "Just noticeable difference estimation for images with free-energy principle", "Model-based referenceless quality metric of 3D synthesized images using local image description", "No-reference and robust image sharpness evaluation based on multiscale spatial and spectral features", "Just-noticeable difference estimation with pixels in images", "Objective quality assessment for image retargeting based on structural similarity", "Efficient image deblocking based on postfiltering in shifted windows", "End-to-end blind image quality prediction with cascaded deep neural network", "An iterative co-saliency framework for RGBD images", "Image sharpness assessment by sparse representation", "Visual distortion gauge based on discrimination of noticeable contrast changes", "Recurrent air quality predictor based on meteorology-and pollution-related factors", "A dilated inception network for visual saliency prediction", "Learning markov clustering networks for scene text detection", "Semisupervised biased maximum margin analysis for interactive image retrieval", "An engineered CRISPR-Cas12a variant and DNA-RNA hybrid guides enable robust and rapid COVID-19 testing", "Full-reference quality assessment of stereoscopic images by learning binocular receptive field properties", "No reference quality assessment for screen content images with both local and global feature representation", "Learning a blind quality evaluation engine of screen content images", "Fourier transform-based scalable image quality measure", "Reduced-reference quality assessment of screen content images", "Culturing fibroblasts in 3D human hair keratin hydrogels", "SGDNet: An end-to-end saliency-guided deep neural network for no-reference image quality assessment", "Orientation selectivity based visual pattern for reduced-reference image quality assessment", "Objective quality assessment for image retargeting based on perceptual geometric distortion and information loss", "Which has better visual quality: The clear blue sky or a blurry animal?", "Evaluating quality of screen content images via structural variation analysis", "Towards robust curve text detection with conditional spatial expansion", "Generalized biased discriminant analysis for content-based image retrieval", "Salient object detection with spatiotemporal background priors for video", "Perceptual visual signal compression and transmission", "Quality assessment of DIBR-synthesized images by measuring local geometric distortions and global sharpness", "Rate control for videophone using local perceptual cues", "Three dimensional scalable video adaptation via user-end perceptual quality assessment", "No-reference quality assessment of deblocked images", "Visual quality assessment: recent developments, coding applications and future trends", "Robust Image Coding Based upon Compressive Sensing", "A universal framework for salient object detection", "NMF-based image quality assessment using extreme learning machine", "Personality-assisted multi-task learning for generic and personalized image aesthetics assessment", "Low-complexity video quality assessment using temporal quality variations", "Perceptual video coding: Challenges and approaches", "Objective quality assessment and perceptual compression of screen content images", "Visual saliency detection with free energy theory", "Skin heat transfer model of facial thermograms and its application in face recognition", "A paraboost method to image quality assessment", "Toward intelligent sensing: Intermediate deep feature compression", "Just noticeable difference estimation for screen content images", "Efficient deblocking with coefficient regularization, shape-adaptive filtering, and quantization constraint", "Subjective and objective quality assessment of compressed screen content images", "Scene-based movie summarization via role-community networks", "Direct intermode selection for H. 264 video coding using phase correlation", "Just-noticeable-distortion profile with nonlinear additivity model for perceptual masking in color images", "Toward a blind deep quality evaluator for stereoscopic images based on monocular and binocular interactions", "HodgeRank on random graphs for subjective video quality assessment", "Blind image blur identification in cepstrum domain", "Backward registration-based aspect ratio similarity for image retargeting quality assessment", "Low-rank-based nonlocal adaptive loop filter for high-efficiency video compression", "Explore and model better I-frames for video coding", "Pattern masking estimation in image with structural uncertainty", "Unified information fusion network for multi-modal RGB-D and RGB-T salient object detection", "Joint bit allocation and rate control for coding multi-view video plus depth based 3D video", "Conjunctive patches subspace learning with side information for collaborative image retrieval", "PM\u2082. \u2085 monitoring: use information abundance measurement and wide and deep learning", "B-SHOT: A binary feature descriptor for fast and efficient keypoint matching on 3D point clouds", "Geometric optimum experimental design for collaborative image retrieval", "Adaptive downsampling/upsampling for better video compression at low bit rate", "Context-aware deep learning for multi-modal depression detection", "Salient region detection by fusing bottom-up and top-down features extracted from a single image", "Blind blur assessment for vision-based applications", "Robust image compression based on compressive sensing", "Contrast signal-to-noise ratio for image quality assessment", "Visual orientation selectivity based structure description", "Just-noticeable difference-based perceptual optimization for JPEG compression", "A long-term reference frame for hierarchical B-picture-based video coding", "Low-rank decomposition-based restoration of compressed images via adaptive noise estimation", "A closed-form estimate of 3D ICP covariance", "Selective visual attention: computational models and applications", "Content-based image compression for arbitrary-resolution display devices", "No-reference JPEG-2000 image quality metric", "Multiscale natural scene statistical analysis for no-reference quality evaluation of DIBR-synthesized views", "BSD: Blind image quality assessment based on structural degradation", "A new marker-based watershed algorithm", "Blind image quality assessment with active inference", "Studying personality through the content of posted and liked images on Twitter", "Screen image quality assessment incorporating structural degradation measurement", "Reducing location map in prediction-based difference expansion for reversible image data embedding", "BLIQUE-TMI: Blind quality evaluator for tone-mapped images based on local and global feature analyses", "Blind image quality assessment for stereoscopic images using binocular guided quality lookup and visual codebook", "Image quality assessment with degradation on spatial structure", "A visual attention model combining top-down and bottom-up mechanisms for salient object detection", "Guided image contrast enhancement based on retrieved images in cloud", "Visual quality assessment by machine learning", "Visual quality assessment by machine learning", "Visual quality assessment by machine learning", "No-reference noticeable blockiness estimation in images", "Learning structural regularity for evaluating blocking artifacts in JPEG images", "Spread spectrum image watermarking based on perceptual quality metric", "Audio and face video emotion recognition in the wild using deep neural networks and small datasets", "Video coding using the most common frame in scene", "An objective out-of-focus blur measurement", "Multi-task rank learning for image quality assessment", "Do others perceive you as you want them to? Modeling personality based on selfies", "Image quality assessment based on local linear information and distortion-specific compensation", "Sparse representation-based image quality index with adaptive sub-dictionaries", "Using binocular feature combination for blind quality assessment of stereoscopic images", "Nonintrusive quality assessment of noise suppressed speech with mel-filtered energies and support vector regression", "A multi-metric fusion approach to visual quality assessment", "Multiple-level feature-based measure for retargeted image quality", "Automated anterior segment OCT image analysis for angle closure glaucoma mechanisms classification", "Scalable image quality assessment with 2D mel-cepstrum and machine learning approach", "Random partial paired comparison for subjective video quality assessment via hodgerank", "Multimedia Analysis, Processing and Communications", "Measuring the negative impact of frame dropping on perceptual visual quality", "Bayesian error concealment with DCT pyramid for images", "Comparison of video quality metrics on multimedia videos", "Quality assessment for video with degradation along salient trajectories", "A highly efficient blind image quality assessment metric of 3-D synthesized images using outlier detection", "Visual object tracking by structure complexity coefficients", "Learning receptive fields and quality lookups for blind quality assessment of stereoscopic images", "Facial emotion recognition", "Do personality and culture influence perceived video quality and enjoyment?", "Learning ECOC code matrix for multiclass classification with application to glaucoma diagnosis", "Personality modeling based image recommendation", "Computational models for just-noticeable difference", "Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling", "Single image super-resolution quality assessment: a real-world dataset, subjective studies, and an objective metric", "Learning blind quality evaluator for stereoscopic images using joint sparse representation", "Using edge direction information for measuring blocking artifacts of images", "Lossy intermediate deep learning feature compression and evaluation", "QoE-guided warping for stereoscopic image retargeting", "Subjective quality assessment of screen content images", "Efficient quadtree based block-shift filtering for deblocking and deringing", "Perceptual quality and objective quality measurements of compressed videos", "Fine-grained quality assessment for compressed images", "B-SHOT: a binary 3D feature descriptor for fast Keypoint matching on 3D point clouds", "Cross-examination for angle-closure glaucoma feature detection", "Dynamic load-balancing between two processing means for real-time video encoding", "Visual distortion assessment with emphasis on spatially transitional regions", "Collaborative intelligence: Challenges and opportunities", "A prediction backed model for quality assessment of screen content and 3-D synthesized images", "Cmua-watermark: A cross-model universal adversarial watermark for combating deepfakes", "CVIQD: Subjective quality evaluation of compressed virtual reality images", "Video coding with dynamic background", "Video compression artifact reduction via spatio-temporal multi-hypothesis prediction", "Improved super-resolution reconstruction from video", "Perceptual quality assessment for 3D triangle mesh based on curvature", "Optimal compression plane for efficient video coding", "Point cloud saliency detection by local and global feature fusion", "Learning sparse representation for objective image retargeting quality assessment", "On predicting visual comfort of stereoscopic images: A learning to rank based approach", "Method and system for video quality measurements", "A data-driven point cloud simplification framework for city-scale image-based localization", "Adjacent context coordination network for salient object detection in optical remote sensing images", "3DHoPD: A fast low-dimensional 3-D descriptor", "Models of monocular and binocular visual perception in quality assessment of stereoscopic images", "Scale and orientation invariant text segmentation for born-digital compound images", "Surveillance video coding via low-rank and sparse decomposition", "Non-intrusive speech quality assessment with support vector regression", "Image error-concealment via block-based bilateral filtering", "Perceptual quality metric for compressed videos", "Progressive self-guided loss for salient object detection", "Pasadena: Perceptually Aware and Stealthy Adversarial Denoise Attack", "Additive log-logistic model for networked video quality assessment", "Occupancy map guided fast video-based dynamic point cloud coding", "Pairwise-comparison-based rank learning for benchmarking image restoration algorithms", "Perceptual screen content image quality assessment and compression", "Depth map coding for view synthesis based on distortion analyses", "Scalable image quality assessment based on structural vectors", "Method for generating a quality oriented significance map for assessing the quality of an image or video", "Saliency-based stereoscopic image retargeting", "Modelling human factors in perceptual multimedia quality: On the role of personality and culture", "Modelling the influence of personality and culture on affect and enjoyment in multimedia", "Fast edge-preserved postprocessing for compressed images", "Colour perceptual video quality metric", "Learning sparse representation for no-reference quality assessment of multiply distorted stereoscopic images", "Image retargeting quality assessment based on support vector regression", "Multiple description video coding based on human visual system characteristics", "An overview of perceptual processing for digital pictures", "Low-complexity video coding based on two-dimensional singular value decomposition", "Perceptual quality metric for h. 264 low bit rate videos", "Perceptual impact of edge sharpness in images", "Marker-based image segmentation relying on disjoint set union", "Perceptual Region-of-Interest (ROI) based scalable video coding", "An efficient video object segmentation scheme", "Illumination unification for person re-identification", "Reference-free quality assessment of sonar images via contour degradation measurement", "Learning a referenceless stereopair quality engine with deep nonnegativity constrained sparse autoencoder", "Demosaicing with improved edge direction detection", "Measuring blocking artifacts using edge direction information [image and video coding]", "Adversarial exposure attack on diabetic retinopathy imagery", "Cascaded parallel filtering for memory-efficient image-based localization", "Measuring individual video QoE: A survey, and proposal for future directions using social media", "Toward simultaneous visual comfort and depth sensation optimization for stereoscopic 3-D experience", "GridSAR: Grid strength and regularity for robust evaluation of blocking artifacts in JPEG images", "Incremental low-rank and sparse decomposition for compressing videos captured by fixed cameras", "Low bit rate quality assessment based on perceptual characteristics", "Voxel structure-based mesh reconstruction from a 3D point cloud", "Learning a unified blind image quality metric via on-line and off-line big training instances", "Blind image quality assessment with hierarchy: Degradation from local structure to deep semantics", "Saliency-based image retargeting in the compressed domain", "Image and Video Quality Measurement", "Video quality metric for low bitrate compressed videos", "Performance evaluation of visual tracking algorithms on video sequences with quality degradation", "Sparse depth odometry: 3D keypoint based pose estimation from dense depth data", "DECA: Recovering fields of physical quantities from incomplete sensory data", "Object-level attention for aesthetic rating distribution prediction", "Statistical and structural information backed full-reference quality measure of compressed sonar images", "Facial action recognition using very deep networks for highly imbalanced class distribution", "Rate-distortion optimized sparse coding with ordered dictionary for image set compression", "Maximum a posterior and perceptually motivated reconstruction algorithm: A generic framework", "Stereoscopic visual attention guided seam carving for stereoscopic image retargeting", "\u03b2-Phase poly(vinylidene fluoride) films encouraged more homogeneous cell distribution and more significant deposition of fibronectin towards the cell-material interface\u00a0\u2026", "Study of subjective and objective quality assessment of retargeted images", "LGPS: Phase based image quality assessment metric", "No-reference view synthesis quality prediction for 3-D videos based on color\u2013depth interactions", "Toward domain transfer for no-reference quality prediction of asymmetrically distorted stereoscopic images", "Color image quality assessment based on sparse representation and reconstruction residual", "Visual signal quality assessment", "Visual acuity inspired saliency detection by using sparse features", "Performance analysis, parameter selection and extensions to H. 264/AVC FRExt for high resolution video coding", "Recent developments and future trends in visual quality assessment", "PQSM-based RR and NR video quality metrics", "Visual structural degradation based reduced-reference image quality assessment", "Who likes what, and why? insights into personality modeling based on imagelikes\u2019", "Reliable Feature Selection for Automated Angle Closure Glaucoma Mechanism Detection", "Stereoscopic image retargeting based on 3D saliency detection", "Unsupervised malaria parasite detection based on phase spectrum", "Approximate intrinsic voxel structure for point cloud simplification", "No-reference image quality assessment with visual pattern degradation", "Content-insensitive blind image blurriness assessment using weibull statistics and sparse extreme learning machine", "Quality assessment of retargeted images by salient region deformity analysis", "Hierarchical feature degradation based blind image quality assessment", "Enhanced just noticeable difference model with visual regularity consideration", "Toward top-down just noticeable difference estimation of natural images", "Progress and opportunities in modelling just-noticeable difference (JND) for multimedia", "Temporal reasoning guided QoE evaluation for mobile live video broadcasting", "Blind image quality assessment based on joint log-contrast statistics", "Deep visual saliency on stereoscopic images", "Intermediate deep feature compression: the next battlefield of intelligent sensing", "Detecting keypoint sets on 3D point clouds via Histogram of Normal Orientations", "No-reference image quality assessment based on high order derivatives", "A general histogram modification framework for efficient contrast enhancement", "Non-intrusive quality assessment for enhanced speech signals based on spectro-temporal features", "Real time H. 263 video codec using parallel DSP", "Just noticeable difference for natural images using RMS contrast and feed-back mechanism", "Pairwise comparison and rank learning for image quality assessment", "Recent advances and challenges of visual signal quality assessment", "Perceptual quality evaluation on periodic frame-dropping video", "Low resolution information also matters: Learning multi-resolution representations for person re-identification", "Reduced-reference quality assessment of image super-resolution by energy change and texture variation", "Survey of visual just noticeable difference estimation", "Visual-attention-based pixel dimming technique for OLED displays of mobile devices", "Exploring V1 by modeling the perceptual quality of images", "A novel SVD-based image quality assessment metric", "Rotated orthogonal transform (ROT) for motion-compensation residual coding", "Buffer-constrained RD model-based rate control for H. 264/AVC", "Exploring the effectiveness of video perceptual representation in blind video quality assessment", "Selfreformer: Self-refined network with transformer for salient object detection", "Just noticeable difference for deep machine vision", "Lossless image and video compression", "Understanding deep representations learned in modeling users likes", "Video saliency detection in the compressed domain", "Gaussian noise level estimation in SVD domain for images", "Objective quality assessment for compressed video", "Subjective and objective quality evaluation of sonar images for underwater acoustic transmission", "High-efficiency image coding via near-optimal filtering", "Mobile acoustic emotion recognition", "No-reference hybrid video quality assessment based on partial least squares regression", "Learning based screen image compression", "Discovqa: Temporal distortion-content transformers for video quality assessment", "Content-dependency reduction with multi-task learning in blind stitched panoramic image quality assessment", "Aspect ratio similarity (ARS) for image retargeting quality assessment", "An inter-image redundancy measure for image set compression", "Emotional facial expression transfer based on temporal restricted Boltzmann machines", "Discretized-Vapnik-Chervonenkis dimension for analyzing complexity of real function classes", "A no-reference stereoscopic image quality assessment network based on binocular interaction and fusion mechanisms", "Quality assessment of 3D synthesized images via disoccluded region discovery", "Reduced-reference image quality assessment with local binary structural pattern", "McFIS in hierarchical bipredictve pictures-based video coding for referencing the stable area in a scene", "Geometrically determining the leaky bucket parameters for video streaming over constant bit-rate channels", "Discriminative analysis of pixel difference towards picture quality prediction", "Lightweight salient object detection in optical remote-sensing images via semantic matching and edge alignment", "LGGD+: Image retargeting quality assessment by measuring local and global geometric distortions", "A two-stage outlier filtering framework for city-scale localization using 3D SfM point clouds", "Quality assessment for image super-resolution based on energy change and texture variation", "Compression noise estimation and reduction via patch clustering", "Image activity measure (IAM) for screen image segmentation", "Perceptual image quality assessment: recent progress and trends", "Image quality measure using curvature similarity", "Perceptual-quality significance map (pqsm) and its application on video quality distortion metrics", "Collaborative structure and feature learning for multi-view clustering", "MMSMCNet: Modal memory sharing and morphological complementary networks for RGB-T urban scene semantic segmentation", "Disentangling aesthetic and technical effects for video quality assessment of user generated content", "Blind quality assessment of 3D dense point clouds with structure guided resampling", "Gradient-weighted structural similarity for image quality assessments", "Exploiting entropy masking in perceptual graphic rendering", "Subjective quality evaluation of compressed digital compound images", "Objective quality assessment for image retargeting based on perceptual distortion and information loss", "Non-intrusive respiratory monitoring system using Fiber Bragg Grating sensor", "Local visual perceptual clues and its use in videophone rate control", "Neighbourhood representative sampling for efficient end-to-end video quality assessment", "Interaction-matrix based personalized image aesthetics assessment", "Full RGB just noticeable difference (JND) modelling", "Data representation in hybrid coding framework for feature maps compression", "Learning visual saliency from human fixations for stereoscopic images", "Optimal region selection for stereoscopic video subtitle insertion", "Image quality assessment guided deep neural networks training", "Complex wavelet based quality assessment for AS-OCT images with application to angle closure glaucoma diagnosis", "Personalizing User Interfaces for improving quality of experience in VoD recommender systems", "Rate-perceptual-distortion optimization (RpDO) based picture coding\u2014Issues and challenges", "Machine learning based modeling of spatial and temporal factors for video quality assessment", "Fast environment matting extraction using compressive sensing", "Two dimensional singular value decomposition (2D-SVD) based video coding", "McFIS: better I-frame for video coding", "Objective image quality assessment with singular value decomposition", "Adaptive nonlinear diffusion processes for ringing artifacts removal on JPEG 2000 images", "Perceptually adaptive hybrid video encoding based on just-noticeable-distortion profile", "Camera contrast learning for unsupervised person re-identification", "End-to-end ensemble learning by exploiting the correlation between individuals and weights", "Pyramidal modeling of geometric distortions for retargeted image quality evaluation", "Using multiscale analysis for blind quality assessment of DIBR-synthesized images", "Low-complexity depth coding by depth sensitivity aware rate-distortion optimization", "Rate-distortion based sparse coding for image set compression", "Bag-of-words representation for non-intrusive speech quality assessment", "Image retargeting quality assessment", "Feature selection for computer-aided angle closure glaucoma mechanism detection", "Dual-level compressed aggregation: Recovering fields of physical quantities from incomplete sensory data", "Video quality assessment using neural network based on multi-feature extraction", "Learning detail-structure alternative optimization for blind super-resolution", "Deep learning-based perceptual video quality enhancement for 3D synthesized view", "Salient object detection by spatiotemporal and semantic features in real-time video processing systems", "Evaluating visual and textual features for predicting user \u2018likes\u2019", "Dominant SIFT: A novel compact descriptor", "Learning visual saliency for stereoscopic images", "Fast and efficient blind image quality index in spatial domain", "A semantic subspace learning method to exploit relevance feedback log data for image retrieval", "Performance of reconstruction-based super-resolution with regularization", "Gauging image and video quality in industrial applications", "Image deringing using quadtree based block-shift filtering", "Quality evaluation of MPEG-4 and H. 26L coded video for mobile multimedia communications", "Intrinsic and isotropic resampling for 3D point clouds", "StereoARS: Quality evaluation for stereoscopic image retargeting with binocular inconsistency detection", "Video quality assessment with serial dependence modeling", "Saliency detection in computer rendered images based on object-level contrast", "A fusion approach to video quality assessment based on temporal decomposition", "Method for encoding a picture, computer program product and encoder", "Mode-dependent templates and scan order for H. 264/AVC-based intra lossless coding", "Video quality assessment using temporal quality variations and machine learning", "Lossless video compression with optimal compression plane determination", "Video quality metrics-an analysis for low bit rate videos", "Perceived visual quality metric based on error spread and contrast", "A new dct-based just-noticeable distortion estimator for images", "A coding artifacts removal algorithm based on spatial and temporal regularization", "Real time full-duplex H. 263 video codec system", "Efficient geometry surface coding in V-PCC", "Fine-grained patch segmentation and rasterization for 3-d point cloud attribute compression", "Fine-grained image quality assessment: A revisit and further thinking", "Intermediate Deep Feature Compression: Toward Intelligent Sensing.", "Personality, culture, and system factors-impact on affective response to multimedia", "Nonlocal adaptive in-loop filter via content-dependent soft-thresholding for HEVC", "Reduced-reference image quality assessment with orientation selectivity based visual pattern", "Study on subjective quality assessment of digital compound images", "Visual masking estimation based on structural uncertainty", "Introduction to the special issue on new subjective and objective methodologies for audio and visual signal processing", "Perceptual multiview video coding using synthesized just noticeable distortion maps", "Modeling the masking effect of the human visual system with visual attention model", "Spatial selectivity modulated just-noticeable-distortion profile for video", "On incorporating just-noticeable-distortion profile into motion-compensated prediction for video compression", "Video object segmentation based on change detection and region growing", "Integratedpifu: Integrated pixel aligned implicit function for single-view human reconstruction", "Bi-disparity sparse feature learning for 3D visual discomfort prediction", "On creating low dimensional 3D feature descriptors with PCA", "A benchmark for robustness analysis of visual tracking algorithms", "Review of existing objective QoE methodologies", "Dense correspondence based prediction for image set compression", "No-reference perceptual image sharpness index using normalized DCT-based representation", "Rank learning on training set selection and image quality assessment", "Visual-saliency-enhanced image quality assessment indices", "Efficient video coding considering a video as a 3D data cube", "Enhanced just noticeable difference (JND) estimation with image decomposition", "Comparison between H. 264/AVC and Motion jpeg2000 for super-high definition video coding", "Fast visual saliency map extraction from digital video", "Content-based quality evaluation on frame-dropped and blurred video", "Just-noticeable distortion estimation for image pixels", "DSP implementation of very low bit rate videoconferencing system", "Exploring opinion-unaware video quality assessment with semantic affinity criterion", "Exploring video quality assessment on user generated contents from aesthetic and technical perspectives", "SRInpaintor: When super-resolution meets Transformer for image inpainting", "Blind image quality prediction with hierarchical feature aggregation", "Saliency change based reduced reference image quality assessment", "Effective visual tracking by pairwise metric learning", "Detection and estimation of supra-threshold distortion levels of pictures based on just-noticeable difference", "Facial scanning with a digital camera: a novel way of screening for primary angle closure", "3D point cloud simplification for image-based localization", "Operational rate-distortion shape coding with dual error regularization", "Objective visual quality assessment for 3D meshes", "Structural uncertainty based just noticeable difference estimation", "Visual quality metric for perceptual video coding", "Overview of quality assessment for visual signals and newly emerged trends", "Introduction to visual attention", "Proceedings of the 13th Pacific-Rim conference on Advances in Multimedia Information Processing", "Bayesian error concealment with DCT pyramid", "Defocus estimation from a single image", "Shifted window based filtering for alleviating blocking artifacts", "Perceptual video quality evaluation using fuzzy inference system", "Object segmentation with affine motion similarity measure", "Multiple motion object segmentation based on homogenous region merging", "Complementary networks for person re-identification", "A Thorough Benchmark and a New Model for Light Field Saliency Detection", "Minimum Noticeable Difference-Based Adversarial Privacy Preserving Image Generation", "HVS-inspired signal degradation network for just noticeable difference estimation", "Visual saliency and quality evaluation for 3D point clouds and meshes: An overview", "Learning image aesthetic assessment from object-level visual components", "Defense for adversarial videos by self-adaptive JPEG compression and optical texture", "Signal-independent separable KLT by offline training for video coding", "Robustness analysis of pedestrian detectors for surveillance", "Image quality assessment based label smoothing in deep neural network learning", "Optimising ensemble combination based on maximisation of diversity", "Performance scoring of singing voice", "Deep representations to model user \u2018likes\u2019", "Retargeted image quality assessment: Current progresses and future trends", "Selective rendering with graphical saliency model", "Bottom-up saliency detection model based on amplitude spectrum", "A comparative study on attention-based rate adaptation for scalable video coding", "Layered image resizing in compression domain", "An Adaptive Deblocking Filter for ROI-Based Scalable Video Coding", "A wavelet-based visible distortion measure for video quality evaluation", "Fast automatic video object segmentation for content-based applications", "Modelling visual attention and motion effect for visual quality evaluation", "Speech pitch detection in noisy environment using multi-rate adaptive lossless FIR filters", "Disjoint set data structure for morphological area operators", "Systematic analysis and methodology of real-time DSP implementation for hybrid video coding", "Towards Robust Text-Prompted Semantic Criterion for In-the-Wild Video Quality Assessment", "GCFAgg: Global and Cross-view Feature Aggregation for Multi-view Clustering", "A new image codec paradigm for human and machine uses", "Range image based point cloud colorization using conditional generative model", "Visual speech emotion conversion using deep learning for 3D talking head", "A novel distortion criterion of rate-distortion optimization for depth map coding", "No-reference image quality assessment with orientation selectivity mechanism", "Low Bit-rate 3D feature descriptors for depth data from Kinect-style sensors", "Efficient Lagrange multiplier selection algorithm for depth maps coding", "No-reference image quality assessment based on structural and luminance information", "An energy-constrained video retargeting approach for color-plus-depth 3D video", "Is pedestrian detection robust for surveillance?", "Observation model based perceptually motivated bilateral filter for image reconstruction", "Cloud based image contrast enhancement", "Fast synthesized and predicted just noticeable distortion maps for perceptual multiview video coding", "Visual object tracking based on appearance model selection", "A novel NMF-based image quality assessment metric using extreme learning machine", "Background of visual attention-theory and experiments", "No-reference image quality assessment for compressed images based on DCT coefficient distribution and PSNR estimation", "Laplacian Regularized Subspace Learning for interactive image re-ranking", "Yuming; Lin, Weisi; Lee, Bu-Sung; Lau, Chiew-Tong; Chen, Zhenzhong; Lin,\u201cBottom-Up Saliency Detection Model Based on Human Visual Sensitivity and Amplitude Spectrum,\u201d", "Biased subspace learning for SVM relevance feedback in content-based image retrieval", "Image quality assessment based on improved feature similarity metric", "Blind measurement of image blur for vision-based applications", "Two-layer image resizing for scalable codec", "Edge-adaptive color reconstruction for single-sensor digital cameras", "Weisi Lin, Zhongkaiig Lu, Xiaokang Yang, Susir Yao, Feng Pan, Lijilrn Jiarrg, and Fulvio Moscheni,\u201cA NO-REFERENCE QUALITY METRIC FOR MEASURING IMAGE BLUR\u201d", "Towards transparent deep image aesthetics assessment with tag-based content descriptors", "GMS-3DQA: Projection-based Grid Mini-patch Sampling for 3D Model Quality Assessment", "AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment", "Image Aesthetics Assessment with Attribute-Assisted Multimodal Memory Network", "S-PIFu: Integrating Parametric Human Models with PIFu for Single-view Clothed Human Reconstruction", "From Whole Video to Frames: Weakly-Supervised Domain Adaptive Continuous-Time QoE Evaluation", "Blind quality evaluator for screen content images via analysis of structure", "Separable KLT for intra coding in versatile video coding (VVC)", "Predicting visual saliency via a dilated inception module-based model", "Visual-quality guided global backlight dimming for video display on mobile devices", "Object tracking based on stable feature mining using intraframe clustering and interframe association", "Transform-domain in-loop filter with block similarity for HEVC", "No-reference image quality assessment based on local region statistics", "I2RNTU at SemEval-2016 Task 4: Classifier fusion for polarity classification in Twitter", "Quality assessment of contrast-altered images", "Do others Perceive You as You want them To?: Modeling Personality based on Sel es", "Improved salient object detection based on background priors", "2D mel-cepstrum based saliency detection", "Semi-Supervised Biased Maximum Margin Analysis for Interactive Image Retrieval Image Processing", "Image retargeting based on the sensitivity-tuned visual significance map", "Visual attention model for target search in cluttered scene", "Pattern based video coding with uncovered background", "Analysis of the H. 264 advanced video coding standard and an associated rate control scheme", "Unified deblocking for discrete cosine transfer compressed images", "Cross-dimensional quality assessment for low bitrate video", "Image Quality Assessment using Foveated Wavelet Error Sensitivity and Isotropic Contrast", "Demosaicing with Improved Edge Direction Detection", "Modelling Global modulatory factors in Just-Noticeable-Distortion Estimation", "A no-reference quality metric for measuring image blur", "On fast firmware/software-based video coding", "Application of analytical optimum fir compaction filters for tracking pitch of musical signals", "A Fast Video Object Segmentation Scheme for MPEG-4 Video Coding", "Methods for image quality assessment", "Task Division for Parallel Implementation of Object Identification System Based on Alternating Hypothesize-verify-Extend Strategy", "Visual Interaction Perceptual Network for Blind Image Quality Assessment", "AFD-former: A Hybrid Transformer with Asymmetric Flow Division for Synthesized View Quality Enhancement", "Adaptively Isotropic Remeshing based on Curvature Smoothed Field", "Bridging Component Learning with Degradation Modelling for Blind Image Super-Resolution", "Context Region Identification based Quality Assessment of 3D Synthesized Views", "Advanced geometry surface coding for dynamic point cloud compression", "Does deep machine vision have just noticeable difference (JND)", "GSTO: Gated Scale-Transfer Operation for Multi-Scale Feature Learning in Semantic Segmentation", "From technical to aesthetics quality assessment and beyond: challenges and potential", "ATQAM/MAST'20: Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends", "Gsto: Gated scale-transfer operation for multi-scale feature learning in pixel labeling", "Beyond ranking loss: Deep holographic networks for multi-label video search", "Benchmarking screen content image quality evaluation in spatial psychovisual modulation display system", "Quality assessment and perception in computer graphics", "Effect of spatial frequency on facial expression adaptation and awareness of emotion", "Visual pattern degradation based image quality assessment", "Do Others Perceive You As You Want Them To? Modeling Personality based on Selfies", "Image features and feature processing", "Fundamental Knowledge of Machine Learning", "Metrics fusion", "Moving Object Tracking with Structure Complexity Coefficients", "Multi-operator retargeting based on perceptual structural similarity", "Introduction to special issue on multiple sensorial (MulSeMedia) multimodal media: Advances and applications", "A saliency detection model based on sparse features and visual acuity", "To exploit uncertainty masking for adaptive image rendering", "Applications in computer vision, image retrieval and robotics", "Computational models for top-down visual attention", "Just noticeable distortion map prediction for perceptual multiview video coding", "Improve H. 264/AVC lossless coding by coefficients reordering", "HodgeRank on random graphs for subjective video quality assessment", "Adaptive orthogonal transform for motion compensation residual in video compression", "Simultaneous deblocking and error concealment for decoded visual signal", "Video object segmentation", "Recover image coding loss with LMS filtering", "Initial Image Selection and its Influence on Super-Resolution Reconstruction", "Recovery of compressed videos using forward and backward anisotropic diffusion", "Integrated Evaluation of Temporal and Spatial Distortions for Low Bit-rate Videos", "Perceptually-adaptive pre-processing for motion-compensated residue in video coding", "An effective perceptual weighting model for videophone coding", "Application of a Concatenated Coding System with Convolutional Codes and Reed-Solomon Codes to MPEG Advanced Audio Coding", "'Walking on two legs': alternate use of two representations of a match during object recognition and location", "HVS-inspired adversarial image generation with high perceptual quality", "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision", "Uplink-Assist Downlink Remote Sensing Image Compression via Historical Referecing", "Towards 3D Colored Mesh Saliency: Database and Benchmarks", "Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment", "TOPIQ: A Top-down Approach from Semantics to Distortions for Image Quality Assessment", "Regression-free Blind Image Quality Assessment", "Inter-Intra Camera Identity Learning for Person Re-Identification with Training in Single Camera", "Multi-level Part-aware Feature Disentangling for Text-based Person Search", "Advancing Zero-Shot Digital Human Quality Assessment through Text-Prompted Evaluation", "Multi-level Transitional Contrast Learning for Personalized Image Aesthetics Assessment", "You Can Mask More For Extremely Low-Bitrate Image Compression", "Towards Explainable In-the-Wild Video Quality Assessment: a Database and a Language-Prompted Approach", "Blind Multimodal Quality Assessment: A Brief Survey and A Case Study of Low-light Images", "Deep Blind Image Quality Assessment Powered by Online Hard Example Mining", "Object re-identification using multiple cameras", "KSS-ICP: Point Cloud Registration Based on Kendall Shape Space", "The First Comprehensive Dataset with Multiple Distortion Types for Visual Just-Noticeable Differences", "MetaGrad: Adaptive Gradient Quantization with Hypernetworks", "JND-Based Perceptual Optimization For Learned Image Compression", "Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks", "IntegratedPIFu: Integrated Pixel Aligned Implicit Function for Single-view Human Reconstruction", "Channel-Wise Bit Allocation for Deep Visual Feature Quantization", "Visual Signal Assessment, Analysis and Enhancement for Low-resolution or Varying-illumination Environment", "Auto-Weighted Layer Representation Based View Synthesis Distortion Estimation for 3-D Video Coding", "Lightweight Salient Object Detection in Optical Remote Sensing Images via Feature Correlation", "VaNGuard assay protocol for SARS-CoV-2 detection", "VaNGuard assay protocol for SARS-CoV-2 detection (preprint)", "Statistical Convolution On Unordered Point Set", "ATQAM/MAST'20 (Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends) Organization", "Rating Distribution and Personality Prediction for ImageAesthetics Assessment", "Video Frame Synthesis via Plug-and-Play Deep Locally Temporal Embedding", "Robust Super-resolution Using Multiple Bases and 3D Filtering", "No-reference image quality assessment with visual pattern degradation", "Computer Vision\u2013Aided Video Coding", "Technical program overview.", "Optimising ensemble combination based on maximisation", "Object Tracking Based on Stable Feature Mining Using", "Studying Personality through the Content of Posted and Liked Images", "from Kinect-style Sensors, Signal Processing: Image Communication", "Image retargeting quality assessment based on support vector regression, Image Communication, v. 39 n", "Visual Quality Assessment by Machine Learning", "Summary and Remarks for Future Research", "Feature Pooling by Learning", "Orientation selectivity-based structure for texture classification", "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) Volume 11 Issue 1s", "Correlation based universal image/video coding loss recovery", "Special issue on QoE in 2D/3D video systems.", "Introduction to Special issue on Multiple Sensorial (MulSeMedia) Multi-modal Media: Advances and Applications", "Multiple sensorial (MulSeMedia) multimodal media: advances and applications", "Advances in Multimedia Content Analysis and Signal Processing", "Detection of salient objects in computer synthesized images based on object-level contrast", "Visual masking estimation based on structural uncertainty", "QoE Modeling and Applications for Multimedia Systems", "Application of Attention Models in Image Processing", "Summary, Further Discussions and Conclusions", "Low, Yuen Kei Adarina; Zou, Xi; Fang, Yuming; Wang, Junling; Lin, Weisi; Boey, Freddy Yin Chiang; Ng, Kee Woei", "Validation and Evaluation for Visual Attention Models", "Computational Models in the Spatial Domain", "Facial Scanning With a Digital Camera: A Novel Way of Screening for Primary Angle Closure", "DECA: recovering fields of physical quantities from incomplete sensory data", "An overview of perceptual processing for digital pictures", "Joint visual attention and rendering complexity based sample rate estimation in selective rendering", "A psychovisual quality metric in free-energy principle", "Optimal compression plane (OCP)\u2014A new framework for H. 264 video coding", "Perception based down sampling for low bit rate image coding", "Mobile video processing for visual saliency map determination", "A Unified Framework for Removing Blocking Artifacts", "Image Super-Resolution Framework with Multi-Channel Constraints", "Blocking artifacts reduction for DCT-based image compression using neurofuzzy driven anisotropic diffusion", "Task Division for Parallel Implementation of Object Identification System Based on Alternating Hypothesize-verify-Extend Strategy", "Joint Multi-Dimensional Dynamic Attention and Transformer for Efficient Image Restoration", "Supplementary Materials for S-PIFu: Integrating Parametric Human Models with PIFu for Single-view Clothed Human Reconstruction", "BEYOND RANKING LOSS: DEEP HOLOGRAPHIC NETWORKS FOR MULTI-LABEL VIDEO SEARCH", "Visual quality assessment: recent developments, coding applications and future trends", "BLIQUE-TMI: Blind Quality Evaluator for Tone-Mapped Images Based on Local and Global Feature Analyses", "ask Division for Parallel Implementation of Object Identification System Based on Alternating Hypothesize-verify-Extend Strategy", "Visual Signal Quality Assessment \u2013 Quality of Experience (QoE)", "Advances in Multimedia Information Processing  PCM 2012", "Visual Quality Assessment by Machine Learning", "Modeling Selective Visual Attention: Techniques and Applications", "Joint Video Team (JVT) of ISO/IEC MPEG & ITU-T VCEG (ISO/IEC JTC1/SC29/WG11 and ITU-T SG16 Q. 6)", "Pairwise comparison and rank learning for image quality assessment", "A paraboost method to image quality assessment", "On Predicting Visual Comfort of Stereoscopic Images: A Learning to Rank Based Approach\u201d", "Do Personality and Culture influence Perceived Video Quality and Enjoyment?", "Computational Models for Just-noticeable Difference (JND) in Visual Signal", "Reliable Feature Selection for Automated Angle Closure Glaucoma Mechanism Detection", "Visual Orientation Selectivity based Structure", "GC13 WS-QoEMC: Globecom 2013 Workshop-Quality of Experience for Multimedia Communications-Committees and Welcome", "Video Quality Assessment Using Radial Basis Function Neural Network", "Visual Processing Driven by Perceptual Quality Gauge: A Perspective", "On High Level Syntax Related to ROI for Scalable Video Coding Status: Input Document to JVT Purpose: Proposal", "VIDEO QUALITY EVALUATION BASED ON WAVELET VISIBLE DIFFERENCE MEASURE", "Perceptual Region-of-interest (ROI) based Scalable Video Coding Status: Input Document to JVT Purpose: Proposal Authors or Contacts", "A NEW VISUAL QUALITY METRIC FOR COMPRESSED IMAGE AND VIDEO EVALUATION", "PERCEPTION-BASED DISTRIBUTED IMAGE CODING", "Determine Visual Just-noticeable Difference (JND) for Multimedia Applications", "ENHANCED JUST NOTICEABLE DIFFERENCE (JND) ESTIMATION WITH IMAGE DECOMPOSITION FOR SEPARATING EDGE AND TEXTURED REGIONS", "Flexible Complexity Control and Dynamic Load Balancing for Hybrid Video Coding"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:uLbwQdceFCQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:OLNndOjO69MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:tYavs44e6CUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:kiex5tMzGo8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:3NQIlFlcGxIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:3pYxbvHKFu8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:7XCffrwrS2sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:QC-2xSqExF4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:WTSGYGHz1bkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:oYVvnHz_XzQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:HGTzPopzzJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:Tiz5es2fbqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:N5XbD978G_MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:h-xndbdg2koC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:DyXnQzXoVgIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:1xBWf43XMUgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:CHSYGLWDkRkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:XdYaqolBBq8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&citation_for_view=D_S41X4AAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:vofGIMt6cyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:eHo_KFcuhuIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:m44aUaJR3ikC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:gsN89kCJA0AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:aDl3D7KC1E4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:QndmRo8phpgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:0vYOBEH00j0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:yTLRzDEmwhEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:BxcezVm2apwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:YOwf2qJgpHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:3x-KLxxGyuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:WTQy_8Ay2UsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:kzcSZmkxUKAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:DwFgw5hZUzMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:bczYY1dZPtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:ufrVoPGSRksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:bcT4vkklUMwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:cNe27ouKFcQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:EO1llL0aI9sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:G60ApcfeQaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:jFemdcug13IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:IF0FbId8XVwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:TIZ-Mc8IlK0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:69viAa4lnfgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:SrKkpNFED5gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:9AucIJLffosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:r655XaDZu5IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:6Zm5LS9gQ5UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:_Qo2XoVZTnwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:45AZ0Vt6gvEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:I__7AI8a974C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:H0nRGT7Dr7IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:mqcSLZKRP28C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:uHQrz-U2knEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:d4Uf0zfqV5IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:zxqBrjVgvjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:Uo5fLKClJkAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:f13iAvnbnnYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:SEmTV4UfSqwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:0d9pApVQ-n0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:MpfHP-DdYjUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:lDOOmgye57wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:FkBsMxS_Bp0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:BjLbhSWBl98C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:irE4lMk4wWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:sYWwZaPVD1oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:MDBbo4b0KHEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:A7-hzOuI2KQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:XUmZziu-z7kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:Xtec1x7NZGAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:5rMqqAh47xYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:Et1yZiPVzsMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:hXZnTIgIr50C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:Ak0FvsSvgGUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:fXCg-C-QWH4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:XJogQLJr2CkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:Z610oKUqOA4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:cK4Rrx0J3m0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:kVKjLOQJbwAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:TewouNez5YAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:RHpTSmoSYBkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:dyiPZ63SVtYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:DxlTmyU89zoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:DGpvO1n63MYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:5thYEm8kiqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=20&pagesize=80&citation_for_view=D_S41X4AAAAJ:qCpRzq7zkD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:vDZJ-YLwNdEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:kz9GbA2Ns4gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:m03se8k-GH0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:7Hz3ACDFbsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:hwlm9Y4obscC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:vYYylRVofzEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:8AbLer7MMksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:2Q0AJrNhS-QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:2BeMVx_SZpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:tgTmbKTkO1IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:artPoR2Yc-kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:IT5EXw6i2GUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Vch7EZszQGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:DGzKIA18-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:KUazKHuGu6AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:q09DtPQ_434C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:hMod-77fHWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:u9iWguZQMMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:WbkHhVStYXYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:z-B63o8J19IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:bxbQgRQgr4sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:bYbwfsIO_fQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:RZBefGmQYygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:uUvzmPk0f8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:XD-gHx7UXLsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:RW1BPcyHXiwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:WMj-6b1RDO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:EXDW3tg14iEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Dh4RK7yvr34C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:5N-NJrZHaHcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:1DsIQWDZLl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:BZGggv0hN9sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:dJ_BR67V0s4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Yw6v6SrDvuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:IX653JsL2_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:aFwNBOTKqaYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:PbVfkCSoiPoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:njp6nI0QjqAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:svGagg1hbZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:URolC5Kub84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:LwieBGrN4GEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:5ugPr518TE4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:aVq8r21TQD4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:C-SEpTPhZ1wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:0Tn0GYh_KFEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:BSZS1fLCB98C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:URGbnSt0D2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:AvfA0Oy_GE0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:lf0D1wPZyaoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:qzPvQt7yPGwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:9Nmd_mFXekcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Bg7qf7VwUHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:NDuN12AVoxsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:J-pR_7NvFogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:e5wmG9Sq2KIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:PpfTWA6RG0gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:p9YawgimX9oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:OkrdTXRNpVkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:o0eFlWRwRSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Z98IfIjqAwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:xUD2GqFNeDMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:PQ6BjWB6CiEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:gI9wzKcniAoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:ruyezt5ZtCIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZuSUVyMx-TgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:vs4DU1qUSb8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:X1y8G5kAG8gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:SpbeaW3--B0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:P5iVsrg4GywC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:PN3Jufe71OMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:-ZoC36zw86wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:2P1L_qKh6hAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:fGgrif5g-LMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:5s9rAH04UEoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:tL5YfqkXb3gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:twffdjNOitAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:HygtOXotxAUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:2PyGcyYA7d0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:TPlbvKeUp3cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:vbti4gW_9XwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:e-nnqdki048C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:ddB7do2jUx8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:Y0pCki6q_DkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:r5yabEp13iMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=100&pagesize=100&citation_for_view=D_S41X4AAAAJ:PbrqR9PZhrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:edV_OwlUe4UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:p7qoFRH4VUUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:cCmJLe1CRJUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:FGHlONkF9GcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:zTJoPluU4X4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:M8meJADSprsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:wSy_KLzO7YEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:Ns2bVKt8YxIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:RsqFu5Siv-AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:renmPk63pRgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:tz746QTLzJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:jW3pDOCzWhQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:Oi-j_DTgP3cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:rKYHOwt5kWwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:Pqt4MY__2vwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:Ug5p-4gJ2f0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:3s2jc9hNhkQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:TuM7UPshZo8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:WsFh9Szeq2wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:UO8fSLLLPykC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:PD-wXv1Sh1EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:sfsSB7lKuh0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:AXkvAH5U_nMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:kzcrU_BdoSEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:foquWX3nUaYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:k8Z6L05lTy4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:SPgoriM2DtkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:dZbaGXT4iR0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:1HYIo8DVeu0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:i2xiXl-TujoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:lonyfjS0l_UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:A5aiAONn640C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:xggF7591RyAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:aXwx4OqTWR4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZZwcLRaXOV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:mS4qin7VKjkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:Fu4hY69slDoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:lAfYOIClFDQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:lR2ECBI0YV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:zLWjf1WUPmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:k_IJM867U9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:PFhWnMnuT1QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:6ScxedgR18sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:OBSaB-F7qqsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:zCpYd49hD24C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:-Viv1fr_sjoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:tUtwDVdCwjgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:1AS7WB7zg6gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:N7YTE_TVRugC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:nnITTVbzT6kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:43LB_KcVqeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:IaI1MmNe2tcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:YDt8mvgFZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:23Hg5vt_rPQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:5kvG9DmRKWYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:eJD0kABLposC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:yL7DKRohVA8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:fFSKOagxvKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:9UF2BbDYXHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:yBJCJstOu-sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:3AIi9tQMIrsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:rDsFeusoTZkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:9c2xU6iGI7YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:r4zddjZt6C4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:UO6ax3c-pNsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:5bFWG3eDk9wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZVmBtQ9RJaQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:dQJM2trw0wsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:7CU1mCQUTf4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:HHUT0vUrEqMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:Y6EZgx1ah38C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:Ow2R9nchCv8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:q1ZQJjUA47MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:VglVhKISWcQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:w2Aknop99M4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:aJ-3-MYELVsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:EVKlETVVbN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:HevVnK7dagcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:jMZTt8odoasC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:tuHXwOkdijsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:2ZctHUgIzyAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=200&pagesize=100&citation_for_view=D_S41X4AAAAJ:P7PM_jyRRcwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:vxA22ZmNLkoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:hTqO-V9ugBQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:KR6TXPE-FHQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:PQ1NLOpCoVAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:dhZ1Wuf5EGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:ynsZFq2pu0MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:mo9XK3BEATIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:SjuI4pbJlxcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:xtoqd-5pKcoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:OtrppQHxQ5wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:WD7AgJrCjNIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:0D9gKr9vLLUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:d4tt_xEv1X8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:jvrRlaHg2sAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:JWITY9-sCbMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:ziOE8S1-AIUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:WYleWgRJor0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:2C0LhDdYSbcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:5LOebrzo1TYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:3WNXLiBY60kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:_AeoHAGD03cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:1TqUyixsFioC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:KKiikWAUrRgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:Ltc9rfRcsxEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:KEtq3P1Vf8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:S0CDQJw8Yr4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:6_hjMsCP8ZoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:d4paSpBSrDQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:1W67FsDfIBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:Rj7SuZOA3CsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:HbR8gkJAVGIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:M05iB0D1s5AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:xqRlItQsuMMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:AeQkyvggb0MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:Y4-Jr3-UGfkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:KxdiYzj_eL0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:eG7oJ4UONFcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:cocyvO7vrcwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:_xSYboBqXhAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:ClCfbGk0d_YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:P76ttB97BVgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:QLZAdv6BrvsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:8QO3eJiZnkEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:mq6pegT_rlEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:MDQ-9Oe3GGUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:OlbiQ0ttILcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:J-ba04ztB30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:KsTgnNRry18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:RJujIP1NYNUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:oNZyr7d5Mn4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:lLPirIASiZEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:wW8w_uPXRNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:J4wmHkHhN-kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:s_JjmAzd-pQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:4_yl7nwqy4oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:PWMd_Z0sy-4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:vNwWLMROEM0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:Qy-rCirNo-8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:tswL-GKFg8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:HAmI6pRF5skC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:-95Q15plzcUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:7T2F9Uy0os0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:BrmTIyaxlBUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:HE397vMXCloC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:PoWvk5oyLR8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:a0OBvERweLwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:8NHCvSvNRCIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:kGbpvR7Ecy8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:-152x6qmK5kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:A1VvXmYcXXgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:SoGhKUJvMTQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:wuYnf3tzzDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:q82PccF7nXcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:MvIMIWP2nqIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:Zbx7W2Xs4QsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:N2osnxbUuuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:2SmvwDDsShQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:coeFWI40FR8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:rMiSbtrAJs8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:gYnLYBw1zZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:fixghrsIJ_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:AubyX3KqGToC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:xckinRaLORAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:GHsHDPAyICYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:I96H1Mlar6gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:FSHXWovK7t4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=300&pagesize=100&citation_for_view=D_S41X4AAAAJ:x7X4uJ7IbpoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:oH8HCDhqVGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:MtS25d97-7AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:JjBZBFkNMTQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:MXK_kJrjxJIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:738O_yMBCRsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:ns9cj8rnVeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:LO7wyVUgiFcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:b1wdh0AR-JQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:_8F20clBW_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:5Y1KH4bkPm0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:Fr8DH2VBP9sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:_bh1rdP-zDcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:AmQcmOVUwm0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZbXHRDTBJTgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:x8G803Bi31IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:-qpA3cGbmHsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:Kqc1aDSOPooC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:WAzi4Gm8nLoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:uWiczbcajpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:mvPsJ3kp5DgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:P5F9QuxV20EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:wsDUewC2rQsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:MLfJN-KU85MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:X-Dm1JipzzIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:SSGWEqmz6gUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:AUmYgNQ2pq4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:96M4k3P_OWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:kxd3qP2_5uAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:HfY9tUF4VgMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:wH03y5nBhxsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:43B52WW2E64C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:o-PowTg_VKEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:sNmaIFBj_lkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:OU6Ihb5iCvQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:fQNAKQ3IYiAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:abG-DnoFyZgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:dY-VugTTHzcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:kuK5TVdYjLIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:0MsrIDK0EWAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:bQkGhl1z2hUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:G-26V_K0F8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:vM5yiaU9oLoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:l2sC-qQILUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:kuq-04jA658C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:F_tUKv7nyWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:1wZ_wKGpLuwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:fc7zyzPI2QAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:2SFquFhkCoYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:PazO6pb-sMwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:DwWRdx-KAo4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:I2jIoRS3jIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:QKtdBID3u5MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:ealulPZkXgsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:vlMkzkLhH4wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:Mojj43d5GZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:kRWSkSYxWN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:sSrBHYA8nusC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:ipzZ9siozwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:VaXvl8Fpj5cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:LI9QrySNdTsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:aXI_bbQgCfgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:0lma3NU_mqEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:RSvx26AYfG8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:8o7LCxyMrhgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:vlECJaBXBlQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:oQQVFBP0nzwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:ziW8EwMpto0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:iWMDS-dds2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:Cd1N8iHLSCsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:eCFM_hdDfssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:BO8BTsX0K1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:6sy8ByAbWgIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZYLUaBFA95QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:w5CyTnyFq80C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:WqliGbK-hY8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:olpn-zPbct0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:LgRImbQfgY4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:EUQCXRtRnyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:vDijr-p_gm4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:b0M2c_1WBrUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:WZBGuue-350C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:4fKUyHm3Qg0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:B3FOqHPlNUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:lmc2jWPfTJgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:I8ubwoE7ciMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:W2VW_RKN1OwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:3oYmN_DgFiMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:TpYZtc-0n-8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:hEp1lTclR2YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:ewB7xoWXMOgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:in81wS_EFI4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:rlwtDmSc194C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:7q08wCQPkLwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=400&pagesize=100&citation_for_view=D_S41X4AAAAJ:cAWJABFkdiUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:EmjvLWWcsQIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Lpa4s8qvUTIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:YXPZ0dOdYS4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:ve7iT2ZEuL4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:v1_lew4L6wgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:j2GSQqY3pL0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:DCYT7yIMjgYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:q2HS4OCVtYIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:rOcdG6UcVlcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:q-HalDI95KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:N8RR74vhTp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:BzfGm06jWhQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:dIILA_La5fwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Qovp55VTycgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:rQcm2j6_ZE8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Wu2CBzYYpfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:qNJvDq80yDAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:fveVehIkgekC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Q3_nmhWTCy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:w9HJn4dzO2wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:wdLM4YbmhYkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:RF4BjkDOTHkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:07QC26MHY14C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:cAX_JoMERPUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:0aNKY9CYzMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Xc8JtHQmRf8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:CzVmvSWNoUEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:vJdfVD8-6ZYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:HKuYlFsi-qEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Usae8uB1euoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:VDARjI3xf8gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:ohFW0PAxsewC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:PklR0melJeUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:z62hWG5Wpo0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:mlAyqtXpCwEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZfRJV9d4-WMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:EkHepimYqZsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:2KloaMYe4IUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Huv_iBJ06lEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:-FonjvnnhkoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Q9ss7R9eeXsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:NhqRSupF_l8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:3K1WWmVGZRgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:XoXfffV-tXoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:C6rTQemI8T8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:b2v3RnpdjZkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:22I2CSi1iVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:Rc-B-9qnGaUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:PRLG7g5oK-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:PPAp3RzCAaIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:uH1VZYVfkoQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:prvsfHNhuEoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:M3zsPnPgUlUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:C4Vd9JCM9EcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:mpaOjDK6XBIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:WzbvD7BMtBoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:SrsqWtBqNIQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:pXiCqeVOhFwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:HNqp4bORoCIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:V-j82LJCZ3MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:rrpmhsargb8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:pcWPcJyQGiUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:b8m_4JuPjscC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:H-3wYkpcA84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZJ-noXUx9mkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:dT4-KZ621vcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:TY5xIG7f_2sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:vkuYBMKU6wEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:jYy9R3AkpWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:cF7EPgIk0B4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:nOiSByfp82kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:KtrhGiDecpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:SkU8VjQp03IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:8VtEwCQfWZkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:7BrZ7Jt4UNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:fXbrI0tPCuEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:NJ774b8OgUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:eflP2zaiRacC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:VLnqNzywnoUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:fEOibwPWpKIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:uc_IGeMz5qoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:PVjk1bu6vJQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:e_rmSamDkqQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:x9HjRiAMpasC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:SGnGYi2HwEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:nyUGVxR7BcAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:QbuKDewGlxwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:tDdgxD0hSQMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZnWe2zbntUIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:qmyJBmpVKbYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=500&pagesize=100&citation_for_view=D_S41X4AAAAJ:rKOGDx9nJ1EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:bCDyRX4i8-MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:8rj-XRKUtKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:D52hNgOu9GcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Xc-mKOjpdrwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:fTLh7q_iUBEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:YK4ucWkmU_UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Kr3pDLWb32UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:H30kdXGRGPkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:9xhnSCvx0jcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:xUT3DyvLuJwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:GzlcqhCAosUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:l07DEcJES74C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:jkTRWqoJ8oAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:H6myOybQgeYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:HiznBx5RP3QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZbiiB1Sm8G8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:1Lcp1PKUB6cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:tai-Ft5GzhwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:ZEcFV8kAgqMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:YvgfBAebZEkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:KQ7zX_ltr48C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:IuzW4o0J_HAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:12nnf2f32iYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:yKcmA0jEsUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:eK4aujBuqBIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:zG6RlwjYRPQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Fd6TstiuZzAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:IHkkN1K1AlAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:zJAXUfKFhq0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:c6chOJGeGucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:nOKOSwxqtg8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Zp9IZb6oESQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:w2Gke83ceDMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:NWFKKQzSIN4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:bO_hriczGZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:UEFpDhwOD2kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:M7C5BM2v8n8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:MnogvFdIBdwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:SFOYbPikdlgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:8rLWgkbgOXQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:r56sNq9gaawC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:oFKsPyNwwpYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:H2XcuePHLsAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:YXeC4bxG7-IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:PXxrOWvQdDgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Oc-rVwKPngoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:N0GlzNNR4l8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:UCVoGz2p8ukC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:LHWLPdAD5FMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:OWslULmvb_UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:USX2HHcnDqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:EfTNjLFZ3b0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:OWf_fnsf0g4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:04dtUmz_MT0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Vu1dURnyNv8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:-_dYPAW6P2MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:5awf1xo2G04C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:LjlpjdlvIbIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:hbz17DqrwuEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:qjuL_XCUnM8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:bKwnt0rjkrwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:LHtfzE5_5AIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:FnaCo-ypupUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:GfAJFcoWUJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:xpFxhiwfz1QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Y0agIcFmOsQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:z2g7kDSNNyoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:kkSDTGFLcmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Y20HiHuZk70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:-3_NAp5WSNkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:ie8Y0QrpMWAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:D_25heFg6JwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:Pm-n7EBaiV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:XK2cf6JOk9AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:OP2qWFMeUpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:ipvhVhH6zQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:ec1XJgWlWRUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:ibZ2AwG9z6wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:CMPXdcK5v8EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:kWvqk_afx_IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:BwyfMAYsbu0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:j8SEvjWlNXcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:UHK10RUVsp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:dQ2og3OwTAUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:LkGwnXOMwfcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=D_S41X4AAAAJ&cstart=600&pagesize=100&citation_for_view=D_S41X4AAAAJ:9yKSN-GCB0IC"], "Topic": ["Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Computer Scienc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Federated Learning", "Computer Scienc", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Vision", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Computer Scienc", "Computer Scienc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Computer Scienc", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Computer Scienc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Computer Scienc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Computer Scienc", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Computer Vision", "Computer Scienc", "Federated Learning", "Federated Learning", "Computer Vision", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Computer Vision", "Computer Scienc", "Computer Vision", "Federated Learning", "Federated Learning", "Computer Scienc", "Federated Learning", "Hardware Design and Neural Network Optimization", "Computer Vision", "Artificial Intelligenc", "Federated Learning", "Computer Scienc", "Federated Learning", "Computer Vision", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others"], "# of Citations": [1037, 800, 378, 378, 368, 346, 339, 339, 325, 317, 304, 275, 268, 268, 258, 250, 242, 244, 242, 234, 222, 221, 220, 216, 218, 210, 212, 204, 199, 194, 193, 190, 192, 191, 191, 190, 188, 184, 179, 166, 161, 166, 167, 159, 151, 152, 144, 147, 142, 142, 138, 135, 134, 136, 126, 127, 126, 126, 125, 125, 121, 116, 119, 114, 112, 113, 111, 112, 106, 102, 101, 96, 95, 95, 94, 93, 92, 89, 89, 90, 90, 89, 88, 87, 88, 88, 83, 84, 82, 81, 81, 79, 79, 77, 76, 75, 73, 74, 74, 74, 72, 71, 71, 71, 72, 70, 71, 71, 69, 64, 69, 65, 66, 68, 65, 68, 66, 66, 66, 65, 66, 64, 63, 62, 61, 60, 61, 61, 59, 60, 59, 59, 57, 57, 54, 55, 55, 53, 54, 52, 52, 52, 52, 52, 51, 50, 48, 49, 49, 49, 49, 49, 47, 48, 48, 46, 48, 47, 44, 44, 47, 46, 47, 46, 45, 45, 44, 45, 43, 44, 44, 44, 44, 44, 43, 42, 43, 42, 42, 42, 39, 42, 42, 39, 40, 41, 41, 40, 40, 40, 38, 39, 39, 38, 38, 37, 37, 36, 36, 35, 35, 33, 32, 33, 33, 33, 33, 33, 33, 33, 32, 32, 32, 31, 31, 29, 31, 30, 28, 30, 30, 30, 29, 30, 28, 27, 29, 29, 28, 28, 28, 29, 28, 29, 28, 28, 28, 28, 27, 26, 26, 27, 27, 26, 26, 27, 25, 26, 26, 26, 26, 26, 25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 22, 23, 23, 23, 23, 23, 23, 22, 23, 22, 22, 22, 22, 21, 22, 21, 21, 21, 20, 21, 21, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 18, 18, 18, 17, 18, 18, 17, 16, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 14, 15, 15, 15, 15, 15, 13, 14, 14, 14, 14, 14, 13, 13, 12, 13, 13, 13, 13, 13, 13, 10, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 9, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 8, 9, 6, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 3, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics(namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally\u00a0\u2026", "In this paper, we propose a new image quality assessment (IQA) scheme, with emphasis on gradient similarity. Gradients convey important visual information and are crucial to scene understanding. Using such information, structural and contrast changes can be effectively captured. Therefore, we use the gradient similarity to measure the change in contrast and structure in images. Apart from the structural/contrast changes, image quality is also affected by luminance changes, which must be also accounted for complete and more robust IQA. Hence, the proposed scheme considers both luminance and contrast-structural changes to effectively assess image quality. Furthermore, the proposed scheme is designed to follow the masking effect and visibility threshold more closely, i.e., the case when both masked and masking signals are small is more effectively tackled by the proposed scheme. Finally, the effects of the\u00a0\u2026", "In this paper, we investigate into the problem of image quality assessment (IQA) and enhancement via machine learning. This issue has long attracted a wide range of attention in computational intelligence and image processing communities, since, for many practical applications, e.g., object detection and recognition, raw images are usually needed to be appropriately enhanced to raise the visual quality (e.g., visibility and contrast). In fact, proper enhancement can noticeably improve the quality of input images, even better than originally captured images, which are generally thought to be of the best quality. In this paper, we present two most important contributions. The first contribution is to develop a new no-reference (NR) IQA model. Given an image, our quality measure first extracts 17 features through analysis of contrast, sharpness, brightness and more, and then yields a measure of visual quality using a\u00a0\u2026", "We explore a new perceptually-adaptive video coding (PVC) scheme for hybrid video compression, in order to achieve better perceptual coding quality and operational efficiency. A new just noticeable distortion (JND) estimator for color video is first devised in the image domain. How to efficiently integrate masking effects together is a key issue of JND modelling. We integrate spatial masking factors with the nonlinear additivity model for masking (NAMM). The JND estimator applies to all color components and accounts for the compound impact of luminance masking, texture masking and temporal masking. Extensive subjective viewing confirms that it is capable of determining a more accurate visibility threshold that is close to the actual JND bound in human eyes. Secondly, the image-domain JND profile is incorporated into hybrid video encoding via the JND-adaptive motion estimation and residue filtering process\u00a0\u2026", "Proper contrast change can improve the perceptual quality of most images, but it has largely been overlooked in the current research of image quality assessment (IQA). To fill this void, we in this paper first report a new large dedicated contrast-changed image database (CCID2014), which includes 655 images and associated subjective ratings recorded from 22 inexperienced observers. We then present a novel reduced-reference image quality metric for contrast change (RIQMC) using phase congruency and statistics information of the image histogram. Validation of the proposed model is conducted on contrast related CCID2014, TID2008, CSIQ and TID2013 databases, and results justify the superiority and efficiency of RIQMC over a majority of classical and state-of-the-art IQA methods. Furthermore, we combine aforesaid subjective and objective assessments to derive the RIQMC based Optimal HIstogram\u00a0\u2026", "Saliency detection plays important roles in many image processing applications, such as regions of interest extraction and image resizing. Existing saliency detection models are built in the uncompressed domain. Since most images over Internet are typically stored in the compressed domain such as joint photographic experts group (JPEG), we propose a novel saliency detection model in the compressed domain in this paper. The intensity, color, and texture features of the image are extracted from discrete cosine transform (DCT) coefficients in the JPEG bit-stream. Saliency value of each DCT block is obtained based on the Hausdorff distance calculation and feature map fusion. Based on the proposed saliency detection model, we further design an adaptive image retargeting algorithm in the compressed domain. The proposed image retargeting algorithm utilizes multioperator operation comprised of the block\u00a0\u2026", "Contrast distortion is often a determining factor in human perception of image quality, but little investigation has been dedicated to quality assessment of contrast-distorted images without assuming the availability of a perfect-quality reference image. In this letter, we propose a simple but effective method for no-reference quality assessment of contrast distorted images based on the principle of natural scene statistics (NSS). A large scale image database is employed to build NSS models based on moment and entropy features. The quality of a contrast-distorted image is then evaluated based on its unnaturalness characterized by the degree of deviation from the NSS models. Support vector regression (SVR) is employed to predict human mean opinion score (MOS) from multiple NSS features as the input. Experiments based on three publicly available databases demonstrate the promising performance of the\u00a0\u2026", "The visual saliency detection model simulates the human visual system to perceive the scene and has been widely used in many vision tasks. With the development of acquisition technology, more comprehensive information, such as depth cue, inter-image correspondence, or temporal relationship, is available to extend image saliency detection to RGBD saliency detection, co-saliency detection, or video saliency detection. The RGBD saliency detection model focuses on extracting the salient regions from RGBD images by combining the depth information. The co-saliency detection model introduces the inter-image correspondence constraint to discover the common salient object in an image group. The goal of the video saliency detection model is to locate the motion-related salient object in video sequences, which considers the motion cue and spatiotemporal constraint jointly. In this paper, we review different\u00a0\u2026", "The general purpose of seeing a picture is to attain information as much as possible. With it, we in this paper devise a new no-reference/blind metric for image quality assessment (IQA) of contrast distortion. For local details, we lirst roughly remove predicted regions in an image since unpredicted remains are of much information. We then compute entropy of particular unpredicted areas of maximum information via visual saliency. From global perspective, we compare the image histogram with the uniformly distributed histogram of maximum information via the symmetric Kullback-Leibler divergence. The proposed blind IQA method generates an overall quality estimation of a contrast-distorted image by properly combining local and global considerations. Thorough experiments on live databases/subsets demonstrate the superiority of our training-free blind technique over state-of-the-art fulland no-reference IQA\u00a0\u2026", "In this paper, we propose a new no-reference (NR)/ blind sharpness metric in the autoregressive (AR) parameter space. Our model is established via the analysis of AR model parameters, first calculating the energy- and contrast-differences in the locally estimated AR coefficients in a pointwise way, and then quantifying the image sharpness with percentile pooling to predict the overall score. In addition to the luminance domain, we further consider the inevitable effect of color information on visual perception to sharpness and thereby extend the above model to the widely used YIQ color space. Validation of our technique is conducted on the subsets with blurring artifacts from four large-scale image databases (LIVE, TID2008, CSIQ, and TID2013). Experimental results confirm the superiority and efficiency of our method over existing NR algorithms, the state-of-the-art blind sharpness/blurriness estimators, and\u00a0\u2026", "Contrast is a fundamental attribute of images that plays an important role in human visual perception of image quality. With numerous approaches proposed to enhance image contrast, much less work has been dedicated to automatic quality assessment of contrast changed images. Existing approaches rely on global statistics to estimate contrast quality. Here we propose a novel local patch-based objective quality assessment method using an adaptive representation of local patch structure, which allows us to decompose any image patch into its mean intensity, signal strength and signal structure components and then evaluate their perceptual distortions in different ways. A unique feature that differentiates the proposed method from previous contrast quality models is the capability to produce a local contrast quality map, which predicts local quality variations over space and may be employed to guide contrast\u00a0\u2026", "Objective image quality assessment (IQA) aims to evaluate image quality consistently with human perception. Most of the existing perceptual IQA metrics cannot accurately represent the degradations from different types of distortion, e.g., existing structural similarity metrics perform well on content-dependent distortions while not as well as peak signal-to-noise ratio (PSNR) on content-independent distortions. In this paper, we integrate the merits of the existing IQA metrics with the guide of the recently revealed internal generative mechanism (IGM). The IGM indicates that the human visual system actively predicts sensory information and tries to avoid residual uncertainty for image perception and understanding. Inspired by the IGM theory, we adopt an autoregressive prediction algorithm to decompose an input scene into two portions, the predicted portion with the predicted visual content and the disorderly portion\u00a0\u2026", "In this paper, we propose a new psychovisual quality metric of images based on recent developments in brain theory and neuroscience, particularly the free-energy principle. The perception and understanding of an image is modeled as an active inference process, in which the brain tries to explain the scene using an internal generative model. The psychovisual quality is thus closely related to how accurately visual sensory data can be explained by the generative model, and the upper bound of the discrepancy between the image signal and its best internal description is given by the free energy of the cognition process. Therefore, the perceptual quality of an image can be quantified using the free energy. Constructively, we develop a reduced-reference free-energy-based distortion metric (FEDM) and a no-reference free-energy-based quality metric (NFEQM). The FEDM and the NFEQM are nearly invariant to many\u00a0\u2026", "With the widespread adoption of multidevice communication, such as telecommuting, screen content images (SCIs) have become more closely and frequently related to our daily lives. For SCIs, the tasks of accurate visual quality assessment, high-efficiency compression, and suitable contrast enhancement have thus currently attracted increased attention. In particular, the quality evaluation of SCIs is important due to its good ability for instruction and optimization in various processing systems. Hence, in this paper, we develop a new objective metric for research on perceptual quality assessment of distorted SCIs. Compared to the classical MSE, our method, which mainly relies on simple convolution operators, first highlights the degradations in structures caused by different types of distortions and then detects salient areas where the distortions usually attract more attention. A comparison of our algorithm with the\u00a0\u2026", "Blur is a key determinant in the perception of image quality. Generally, blur causes spread of edges, which leads to shape changes in images. Discrete orthogonal moments have been widely studied as effective shape descriptors. Intuitively, blur can be represented using discrete moments since noticeable blur affects the magnitudes of moments of an image. With this consideration, this paper presents a blind image blur evaluation algorithm based on discrete Tchebichef moments. The gradient of a blurred image is first computed to account for the shape, which is more effective for blur representation. Then the gradient image is divided into equal-size blocks and the Tchebichef moments are calculated to characterize image shape. The energy of a block is computed as the sum of squared non-DC moment values. Finally, the proposed image blur score is defined as the variance-normalized moment energy, which is\u00a0\u2026", "Researchers have been taking advantage of visual attention in various image processing applications such as image retargeting, video coding, etc. Recently, many saliency detection algorithms have been proposed by extracting features in spatial or transform domains. In this paper, a novel saliency detection model is introduced by utilizing low-level features obtained from the wavelet transform domain. Firstly, wavelet transform is employed to create the multi-scale feature maps which can represent different features from edge to texture. Then, we propose a computational model for the saliency map from these features. The proposed model aims to modulate local contrast at a location with its global saliency computed based on the likelihood of the features, and the proposed model considers local center-surround differences and global contrast in the final saliency map. Experimental evaluation depicts the\u00a0\u2026", "Saliency detection is widely used to extract regions of interest in images for various image processing applications. Recently, many saliency detection models have been proposed for video in uncompressed (pixel) domain. However, video over Internet is always stored in compressed domains, such as MPEG2, H.264, and MPEG4 Visual. In this paper, we propose a novel video saliency detection model based on feature contrast in compressed domain. Four types of features including luminance, color, texture, and motion are extracted from the discrete cosine transform coefficients and motion vectors in video bitstream. The static saliency map of unpredicted frames (I frames) is calculated on the basis of luminance, color, and texture features, while the motion saliency map of predicted frames (P and B frames) is computed by motion feature. A new fusion method is designed to combine the static saliency and motion\u00a0\u2026", "In just noticeable difference (JND) models, evaluation of contrast masking (CM) is a crucial step. More specifically, CM due to edge masking (EM) and texture masking (TM) needs to be distinguished due to the entropy masking property of the human visual system. However, TM is not estimated accurately in the existing JND models since they fail to distinguish TM from EM. In this letter, we propose an enhanced pixel domain JND model with a new algorithm for CM estimation. In our model, total-variation based image decomposition is used to decompose an image into structural image (i.e., cartoon like, piecewise smooth regions with sharp edges) and textural image for estimation of EM and TM, respectively. Compared with the existing models, the proposed one shows its advantages brought by the better EM and TM estimation. It has been also applied to noise shaping and visual distortion gauge, and favorable\u00a0\u2026", "Recent years have witnessed a growing number of image and video centric applications on mobile, vehicular, and cloud platforms, involving a wide variety of digital screen content images. Unlike natural scene images captured with modern high fidelity cameras, screen content images are typically composed of fewer colors, simpler shapes, and a larger frequency of thin lines. In this paper, we develop a novel blind/no-reference (NR) model for accessing the perceptual quality of screen content pictures with big data learning. The new model extracts four types of features descriptive of the picture complexity, of screen content statistics, of global brightness quality, and of the sharpness of details. Comparative experiments verify the efficacy of the new model as compared with existing relevant blind picture quality assessment algorithms applied on screen content image databases. A regression module is trained on a\u00a0\u2026", "With the fast development of visual noise-shaping related applications (visual compression, error resilience, watermarking, encryption, and display), there is an increasingly significant demand on incorporating perceptual characteristics into these applications for improved performance. In this paper, a very important mechanism of the human brain, visual attention, is introduced for visual sensitivity and visual quality evaluation. Based upon the analysis, a new numerical measure for visual attention's modulatory aftereffects, perceptual quality significance map (PQSM), is proposed. To a certain extent, the PQSM reflects the processing ability of the human brain on local visual contents statistically. The PQSM is generated with the integration of local perceptual stimuli from color contrast, texture contrast, motion, as well as cognitive features (skin color and face in this study). Experimental results with subjective viewing\u00a0\u2026", "We present a motion-compensated residue signal preprocessing scheme in video coding scheme based on just-noticeable-distortion (JND) profile. Human eyes cannot sense any changes below the JND threshold around a pixel due to their underlying spatial/temporal masking properties. An appropriate (even imperfect) JND model can significantly help to improve the performance of video coding algorithms. From the viewpoint of signal compression, smaller variance of signal results in less objective distortion of the reconstructed signal for a given bit rate. In this paper, a new JND estimator for color video is devised in image-domain with the nonlinear additivity model for masking (NAMM) and is incorporated into a motion-compensated residue signal preprocessor for variance reduction toward coding quality enhancement. As the result, both perceptual quality and objective quality are enhanced in coded video at a\u00a0\u2026", "We propose a novel algorithm to detect visual saliency from video signals by combining both spatial and temporal information and statistical uncertainty measures. The main novelty of the proposed method is twofold. First, separate spatial and temporal saliency maps are generated, where the computation of temporal saliency incorporates a recent psychological study of human visual speed perception. Second, the spatial and temporal saliency maps are merged into one using a spatiotemporally adaptive entropy-based uncertainty weighting approach. The spatial uncertainty weighing incorporates the characteristics of proximity and continuity of spatial saliency, while the temporal uncertainty weighting takes into account the variations of background motion and local contrast. Experimental results show that the proposed spatiotemporal uncertainty weighting algorithm significantly outperforms state-of-the-art video\u00a0\u2026", "A fast reliable computational quality predictor is eagerly desired in practical image/video applications, such as serving for the quality monitoring of real-time coding and transcoding. In this paper, we propose a new perceptual image quality assessment (IQA) metric based on the human visual system (HVS). The proposed IQA model performs efficiently with convolution operations at multiscales, gradient magnitude, and color information similarity, and a perceptual-based pooling. Extensive experiments are conducted using four popular large-size image databases and two multiply distorted image databases, and results validate the superiority of our approach over modern IQA measures in efficiency and efficacy. Our metric is built on the theoretical support of the HVS with lately designed IQA methods as special cases.", "Research on screen content images (SCIs) becomes important as they are increasingly used in multi-device communication applications. In this paper, we present a study on perceptual quality assessment of distorted SCIs subjectively and objectively. We construct a large-scale screen image quality assessment database (SIQAD) consisting of 20 source and 980 distorted SCIs. In order to get the subjective quality scores and investigate, which part (text or picture) contributes more to the overall visual quality, the single stimulus methodology with 11 point numerical scale is employed to obtain three kinds of subjective scores corresponding to the entire, textual, and pictorial regions, respectively. According to the analysis of subjective data, we propose a weighting strategy to account for the correlation among these three kinds of subjective scores. Furthermore, we design an objective metric to measure the visual\u00a0\u2026", "Perceptual visibility threshold estimation, based upon characteristics of the human visual system (HVS), has wide applications in digital image/video processing. An improved scheme for estimating just-noticeable distortion (JND) is proposed in this paper. It is proved to outperform the DCTune model, with the major contributions of a new formula for luminance adaptation adjustment and the incorporation of block classification for contrast masking. The HVS visibility threshold for digital images exhibits an approximately parabolic curve versus gray levels and this has been formulated to yield a more accurate base threshold. Moreover, edge regions have been differentiated via block classification to effectively avoid over-estimation of JND in the said regions. Experiments with different images and the associated subjective tests show improved performance of the proposed scheme over the DCTune model for luminance\u00a0\u2026", "Perceptual quality assessment is a challenging issue in 3D signal processing research. It is important to study 3D signal directly instead of studying simple extension of the 2D metrics directly to the 3D case as in some previous studies. In this paper, we propose a new perceptual full-reference quality assessment metric of stereoscopic images by considering the binocular visual characteristics. The major technical contribution of this paper is that the binocular perception and combination properties are considered in quality assessment. To be more specific, we first perform left-right consistency checks and compare matching error between the corresponding pixels in binocular disparity calculation, and classify the stereoscopic images into non-corresponding, binocular fusion, and binocular suppression regions. Also, local phase and local amplitude maps are extracted from the original and distorted stereoscopic\u00a0\u2026", "In practice, images available to consumers usually undergo several stages of processing including acquisition, compression, transmission, and presentation, and each stage may introduce certain type of distortion. It is common that images are simultaneously distorted by multiple types of distortions. Most existing objective image quality assessment (IQA) methods have been designed to estimate perceived quality of images corrupted by a single image processing stage. In this letter, we propose a no-reference (NR) IQA method to predict the visual quality of multiply-distorted images based on structural degradation. In the proposed method, a novel structural feature is extracted as the gradient-weighted histogram of local binary pattern (LBP) calculated on the gradient map (GWH-GLBP), which is effective to describe the complex degradation pattern introduced by multiple distortions. Extensive experiments conducted\u00a0\u2026", "High dynamic range (HDR) imaging techniques have been working constantly, actively, and validly in the fault detection and disease diagnosis in the astronomical and medical fields, and currently they have also gained much more attention from digital image processing and computer vision communities. While HDR imaging devices are starting to have friendly prices, HDR display devices are still out of reach of typical consumers. Due to the limited availability of HDR display devices, in most cases tone mapping operators (TMOs) are used to convert HDR images to standard low dynamic range (LDR) images for visualization. But existing TMOs cannot work effectively for all kinds of HDR images, with their performance largely depending on brightness, contrast, and structure properties of a scene. To accurately measure and compare the performance of distinct TMOs, in this paper develop an effective and efficient no\u00a0\u2026", "A new methodology for objective image quality assessment (IQA) with multi-method fusion (MMF) is presented in this paper. The research is motivated by the observation that there is no single method that can give the best performance in all situations. To achieve MMF, we adopt a regression approach. The new MMF score is set to be the nonlinear combination of scores from multiple methods with suitable weights obtained by a training process. In order to improve the regression results further, we divide distorted images into three to five groups based on the distortion types and perform regression within each group, which is called \u201ccontext-dependent MMF\u201d (CD-MMF). One task in CD-MMF is to determine the context automatically, which is achieved by a machine learning approach. To further reduce the complexity of MMF, we perform algorithms to select a small subset from the candidate method set. The result is\u00a0\u2026", "State-of-the-art algorithms for blind image quality assessment (BIQA) typically have two categories. The first category approaches extract natural scene statistics (NSS) as features based on the statistical regularity of natural images. The second category approaches extract features by feature encoding with respect to a learned codebook. However, several problems need to be addressed in existing codebook-based BIQA methods. First, the high-dimensional codebook-based features are memory-consuming and have the risk of over-fitting. Second, there is a semantic gap between the constructed codebook by unsupervised learning and image quality. To address these problems, we propose a novel codebook-based BIQA method by optimizing multistage discriminative dictionaries (MSDDs). To be specific, MSDDs are learned by performing the label consistent K-SVD (LC-KSVD) algorithm in a stage-by-stage\u00a0\u2026", "In this paper, a method for measuring the perceptual quality of blurred images has been proposed. Here, the amount of image blur is characterized by the average extent of edges in the image, or more specifically the average extent of the slope's spread of an edge in the opposing gradients' directions. The effectiveness of such method is validated using subjective tests on blurred images, including JPEG-2000 coded images, and the experimental results show that the proposed method can provide results that correlate relatively well with human subjective ratings.", "Objective image quality estimation is useful in many visual processing systems, and is difficult to perform in line with the human perception. The challenge lies in formulating effective features and fusing them into a single number to predict the quality score. In this brief, we propose a new approach to address the problem, with the use of singular vectors out of singular value decomposition (SVD) as features for quantifying major structural information in images and then support vector regression (SVR) for automatic prediction of image quality. The feature selection with singular vectors is novel and general for gauging structural changes in images as a good representative of visual quality variations. The use of SVR exploits the advantages of machine learning with the ability to learn complex data patterns for an effective and generalized mapping of features into a desired score, in contrast with the oft-utilized feature\u00a0\u2026", "Just-noticeable distortion (JND), which refers to the maximum distortion that the human visual system (HVS) cannot perceive, plays an important role in perceptual image and video processing. In comparison with JND estimation for images, estimation of the JND profile for video needs to take into account the temporal HVS properties in addition to the spatial properties. In this paper, we develop a spatio-temporal model estimating JND in the discrete cosine tranform domain. The proposed model incorporates the spatio-temporal contrast sensitivity function, the influence of eye movements, luminance adaptation, and contrast masking to be more consistent with human perception. It is capable of yielding JNDs for both still images and video with significant motion. The experiments conducted in this study have demonstrated that the JND values estimated for video sequences with moving objects by the model are in line\u00a0\u2026", "Many saliency detection models for 2D images have been proposed for various multimedia processing applications during the past decades. Currently, the emerging applications of stereoscopic display require new saliency detection models for salient region extraction. Different from saliency detection for 2D images, the depth feature has to be taken into account in saliency detection for stereoscopic images. In this paper, we propose a novel stereoscopic saliency detection framework based on the feature contrast of color, luminance, texture, and depth. Four types of features, namely color, luminance, texture, and depth, are extracted from discrete cosine transform coefficients for feature contrast calculation. A Gaussian model of the spatial distance between image patches is adopted for consideration of local and global contrast calculation. Then, a new fusion method is designed to combine the feature maps to\u00a0\u2026", "Mulsemedia\u2014multiple sensorial media\u2014captures a wide variety of research efforts and applications. This article presents a historic perspective on mulsemedia work and reviews current developments in the area. These take place across the traditional multimedia spectrum\u2014from virtual reality applications to computer games\u2014as well as efforts in the arts, gastronomy, and therapy, to mention a few. We also describe standardization efforts, via the MPEG-V standard, and identify future developments and exciting challenges the community needs to overcome.", "Accurate estimation of Gaussian noise level is of fundamental interest in a wide variety of vision and image processing applications as it is critical to the processing techniques that follow. In this paper, a new effective noise level estimation method is proposed on the basis of the study of singular values of noise-corrupted images. Two novel aspects of this paper address the major challenges in noise estimation: 1) the use of the tail of singular values for noise estimation to alleviate the influence of the signal on the data basis for the noise estimation process and 2) the addition of known noise to estimate the content-dependent parameter, so that the proposed scheme is adaptive to visual signals, thereby enabling a wider application scope of the proposed scheme. The analysis and experiment results demonstrate that the proposed algorithm can reliably infer noise levels and show robust behavior over a wide range of\u00a0\u2026", "For computer vision-based inspection of electronic chips or dies in semiconductor production lines, we propose a new method to effectively and efficiently detect defects in images. Different from the traditional methods that compare the image of each test chip or die with the template image one by one, which are sensitive to misalignment between the test and template images, a collection of multiple test images are used as the input image for processing simultaneously in our method with two steps. The first step is to obtain salient regions of the whole collection of test images, and the second step is to evaluate local discrepancy between salient regions in test images and the corresponding regions in the defect-free template image. To be more specific, in the first step of our method, phase-only Fourier transform (POFT), which is computationally efficient for online applications in industry, is used for saliency detection\u00a0\u2026", "Digital images in the real world are created by a variety of means and have diverse properties. A photographical natural scene image (NSI) may exhibit substantially different characteristics from a computer graphic image (CGI) or a screen content image (SCI). This casts major challenges to objective image quality assessment, for which existing approaches lack effective mechanisms to capture such content type variations, and thus are difficult to generalize from one type to another. To tackle this problem, we first construct a cross-content-type (CCT) database, which contains 1,320 distorted NSIs, CGIs, and SCIs, compressed using the high efficiency video coding (HEVC) intra coding method and the screen content compression (SCC) extension of HEVC. We then carry out a subjective experiment on the database in a well-controlled laboratory environment. Moreover, we propose a unified content-type adaptive\u00a0\u2026", "With the wide applications of saliency information in visual signal processing, many saliency detection methods have been proposed. However, some key characteristics of the human visual system (HVS) are still neglected in building these saliency detection models. In this paper, we propose a new saliency detection model based on the human visual sensitivity and the amplitude spectrum of quaternion Fourier transform (QFT). We use the amplitude spectrum of QFT to represent the color, intensity, and orientation distributions for image patches. The saliency value for each image patch is calculated by not only the differences between the QFT amplitude spectrum of this patch and other patches in the whole image, but also the visual impacts for these differences determined by the human visual sensitivity. The experiment results show that the proposed saliency detection model outperforms the state-of-the-art\u00a0\u2026", "We study the use of machine learning for visual quality evaluation with comprehensive singular value decomposition (SVD)-based visual features. In this paper, the two-stage process and the relevant work in the existing visual quality metrics are first introduced followed by an in-depth analysis of SVD for visual quality assessment. Singular values and vectors form the selected features for visual quality assessment. Machine learning is then used for the feature pooling process and demonstrated to be effective. This is to address the limitations of the existing pooling techniques, like simple summation, averaging, Minkowski summation, etc., which tend to be ad hoc. We advocate machine learning for feature pooling because it is more systematic and data driven. The experiments show that the proposed method outperforms the eight existing relevant schemes. Extensive analysis and cross validation are performed with\u00a0\u2026", "Existing RGB-D Salient Object Detection (SOD) methods take advantage of depth cues to improve the detection accuracy, while pay insufficient attention to the quality of depth information. In practice, a depth map is often with uneven quality and sometimes suffers from distractors, due to various factors in the acquisition procedure. In this article, to mitigate distractors in depth maps and highlight salient objects in RGB images, we propose a Hierarchical Alternate Interactions Network (HAINet) for RGB-D SOD. Specifically, HAINet consists of three key stages: feature encoding, cross-modal alternate interaction, and saliency reasoning. The main innovation in HAINet is the Hierarchical Alternate Interaction Module (HAIM), which plays a key role in the second stage for cross-modal feature interaction. HAIM first uses RGB features to filter distractors in depth features, and then the purified depth features are exploited to\u00a0\u2026", "Reduced-reference (RR) image quality assessment (IQA) aims to use less data about the reference image and achieve higher evaluation accuracy. Recent research on brain theory suggests that the human visual system (HVS) actively predicts the primary visual information and tries to avoid the residual uncertainty for image perception and understanding. Therefore, the perceptual quality relies to the information fidelities of the primary visual information and the residual uncertainty. In this paper, we propose a novel RR IQA index based on visual information fidelity. We advocate that distortions on the primary visual information mainly disturb image understanding, and distortions on the residual uncertainty mainly change the comfort of perception. We separately compute the quantities of the primary visual information and the residual uncertainty of an image. Then the fidelities of the two types of information are\u00a0\u2026", "Blind image quality assessment (BIQA) aims to develop quantitative measures to automatically and accurately estimate perceptual image quality without any prior information about the reference image. In this paper, we introduce a novel BIQA metric by structural and luminance information, based on the characteristics of human visual perception for distorted image. We extract the perceptual structural features of distorted image by the local binary pattern distribution. Besides, the distribution of normalized luminance magnitudes is extracted to represent the luminance changes in distorted image. After extracting the features for structures and luminance, support vector regression is adopted to model the complex nonlinear relationship from feature space to quality measure. The proposed BIQA model is called no-reference quality assessment using statistical structural and luminance features (NRSL). Extensive\u00a0\u2026", "At low bit rates, better coding quality can be achieved by downsampling the image prior to compression and estimating the missing portion after decompression. This paper presents a new algorithm in such a paradigm, based on the adaptive decision of appropriate downsampling directions/ratios and quantization steps, in order to achieve higher coding quality with low bit rates with the consideration of local visual significance. The full-resolution image can be restored from the DCT coefficients of the downsampled pixels so that the spatial interpolation required otherwise is avoided. The proposed algorithm significantly raises the critical bit rate to approximately 1.2 bpp, from 0.15-0.41 bpp in the existing downsample-prior-to-JPEG schemes and, therefore, outperforms the standard JPEG method in a much wider bit-rate scope. The experiments have demonstrated better PSNR improvement over the existing\u00a0\u2026", "Smoke detection plays an important role in industrial safety warning systems and fire prevention. Due to the complicated changes in the shape, texture, and color of smoke, identifying the smoke from a given image still remains a substantial challenge, and this has accordingly aroused a considerable amount of research attention recently. To address the problem, we devise a new deep dual-channel neural network (DCNN) for smoke detection. In contrast to popular deep convolutional networks (e.g., Alex-Net, VGG-Net, Res-Net, and Dense-Net and the DNCNN that is specifically devoted to detecting smoke), our proposed end-to-end network is mainly composed of dual channels of deep subnetworks. In the first subnetwork, we sequentially connect multiple convolutional layers and max-pooling layers. Then, we selectively append the batch normalization layer to each convolutional layer for overfitting reduction and\u00a0\u2026", "Most studies in the literature for video quality assessment have been focused on the evaluation of quantized video sequences at fixed and high spatial and temporal resolutions. Only limited work has been reported for assessing video quality under different spatial and temporal resolutions. In this paper, we consider a wider scope of video quality assessment in the sense of considering multiple dimensions. In particular, we address the problem of evaluating perceptual visual quality of low bit-rate videos under different settings and requirements. Extensive subjective view tests for assessing the perceptual quality of low bit-rate videos have been conducted, which cover 150 test scenarios and include five distinctive dimensions: encoder type, video content, bit rate, frame size, and frame rate. Based on the obtained subjective testing results, we perform thorough statistical analysis to study the influence of different\u00a0\u2026", "This paper presents the result of a recent large-scale subjective study of image retargeting quality on a collection of images generated by several representative image retargeting methods. Owning to many approaches to image retargeting that have been developed, there is a need for a diverse independent public database of the retargeted images and the corresponding subjective scores to be freely available. We build an image retargeting quality database, in which 171 retargeted images (obtained from 57 natural source images of different contents) were created by several representative image retargeting methods. And the perceptual quality of each image is subjectively rated by at least 30 viewers, meanwhile the mean opinion scores (MOS) were obtained. It is revealed that the subject viewers have arrived at a reasonable agreement on the perceptual quality of the retargeted image. Therefore, the MOS\u00a0\u2026", "Image quality assessment (IQA) has been an active research area during last decades. Many existing objective IQA models share a similar two-step structure with measuring local distortion before pooling. Compared with the rapid development for local distortion measurement, seldom effort has been made dedicated to effective pooling schemes. In this paper, we design a new pooling model via the analysis of distortion distribution affected by image content and distortion. That is, distributions of distortion position, distortion intensity, frequency changes, and histogram changes are comprehensively considered to infer an overall quality score. Experimental results conducted on four large-scale image quality databases (LIVE, TID2008, CSIQ, and CCID2014) concluded with three valuable findings. First, the proposed technique leads to consistent improvement in the IQA performance for studied local distortion\u00a0\u2026", "Block transform coding is the most popular approach for image and video compression. The objective measurement of blocking artifacts plays an important role in the design, optimization, and assessment of image and video coding systems. This paper presents a new algorithm for measuring blocking artifacts in images and videos. It exhibits unique and useful features: 1) it examines the blocks individually so that it can measure the severity of blocking artifacts locally; 2) it is a one-pass algorithm in the sense that the image needs to be accessed only once; 3) it takes into account the blocking artifacts for high bit rate images and the flatness for the very low bit rate images; 4) the blocking artifacts measure is well-defined in the range of 0-10. Experiments on various still images and videos show that this blockiness measure is very efficient in terms of computational complexity and memory usage, and can produce\u00a0\u2026", "The just noticeable difference (JND) in an image, which reveals the visibility limitation of the human visual system (HVS), is widely used for visual redundancy estimation in signal processing. To determine the JND threshold with the current schemes, the spatial masking effect is estimated as the contrast masking, and this cannot accurately account for the complicated interaction among visual contents. Research on cognitive science indicates that the HVS is highly adapted to extract the repeated patterns for visual content representation. Inspired by this, we formulate the pattern complexity as another factor to determine the total masking effect: the interaction is relatively straightforward with a limited masking effect in a regular pattern, and is complicated with a strong masking effect in an irregular pattern. From the orientation selectivity mechanism in the primary visual cortex, the response of each local receptive field\u00a0\u2026", "In this paper, we introduce a novel just noticeable difference (JND) estimation model based on the unified brain theory, namely the free-energy principle. The existing pixel-based JND models mainly consider the orderly factors and always underestimate the JND threshold of the disorderly region. Recent research indicates that the human visual system (HVS) actively predicts the orderly information and avoids the residual disorderly uncertainty for image perception and understanding. Thus, we suggest that there exists disorderly concealment effect which results in high JND threshold of the disorderly region. Beginning with the Bayesian inference, we deduce an autoregressive model to imitate the active prediction of the HVS. Then, we estimate the disorderly concealment effect for the novel JND model. Experimental results confirm that the proposed JND model outperforms the relevant existing ones. Furthermore\u00a0\u2026", "New challenges have been brought out along with the emerging of 3D-related technologies, such as virtual reality, augmented reality (AR), and mixed reality. Free viewpoint video (FVV), due to its applications in remote surveillance, remote education, and so on, based on the flexible selection of direction and viewpoint, has been perceived as the development direction of next-generation video technologies and has drawn a wide range of researchers' attention. Since FVV images are synthesized via a depth image-based rendering (DIBR) procedure in the \u201cblind\u201d environment (without reference images), a reliable real-time blind quality evaluation and monitoring system is urgently required. But existing assessment metrics do not render human judgments faithfully mainly because geometric distortions are generated by DIBR. To this end, this paper proposes a novel referenceless quality metric of DIBR-synthesized\u00a0\u2026", "The human visual system exhibits multiscale characteristic when perceiving visual scenes. The hierarchical structures of an image are contained in its scale space representation, in which the image can be portrayed by a series of increasingly smoothed images. Inspired by this, this paper presents a no-reference and robust image sharpness evaluation (RISE) method by learning multiscale features extracted in both the spatial and spectral domains. For an image, the scale space is first built. Then sharpness-aware features are extracted in gradient domain and singular value decomposition domain, respectively. In order to take into account the impact of viewing distance on image quality, the input image is also down-sampled by several times, and the DCT-domain entropies are calculated as quality features. Finally, all features are utilized to learn a support vector regression model for sharpness prediction\u00a0\u2026", "Perceptual visibility threshold estimation, based upon characteristics of the human visual system (HVS), is widely used in digital image and video processing. We propose in this paper a scheme for estimating JND (just-noticeable difference) with explicit formulation for image pixels, by summing the effects of the visual thresholds in sub-bands. The factors being considered include spatial contrast sensitivity function (CSF), luminance adaptation, and adaptive inter- and intra-band contrast masking. The proposed scheme demonstrates favorable results in noise shaping and perceptual visual distortion gauge for different images, in comparison with the relevant existing JND estimators.", "We propose an objective quality assessment method for image retargeting. The key step in our approach is to generate a structural similarity (SSIM) map that indicates at each spatial location in the source image how the structural information is preserved in the retargeted image. A spatial pooling method employing both bottom-up and top-down visual saliency estimations is then applied to provide an overall evaluation of the retargeted image. To evaluate the performance of the proposed IR-SSIM algorithm, we created an image database that contains images produced by different retargeting algorithms and carried out subjective tests to assess the quality of the retargeted images. Our experimental results show that IR-SSIM is better correlated with subjective evaluations than existing methods in the literature. To further demonstrate the advantages and potential applications of IR-SSIM, we embed it into a multi\u00a0\u2026", "We propose a simple yet effective deblocking method for JPEG compressed image through postfiltering in shifted windows (PSW) of image blocks. The MSE is compared between the original image block and the image blocks in shifted windows, so as to decide whether these altered blocks are used in the smoothing procedure. Our research indicates that there exists strong correlation between the optimal mean squared error threshold and the image quality factor Q, which is selected in the encoding end and can be computed from the quantization table embedded in the JPEG file. Also we use the standard deviation of each original block to adjust the threshold locally so as to avoid the over-smoothing of image details. With various image and bit-rate conditions, the processed image exhibits both great visual effect improvement and significant peak signal-to-noise ratio gain with fairly low computational complexity\u00a0\u2026", "The deep convolutional neural network (CNN) has achieved great success in image recognition. Many image quality assessment (IQA) methods directly use recognition-oriented CNN for quality prediction. However, the properties of IQA task is different from image recognition task. Image recognition should be sensitive to visual content and robust to distortion, while IQA should be sensitive to both distortion and visual content. In this paper, an IQA-oriented CNN method is developed for blind IQA (BIQA), which can efficiently represent the quality degradation. CNN is large-data driven, while the sizes of existing IQA databases are too small for CNN optimization. Thus, a large IQA dataset is firstly established, which includes more than one million distorted images (each image is assigned with a quality score as its substitute of Mean Opinion Score (MOS), abbreviated as pseudo-MOS). Next, inspired by the hierarchical\u00a0\u2026", "As a newly emerging and significant topic in computer vision community, co-saliency detection aims at discovering the common salient objects in multiple related images. The existing methods often generate the co-saliency map through a direct forward pipeline which is based on the designed cues or initialization, but lack the refinement-cycle scheme. Moreover, they mainly focus on RGB image and ignore the depth information for RGBD images. In this paper, we propose an iterative RGBD co-saliency framework, which utilizes the existing single saliency maps as the initialization, and generates the final RGBD co-saliency map by using a refinement-cycle model. Three schemes are employed in the proposed RGBD co-saliency framework, which include the addition scheme, deletion scheme, and iteration scheme. The addition scheme is used to highlight the salient regions based on intra-image depth propagation\u00a0\u2026", "Recent advances in sparse representation show that overcomplete dictionaries learned from natural images can capture high-level features for image analysis. Since atoms in the dictionaries are typically edge patterns and image blur is characterized by the spread of edges, an overcomplete dictionary can be used to measure the extent of blur. Motivated by this, this paper presents a no-reference sparse representation-based image sharpness index. An overcomplete dictionary is first learned using natural images. The blurred image is then represented using the dictionary in a block manner, and block energy is computed using the sparse coefficients. The sharpness score is defined as the variance-normalized energy over a set of selected high-variance blocks, which is achieved by normalizing the total block energy using the sum of block variances. The proposed method is not sensitive to training images, so a\u00a0\u2026", "This paper presents a method to discriminate pixel differences according to their impact toward perceived visual quality. Noticeable local contrast changes are formulated firstly since contrast is the basic sensory feature in the human visual system (HVS) perception. The analysis aims at quantifying the actual impact of such changes (further divided into increases and decreases on edges) in different signal contexts. An associated full-reference distortion metric proposed next provides better match with the HVS viewing. Experiments have used two independent visual data sets and the related subjective viewing results, and demonstrated the performance improvement of the proposed metric over the relevant existing ones with various video/images and under diversified test conditions. The proposed metric is particularly effective to visual signal with blurring and luminance fluctuations as the major artifacts, and brings\u00a0\u2026", "Air quality is currently arousing drastically increasing attention from the governments and populace all over the world. In this paper, we propose a heuristic recurrent air quality predictor (RAQP) to infer air quality. The RAQP exploits some key meteorology- and pollution-related variables to infer air pollutant concentrations (APCs), e.g. the fine particulate matter (PM2.5). It is natural that the meteorological factors and APCs at the current time have strong influences on air quality the next adjacent moment, that is to say, there exist high correlations between them. With this consideration, applying simple machine learners to the current meteorology- and pollution-related factors can reliably predict the air quality indices at a time later. However, owing to the nonlinear and chaotic reasons, the above correlations decline with the time interval enlarged. In such cases, it fails to forecast the air quality after several hours by only\u00a0\u2026", "Recently, with the advent of deep convolutional neural networks (DCNN), the improvements in visual saliency prediction research are impressive. One possible direction to approach the next improvement is to fully characterize the multi-scale saliency-influential factors with a computationally-friendly module in DCNN architectures. In this work, we propose an end-to-end dilated inception network (DINet) for visual saliency prediction. It captures multi-scale contextual features effectively with very limited extra parameters. Instead of utilizing parallel standard convolutions with different kernel sizes as the existing inception module, our proposed dilated inception module (DIM) uses parallel dilated convolutions with different dilation rates which can significantly reduce the computation load while enriching the diversity of receptive fields in feature maps. Moreover, the performance of our saliency model is further improved\u00a0\u2026", "A novel framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. MCN predicts instance-level bounding boxes by firstly converting an image into a Stochastic Flow Graph (SFG) and then performing Markov Clustering on this graph. Our method can detect text objects with arbitrary size and orientation without prior knowledge of object size. The stochastic flow graph encode objects' local correlation and semantic information. An object is modeled as strongly connected nodes, which allows flexible bottom-up detection for scale-varying and rotated objects. MCN generates bounding boxes without using Non-Maximum Suppression, and it can be fully parallelized on GPUs. The evaluation on public benchmarks shows that our method outperforms the existing methods by a large margin in detecting multioriented text objects. MCN achieves new state-of-art performance on challenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and F-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34 FPS, which is  speedup when compared with the fastest scene text detection algorithm.", "With many potential practical applications, content-based image retrieval (CBIR) has attracted substantial attention during the past few years. A variety of relevance feedback (RF) schemes have been developed as a powerful tool to bridge the semantic gap between low-level visual features and high-level semantic concepts, and thus to improve the performance of CBIR systems. Among various RF approaches, support-vector-machine (SVM)-based RF is one of the most popular techniques in CBIR. Despite the success, directly using SVM as an RF scheme has two main drawbacks. First, it treats the positive and negative feedbacks equally, which is not appropriate since the two groups of training feedbacks have distinct properties. Second, most of the SVM-based RF techniques do not take into account the unlabeled samples, although they are very helpful in constructing a good classifier. To explore solutions to\u00a0\u2026", "Extensive testing is essential to break the transmission of SARS-CoV-2, which causes the ongoing COVID-19 pandemic. Here, we present a CRISPR-based diagnostic assay that is robust to viral genome mutations and temperature, produces results fast, can be applied directly on nasopharyngeal (NP) specimens without RNA purification, and incorporates a human internal control within the same reaction. Specifically, we show that the use of an engineered AsCas12a enzyme enables detection of wildtype and mutated SARS-CoV-2 and allows us to perform the detection step with loop-mediated\u00a0isothermal amplification (LAMP) at 60-65\u2009\u00b0C. We also find that the use of hybrid DNA-RNA guides increases the rate of reaction, enabling our test to be completed within 30\u2009minutes. Utilizing clinical samples from 72 patients with COVID-19 infection and 57 healthy individuals, we demonstrate that our test exhibits a\u00a0\u2026", "Quality assessment of 3D images encounters more challenges than its 2D counterparts. Directly applying 2D image quality metrics is not the solution. In this paper, we propose a new full-reference quality assessment for stereoscopic images by learning binocular receptive field properties to be more in line with human visual perception. To be more specific, in the training phase, we learn a multiscale dictionary from the training database, so that the latent structure of images can be represented as a set of basis vectors. In the quality estimation phase, we compute sparse feature similarity index based on the estimated sparse coefficient vectors by considering their phase difference and amplitude difference, and compute global luminance similarity index by considering luminance changes. The final quality score is obtained by incorporating binocular combination based on sparse energy and sparse complexity\u00a0\u2026", "In this paper, we propose a novel no reference quality assessment method by incorporating statistical luminance and texture features (NRLT) for screen content images (SCIs) with both local and global feature representation. The proposed method is designed inspired by the perceptual property of the human visual system (HVS) that the HVS is sensitive to luminance change and texture information for image perception. In the proposed method, we first calculate the luminance map through the local normalization, which is further used to extract the statistical luminance features in global scope. Second, inspired by existing studies from neuroscience that high-order derivatives can capture image texture, we adopt four filters with different directions to compute gradient maps from the luminance map. These gradient maps are then used to extract the second-order derivatives by local binary pattern. We further extract the\u00a0\u2026", "We in this paper investigate how to blindly predict the visual quality of a screen content image (SCI). With the popularity of multi-client and remote-controlling systems, SCIs and the relevant applications have been a hot research topic. In general, SCIs contain texts or graphics in cartoons, ebooks or captures of computer screens. As for blind quality assessment (QA) of natural scene images (NSIs), it has been well established since NSIs possess certain statistical properties. SCIs however do not have reliable statistic models so far and thus the associated blind QA task is hard to be addressed. Aiming at solving this problem, we first extract 13 perceptual-inspired features with the free energy based brain theory and structural degradation model. In order to avoid the overfitting and guarantee the independence of training and testing samples, we then collect 100,000 images and use their objective quality scores\u00a0\u2026", "We present a new image quality assessment algorithm based on the phase and magnitude of the 2-D discrete Fourier transform. The basic idea is to compare the phase and magnitude of the reference and distorted images to compute the quality score. However, it is well known that the human visual system's sensitivity to different frequency components is not the same. We accommodate this fact via a simple yet effective strategy of non-uniform binning of the frequency components. This process also leads to reduced space representation of the image thereby enabling the reduced-reference (RR) prospects of the proposed scheme. We employ linear regression to integrate the effects of the changes in phase and magnitude. In this way, the required weights are determined via proper training and hence more convincing and effective. Last, using the fact that phase usually conveys more information than magnitude\u00a0\u2026", "The screen content images (SCIs) quality influences the user experience and the interactive performance of remote computing systems. With numerous approaches proposed to evaluate the quality of natural images, much less work has been dedicated to reduced-reference image quality assessment (RR-IQA) of SCIs. Here, we propose an RR-IQA method from the perspective of SCI visual perception. In particular, the quality of the distorted SCI is evaluated by comparing a set of extracted statistical features that consider both primary visual information and unpredictable uncertainty. A unique property that differentiates the proposed method from previous RR-IQA methods for natural images is the consideration of behaviors when human subjects view the screen content, which motivates us to establish the perceptual model according to the distinct properties of SCIs. Validations based on the screen content IQA\u00a0\u2026", "Human hair keratins are readily available, easy to extract, and eco-friendly materials with natural bioactivities. Keratin-based materials have been studied for applications such as cell culture substrates, internal hemostats for liver injury, and conduits for peripheral nerve repair. However, there are limited reports of using keratin-based 3D scaffolds for cell culture in vitro. Here, we describe the development of a 3D hair keratin hydrogel, which allows for living cell encapsulation under near physiological conditions. The convenience of making the hydrogels from keratin solutions in a simple and controllable manner is demonstrated, giving rise to constructs with tunable physical properties. This keratin hydrogel is comparable to collagen hydrogels in supporting the viability and proliferation of L929 murine fibroblasts. Notably, the keratin hydrogels contract less significantly as compared to the collagen hydrogels, over a 16\u00a0\u2026", "We propose an end-to-end saliency-guided deep neural network (SGDNet) for no-reference image quality assessment (NR-IQA). Our SGDNet is built on an end-to-end multi-task learning framework in which two sub-tasks including visual saliency prediction and image quality prediction are jointly optimized with a shared feature extractor. The existing multi-task CNN-based NR-IQA methods which usually consider distortion identification as the auxiliary sub-task cannot accurately identify the complex mixtures of distortions exist in authentically distorted images. By contrast, our saliency prediction sub-task is more universal because visual attention always exists when viewing every image, regardless of its distortion type. More importantly, related works have reported that saliency information is highly correlated with image quality while this property is fully utilized in our proposed SGNet by training the model with more\u00a0\u2026", "Image quality assessment (IQA) is in great demand for high quality image selection in the big data era. The challenge of reduced-reference (RR) IQA is how to use limited data to effectively represent the visual content of an image in the context of IQA. Research on neuroscience indicates that the human visual system (HVS) exhibits obvious orientation selectivity (OS) mechanism for visual content extraction. Inspired by this, an OS based visual pattern (OSVP) is proposed to extract visual content for RR IQA in this paper. The OS arises from the arrangement of the excitatory and inhibitory interactions among connected cortical neurons in a local receptive field. According to the OS mechanism, the similarity of preferred orientations between two nearby pixels is first analyzed. Then, the orientation similarities of pixels in a local neighborhood are arranged, and the OSVP is built for visual information representation. With\u00a0\u2026", "Image retargeting techniques aim to obtain retargeted images with different sizes or aspect ratios for various display screens. Various content-aware image retargeting algorithms have been proposed recently. However, there is still no effective objective metric for visual quality assessment of retargeted images. In this paper, we propose a novel full-reference objective metric for assessing visual quality of a retargeted image based on perceptual geometric distortion and information loss. The proposed metric measures the geometric distortion of a retargeted image based on the local variance of SIFT flow vector fields of the image. Furthermore, a visual saliency map is derived to characterize human perception of the geometric distortion. Besides, the information loss in the retargeted image, which is estimated based on the saliency map, is also taken into account in the proposed metric. Subjective tests are conducted to\u00a0\u2026", "Image content variation is a typical and challenging problem in no-reference image-quality assessment (NR-IQA). This work pays special attention to the impact of image content variation on NR-IQA methods. To better analyze this impact, we focus on blur-dominated distortions to exclude the impacts of distortion-type variations. We empirically show that current NR-IQA methods are inconsistent with human visual perception when predicting the relative quality of image pairs with different image contents. In view of deep semantic features of pretrained image classification neural networks always containing discriminative image content information, we put forward a new NR-IQA method based on semantic feature aggregation (SFA) to alleviate the impact of image content variation. Specifically, instead of resizing the image, we first crop multiple overlapping patches over the entire distorted image to avoid introducing\u00a0\u2026", "With the quick development and popularity of computers, computer-generated signals have drastically invaded into our daily lives. Screen content image is a typical example, since it also includes graphic and textual images as components as compared with natural scene images which have been deeply explored, and thus screen content image has posed novel challenges to current researches, such as compression, transmission, display, quality assessment, and more. In this paper, we focus our attention on evaluating the quality of screen content images based on the analysis of structural variation, which is caused by compression, transmission, and more. We classify structures into global and local structures, which correspond to basic and detailed perceptions of humans, respectively. The characteristics of graphic and textual images, e.g., limited color variations, and the human visual system are taken into\u00a0\u2026", "It is challenging to detect curve texts due to their irregular shapes and varying sizes. In this paper, we first investigate the deficiency of the existing curve detection methods and then propose a novel Conditional Spatial Expansion (CSE) mechanism to improve the performance of curve detection. Instead of regarding the curve text detection as a polygon regression or a segmentation problem, we formulate it as a sequence prediction on the spatial domain. CSE starts with a seed arbitrarily chosen within a text region and progressively merges neighborhood regions based on the extracted local features by a CNN and contextual information of merged regions. The CSE is highly parameterized and can be seamlessly integrated into existing object detection frameworks. Enhanced by the data-dependent CSE mechanism, our curve text detection system provides robust instance-level text region extraction with minimal post-processing. The analysis experiment shows that our CSE can handle texts with various shapes, sizes, and orientations, and can effectively suppress the false-positives coming from text-like textures or unexpected texts included in the same RoI. Compared with the existing curve text detection algorithms, our method is more robust and enjoys a simpler processing flow. It also creates a new state-of-art performance on curve text benchmarks with F-measurement of up to 78.4%.", "Biased discriminant analysis (BDA) is one of the most promising relevance feedback (RF) approaches to deal with the feedback sample imbalance problem for content-based image retrieval (CBIR). However, the singular problem of the positive within-class scatter and the Gaussian distribution assumption for positive samples are two main obstacles impeding the performance of BDA RF for CBIR. To avoid both of these intrinsic problems in BDA, in this paper, we propose a novel algorithm called generalized BDA (GBDA) for CBIR. The GBDA algorithm avoids the singular problem by adopting the differential scatter discriminant criterion (DSDC) and handles the Gaussian distribution assumption by redesigning the between-class scatter with a nearest neighbor approach. To alleviate the overfitting problem, GBDA integrates the locality preserving principle; therefore, a smooth and locally consistent transform can also\u00a0\u2026", "Saliency detection for images has been studied for many years, for which a lot of methods have been designed. In saliency detection, background priors, which are often regarded as pseudo-background, are effective clues to find salient objects in images. Although image boundary is commonly used as background priors, it does not work well for images of complex scenes and videos. In this paper, we explore how to identify the background priors for a video and propose a saliency-based method to detect the visual objects by using the background priors. For a video, we integrate multiple pairs of scale-invariant feature transform flows from long-range frames, and a bidirectional consistency propagation is conducted to obtain the accurate and sufficient temporal background priors, which are combined with spatial background priors to generate spatiotemporal background priors. Next, a novel dual-graph-based\u00a0\u2026", "One- and two-way communication with digital compressed visual signals is now an integral part of the daily life of millions. Such commonplace use has been realized by decades of advances in visual signal compression. The design of effective, efficient compression and transmission strategies for visual signals may benefit from proper incorporation of human visual system (HVS) characteristics. This paper overviews psychophysics and engineering associated with the communication of visual signals. It presents a short history of advances in perceptual visual signal compression, and describes perceptual models and how they are embedded into systems for compression and transmission, both with and without current compression standards.", "Depth-image-based rendering (DIBR) is a fundamental technique in free viewpoint video, which is widely adopted to synthesize virtual viewpoints. The warping and rendering operations in DIBR generally introduce geometric distortions and sharpness change. The state-of-the-art quality indices are limited in dealing with such images since they are sensitive to geometric changes. In this paper, a new quality model for DIBR-synthesized view images is presented by measuring LOcal Geometric distortions in disoccluded regions and global Sharpness (LOGS). A disoccluded region detection method is first proposed using SIFT-flow-based warping. Then, the sizes and distortion strength of local disoccluded regions are combined to generate a score. Furthermore, a reblurring-based strategy is proposed to quantify the global sharpness. Finally, the overall quality score is calculated by pooling the scores of local\u00a0\u2026", "We present a method for extracting local visual perceptual cues and its application for rate control of videophone, in order to ensure the scarce bits to be assigned for maximum perceptual coding quality. The optimum quantization step is determined with the rate-distortion model considering the local perceptual cues in the visual signal. For extraction of the perceptual cues, luminance adaptation and texture masking are used as the stimulus-driven factors, while skin color serves as the cognition-driven factor in the current implementation. Both objective and subjective quality evaluations are given by evaluating the proposed perceptual rate control (PRC) scheme in the H.263 platform, and the evaluations show that the proposed PRC scheme achieves significant quality improvement in block-based coding for bandwidth-hungry applications.", "For wireless video streaming, the three dimensional scalabilities (spatial, temporal and SNR) provided by the advanced scalable video coding (SVC) technique can be directly utilized to adapt video streams to dynamic wireless network conditions and heterogeneous wireless devices. However, the question is how to optimally trade off among the three dimensional scalabilities so as to maximize the perceived video quality, given the available resource. In this paper, we propose a low-complexity algorithm that executes at resource-limited user end to quantitatively and perceptually assess video quality under different spatial, temporal and SNR combinations. Based on the video quality measures, we further propose an efficient adaptation algorithm, which dynamically adapts scalable video to a suitable three dimension combination. Experimental results demonstrate the effectiveness of our proposed perceptual video\u00a0\u2026", "JPEG is the most commonly used image compression standard. In practice, JPEG images are easily subject to blocking artifacts at low bit rates. To reduce the blocking artifacts, many deblocking algorithms have been proposed. However, they also introduce certain degree of blur, so the deblocked images contain multiple distortions. Unfortunately, the current quality metrics are not designed for multiply distorted images, so they are limited in evaluating the quality of deblocked images. To solve the problem, this paper presents a no-reference (NR) quality metric for deblocked images. A DeBlocked Image Database (DBID) is first built with subjective Mean Opinion Score (MOS) as ground truth. Then a NR DeBlocked Image Quality (DBIQ) metric is proposed by simultaneously evaluating blocking artifacts in smooth regions and blur in textured regions. Experimental results conducted on the DBID database demonstrate\u00a0\u2026", "Research on visual quality assessment has been active during the last decade. In this work, we provide an in-depth review of recent developments in the field. As compared with existing survey papers, our current work has several unique contributions. First, besides image quality databases and metrics, we put equal emphasis on video quality databases and metrics as this is a less investigated area. Second, we discuss the application of visual quality evaluation to perceptual coding as an example for applications. Third, we benchmark the performance of state-of-the-art visual quality metrics with experiments. Finally, future trends in visual quality assessment are discussed.", "Multiple description coding (MDC) is one of the widely used mechanisms to combat packet-loss in non-feedback systems. However, the number of descriptions in the existing MDC schemes is very small (typically 2). With the number of descriptions increasing, the coding complexity increases drastically and many decoders would be required. In this paper, the compressive sensing (CS) principles are studied and an alternative coding paradigm with a number of descriptions is proposed based upon CS for high packet loss transmission. Two-dimentional discrete wavelet transform (DWT) is applied for sparse representation. Unlike the typical wavelet coders (e.g., JPEG 2000), DWT coefficients here are not directly encoded, but re-sampled towards equal importance of information instead. At the decoder side, by fully exploiting the intra-scale and inter-scale correlation of multiscale DWT, two different CS recovery\u00a0\u2026", "In this paper, we propose a novel universal framework for salient object detection, which aims to enhance the performance of any existing saliency detection method. First, rough salient regions are extracted from any existing saliency detection model with distance weighting, adaptive binarization, and morphological closing. With the superpixel segmentation, a Bayesian decision model is adopted to refine the rough saliency map to obtain a more accurate saliency map. An iterative optimization method is designed to obtain better saliency results by exploiting the characteristics of the output saliency map each time. Through the iterative optimization process, the rough saliency map is updated step by step with better and better performance until an optimal saliency map is obtained. Experimental results on the public salient object detection datasets with ground truth demonstrate the promising performance of the\u00a0\u2026", "Numerous state-of-the-art perceptual image quality assessment (IQA) algorithms share a common two-stage process: distortion description followed by distortion effects pooling. As for the first stage, the distortion descriptors or measurements are expected to be effective representatives of human visual variations, while the second stage should well express the relationship among quality descriptors and the perceptual visual quality. However, most of the existing quality descriptors (e.g., luminance, contrast, and gradient) do not seem to be consistent with human perception, and the effects pooling is often done in ad-hoc ways. In this paper, we propose a novel full-reference IQA metric. It applies non-negative matrix factorization (NMF) to measure image degradations by making use of the parts-based representation of NMF. On the other hand, a new machine learning technique [extreme learning machine (ELM)] is\u00a0\u2026", "Traditional image aesthetics assessment (IAA) approaches mainly predict the average aesthetic score of an image. However, people tend to have different tastes on image aesthetics, which is mainly determined by their subjective preferences. As an important subjective trait, personality is believed to be a key factor in modeling individual's subjective preference. In this paper, we present a personality-assisted multi-task deep learning framework for both generic and personalized image aesthetics assessment. The proposed framework comprises two stages. In the first stage, a multi-task learning network with shared weights is proposed to predict the aesthetics distribution of an image and Big-Five (BF) personality traits of people who like the image. The generic aesthetics score of the image can be generated based on the predicted aesthetics distribution. In order to capture the common representation of generic\u00a0\u2026", "Objective video quality assessment (VQA) is the use of computational models to evaluate the video quality in line with the perception of the human visual system (HVS). It is challenging due to the underlying complexity, and the relatively limited understanding of the HVS and its intricate mechanisms. There are three important issues that arise in objective VQA in comparison with image quality assessment: 1) the temporal factors apart from the spatial ones also need to be considered, 2) the contribution of each factor (spatial and temporal) and their interaction to the overall video quality need to be determined, and 3) the computational complexity of the resultant method. In this paper, we seek to tackle the first issue by utilizing the worst case pooling strategy and the variations of spatial quality along the temporal axis with proper analysis and justification. The second issue is addressed by the use of machine learning\u00a0\u2026", "Investigation on the human perception can play an important role in video signal processing. Recently, there has been great interest in incorporating the human perception in video coding systems to enhance the perceptual quality of the represented visual signal. However, the limited understanding of the human visual system and high complexity of computational models of human visual system make it a challenging task. Furthermore, the hybrid video coding structure brings difficulties to integrate computational models with coding components to fulfill the requirements. In this paper, we review the physiological characteristics of human perception and address the most relevant aspects to video coding applications. Moreover, we discuss the computational models and metrics which guide the design and implementation of the video coding system, as well as the recent advances in perceptual video coding. To\u00a0\u2026", "Screen content image (SCI) has recently emerged as an active topic due to the rapidly increasing demand in many graphically rich services such as wireless displays and virtual desktops. SCIs are often composed of pictorial regions and computer generated textual/graphical content, which exhibit different statistical properties that often lead to different viewer behaviors. Inspired by this, we propose an objective quality assessment approach for SCIs that incorporates both visual field adaptation and information content weighting into structural similarity based local quality assessment. Furthermore, we develop a perceptual screen content coding scheme based on the newly proposed quality assessment measure, targeting at further improving the SCI compression performance. Experimental results show that the proposed quality assessment method not only better predicts the perceptual quality of SCIs, but also\u00a0\u2026", "Visual saliency can be thought of as the product of human brain activity. Most existing models were built upon local features or global features or both. Lately, a so-called free energy principle unifies several brain theories within one framework, and tells where easily surprise human viewers in a visual stimulus through a psychological measure. We believe that this \u201csurprise\u201d should be highly related to visual saliency, and thereby introduce a novel computational Free Energy inspired Saliency detection technique (FES). Our method computes the local entropy of the gap between an input image signal and its predicted counterpart that is reconstructed from the input one with a semi-parametric model. Experimental results prove that our algorithm predicts human fixation points accurately and is superior to classical/state-of-the-art competitors.", "It has been found that facial thermograms vary with ambient temperature, as well as other internal and external conditions, and result in severe decline in the facial recognition rate. To tackle this problem, a skin heat transfer (SHT) model based on thermal physiology is derived in this paper. The proposed model converts the facial thermograms into blood-perfusion data, which is revealed to reduce the within-class scatter of face images. The advantage of the derived blood-perfusion data over the raw thermograms for recognition is analyzed by the normalized reverse cumulative histogram. It is shown that blood-perfusion data are more consistent in representing facial features. The experiments conducted on both same-session and time-lapse data have further demonstrated that (1) the blood-perfusion data are less sensitive to ambient temperature, physiological and psychological conditions if the human bodies are\u00a0\u2026", "An ensemble method for full-reference image quality assessment (IQA) based on the parallel boosting (ParaBoost) idea is proposed in this paper. We first extract features from existing image quality metrics and train them to form basic image quality scorers (BIQSs). Then, we select additional features to address specific distortion types and train them to construct auxiliary image quality scorers (AIQSs). Both BIQSs and AIQSs are trained on small image subsets of certain distortion types and, as a result, they are weak performers with respect to a wide variety of distortions. Finally, we adopt the ParaBoost framework, which is a statistical scorer selection scheme for support vector regression (SVR), to fuse the scores of BIQSs and AIQSs to evaluate the images containing a wide range of distortion types. This ParaBoost methodology can be easily extended to images of new distortion types. Extensive experiments are\u00a0\u2026", "The recent advances of hardware technology have made the intelligent analysis equipped at the front-end with deep learning more prevailing and practical. To better enable the intelligent sensing at the front-end, instead of compressing and transmitting visual signals or the ultimately utilized top-layer deep learning features, we propose to compactly represent and convey the intermediate-layer deep learning features with high generalization capability, to facilitate the collaborating approach between front and cloud ends. This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis. Moreover, the presented strategy also makes the standardization of deep feature coding more feasible and promising, as a series of tasks can simultaneously benefit from the\u00a0\u2026", "We propose a novel just noticeable difference (JND) model for a screen content image (SCI). The distinct properties of the SCI result in different behaviors of the human visual system when viewing the textual content, which motivate us to employ a local parametric edge model with an adaptive representation of the edge profile in JND modeling. In particular, we decompose each edge profile into its luminance, contrast, and structure, and then evaluate the visibility threshold in different ways. The edge luminance adaptation, contrast masking, and structural distortion sensitivity are studied in subjective experiments, and the final JND model is established based on the edge profile reconstruction with tolerable variations. Extensive experiments are conducted to verify the proposed JND model, which confirm that it is accurate in predicting the JND profile, and outperforms the state-of-the-art schemes in terms of the\u00a0\u2026", "We propose an effective deblocking scheme with extremely low computational complexity. The algorithm involves three parts: local ac coefficient regularization (ACR) of shifted blocks in the discrete cosine transform (DCT) domain, block-wise shape adaptive filtering (BSAF) in the spatial domain, and quantization constraint (QC) in the DCT domain. The DCT domain ACR suppresses the grid noise (blockiness) in monotone areas. The spatial-domain BSAF alleviates the staircase noise along the edge, and the ringing near the edge and the corner outliers. The narrow quantization constraint set is imposed to prevent possible oversmoothing and improve PSNR performance. Extensive simulation results and comparative studies are provided to justify the effectiveness and efficiency of the proposed deblocking algorithm.", "Objectively accessing the quality of screen content images (SCIs) is a challenging problem, as SCIs may not always have identical properties as natural scenes. Here we conduct comprehensive studies on the subjective and objective quality assessment of the compressed SCIs. Firstly, we build a database that contains the distorted SCIs generated by the high efficiency video coding standard as well as its extension on screen content compression. Subsequently, subjective experiments are conducted to evaluate the perceived quality of these SCIs with compression artifacts. To automatically predict the subjective quality, a reduced-reference quality assessment model is further learnt by a set of wavelet domain features concerning the generalized spectral behavior, the fluctuations of the energy, and the information content with relatively large scale training samples. Our experimental results show that the learnt\u00a0\u2026", "Video summarization techniques aim at condensing a full-length video to a significantly shortened version that still preserves the major semantic content of the original video. Movie summarization, being a special class of video summarization, is particularly challenging since a large variety of movie scenarios and film styles complicate the problem. In this paper, we propose a two-stage scene-based movie summarization method based on mining the relationship between role-communities since the role-communities in earlier scenes are usually used to develop the role relationship in later scenes. In the analysis stage, we construct a social network to characterize the interactions between role-communities. As a result, the social power of each role-community is evaluated by the community's centrality value and the role communities are clustered into relevant groups based on the centrality values. In the\u00a0\u2026", "The H.264 video coding standard exhibits higher performance compared to the other existing standards such as H.263, MPEG-X. This improved performance is achieved mainly due to the multiple-mode motion estimation and compensation. Recent research tried to reduce the computational time using the predictive motion estimation, early zero motion vector detection, fast motion estimation, and fast mode decision, etc. These approaches reduce the computational time substantially, at the expense of degrading image quality and/or increase bitrates to a certain extent. In this paper, we use phase correlation to capture the motion information between the current and reference blocks and then devise an algorithm for direct motion estimation mode prediction, without excessive motion estimation. A bigger amount of computational time is reduced by the direct mode decision and exploitation of available motion vector\u00a0\u2026", "We propose a new spatial just noticeable distortion (JND) profile for color image processing. The JND threshold depends on various masking effects underlying existing in the human vision system (HVS). How to efficiently integrate different masking effects together is the key issue of modelling the JND profile. Based on recent vision research results, we model the masking effects in different stimulus dimensions as a nonlinear additivity model for masking (NAMM). It applies to all color components and accounts for the compound impact of luminance masking and texture masking to estimate the JND threshold in images. In our PSNR and subjective comparison to the related work, the proposed NAMM scheme provides a more accurate JND profile towards the actual JND bound in the HVS.", "During recent years, blind image quality assessment (BIQA) has been intensively studied with different machine learning tools. Existing BIQA metrics, however, do not design for stereoscopic images. We believe this problem can be resolved by separating 3D images and capturing the essential attributes of images via deep neural network. In this paper, we propose a blind deep quality evaluator (DQE) for stereoscopic images (denoted by 3D-DQE) based on monocular and binocular interactions. The key technical steps in the proposed 3D-DQE are to train two separate 2D deep neural networks (2D-DNNs) from 2D monocular images and cyclopean images to model the process of monocular and binocular quality predictions, and combine the measured 2D monocular and cyclopean quality scores using different weighting schemes. Experimental results on four public 3D image quality assessment databases\u00a0\u2026", "This paper introduces a novel framework, HodgeRank on Random Graphs, based on paired comparison, for subjective video quality assessment. Two types of random graph models are studied, i.e., Erd\u00f6s-R\u00e9nyi random graphs and random regular graphs. Hodge decomposition of paired comparison data may derive, from incomplete and imbalanced data, quality scores of videos and inconsistency of participants' judgments. We demonstrate the effectiveness of the proposed framework on LIVE video database. Both of the two random designs are promising sampling methods without jeopardizing the accuracy of the results. In particular, due to balanced sampling, random regular graphs may achieve better performances when sampling rates are small. However, when the number of videos is large or when sampling rates are large, their performances are so close that Erd\u00f6s-R\u00e9nyi random graphs, as the simplest\u00a0\u2026", "The type and extent of blur affect image quality and therefore its evaluation. This paper presents an accurate method for blur identification and parameter estimation from one image without a priori knowledge. The key idea of the proposed method is to perform Fourier transform of logarithm spectrum to detect the periodic blur pattern in cepstrum domain instead of spectral nulls. Accordingly, the estimation of blur parameters is more accurate and robust to noise. The experimental results validate the accuracy of the proposed method.", "During the past few years, there have been various kinds of content-aware image retargeting operators proposed for image resizing. However, the lack of effective objective retargeting quality assessment metrics limits the further development of image retargeting techniques. Different from traditional image quality assessment (IQA) metrics, the quality degradation during image retargeting is caused by artificial retargeting modifications, and the difficulty for image retargeting quality assessment (IRQA) lies in the alternation of the image resolution and content, which makes it impossible to directly evaluate the quality degradation like traditional IQA. In this paper, we interpret the image retargeting in a unified framework of resampling grid generation and forward resampling. We show that the geometric change estimation is an efficient way to clarify the relationship between the images. We formulate the geometric change\u00a0\u2026", "In video coding, the in-loop filtering has emerged as a key module due to its significant improvement on compression performance since H.264/Advanced Video Coding. Existing incorporated in-loop filters in video coding standards mainly take advantage of the local smoothness prior model used for images. In this paper, we propose a novel adaptive loop filter utilizing image nonlocal prior knowledge by imposing the low-rank constraint on similar image patches for compression noise reduction. In the filtering process, the reconstructed frame is first divided into image patch groups according to image patch similarity. The proposed in-loop filtering is formulated as an optimization problem with low-rank constraint for every group of image patches independently. It can be efficiently solved by soft-thresholding singular values of the matrix composed of image patches in the same group. To adapt the properties of the input\u00a0\u2026", "In video coding, an intra (I)-frame is used as an anchor frame for referencing the subsequence frames, as well as error propagation prevention, indexing, and so on. To get better rate-distortion performance, a frame should have the following quality to be an ideal I-frame: the best similarity with the frames in a group of picture (GOP), so that when it is used as a reference frame for a frame in the GOP we need the least bits to achieve the desired image quality, minimize the temporal fluctuation of quality, and also maintain a more consistent bit count per frame. In this paper we use a most common frame of a scene in a video sequence with dynamic background modeling and then encode it to replace the conventional I-frame. The extensive experimental results confirm the superiority of our proposed scheme in comparison with the existing state-of-art methods by significant image quality improvement and computational\u00a0\u2026", "A model of visual masking, which reveals the visibility of stimuli in the human visual system (HVS), is useful in perceptual based image/video processing. The existing visual masking function mainly considers luminance contrast, which always overestimates the visibility threshold of the edge region and underestimates that of the texture region. Recent research on visual perception indicates that the HVS is sensitive to orderly regions that possess regular structures and insensitive to disorderly regions that possess uncertain structures. Therefore, structural uncertainty is another determining factor on visual masking. In this paper, we introduce a novel pattern masking function based on both luminance contrast and structural uncertainty. Through mimicking the internal generative mechanism of the HVS, a prediction model is firstly employed to separate out the unpredictable uncertainty from an input image. In addition\u00a0\u2026", "The use of complementary information, namely depth or thermal information, has shown its benefits to salient object detection (SOD) during recent years. However, the RGB-D or RGB-T SOD problems are currently only solved independently, and most of them directly extract and fuse raw features from backbones. Such methods can be easily restricted by low-quality modality data and redundant cross-modal features. In this work, a unified end-to-end framework is designed to simultaneously analyze RGB-D and RGB-T SOD tasks. Specifically, to effectively tackle multi-modal features, we propose a novel multi-stage and multi-scale fusion network (MMNet), which consists of a cross-modal multi-stage fusion module (CMFM) and a bi-directional multi-scale decoder (BMD). Similar to the visual color stage doctrine in the human visual system (HVS), the proposed CMFM aims to explore important feature representations in\u00a0\u2026", "In three-dimensional (3D) video coding, distortion in texture video and depth maps can all affect the quality of the synthesized virtual views. Therefore, under the total bitrate constraint, effective bit allocation between texture and depth information is very important for 3D video coding. In this paper, the major technical contribution is to formulate view synthesis quality for optimal resource allocation in 3D video coding, since such quality is what that matters most to the ultimate user (i.e., the viewer) of the system; to be more specific, a new joint bit allocation and rate control method for multi-view video plus depth (MVD) based 3D video coding is proposed accordingly. We firstly derive a view synthesis distortion model to characterize the effect of coding distortion of texture video and depth maps on the synthesized virtual views. Based on this model, we derive a rate-distortion model to characterize the relationship between\u00a0\u2026", "Content-based image retrieval (CBIR) has attracted substantial attention during the past few years for its potential practical applications to image management. A variety of relevance feedback schemes have been designed to bridge the semantic gap between low-level visual features and high-level semantic concepts for an image retrieval task. Various collaborative image retrieval (CIR) schemes aim to utilize the user historical feedback log data with similar and dissimilar pairwise constraints to improve the performance of a CBIR system. However, existing subspace learning approaches with explicit label information cannot be applied for a CIR task although the subspace learning techniques play a key role in various computer vision tasks, e.g., face recognition and image classification. In this paper, we propose a novel subspace learning framework, i.e., conjunctive patches subspace learning (CPSL) with side\u00a0\u2026", "This article devises a photograph-based monitoring model to estimate the real-time PM 2.5  concentrations, overcoming currently popular electrochemical sensor-based PM 2.5  monitoring methods\u2019 shortcomings such as low-density spatial distribution and time delay. Combining the proposed monitoring model, the photographs taken by various camera devices (e.g., surveillance camera, automobile data recorder, and mobile phone) can widely monitor PM 2.5  concentration in megacities. This is beneficial to offering helpful decision-making information for atmospheric forecast and control, thus reducing the epidemic of COVID-19. To specify, the proposed model fuses Information Abundance measurement and Wide and Deep learning, dubbed as IAWD, for PM 2.5  monitoring. First, our model extracts two categories of features in a newly proposed DS transform space to measure the information abundance (IA) of a\u00a0\u2026", "In this paper, we introduce the very first `binary' 3D feature descriptor, B-SHOT, for fast and efficient keypoint matching on 3D point clouds. We propose a binary quantization method that converts a real valued vector to a binary vector. We apply this method on a state-of-the-art 3D feature descriptor, SHOT [1], and create a new binary 3D feature descriptor. B-SHOT requires 32 times lesser memory for its representation while being 6 times faster in feature descriptor matching, when compared to the SHOT feature descriptor. Experimental evaluation shows that B-SHOT offers comparable keypoint matching performance to that of the state-of-the-art 3D feature descriptors on a standard benchmark dataset.", "Relevance feedback (RF) schemes have been widely designed to improve the performance of content-based image retrieval. Despite the success, it is not appropriate to require the user to label a large number of samples in RF. Collaborative image retrieval (CIR) aims to reduce the labeling efforts of the user by resorting to the auxiliary information. Support vector machine (SVM) active learning can select ambiguous samples as the most informative ones for the user to label with the help of the optimal hyperplane of SVM, and thus alleviate the labeling efforts of conventional RF. However, the optimal hyperplane of SVM is usually unstable and inaccurate with small-sized training data, and this is always the case in image retrieval since the user would not like to label a large number of feedback samples and cannot label each sample accurately all the time. In this paper, we propose a novel active learning method, i.e\u00a0\u2026", "To transmit video contents over limited bandwidth network, video bitstreams may need to reduce the bit rate by encoding with coarse quantization parameters at the expense of degrading quality. At low bit rates, better coding quality can be achieved by downsampling the video prior to compression and upsampling later after decompression. In this paper, we present an adaptive downsampling/upsampling video coding scheme in order to achieve better video quality at low bit rates in terms of both measure and visual quality. In particular, appropriate downsampling directions/ratios and quantization step sizes are adaptively decided for encoding different regions of video frame with the consideration of local contents. Experimental results have shown the better performance of the proposed scheme over the regular coding and downsampling-based coding scheme with fixed downscaling ratio. In addition, the proposed\u00a0\u2026", "In this study, we focus on automated approaches to detect depression from clinical interviews using machine learning approached, which the models are trained on multi-modal data. Differentiating from successful machine learning approaches such as context-aware analysis through feature engineering and end-to-end deep neural networks to depression detection utilizing the Distress Analysis Interview Corpus, we propose a novel method that incorporates a data augmentation procedure based on topic modelling using transformer and deep 1D convolutional neural network (CNN) for acoustic feature modeling. The simulation results demonstrate the effectiveness of the proposed method for training multi-modal deep learning models. Our deep 1D CNN and transformer models achieve the state-of-the-art performance for the audio and text modalities respectively, while our multi-modal results are comparable with\u00a0\u2026", "Recently, some global contrast-based salient region detection models have been proposed based on only the low-level feature of color. It is necessary to consider both color and orientation features to overcome their limitations, and thus improve the performance of salient region detection for images with low-contrast in color and high-contrast in orientation. In addition, the existing fusion methods for different feature maps, like the simple averaging method and the selective method, are not effective sufficiently. To overcome these limitations of existing salient region detection models, we propose a novel salient region model based on the bottom-up and top-down mechanisms: the color contrast and orientation contrast are adopted to calculate the bottom-up feature maps, while the top-down cue of depth-from-focus from the same single image is used to guide the generation of final salient regions, since depth-from\u00a0\u2026", "In this paper, a criterion for objective defocus blur measurement is theoretically derived from one image. The essential idea is to estimate the point spread function (PSF) from the line spread function (LSF), whereas the LSF is constructed from edge information. It is proven that an edge point corresponds to the local maximal gradient in a blurred image, and therefore edges can be extracted from blurred images by conventional edge detectors. To achieve high accuracy, local Radon transform is implemented and a number of LSFs are extracted from each edge. The experimental results on a variety of synthetic and real blurred images validate the proposed method. The algorithm can be implemented for image quality evaluation in vision-based applications as no reference images are needed.", "The existing image compression methods (e.g., JPEG2000, etc.) are vulnerable to bit-loss, and this is usually tackled by channel coding that follows. However, source coding and channel coding have conflicting requirement. In this paper, we address the problem with an alternative paradigm, and a novel compressive sensing (CS) based compression scheme is therefore proposed. Discrete wavelet transform (DWT) is applied for sparse representation, and based on the property of 2-D DWT, a fast CS measurements taking method is presented. Unlike the unequally important discrete wavelet coefficients, the resultant CS measurements carry nearly the same amount of information and have minimal effects for bit-loss. At the decoder side, one can simply reconstruct the image via l 1  minimization. Experimental results show that the proposed CS-based image codec without resorting to error protection is more robust\u00a0\u2026", "Peak signal-to-noise ratio (PSNR) is commonly used as an objective quality metric in signal processing. However, PSNR correlates poorly with the subjective quality rating. In this paper, we propose a new metric using contrast signal-to-noise ratio (CSNR), which measures the ratio of the contrast information level of distorted signal to the contrast level of the error signal. The performance of the proposed method has been verified using a database of images compressed with JPEG and JPEG 2000. The results show that it can achieve very good correlation with the subjective mean opinion scores in terms of prediction accuracy, monotonicity and consistency.", "The human visual system is highly adaptive to extract structure information for scene perception, and structure character is widely used in perception-oriented image processing works. However, the existing structure descriptors mainly describe the luminance contrast of a local region, but cannot effectively represent the spatial correlation of structure. In this paper, we introduce a novel structure descriptor according to the orientation selectivity mechanism in the primary visual cortex. Research on cognitive neuroscience indicate that the arrangement of excitatory and inhibitory cortex cells arise orientation selectivity in a local receptive field, within which the primary visual cortex performs visual information extraction for scene understanding. Inspired by the orientation selectivity mechanism, we compute the correlations among pixels in a local region based on the similarities of their preferred orientation. By imitating the\u00a0\u2026", "The Quantization table in JPEG, which specifies the quantization scale for each discrete cosine transform (DCT) coefficient, plays an important role in image codec optimization. However, the generic quantization table design that is based on the characteristics of human visual system (HVS) cannot adapt to the variations of image content. In this letter, we propose a just-noticeable difference (JND) based quantization table derivation method for JPEG by optimizing the rate-distortion costs for all the frequency bands. To achieve better perceptual quality, the DCT domain JND-based distortion metric is utilized to model the stair distortion perceived by HVS. The rate-distortion cost for each band is derived by estimating the rate with the first-order entropy of quantized coefficients. Subsequently, the optimal quantization table is obtained by minimizing the total rate-distortion costs of all the bands. Extensive experimental\u00a0\u2026", "Generally, H.264/AVC video coding standard with hierarchical bipredictive picture (HBP) structure outperforms the classical prediction structures such as \u201cIPPP...\u201d and \u201cIBBP...\u201d through better exploitation of data correlation using reference frames and unequal quantization setting among frames. However, multiple reference frames (MRFs) techniques are not fully exploited in the HBP scheme because of the computational requirement for B-frames, unavailability of adjacent reference frames, and with no explicit sorting of the reference frames for foreground or background being used. To exploit MRFs fully and explicitly in background referencing, we observe that not a single frame of a video is appropriate to be the reference frame as no one covers adequate background of a video. To overcome the problems, we propose a new coding scheme with the HBP, which uses the most common frame in scene (McFIS\u00a0\u2026", "Images coded at low bit rates in real-world applications usually suffer from significant compression noise, which significantly degrades the visual quality. Traditional denoising methods are not suitable for the content-dependent compression noise, which usually assume that noise is independent and with identical distribution. In this paper, we propose a unified framework of content-adaptive estimation and reduction for compression noise via low-rank decomposition of similar image patches. We first formulate the framework of compression noise reduction based upon low-rank decomposition. Compression noises are removed by soft thresholding the singular values in singular value decomposition of every group of similar image patches. For each group of similar patches, the thresholds are adaptively determined according to compression noise levels and singular values. We analyze the relationship of image\u00a0\u2026", "We present a closed-form solution to estimate the covariance of the resultant transformation provided by the Iterative Closest Point (ICP) algorithm for 3D point cloud registration. We extend an existing work [1] that estimates ICP's covariance in 2D with point to plane error metric to 3D with point to point and point to plane error metrics. Moreover, we do not make any assumption on the noise present in the sensor data and have no constraints on the estimated rigid transformation. The source code of our implementation is made publicly available, which can be adapted to work for ICP with different error metrics with minor changes. Our preliminary results show that ICP's covariance is lower at a global minimum than at a local minima.", "Visual attention is a relatively new area of study combining a number of disciplines: artificial neural networks, artificial intelligence, vision science and psychology. The aim is to build computational models similar to human vision in order to solve tough problems for many potential applications including object recognition, unmanned vehicle navigation, and image and video coding and processing. In this book, the authors provide an up to date and highly applied introduction to the topic of visual attention, aiding researchers in creating powerful computer vision systems. Areas covered include the significance of vision research, psychology and computer vision, existing computational visual attention models, and the authors' contributions on visual attention models, and applications in various image and video processing tasks. This book is geared for graduates students and researchers in neural networks, image processing, machine learning, computer vision, and other areas of biologically inspired model building and applications. The book can also be used by practicing engineers looking for techniques involving the application of image coding, video processing, machine vision and brain-like robots to real-world systems. Other students and researchers with interdisciplinary interests will also find this book appealing. Provides a key knowledge boost to developers of image processing applications Is unique in emphasizing the practical utility of attention mechanisms Includes a number of real-world examples that readers can implement in their own work: robot navigation and object selection image and video quality assessment image and video\u00a0\u2026", "The existing image coding methods cannot support content-based spatial scalability with high compression. In mobile multimedia communications, image retargeting is generally required at the user end. However, content-based image retargeting (e.g., seam carving) is with high computational complexity and is not suitable for mobile devices with limited computing power. The work presented in this paper addresses the increasing demand of visual signal delivery to terminals with arbitrary resolutions, without heavy computational burden to the receiving end. In this paper, the principle of seam carving is incorporated into a wavelet codec (i.e., SPIHT ). For each input image, block-based seam energy map is generated in the pixel domain. In the meantime, multilevel discrete wavelet transform (DWT) is performed. Different from the conventional wavelet-based coding schemes, DWT coefficients here are grouped and\u00a0\u2026", "In this paper, a method for measuring the perceptual image quality of JPEG-2000 coded images has been proposed. The image quality is characterized by the average edge-spread in the image, or more specifically the average extent of the slope's spread of an edge in the opposing gradients' directions. The proposed method is, in effect, a way of measuring the amount of blurring in the image. The effectiveness of such method is validated using subjective tests and the experimental results show that the proposed method can provide results that correlate relatively well with human subjective ratings.", "This paper proposes to blindly evaluate the quality of images synthesized via a depth image-based rendering (DIBR) procedure. As a significant branch of virtual reality (VR), superior DIBR techniques provide free viewpoints in many real applications, including remote surveillance and education; however, limited efforts have been made to measure the performance of DIBR techniques, or equivalently the quality of DIBR-synthesized views, especially in the condition when references are unavailable. To achieve this aim, we develop a novel blind image quality assessment (IQA) method via multiscale natural scene statistical analysis (MNSS). The design principle of our proposed MNSS metric is based on two new natural scene statistics (NSS) models specific to the DBIR-synthesized IQA. First, the DIBR-introduced geometric distortions damage the local self-similarity characteristic of natural images, and the damage\u00a0\u2026", "Research in biological vision and neurology has evidenced that there are separate mechanisms in human visual cortex to process the first- and second-order patterns. Image structures detected by a linear filter are the first-order patterns which describe luminance changes, while patterns that are invisible to linear filters are often referred as the second-order structures. In this paper, we propose a general-purpose blind image quality assessment (BIQA) method by taking account of both the first- and second-order image structures. Specifically, the Prewitt linear filters are used to extract first-order image structures and the local contrast normalization is employed to extract second-order image structures. Perceptual features are extracted from these two image structural maps and used as the input to a support vector regression to model the nonlinear relationship between feature space to human opinion score. Extensive\u00a0\u2026", "The marker-based watershed approach is a very efficient means for image segmentation and has been widely used in recent years. The conventional marker-based algorithms are realized using hierarchical queues. A new marker-based watershed algorithm based on the disjoint set data structure is proposed in this paper. It consists of two steps: the flooding step and the resolving step. This algorithm has significantly lower memory requirement as compared with the conventional algorithms while maintaining the computational complexity of O(N) where N is the image size. Experimental results further show that the new algorithm implemented in C language runs much faster than the conventional algorithm based on the hierarchical queues as a result of savings from huge memory allocation and releasing operations.", "Blind image quality assessment (BIQA) is a useful but challenging task. It is a promising idea to design BIQA methods by mimicking the working mechanism of human visual system (HVS). The internal generative mechanism (IGM) indicates that the HVS actively infers the primary content (i.e., meaningful information) of an image for better understanding. Inspired by that, this paper presents a novel BIQA metric by mimicking the active inference process of IGM. Firstly, an active inference module based on the generative adversarial network (GAN) is established to predict the primary content, in which the semantic similarity and the structural dissimilarity (i.e., semantic consistency and structural completeness) are both considered during the optimization. Then, the image quality is measured on the basis of its primary content. Generally, the image quality is highly related to three aspects, i.e., the scene information (content\u00a0\u2026", "Interacting with images through social media has become widespread due to ubiquitous Internet access and multimedia enabled devices. Through images, users generally present their daily activities, preferences or interests. This study aims to identify the way and extent to which personality differences, measured using the Big Five model, are related to online image posting and liking. In two experiments, the larger consisting of ~1.5 million Twitter images both posted and liked by ~4,000 users, we extract interpretable semantic concepts using large-scale image content analysis and analyze differences specific of each personality trait. Predictive results show that image content can predict personality traits, and that there can be significant performance gain by fusing the signal from both posted and liked images.", "Screen content is typically composed of computer generated text and graphics. The contents shown on the screen exhibit various unnatural properties, such as sharp edges and thin lines with few color variations. In this paper we design a novel structure-induced quality metric (SIQM) for assessing the screen image quality. The proposed SIQM works by weighting the benchmark structural similarity index (SSIM) with the structural degradation measurement that is computed using SSIM as well. Experimental results conducted on the newly released subjective quality database concerning screen images show that on one hand the proposed technique is superior to existing quality measures, and on the other hand our model is able to optimize screen video coding and thus introduce remarkable visual quality improvement.", "In this paper, we present a reversible data embedding scheme based on an adaptive edge-directed prediction for images. It is known that the difference expansion is an efficient data embedding method. Since the expansion on a large difference will cause a significant embedding distortion, a location map is usually employed to select small differences for expansion and to avoid overflow/underflow problems caused by expansion. However, location map bits lower payload capacity for data embedding. To reduce the location map, our proposed scheme aims to predict small prediction errors for expansion by using an edge detector. Moreover, to generate a small prediction error for each pixel, an adaptive edge-directed prediction is employed which adapts reasonably well between smooth regions and edge areas. Experimental results show that our proposed data embedding scheme for natural images can achieve\u00a0\u2026", "High dynamic range (HDR) image, which has a powerful capacity to represent the wide dynamic range of real-world scenes, has been receiving attention from both academic and industrial communities. Although HDR imaging devices have become prevalent, the display devices for HDR images are still limited. To facilitate the visualization of HDR images in standard low dynamic range displays, many different tone mapping operators (TMOs) have been developed. To create a fair comparison of different TMOs, this paper proposes a BLInd QUality Evaluator to blindly predict the quality of Tone-Mapped Images (BLIQUE-TMI) without accessing the corresponding HDR versions. BLIQUE-TMI measures the quality of TMIs by considering the following aspects: 1) visual information; 2) local structure; and 3) naturalness. To be specific, quality-aware features related to the former two aspects are extracted in a local manner\u00a0\u2026", "The field of assessing three-dimensional (3-D) visual experience is challenging. In this paper, we propose a new blind image quality assessment for stereoscopic images by using binocular guided quality lookup and visual codebook. To be more specific, in the training stage, we construct phase-tuned quality lookup (PTQL) and phase-tuned visual codebook (PTVC) from the binocular energy responses based on stimuli from different spatial frequencies, orientations, and phase shifts. In the test stage, blind quality pooling can be easily achieved by searching the PTQL and PTVC, and the quality score is obtained by averaging the largest values of all patch's quality. Experimental results on three 3-D image quality assessment databases demonstrate that in comparison with the most related existing methods, the devised algorithm achieves high consistency alignment with subjective assessment and low-complexity\u00a0\u2026", "In this letter, we introduce an improved structural degradation based image quality assessment (IQA) method. Most of the existing structural similarity based IQA metrics mainly consider the spatial contrast degradation but have not fully considered the changes on the spatial distribution of structures. Since the human visual system (HVS) is sensitive to degradations on both spatial contrast and spatial distribution, both factors need to be considered for IQA. In order to measure the structural degradation on spatial distribution, the local binary patterns (LBPs) are first employed to extract structural information. And then, the LBP shift between the reference and distorted images is computed, because noise distorts structural patterns. Finally, the spatial contrast degradation on each pair of LBP shifts is calculated for quality assessment. Experimental results on three large benchmark databases confirm that the proposed IQA\u00a0\u2026", "Selective attention in the human visual system is performed as the way that humans focus on the most important parts when observing a visual scene. Many bottom-up computational models of visual attention have been devised to get the saliency map for an image, which are data-driven or task-independent. However, studies show that the task-driven or top-down mechanism also plays an important role during the formation of visual attention, especially with the cases of object detection and location. In this paper, we proposed a new computational visual attention model by combining bottom-up and top-down mechanisms for man-made object detection in scenes. This model shows that the statistical characteristics of orientation features can be used as top-down clues to help for determining the location for salient objects in natural scenes. Experiments confirm the effectiveness of this visual attention model.", "We propose a guided image contrast enhancement framework based on cloud images, in which the context- sensitive and context-free contrast is jointly improved via solving a multi-criteria optimization problem. In particular, the context-sensitive contrast is improved by performing advanced unsharp masking on the input and edge-preserving filtered images, while the context-free contrast enhancement is achieved by the sigmoid transfer mapping. To automatically determine the contrast enhancement level, the parameters in the optimization process are estimated by taking advantages of the retrieved images with similar content. For the purpose of automatically avoiding the involvement of low-quality retrieved images as the guidance, a recently developed no-reference image quality metric is adopted to rank the retrieved images from the cloud. The image complexity from the free-energy-based brain theory and the\u00a0\u2026", null, null, null, "Blocking artifact is a prevailing degradation caused by the block-based Discrete Cosine Transform (BDCT) coding technique under low bit-rate conditions. In this paper, we propose an efficient no-reference noticeable blockiness estimation algorithm for BDCT coded images. The difference on block boundaries is first measured and then transformed into the block discontinuity map. We consider the effects of luminance adaptation and texture masking on blocking and integrate them using a nonlinear operator to form an overall masking map. This map is finally incorporated with the discontinuity map to generate Noticeable Blockiness Map (NBM), which can be used to guide perceptual quality assessment, codec parameter optimization, post-processing, etc. We have demonstrated the validity of the NBM through its applications in no-reference image quality assessment and image deblocking.", "Image degradation damages genuine visual structures and causes pseudo structures. Pseudo structures are usually present with regularities. This letter proposes a machine learning based blocking artifacts metric for JPEG images by measuring the regularities of pseudo structures. Image corner, block boundary and color change properties are used to differentiate the blocking artifacts. A support vector regression (SVR) model is adopted to learn the underlying relations between these features and perceived blocking artifacts. The blocking artifacts score of a test image is predicted using the trained model. Extensive experiments demonstrate the effectiveness of the method.", "Efficient image watermarking calls for full exploitation of the perceptual distortion constraint. Second-order statistics of visual stimuli are regarded as critical features for perception. This paper proposes a second-order statistics (SOS)-based image quality metric, which considers the texture masking effect and the contrast sensitivity in Karhunen-Lo\u00e8ve transform domain. Compared with the state-of-the-art metrics, the quality prediction by SOS better correlates with several subjectively rated image databases, in which the images are impaired by the typical coding and watermarking artifacts. With the explicit metric definition, spread spectrum watermarking is posed as an optimization problem: we search for a watermark to minimize the distortion of the watermarked image and to maximize the correlation between the watermark pattern and the spread spectrum carrier. The simple metric guarantees the optimal watermark a\u00a0\u2026", "This paper presents the techniques used in our contribution to Emotion Recognition in the Wild 2016\u2019s video based sub-challenge. The purpose of the sub-challenge is to classify the six basic emotions (angry, sad, happy, surprise, fear & disgust) and neutral. Compared to earlier years\u2019 movie based datasets, this year\u2019s test dataset introduced reality TV videos containing more spontaneous emotion. Our proposed solution is the fusion of facial expression recognition and audio emotion recognition subsystems at score level. For facial emotion recognition, starting from a network pre-trained on ImageNet training data, a deep Convolutional Neural Network is fine-tuned on FER2013 training data for feature extraction. The classifiers, i.e., kernel SVM, logistic regression and partial least squares are studied for comparison. An optimal fusion of classifiers learned from different kernels is carried out at the score level to\u00a0\u2026", "Motion estimation (ME) and motion compensation (MC) using variable block size, fractional search, and multiple reference frames (MRFs) help the recent video coding standard H.264 to improve the coding performance significantly over the other contemporary coding standards. The concept of MRF achieves better coding performance in the cases of repetitive motion, uncovered background, non-integer pixel displacement, lighting change, etc. The requirement of index codes of the reference frames, computational time in ME&MC, and memory buffer for pre-coded frames limits the number of reference frames used in practical applications. In typical video sequence, the previous frame is used as a reference frame with 68~92% of cases. In this paper, we propose a new video coding method using a reference frame (i.e., the most common frame in scene (McFIS)) generated by the Gaussian mixture based dynamic\u00a0\u2026", "This paper presents a new method for objective measurement of out-of-focus blur images. The essential idea is to derive the point spread function (PSF) of an out-of-focus blur image from the line spread function (LSF). In this method, it is assumed that at least one sharp edge could be detected. Then, the LSF can be constructed from the edge(s). This approach has fast speed as it works in spatial domain without complex Fourier transform or iterative computation. Moreover, a generic criterion for out-of-focus blur measurement is proposed and it can be applied to all images without depending on image contents. The experimental results validate the accuracy and efficiency of the proposed method.", "In practice, images are distorted by more than one distortion. For image quality assessment (IQA), existing machine learning (ML)-based methods generally establish a unified model for all the distortion types, or each model is trained independently for each distortion type, which is therefore distortion aware. In distortion-aware methods, the common features among different distortions are not exploited. In addition, there are fewer training samples for each model training task, which may result in overfitting. To address these problems, we propose a multi-task learning framework to train multiple IQA models together, where each model is for each distortion type; however, all the training samples are associated with each model training task. Thus, the common features among different distortion types and the said underlying relatedness among all the learning tasks are exploited, which would benefit the generalization\u00a0\u2026", "In this work, selfies (self-portrait images) of users are used to computationally predict and understand their personality. For users to convey a certain impression with selfie, and for the observers to build a certain impression about the users, many visual cues play a significant role. It is interesting to analyse what these cues are and how they influence our understanding of personality profiles. Selfies of users (from a popular microblogging site, Sina Weibo) were annotated with mid-level cues (such as presence of duckface, if the user is alone, emotional positivity etc.) relevant to portraits (especially selfies). Low-level visual features were used to train models to detect these mid-level cues, which are then used to predict users' personality (based on Five Factor Model). The mid-level cue detectors are seen to outperform state-of-the-art features for most traits. Using the trained computational models, we then present\u00a0\u2026", "Image quality assessment (IQA) is a fundamental yet constantly developing task for computer vision and image processing. Most IQA evaluation mechanisms are based on the pertinence of subjective and objective estimation. Each image distortion type has its own property correlated with human perception. However, this intrinsic property may not be fully exploited by existing IQA methods. In this paper, we make two main contributions to the IQA field. First, a novel IQA method is developed based on a local linear model that examines the distortion between the reference and the distorted images for better alignment with human visual experience. Second, a distortion-specific compensation strategy is proposed to offset the negative effect on IQA modeling caused by different image distortion types. These score offsets are learned from several known distortion types. Furthermore, for an image with an unknown\u00a0\u2026", "Distortions cause structural changes in digital images, leading to degraded visual quality. Dictionary-based sparse representation has been widely studied recently due to its ability to extract inherent image structures. Meantime, it can extract image features with slightly higher level semantics. Intuitively, sparse representation can be used for image quality assessment, because visible distortions can cause significant changes to the sparse features. In this paper, a new sparse representation-based image quality assessment model is proposed based on the construction of adaptive sub-dictionaries. An over-complete dictionary trained from natural images is employed to capture the structure changes between the reference and distorted images by sparse feature extraction via adaptive sub-dictionary selection. Based on the observation that image sparse features are invariant to weak degradations and the perceived\u00a0\u2026", "The quality assessment of 3D images is more challenging than its 2D counterparts, and little investigation has been dedicated to blind quality assessment of stereoscopic images. In this letter, we propose a novel blind quality assessment for stereoscopic images based on binocular feature combination. The prominent contribution of this work is that we simplify the process of binocular quality prediction as monocular feature encoding and binocular feature combination. Experimental results on two publicly available 3D image quality assessment databases demonstrate the promising performance of the proposed method.", "Objective speech quality assessment is a challenging task which aims to emulate human judgment in the complex and time consuming task of subjective assessment. It is difficult to perform in line with the human perception due the complex and nonlinear nature of the human auditory system. The challenge lies in representing speech signals using appropriate features and subsequently mapping these features into a quality score. This paper proposes a nonintrusive metric for the quality assessment of noise-suppressed speech. The originality of the proposed approach lies primarily in the use of Mel filter bank energies (FBEs) as features and the use of support vector regression (SVR) for feature mapping. We utilize the sensitivity of FBEs to noise in order to obtain an effective representation of speech towards quality assessment. In addition, the use of SVR exploits the advantages of kernels which allow the\u00a0\u2026", "This paper presents a new methodology for objective visual quality assessment with multi-metric fusion (MMF). The current research is motivated by the observation that there is no single metric that gives the best performance scores in all situations. To achieve MMF, we adopt a regression approach. First, we collect a large number of image samples, each of which has a score labeled by human observers and scores associated with different metrics. The new MMF score is set to be the nonlinear combination of multiple metrics with suitable weights obtained by a training process. Furthermore, we divide image distortions into groups and perform regression within each group, which is called \u201ccontext-dependent MMF\u201d (CD-MMF). One task in CD-MMF is to determine the context automatically, which is achieved by a machine learning approach. It is shown by experimental results that the proposed MMF metric\u00a0\u2026", "Objective image retargeting quality assessment aims to use computational models to predict the retargeted image quality consistent with subjective perception. In this paper, we propose a multiple-level feature (MLF)-based quality measure to predict the perceptual quality of retargeted images. We first provide an in-depth analysis on the low-level aspect ratio similarity feature, and then propose a mid-level edge group similarity feature, to better address the shape/structure related distortion. Furthermore, a high-level face block similarity feature is designed to deal with sensitive region deformation. The multiple-level features are complementary as they quantify different aspects of quality degradation in the retargeted image, and the MLF measure learned by regression is used to predict the perceptual quality of retargeted images. Extensive experimental results performed on two public benchmark databases\u00a0\u2026", "Background and objectivesAngle closure glaucoma (ACG) is an eye disease prevalent throughout the world. ACG is caused by four major mechanisms: exaggerated lens vault, pupil block, thick peripheral iris roll, and plateau iris. Identifying the specific mechanism in a given patient is important because each mechanism requires a specific medication and treatment regimen. Traditional methods of classifying these four mechanisms are based on clinically important parameters measured from anterior segment optical coherence tomography (AS-OCT) images, which rely on accurate segmentation of the AS-OCT image and identification of the scleral spur in the segmented AS-OCT images by clinicians.MethodsIn this work, a fully automated method of classifying different ACG mechanisms based on AS-OCT images is proposed. Since the manual diagnosis mainly based on the morphology of each mechanism, in this\u00a0\u2026", "Measurement of image quality is of fundamental importance to numerous image and video processing applications. Objective image quality assessment (IQA) is a two-stage process comprising of the following: (a) extraction of important information and discarding the redundant one, (b) pooling the detected features using appropriate weights. These two stages are not easy to tackle due to the complex nature of the human visual system (HVS). In this paper, we first investigate image features based on two-dimensional (2D) mel-cepstrum for the purpose of IQA. It is shown that these features are effective since they can represent the structural information, which is crucial for IQA. Moreover, they are also beneficial in a reduced-reference scenario where only partial reference image information is used for quality assessment. We address the second issue by exploiting machine learning. In our opinion, the well\u00a0\u2026", "Subjective visual quality evaluation provides the groundtruth and source of inspiration in building objective visual quality metrics. Paired comparison is expected to yield more reliable results; however, this is an expensive and timeconsuming process. In this paper, we propose a novel framework of HodgeRank on Random Graphs (HRRG) to achieve efficient and reliable subjective Video Quality Assessment (VQA). To address the challenge of a potentially large number of combinations of videos to be assessed, the proposed methodology does not require the participants to perform the complete comparison of all the paired videos. Instead, participants only need to perform a random sample of all possible paired comparisons, which saves a great amount of time and labor. In contrast to the traditional deterministic incomplete block designs, our random design is not only suitable for traditional laboratory and focus\u00a0\u2026", null, "Work presented in the paper includes two parts: first we measured the detectability and annoyance of frame dropping's effect on perceptual visual quality evaluation under different motion and framesize conditions. Then, a new logistics function and an effective yet simple motion content representation are selected to model the relationship among motion, framerate and negative impact of frame-dropping on visual quality, in one formula. The high Pearson and Spearman correlation results between the MOS and predicted MOSp, as well as the results of other two error metrics, confirm the success of the selected logistic function and motion content representation.", "In this paper, the problem of concealing missing image blocks is casted into a framework of Bayesian estimation. The conditional expectation of the missing block vector is taken over a pilot vector of correctly decoded pixels near the missing block. Multiple observations of the missing vector and pilot vectors obtained in a neighborhood are used to approximate the expectation. We design a multiscale estimation approach with discrete cosine transform pyramid to improve estimation efficiency. The DC image of the missing block is recovered first, and then more details related to high-frequency AC coefficients are recovered successively. Moreover, the algorithm operates in an iterative mode through using estimated block to refine the searching process for the next estimation. The algorithm is found to perform very well for a wide range of block loss rates. Substantial improvement over 14 existing error concealment\u00a0\u2026", "The state-of-the-art objective video quality metrics reported in the literature have so far been focused on TV signals with large frame sizes, full TV frame rates, and high compressed bit rates. The aim of this paper is to present the performance of three widely-known objective video quality metrics but tested on multimedia videos with small frame sizes, various frame rates, and low bit rates. The video quality metrics used were the NTIA video quality metrics (the \"general\" and the \"videoconferencing\" models), a modified Watson's DVQ metric, and the VSSIM metric. The test videos consist of both H.264 and MPEG-4 compressed videos with GIF and QCIF frame sizes, at various bit rates (24 kbps-384 kbps) and frame rates (7.5 fps-30 fps). Here, results and conclusions derived from the comparison of the three metrics will be provided.", "With the rapid growth of digital video through the Internet, a reliable objective video-quality assessment (VQA) algorithm is in great demand for video management. Motion information plays a dominant role for video perception, and the human visual system (HVS) is able to track moving objects effectively with eye movement. Moreover, the middle temporal area of the brain is selective for moving objects with particular velocities. In other words, visual contents that are along the motion trajectories will automatically attract our attention for dedicated processing. Inspired by the motion-related process in the HVS, we suggest analyzing the degradation along attended motion trajectories for VQA. The characteristic of motion velocity along each trajectory is analyzed for temporal quality measurement. Meanwhile, visual information along each trajectory is extracted for joint spatial-temporal quality measurement. Finally\u00a0\u2026", "With multitudes of image processing applications, image quality assessment (IQA) has become a prerequisite for obtaining maximally distinctive statistics from images. Despite the widespread research in this domain over several years, existing IQA algorithms have a number of key limitations concerning different image distortion types and algorithms' computational efficiency. Images that are synthesized using depth image-based rendering have applications in various disciplines, such as free viewpoint videos, which enable synthesis of novel realistic images in the referenceless environment. In the literature, very few no-reference (NR) quality assessment metrics of three-dimensional (3-D) synthesized images are proposed, and most of them are computationally expensive, which makes it difficult for them to be deployed in real-time applications. In this paper, we attribute the geometrically distorted pixels as outliers\u00a0\u2026", "Appearance change of moving targets is a challenging problem in visual tracking. In this paper, we present a novel visual object tracking algorithm based on the observation dependent hidden Markov model (OD-HMM) framework. The observation dependency is computed by structure complexity coefficients (SCC) which is defined to predict the target appearance change. Unlike conventional methods addressing the appearance change problem by investigating different online appearance models, we handle this problem by addressing the fundamental reason of motion -related appearance change during visual tracking. Based on the analysis of motion-related appearance change, we investigate the relationship between the structure of the object surface and the appearance stability. The appearance of complex structural regions is easier to change compared with that of smooth structural regions with object\u00a0\u2026", "Blind quality assessment of 3D images encounters more new challenges than its 2D counterparts. In this paper, we propose a blind quality assessment for stereoscopic images by learning the characteristics of receptive fields (RFs) from perspective of dictionary learning, and constructing quality lookups to replace human opinion scores without performance loss. The important feature of the proposed method is that we do not need a large set of samples of distorted stereoscopic images and the corresponding human opinion scores to learn a regression model. To be more specific, in the training phase, we learn local RFs (LRFs) and global RFs (GRFs) from the reference and distorted stereoscopic images, respectively, and construct their corresponding local quality lookups (LQLs) and global quality lookups (GQLs). In the testing phase, blind quality pooling can be easily achieved by searching optimal GRF and LRF\u00a0\u2026", "The interest on emotional computing has been increasing as many applications were in demand by multiple markets. This paper mainly focuses on different learning methods, and has implemented several methods: Support Vector Machine (SVM) and Deep Boltzmann Machine (DBM) for facial emotion recognition. The training and testing data sets of facial emotion prediction are from FERA 2015, and geometric features and appearance features are combined together. Different prediction systems are developed and the prediction results are compared. This paper aims to design an suitable system for facial emotion recognition.", "The interplay between system, context, and human factors is important in perception of multimedia quality. However, studies on human factors are very limited in comparison to those for system and context factors. This article presents an attempt to explore the influence of personality and cultural traits on perception of multimedia quality. As a first step, a database consisting of 144 video sequences from 12 short movie excerpts has been assembled and rated by 114 participants from a cross-cultural population, thereby providing a useful ground-truth for this (as well as future) study. As a second step, three statistical models are compared: (i) a baseline model to only consider system factors; (ii) an extended model to include personality and culture; and (iii) an optimistic model in which each participant is modeled. As a third step, predictive models based on content, affect, system, and human factors are trained to\u00a0\u2026", "Classification of different mechanisms of angle closure glaucoma (ACG) is important for medical diagnosis. Error-correcting output code (ECOC) is an effective approach for multiclass classification. In this study, we propose a new ensemble learning method based on ECOC with application to classification of four ACG mechanisms. The dichotomizers in ECOC are first optimized individually to increase their accuracy and diversity (or interdependence) which is beneficial to the ECOC framework. Specifically, the best feature set is determined for each possible dichotomizer and a wrapper approach is applied to evaluate the classification accuracy of each dichotomizer on the training dataset using cross-validation. The separability of the ECOC codes is maximized by selecting a set of competitive dichotomizers according to a new criterion, in which a regularization term is introduced in consideration of the binary\u00a0\u2026", "With the increasing proliferation of data production technologies (like cameras) and consumption avenues (like social media) multimedia has become an interaction channel among users today. Images and videos are being used by the users to convey innate preferences and tastes. This has led to the possibility of using multimedia as a source for user-modeling, thereby contributing to the field of personalization, recommender systems, content generation systems and so on. This work investigates approaches for modeling personality traits (based on the Five Factor Modeling approach) of users based on a collection of images they tag as \u2018favorite\u2019 on Flickr. It presents several insights for improving the personality estimation performance by proposing better features and modeling approaches. The efficacy of the improved personality modeling approach is demonstrated by its use in an image recommendation\u00a0\u2026", null, "Current deep video quality assessment (VQA) methods are usually with high computational costs when evaluating high-resolution videos. This cost hinders them from learning better video-quality-related representations via end-to-end training. Existing approaches typically consider naive sampling to reduce the computational cost, such as resizing and cropping. However, they obviously corrupt quality-related information in videos and are thus not optimal to learn good representations for VQA. Therefore, there is an eager need to design a new quality-retained sampling scheme for VQA. In this paper, we propose Grid Mini-patch Sampling (GMS), which allows consideration of local quality by sampling patches at their raw resolution and covers global quality with contextual relations via mini-patches sampled in uniform grids. These mini-patches are spliced and aligned temporally, named as fragments. We further build the\u00a0\u2026", "Numerous single image super-resolution (SISR) algorithms have been proposed during the past years to reconstruct a high-resolution (HR) image from its low-resolution (LR) observation. However, how to fairly compare the performance of different SISR algorithms/results remains a challenging problem. So far, the lack of comprehensive human subjective study on large-scale real-world SISR datasets and accurate objective SISR quality assessment metrics makes it unreliable to truly understand the performance of different SISR algorithms. We in this paper make efforts to tackle these two issues. Firstly, we construct a real-world SISR quality dataset (i.e.,  RealSRQ ) and conduct human subjective studies to compare the performance of the representative SISR algorithms. Secondly, we propose a new objective metric, i.e.,  KLTSRQA , based on the Karhunen-Lo\u00e9ve Transform (KLT) to evaluate the quality of SISR\u00a0\u2026", "Perceptual quality prediction for stereoscopic images is of fundamental importance in determining the level of quality perceived by humans in terms of the 3D viewing experience. However, the existing no-reference quality assessment (NR-IQA) framework has its limitation in addressing binocular combination for stereoscopic images. In this paper, we propose a new NR-IQA for stereoscopic images using joint sparse representation. We analyze the relationship between left and right quality predictors, and formulate stereoscopic quality prediction as a combination of feature-prior and feature-distribution. Based on this finding, we extract feature vector that handles different features to be interacted by joint sparse representation, and use support vector regression to characterize feature-prior. Meanwhile, we implement feature-distribution using sparsity regularization as the basis of weights for binocular combination to\u00a0\u2026", "Block-based transform coding is the most popular approach for image and video compression. The objective measurement of blocking artifacts plays an important role in the design, optimization, and assessment of image and video coding systems. This paper presents a new algorithm for measuring blocking artifacts in images and videos. Instead of using the traditional pixel discontinuity along the block boundary, we use the edge directional information of the images. The new algorithm does not need the exact location of the block boundary thus is invariant to the displacement, rotation and scaling of the images. Experiments on various still images and videos show that the new blockiness measure is very efficient in terms of computational complexity and memory usage, and can produce blocking artifacts measurement consistent with subjective rating.", "With the unprecedented success of deep learning in computer vision tasks, many cloud-based visual analysis applications are powered by deep learning models. However, the deep learning models are also characterized with high computational complexity and are task-specific, which may hinder the large-scale implementation of the conventional data communication paradigms. To enable a better balance among bandwidth usage, computational load and the generalization capability for cloud-end servers, we propose to compress and transmit intermediate deep learning features instead of visual signals and ultimately utilized features. The proposed strategy also provides a promising way for the standardization of deep feature coding. As the first attempt to this problem, we present a lossy compression framework and evaluation metrics for intermediate deep feature compression. Comprehensive experimental\u00a0\u2026", "In the field of stereoscopic 3D (S3D) display, it is an interesting as well as meaningful issue to retarget the stereoscopic images to the target resolution, while the existing stereoscopic image retargeting methods do not fully take user's Quality of Experience (QoE) into account. In this paper, we have presented a QoE-guided warping method for stereoscopic image retargeting, which retarget the stereoscopic image and adapt its depth range to the target display while promoting user's QoE. Our method takes shape preservation, visual comfort preservation, and depth perception preservation energies into account, and simultaneously optimizes the 2D coordinates and depth information in 3D space. It also considers the specific viewing configuration in the visual comfort and depth perception preservation energy constraints. Experimental results on visually uncomfortable and comfortable stereoscopic images\u00a0\u2026", "Research on Screen Content Images (SCIs) becomes important as they are increasingly used in multi-device communication applications. In this paper, we present a study of subjective quality assessment for distorted SCIs, and investigate which part (text or picture) contributes more to the overall visual quality. We construct a large-scale Screen Image Quality Assessment Database (SIQAD) consisting of 20 source and 980 distorted SCIs. The 11-category Absolute Category Rating (ACR) is employed to obtain three subjective quality scores corresponding to the entire image, textual and pictorial regions respectively. Based on the subjective data, we investigate the applicability of 12 state-of-the-art Image Quality Assessment (IQA) methods for objectively assessing the quality of SCIs. The results indicate that existing IQA methods are limited in predicting human quality judgement of SCIs. Moreover, we propose a\u00a0\u2026", "The existing implementations of block-shift based filtering algorithms for deblocking are hard to achieve good smoothing performance and low computation complexity simultaneously due to their fixed block size and small shifting range. In this paper, we propose to integrate quadtree (QT) decomposition with the block-shift filtering for deblocking. By incorporating the QT decomposition, we can easily find the locations of uniform regions and determine the corresponding suitable block sizes. The variable block sizes generated by the QT decomposition facilitate the later block-shift filtering with low computational cost. In addition, large block based shift filtering can provide better deblocking results because the smoothing range of large blocks spans over the conventional 8\u00d78 block size. Furthermore, we extend the proposed QT based block-shifting algorithm for deringing JPEG2000 coded images. Experimental results\u00a0\u2026", "This paper firstly presents a study of video quality for H.264 compressed videos compared to MPEG-4 (simple profile) from a perceptual point of view. Traditionally, peak signal-to-noise ratio (PSNR) has been used to represent the quality of a compressed video sequence. However, PSNR has been found to correlate poorly with subjective quality ratings, particularly at much lower bitrates and low frame rates. Thus, an alternative that has been commonly used is to perform subjective test where a large number of human subjects are used to gauge the quality of a video. However, this process is not only time-consuming but also tedious and expensive to perform. Hence, this paper further proposes an objective video quality measurement method to automatically measure the perceptual quality of a stream of video images. The proposed method has been tested on multimedia videos, consisting of CIF and QCIF video\u00a0\u2026", "Image quality assessment (IQA) has attracted more and more attention due to the urgent demand in image services. The perceptual-based image compression is one of the most prominent applications that require IQA metrics to be highly correlated with human vision. To explore IQA algorithms that are more consistent with human vision, several calibrated databases have been constructed. However, the distorted images in the existing databases are usually generated by corrupting the pristine images with various distortions in coarse levels, such that the IQA algorithms validated on them may be inefficient to optimize the perceptual-based image compression with fine-grained quality differences. In this paper, we construct a large-scale image database which can be used for fine-grained quality assessment of compressed images. In the proposed database, reference images are compressed at constant bitrate levels\u00a0\u2026", "We present the first attempt in creating a binary 3D feature descriptor for fast and efficient keypoint matching on 3D point clouds. Specifically, we propose a binarization technique and apply it on the state-of-the-art 3D feature descriptor, SHOT (Salti et al., Comput Vision Image Underst 125:251\u2013264, 2014) to create the first binary 3D feature descriptor, which we call B-SHOT. B-SHOT requires 32 times lesser memory for its representation while being six times faster in feature descriptor matching, when compared to the SHOT feature descriptor. Next, we propose a robust evaluation metric, specifically for 3D feature descriptors. A comprehensive evaluation on standard benchmarks reveals that B-SHOT offers comparable keypoint matching performance to that of the state-of-the-art real valued 3D feature descriptors, albeit at dramatically lower computational and memory costs.", "Effective feature selection plays a vital role in anterior segment imaging for determining the mechanism involved in angle-closure glaucoma (ACG) diagnosis. This research focuses on the use of redundant features for complex disease diagnosis such as ACG using anterior segment optical coherence tomography images. Both supervised [minimum redundancy maximum relevance (MRMR)] and unsupervised [Laplacian score (L-score)] feature selection algorithms have been cross-examined with different ACG mechanisms. An AdaBoost machine learning classifier is then used for classifying the five various classes of ACG mechanism such as iris roll, lens, pupil block, plateau iris, and no mechanism using both feature selection methods. The overall accuracy has shown that the usefulness of redundant features by L-score method in improved ACG diagnosis compared to minimum redundant features by MRMR\u00a0\u2026", "A method for use in a real-time video encoder based on two processing means to optimise the use of computing power of the processing means. This is achieved by dynamically load-balancing between two processing means. The load pattern is determined with measures for the progress of motion estimation in one processing means and the idle time in the other. Adaptive adjustment of load balancing is fulfilled via making a decision on whether the DCT/IDCT/Quantization/Dequantization sub-process is skipped for a macroblock. The invention is not constrained to particular motion estimation techniques.", "It is known that the human visual system (HVS) does not pay equal attention to each error and even region in judging picture quality. In this paper, we combine a perceptual model with an integrated detection of the spatially transitional regions in visual distortion evaluation to better match the HVS perception to visual quality. For decompressed images or video, the spatially transitional regions are the regions where major perceptually disturbing artefacts caused by edge impairments (mainly due to blurring and locations where the edge information is not adequately represented) and the presence of false edges (mainly due to blockiness and the presence of strong rippling effects of ringing) usually occur. Such regions are efficiently detected based on a single two-dimensional spatial high-pass filter in our work. Good correlation between the proposed method and the human perception has been demonstrated with the\u00a0\u2026", "This paper presents an overview of the emerging area of collaborative intelligence (CI). Our goal is to raise awareness in the signal processing community of the challenges and opportunities in this area of growing importance, where key developments are expected to come from signal processing and related disciplines. The paper surveys the current state of the art in CI, with special emphasis on signal processing-related challenges in feature compression, error resilience, privacy, and system-level design.", "In this paper, we address problems associated with free-energy-principle-based image quality assessment (IQA) algorithms for objectively assessing the quality of Screen Content (SC) and three-dimensional (3-D) synthesized images and also propose a very fast and efficient IQA algorithm to address these issues. These algorithms separate an image into predicted and disorder residual parts and assume disorder residual part does not contribute much to the overall perceptual quality. These algorithms fail for quality estimation of SC images as information of textual regions in SC images are largely separated into the disorder residual part and less information in the predicted part and subsequently, given a negligible emphasis. However, this is in contrast with the characteristics of human vision. Since our eyes are well trained to detect text in daily life. So, our human vision has prior information about text regions and\u00a0\u2026", "Malicious applications of deepfakes (ie, technologies generating target facial attributes or entire faces from facial images) have posed a huge threat to individuals' reputation and security. To mitigate these threats, recent studies have proposed adversarial watermarks to combat deepfake models, leading them to generate distorted outputs. Despite achieving impressive results, these adversarial watermarks have low image-level and model-level transferability, meaning that they can protect only one facial image from one specific deepfake model. To address these issues, we propose a novel solution that can generate a Cross-Model Universal Adversarial Watermark (CMUA-Watermark), protecting a large number of facial images from multiple deepfake models. Specifically, we begin by proposing a cross-model universal attack pipeline that attacks multiple deepfake models iteratively. Then, we design a two-level perturbation fusion strategy to alleviate the conflict between the adversarial watermarks generated by different facial images and models. Moreover, we address the key problem in cross-model optimization with a heuristic approach to automatically find the suitable attack step sizes for different models, further weakening the model-level conflict. Finally, we introduce a more reasonable and comprehensive evaluation method to fully test the proposed method and compare it with existing ones. Extensive experimental results demonstrate that the proposed CMUA-Watermark can effectively distort the fake facial images generated by multiple deepfake models while achieving a better performance than existing methods. Our code is available at\u00a0\u2026", "The 360-degree spherical images/videos, also called Virtual Reality (VR) images/videos, can provide immersive experience of the real-world scenes in some specific systems. This makes it widely employed in concerts/sports events live and VR movies. However, it is difficult to transport, compress or store VR images/videos due to their high resolution. So it is significant to research how the popular coding technologies influence the quality of VR images. To this aim, this paper carries out subjective quality evaluation of compressed VR images and examines the correlation performance of popular objective quality measures in accordance with the aforesaid subjective ratings. We first establish a Compressed VR Image Quality Database (CVIQD), which includes five source VR images and associated 165 compressed images under three prevailing coding technologies. The Single-Stimulus (SS) method is exploited to\u00a0\u2026", "Motion estimation (ME) and motion compensation (MC) using variable block size, sub-pixel search, and multiple reference frames (MRFs) are the major reasons for improved coding performance of the H.264 video coding standard over other contemporary coding standards. The concept of MRFs is suitable for repetitive motion, uncovered background, non-integer pixel displacement, lighting change, etc. The requirement of index codes of the reference frames, computational time in ME & MC, and memory buffer for coded frames limits the number of reference frames used in practical applications. In typical video sequences, the previous frame is used as a reference frame with 68\u201392% of cases. In this article, we propose a new video coding method using a reference frame [i.e., the most common frame in scene (McFIS)] generated by dynamic background modeling. McFIS is more effective in terms of rate-distortion and computational time performance compared to the MRFs techniques. It has also inherent capability of scene change detection (SCD) for adaptive group of picture (GOP) size determination. As a result, we integrate SCD (for GOP determination) with reference frame generation. The experimental results show that the proposed coding scheme outperforms the H.264 video coding with five reference frames and the two relevant state-of-the-art algorithms by 0.5\u20132.0 dB with less computational time.", "Annoying compression artifacts exist in most of lossy coded videos at low bit rates, which are caused by coarse quantization of transform coefficients or motion compensation from distorted frames. In this paper, we propose a compression artifact reduction approach that utilizes both the spatial and the temporal correlation to form multi-hypothesis predictions from spatio-temporal similar blocks. For each transform block, three predictions with their reliabilities are estimated, respectively. The first prediction is constructed by inversely quantizing transform coefficients directly, and its reliability is determined by the variance of quantization noise. The second prediction is derived by representing each transform block with a temporal auto-regressive (TAR) model along its motion trajectory, and its corresponding reliability is estimated from local prediction errors of the TAR model. The last prediction infers the original\u00a0\u2026", "Super-resolution (SR) reconstruction usually consists of four steps: registration, interpolation, restoration, and postprocessing. The registration precision (RP) and the initial SR image estimation (ISIE) greatly influence the quality of reconstructed images. A scheme to enhance RP and ISIE is proposed in this paper. Before the registration, each video frame is iteratively upsampled, the registration from current SR reconstructed frame and its adjacent upsampled frames are then estimated, and adjacent frames are warped with registrations to form the high-definition (HD) constraint set, while input frames are used to construct the low-definition (LD) constraint set. The SR reconstructed image corresponds to the minimum difference with the HD constraint set, and its warped and downsampled form corresponds to the minimum difference with the LD constraint set. ISIE can thus be improved from the HD constraint set. In this\u00a0\u2026", "Triangle meshes are used in representation of 3D geometric models, and they are subject to various visual distortions during geometrical processing and transmission. In this study, we propose a novel objective quality assessment method for 3D meshes based on curvature information; according to characteristics of the human visual system (HVS), two new components including visual masking and saturation effect are designed for the proposed method. Besides, inspired by the fact that the HVS is sensitive to structural information, we compute the structure distortion of 3D meshes. We test the performance of the proposed method on three publicly available databases of 3D mesh quality evaluation. We rotate among these databases for parameter determination to demonstrate the robustness of the proposed scheme. Experimental results demonstrate that the proposed method can predict consistent results in terms\u00a0\u2026", "All existing video coding standards developed so far deem video as a sequence of natural frames (formed in the XY plane), and treat spatial redundancy (redundancy along X and Y directions) and temporal redundancy (redundancy along T direction) differently and separately. In this paper, we investigate into a new compression (redundancy reduction) method for video in which the frames are allowed to be formed in a non-XY plane. We are to exploit fuller extent of video redundancy, and propose an adaptive optimal compression plane determination process to be used as a preprocessing step prior to any standard video coding scheme. The essence of the scheme is to form the frames in the plane formed by two axes (among X, Y, and T) corresponding to signal correlation evaluation, which enables better prediction (therefore better compression). In spite of the simplicity of the proposed method, it can be used for\u00a0\u2026", "Inspired by the characteristics of the human visual system, a novel method is proposed for detecting the visually salient regions on 3D point clouds. First, the local distinctness of each point is evaluated based on the difference with its local surroundings. Then, the point cloud is decomposed into small clusters, and the initial global rarity value of each cluster is calculated; a random walk ranking method is then used to introduce cluster-level global rarity refinement to each point in all the clusters. Finally, an optimization framework is proposed to integrate both the local distinctness and the global rarity values to obtain the final saliency detection result of the point cloud. We compare the proposed method with several relevant algorithms and apply it to some computer graphics applications, such as interest point detection, viewpoint selection, and mesh simplification. The experimental results demonstrate the superior\u00a0\u2026", "The goal of image retargeting is to adapt source images to target displays with different sizes and aspect ratios. Different retargeting operators create different retargeted images, and a key problem is to evaluate the performance of each retargeting operator. Subjective evaluation is most reliable, but it is cumbersome and labor-consuming, and more importantly, it is hard to be embedded into online optimization systems. This paper focuses on exploring the effectiveness of sparse representation for objective image retargeting quality assessment. The principle idea is to extract distortion sensitive features from one image (e.g., retargeted image) and further investigate how many of these features are preserved or changed in another one (e.g., source image) to measure the perceptual similarity between them. To create a compact and robust feature representation, we learn two overcomplete dictionaries to represent the\u00a0\u2026", "Predicting the degree of experienced visual comfort in the context of stereoscopic 3-D (S3D) viewing is particularly challenging. In this letter, a simple yet effective visual comfort assessment (VCA) approach for stereoscopic images is proposed from the perspective of learning to rank (L2R). The proposed L2R-based VCA (L2R-VCA) approach is inspired by the traditional absolute categorical rating (ACR) methodology in subjective study and is to characterize the qualitative description behavior of human subjective study. Experimental results on our recently built database confirm the promising performance of the proposed L2R-VCA approach, yielding higher consistency with human subject judgment results.", "A method of measuring a quality of a test video stream, the method comprising measuring a content richness fidelity feature of the test video stream based on occurrences of color values in image frames of the test video stream; measuring a block-fidelity feature of the test video stream based on distortion at block-boundaries in the image frames of the test video stream; measuring a distortion-invisibility feature of the test video stream based on distortion at pixels of the image frames of the test video stream; and determining a quality rating for the test video stream based on the content richness fidelity feature, the block-fidelity feature and the distortion-invisibility feature measured.", "City-scale 3D point clouds reconstructed via structure-from-motion from a large collection of Internet images are widely used in the image-based localization task to estimate a 6-DOF camera pose of a query image. Due to prohibitive memory footprint of city-scale point clouds, image-based localization is difficult to be implemented on devices with limited memory resources. Point cloud simplification aims to select a subset of points to achieve a comparable localization performance using the original point cloud. In this paper, we propose a data-driven point cloud simplification framework by taking it as a weighted K-Cover problem, which mainly includes two complementary parts. First, a utility-based parameter determination method is proposed to select a reasonable parameter K for K-Cover-based approaches by evaluating the potential of a point cloud for establishing sufficient 2D-3D feature correspondences\u00a0\u2026", "Salient object detection (SOD) in optical remote sensing images (RSIs), or RSI-SOD, is an emerging topic in understanding optical RSIs. However, due to the difference between optical RSIs and natural scene images (NSIs), directly applying NSI-SOD methods to optical RSIs fails to achieve satisfactory results. In this article, we propose a novel adjacent context coordination network (ACCoNet) to explore the coordination of adjacent features in an encoder\u2013decoder architecture for RSI-SOD. Specifically, ACCoNet consists of three parts: 1) an encoder; 2) adjacent context coordination modules (ACCoMs); and 3) a decoder. As the key component of ACCoNet, ACCoM activates the salient regions of output features of the encoder and transmits them to the decoder. ACCoM contains a local branch and two adjacent branches to coordinate the multilevel features simultaneously. The local branch highlights the salient\u00a0\u2026", "Three-dimensional feature descriptors are heavily employed in various 3-D perception applications to find keypoint correspondences between two point clouds. The availability of mobile devices equipped with depth sensors compels the developed applications to be both memory and computationally efficient. Toward this, in this letter, we present 3DHoPD, a new low-dimensional 3-D feature descriptor that is extremely fast to compute. The novelty lies in compactly encoding the \u201c3-D\u201d keypoint position by transforming it to a new 3-D space, where the keypoints arising from similar 3-D surface patches lie close to each other. Then, we propose histograms of point distributions (HoPD) to capture the neighborhood structure, thus forming 3DHoPD (3D+HoPD). We propose a tailored feature descriptor matching technique, wherein the \u201c3-D\u201d keypoint position in the new 3-D space is used to remove false positive matches\u00a0\u2026", "Assessing quality of experience (QoE) for three-dimensional (3-D) video is challenging. In this paper, we propose a new full-reference stereoscopic image quality metric, by simulating the behaviors of visual perception with simple and complex receptive field properties and constructing the models of monocular and binocular visual perception. To be more specific, the stereoscopic images are first classified into noncorresponding and corresponding regions. Then, monocular energy responses are generated for the noncorresponding region based on stimuli from different spatial frequencies and orientations, and binocular energy responses are generated for the corresponding region based on stimuli from different spatial frequencies, orientations and disparities, respectively. Finally, gradient similarities between the energy responses of the original and distorted stereoscopic images are measured for\u00a0\u2026", "Many recent applications require text segmentation for born-digital compound images. To this end, we propose a coarse-to-fine framework for segmenting texts of arbitrary scales and orientations in born-digital compound images. In the coarse stage, the local image activity measure is designed based upon the variation distribution of characters, to highlight the difference between textual and pictorial regions. This stage outputs a coarse textual layer including textual regions as well as a few pictorial regions with high activity. In the fine stage, a textual connected component (TCC) based refinement is proposed to eliminate the survived pictorial regions. In particular, a scale and orientation invariant grouping algorithm is proposed to adaptively generate TCCs with uniform statistical features. The minimum average distance and morphological operations are employed to assist the formation of candidate TCCs. Then\u00a0\u2026", "Surveillance videos are usually with a static or gradually changed background. The state-of-the-art block-based codec, H.264/AVC, is not sufficiently efficient for encoding surveillance videos since it cannot exploit the strong background temporal redundancy in a global manner. In this paper, motivated by the recent advance on low-rank and sparse decomposition (LRSD), we propose to apply it for the compression of surveillance videos. In particular, the LRSD is employed to decompose a surveillance video into the low-rank component, representing the background, and the sparse component, representing the moving objects. Then, we design different coding methods for the two different components. We represent the frames of the background by very few independent frames based on their linear dependency, which dramatically removes the temporal redundancy. Experimental results show that, for the\u00a0\u2026", "We propose a new non-intrusive speech quality assessment algorithm based on Support Vector Regression (SVR) and Mel Frequency Cepstral Coefficients (MFCCs). The basic idea is to map the MFCCs into the desired quality score using SVR. The sensitivity of the MFCCs to external noise is exploited to gauge the changes in the speech signal to evaluate its perceptual quality. The use of SVR exploits the advantages of machine learning with the ability to learn complex data patterns for an effective and generalized mapping of features into a perceptual score, in contrast with the oft-utilized feature pooling process in the existing speech quality estimators. Experimental results indicate that the proposed approach outperforms the standard P.563 algorithm for non-intrusive assessment of speech quality with a total of 1792 speech files and the associated subjective scores.", "We propose to use Block-based Bilateral Filter (BBF), which extends the classical Bilateral Filter (BF) through operating in block-wise manner, to conceal missing image blocks in the application of compressed image transmission over wireless channels. We show that the problem of error-concealment using BBF can be considered as a superset of image denoising using BF. The BBF has the ability to capture the block-level similarity that well matches the need of error-concealment for block based image compression. Simulation results suggest significant visual and PSNR improvements (up to 11 dB) over various classic and state-of-the-art error-concealment algorithms.", "The paper proposes an objective perceptual video quality metric to assess the perceived quality of digital videos automatically. Traditionally, peak signal-to-noise ratio (PSNR) has been used to represent the quality of a compressed video sequence. However, PSNR has been found to correlate poorly with subjective quality ratings, particularly at much lower bit rates and frame rates. Computational models are applied to emulate human visual perception based on block-fidelity, content richness fidelity, spatial-textural, colour, and temporal maskings. The proposed video quality metric has been tested on CIF and QCIF video sequences compressed at various bit rates and frame rates. It has been shown to give significantly better correlation to human perception than PSNR.", "We present a simple yet effective progressive self-guided loss function to facilitate deep learning-based salient object detection (SOD) in images. The saliency maps produced by the most relevant works still suffer from incomplete predictions due to the internal complexity of salient objects. Our proposed progressive self-guided loss simulates a morphological closing operation on the model predictions for progressively creating auxiliary training supervisions to step-wisely guide the training process. We demonstrate that this new loss function can guide the SOD model to highlight more complete salient objects step-by-step and meanwhile help to uncover the spatial dependencies of the salient object pixels in a region growing manner. Moreover, a new feature aggregation module is proposed to capture multi-scale features and aggregate them adaptively by a branch-wise attention mechanism. Benefiting from this\u00a0\u2026", "Image denoising can remove natural noise that widely exists in images captured by multimedia devices due to low-quality imaging sensors, unstable image transmission processes, or low light conditions. Recent works also find that image denoising benefits the high-level vision tasks,  e.g ., image classification. In this work, we try to challenge this common sense and explore a totally new problem,  i.e ., whether the image denoising can be given the capability of fooling the state-of-the-art deep neural networks (DNNs) while enhancing the image quality. To this end, we initiate the very first attempt to study this problem from the perspective of adversarial attack and propose the  adversarial denoise attack . More specifically, our main contributions are three-fold:  First , we identify a new task that stealthily embeds attacks inside the image denoising module widely deployed in multimedia devices as an image post\u00a0\u2026", "Modeling subjective opinions on visual quality is a challenging problem, which closely relates to many factors of the human perception. In this paper, the additive log-logistic model (ALM) is proposed to formulate such a multidimensional nonlinear problem. The log-logistic model has flexible monotonic or nonmonotonic partial derivatives and thus is suitable to model various uni-type impairments. The proposed ALM metric adds the distortions due to each type of impairment in a log-logistic transformed space of subjective opinions. The features can be evaluated and selected by classic statistical inference, and the model parameters can be easily estimated. Cross validations on five Telecommunication Standardization Sector of International Telecommunication Union (ITU-T) subjectively-rated databases confirm that: 1) based on the same features, the ALM outper-forms the support vector regression and the logistic\u00a0\u2026", "In video-based dynamic point cloud compression (V-PCC), 3D point clouds are projected into patches, and then the patches are padded into 2D images suitable for the video compression framework. However, the patch projection-based method produces a large number of empty pixels; the far and near components are projected to generate different 2D images (video frames), respectively. As a result, the generated video is with high resolutions and double frame rates, so the V-PCC has huge computational complexity. This paper proposes an occupancy map guided fast V-PCC method. Firstly, the relationship between the prediction coding and block complexity is studied based on a local linear image gradient model. Secondly, according to the V-PCC strategies of patch projection and block generation, we investigate the differences of rate-distortion characteristics between different types of blocks, and the\u00a0\u2026", "Image restoration has attracted substantial attention recently and many image restoration algorithms have been proposed for restoring latent clear images from degraded images. However, determining how to objectively evaluate the performances of these algorithms remains an open problem, which may hinder the further development of advanced image restoration techniques. Most image restoration-quality metrics are designed for specific restoration applications; hence, their generalization ability is limited. For benchmarking image restoration algorithms, the ranking of restored images that are generated via various algorithms, is the most heavily considered factor. Inspired by this, this paper presents a pairwise-comparison-based rank learning framework for benchmarking the performances of image restoration algorithms, which focuses on the relative quality ranking of restored images. Under the proposed\u00a0\u2026", "Compression of screen content has recently emerged as an active research topic due to the increasing demand in many applications such as wireless display and virtual desktop infrastructure. Screen content images (SCIs) exhibit different statistical properties in textual and pictorial regions, and the human visual system (HVS) also behaves differently when viewing the textual and pictorial regions in terms of the extent of visual field. Here we propose a perceptual SCI quality assessment approach that incorporates visual field adaptation and information content weighting. Furthermore, we propose a perceptual coding scheme in an attempt to optimize the HEVC Screen Content Coding encoder. Experimental results show that the proposed quality assessment method not only better predicts the perceptual quality of SCIs, but also leads to an effective way to optimize screen content coding schemes.", "In 3-D video, view synthesis with depth-image-based rendering is employed to generate any virtual view between available camera views. Distortions in depth map induce geometry changes in the virtual views, and thus degrade the performance of view synthesis. This paper proposes a depth map coding method to improve the performance of view synthesis based on distortion analyses. The major technical innovation of this paper is to formulate maximum tolerable depth distortion (MTDD) and depth disocclusion mask (DDM), since such depth sensitivity for view synthesis and inter-view redundancy can be well utilized in coding. To be more specific, we define two different encoders (e.g., base encoder and side encoder) for depth maps in left and right views, respectively. For base encoding, different types of coding units are extracted based on the distribution of MTDD and assigned with different quantitative\u00a0\u2026", "Image quality assessment is useful in many visual processing systems and a great deal of research effort has been put in during the recent years to develop objective image quality metrics that correlate well with the perceived quality measurement. Assessing visual quality of images is not easy since the Human Visual System (HVS) is complicated and difficult to be modelled. It is well known that the HVS is sensitive to spatial frequencies and structure in images, so accounting for structure degradation in images is essential for effective picture quality prediction. In this paper, we propose the use of singular vectors out of Singular Value Decomposition as effective structuring elements in images and use them to quantify the loss in structural information in images. The scalability of the proposed metric has been also explored since singular vectors are ordered according to their visual significance. The proposed metric\u00a0\u2026", null, "In this paper, we propose a new saliency based stereoscopic image retargeting method based on the characteristics of the Human Visual System (HVS). A new stereoscopic saliency detection method is designed by adopting low-level features of intensity, color, texture and depth. Besides, the viewing bias factors including center bias and depth bias existing in the HVS are adopted for stereoscopic visual attention modeling. Since the HVS is sensitive to edges in images, we fuse the saliency and edge maps to predict the visual significance of image pixels for image resizing. With the visual significance measure, we propose an image resizing method by minimizing the structure and depth distortion for stereoscopic image retargeting. Experimental results have shown that both our stereoscopic saliency detection and image retargeting methods can obtain better performance than the existing related methods on the\u00a0\u2026", "Perception of multimedia quality is shaped by a rich interplay between system, context and human factors. While system and context factors are widely researched, few studies consider human factors as sources of systematic variance. This paper presents an analysis on the influence of personality and cultural traits on the perception of multimedia quality. A set of 144 video sequences (from 12 short movie excerpts) were rated by 114 participants from a cross-cultural population, producing 1232 ratings. On this data, three models are compared: a baseline model that only considers system factors; an extended model that includes personality and culture as human factors; and an optimistic model in which each participant is modelled as a random effect. An analysis shows that personality and cultural traits represent 9.3\\% of the variance attributable to human factors while human factors overall predict an equal or\u00a0\u2026", "Affect is evoked through an intricate relationship between the characteristics of stimuli, individuals, and systems of perception. While affect is widely researched, few studies consider the combination of multimedia system characteristics and human factors together. As such, this paper explores the influence of personality (Five-Factor Model) and cultural traits (Hofstede Model) on the intensity of multimedia-evoked positive and negative affects (emotions). A set of 144 video sequences (from 12 short movie clips) were evaluated by 114 participants from a cross-cultural population, producing 1232 ratings. On this data, three multilevel regression models are compared: a baseline model that only considers system factors; an extended model that includes personality and culture; and an optimistic model in which each participant is modelled. An analysis shows that personal and cultural traits represent 5.6% of the variance\u00a0\u2026", "Images are often coded using block-based discrete cosine transform (DCT), where blocking and ringing artifacts are the most common visual distortion. In this letter, a fast algorithm is proposed to alleviate the said artifacts in the DCT domain. The new concept is to decompose a row or column image vector to a gradually changed signal and a fast variational signal, which correspond to low-frequency (LF) and high-frequency (HF) DCT subbands, respectively. Blocking artifacts between adjacent LF blocks are suppressed by smoothing LF components and discarding invalid HF ones, and ringing artifacts inside HF vectors are reduced by a simplified bilateral filter. With such a process, edges are preserved while blockiness and ringing are alleviated. Analytic and experimental results confirm the robustness and computational efficiency of the proposed method", "This paper proposed an objective colour perceptual video quality metric to automatically assess the perceived quality of colour digital videos. The rationale in proposing perceptual-based metric is because peak signal-to-noise ratio (PSNR), which has been used traditionally to represent the quality of a compressed video sequence, has been found to correlate poorly with subjective quality ratings, particularly at much lower bit rates and frame rates. In this paper, computational models have been applied to emulate human visual perception based on a combination of local and global modulating factors. The proposed video quality metric has been tested on CIF and QCIF colour video sequences compressed at various bit rates and frame rates. It has been shown to give significantly better correlations to human perception than PSNR.", "Binocular combination under different distortion types poses a great challenge to three-dimensional image quality assessment (3D-IQA). However, the research works on 3D-IQA with multiple distortion types are very limited. In this paper, we first construct a new multiply distorted stereoscopic image database (NBU-MDSID), which is composed of 270 multiply distorted stereoscopic images and 90 singly distorted stereoscopic images that are corrupted simultaneously and independently by blurring, JPEG compression, and noise injection. We then propose a new multimodal blind metric for quality assessment of multiply distorted stereoscopic images. Inspired by multimodal sparse representation framework, modality-specific dictionaries and the corresponding projection matrices are learned from the singly distorted training database at the training stage, and the testing stage only needs to estimate the quality score\u00a0\u2026", "Image retargeting is to re-represent the image, usually with a different image size to cater for the display size of the end-user (e.g., computer, smartphone). In this paper, we propose an image retargeting quality assessment scheme based on four quality factors and support vector regression. The accounted quality factors are of two categorizes: 1) shape distortions, they are visual unpleasant artifacts introduced by retargeting, e.g., a straight line may be retargeted as a curve, a circle may be retargeted as an ellipse, etc.; 2) visual content changes, they mainly refer to the loss of visual information due to retargeting. Both spatial and frequency information are used toward a complete assessment. The spatial quality factors measure shape distortions and visual content changes separately and independently, and therefore it is possible to control their relative importance to the overall assessment. Since spatial quality\u00a0\u2026", "In this paper, a novel multiple description video coding scheme is proposed based on the characteristics of the human visual system (HVS). Due to the underlying spatial-temporal masking properties, human eyes cannot sense any changes below the just noticeable difference (JND) threshold. Therefore, at an encoder, only the visual information that cannot be predicted well within the JND tolerance needs to be encoded as redundant information, which leads to more effective redundancy allocation according to the HVS characteristics. Compared with the relevant existing schemes, the experimental results exhibit better performance of the proposed scheme at same bit rates, in terms of perceptual evaluation and subjective viewing.", "This paper presents an overview of state-of-the-art technologies for perceptual processing of digital pictures, as well as a discussion of the issues related to their implementation, optimization and testing. The paper begins with a brief description of the main computational modules that are used as part of a perceptual-based visual signal processing framework. Then, a number of perceptual-based visual processing techniques and applications to which perceptual models are presented, including image/video compression, visual signal quality evaluation, and computer graphics. The most significant research efforts are highlighted for each topic, and a number of issues and views are put forward regarding the related research and opportunities.", "In this paper, we propose a low-complexity video coding scheme based upon 2-D singular value decomposition (2-D SVD), which exploits basic temporal correlation in visual signals without resorting to motion estimation (ME). By exploring the energy compaction property of 2-D SVD coefficient matrices, high coding efficiency is achieved. The proposed scheme is for the better compromise of computational complexity and temporal redundancy reduction, i.e., compared with the existing video coding methods. In addition, the problems caused by frame decoding dependence in hybrid video coding, such as unavailability of random access, are avoided. The comparison of the proposed 2-D SVD coding scheme with the existing relevant non-ME-based low-complexity codecs shows its advantages and potential in applications.", "This paper proposed an objective video quality metric designed for automatically assessing the perceived quality of digitally compressed multimedia videos using H.264 video compression. The rationale in proposing perceptual-based metric is because traditional measure, peak signal-to-noise ratio (PSNR), has been found to correlate poorly with subjective quality ratings, particularly at much lower bit rates. In this paper, computational models have been applied to emulate human visual perception based on a combination of local and global modulating factors. The proposed video quality metric has been tested on GIF and QCIF video sequences compressed using H.264 video compression technique at various bit rates (24-384 Kbps) and frame rates (7.5-30 Hz). Performance of the proposed metric with respect to subjective scores will be reported and a comparison with PSNR and also the video structural\u00a0\u2026", "A method based upon subjective viewing tests to evaluate the perceptual impact of different extents of edge sharpness is presented and the most eye-pleasing sharpness (MEPS) for an image edge-sharpening process is derived. The findings with Laplacian of Gaussian edge-enhancement filter show that the baseline MEPS is about 2.6 times that of the local just-noticeable difference, and the actual MEPS is also dependent on the contrast increase in the surrounding areas. The proposed methodology can be used to determine the MEPS for a particular edge-enhancement process, and the resultant formulation for the perceptual impact of edge sharpness can be used for reference in control of edge enhancement, image reconstruction and de-blurring processing, as well as objective visual quality gauge.", "Marker-based image segmentation has been widely used in image analysis and understanding. The well-known Meyer's marker-based watershed algorithm by immersion is realized using the hierarchical circular queues. A new marker-based segmentation algorithm relying on disjoint set union is proposed in this paper. It consists of three steps, namely: pixel sorting, set union, and pixel resolving. The memory requirement for the proposed algorithm is fixed as 2\u00d7N integers (N is the image size), whereas the memory requirement for Meyer's algorithm is image dependent. The advantage of the proposed algorithm lies at its regularity and simplicity in software/firmware/hardware implementation.", null, null, "The performance of person re-identification (re-ID) is easily affected by illumination variations caused by different shooting times, places and cameras. Existing illumination-adaptive methods usually require annotating cross-camera pedestrians on each illumination scale, which is unaffordable for a long-term person retrieval system. The cross-illumination person retrieval problem presents a great challenge for accurate person matching. In this paper, we propose a novel method to tackle this task, which only needs to annotate pedestrians on one illumination scale. Specifically, (i) we propose a novel Illumination Estimation and Restoring framework (IER) to estimate the illumination scale of testing images taken at different illumination conditions and restore them to the illumination scale of training images, such that the disparities between training images with uniform illumination and testing images with varying\u00a0\u2026", "Sonar imagery plays a significant role in oceanic applications since there is little natural light underwater, and light is irrelevant to sonar imaging. Sonar images are very likely to be affected by various distortions during the process of transmission via the underwater acoustic channel for further analysis. At the receiving end, the reference image is unavailable due to the complex and changing underwater environment and our unfamiliarity with it. To the best of our knowledge, one of the important usages of sonar images is target recognition on the basis of contour information. The contour degradation degree for a sonar image is relevant to the distortions contained in it. To this end, we developed a new no-reference contour degradation measurement for perceiving the quality of sonar images. The sparsities of a series of transform coefficient matrices, which are descriptive of contour information, are first extracted as\u00a0\u2026", "This paper proposes a no-reference (NR)/referenceless quality evaluation method for stereoscopic three-dimensional (S3D) images based on deep nonnegativity constrained sparse autoencoder (DNCSAE). To address the quality issue of stereopairs whose perceived quality is not only determined by the individual left and right image qualities but also their interactions, a three-column DNCSAE framework is customized with individual DNCSAE module coping with the left image, the right image, and the cyclopean image, respectively. In the proposed framework, each individual DNCSAE module shares the same network architecture consisting of multiple stacked NCSAE layers and one Softmax regression layer at the end. The contribution of our model is that hierarchical feature evolution and nonlinear feature mapping are jointly optimized in a unified and perceptual-aware deep network (DNCSAE), which well\u00a0\u2026", "A new edge adaptive color demosaicing method for Bayer pattern images of single-sensor digital cameras is presented in this paper. An edge direction detector for narrow edges is proposed by making full use of inter-channel correlation to determine edge directions within the smallest detection radius. A combinative criterion is then formulated to cater for the diversity of edge occurrence in real-world scenes. The improvement of the proposed demosaicing scheme is achieved by an effective detection on edge directions, emphasizing on green channel restoration and the overall refinement. Experimental results show that the new scheme preserves better edge details, reduces color aliasing artifacts, and achieves both higher signal fidelity and higher visual image quality as compared with some existing schemes.", "Block-based transform coding is the most popular approach for image and video compression. The objective measurement of blocking artifacts plays an important role in the design, optimization. and assessment of image and video coding systems. This paper presents a new algorithm for measuring blocking artifacts in images and videos. Instead of using the traditional pixel discontinuity along the block boundary, we use the edge directional information of the images. The new algorithm does not need the exact location of the block boundary thus is invariant to the displacement, rotation and scaling of the images. Experiments on various still images and videos show that the new blockiness measure is very efficient in terms of computational complexity and memory usage, and can produce blocking artifact measurement consistent with subjective rating", "Diabetic retinopathy (DR) is a leading cause of vision loss in the world and numerous cutting-edge works have built powerful deep neural networks (DNNs) to automatically classify the DR cases via the retinal fundus images (RFIs). However, RFIs are usually affected by the widely existing camera exposure while the robustness of DNNs to the exposure is rarely explored. In this paper, we study this problem from the viewpoint of adversarial attack and identify a totally new task, i.e., adversarial exposure attack generating adversarial images by tuning image exposure to mislead the DNNs with significantly high transferability. To this end, we first implement a straightforward method, i.e., multiplicative-perturbation-based exposure attack, and reveal the big challenges of this new task. Then, to make the adversarial image naturalness, we propose the adversarial bracketed exposure fusion that regards the exposure attack as an element-wise bracketed exposure fusion problem in the Laplacian-pyramid space. Moreover, to realize high transferability, we further propose the convolutional bracketed exposure fusion where the element-wise multiplicative operation is extended to the convolution. We validate our method on the real public DR dataset with the advanced DNNs, e.g., ResNet50, MobileNet, and EfficientNet, showing our method can achieve high image quality and success rate of the transfer attack. Our method reveals the potential threats to the DNN-based DR automated diagnosis and can definitely benefit the development of exposure-robust automated DR diagnosis method in the future.", "Image-based localization (IBL) aims to estimate the 6DOF camera pose for a given query image. The camera pose can be computed from 2D-3D matches between a query image and Structure-from-Motion (SfM) models. Despite recent advances in IBL, it remains difficult to simultaneously resolve the memory consumption and match ambiguity problems of large SfM models. In this work, we propose a cascaded parallel filtering method that leverages the feature, visibility and geometry information to filter wrong matches under binary feature representation. The core idea is that we divide the challenging filtering task into two parallel tasks before deriving an auxiliary camera pose for final filtering. One task focuses on preserving potentially correct matches, while another focuses on obtaining high quality matches to facilitate subsequent more powerful filtering. Moreover, our proposed method improves the localization accuracy by introducing a quality-aware spatial reconfiguration method and a principal focal length enhanced pose estimation method. Experimental results on real-world datasets demonstrate that our method achieves very competitive localization performances in a memory-efficient manner.", "The next generation of multimedia services have to be optimized in a personalized way, taking user factors into account for the evaluation of individual experience. Previous works have investigated the influence of user factors mostly in a controlled laboratory environment which often includes a limited number of users and fails to reflect real-life environment. Social media, especially Facebook, provide an interesting alternative for Internet-based subjective evaluation. In this article, we develop (and open-source) a Facebook application, named YouQ1, as an experimental platform for studying individual experience for videos. Our results show that subjective experiments based on YouQ can produce reliable results as compared to a controlled laboratory experiment. Additionally, YouQ has the ability to collect user information automatically from Facebook, which can be used for modeling individual experience.", "Visual comfort and depth sensation are two important incongruent counterparts in determining the overall stereoscopic 3-D experience. In this paper, we proposed a novel simultaneous visual comfort and depth sensation optimization approach for stereoscopic images. The main motivation of the proposed optimization approach is to enhance the overall stereoscopic 3-D experience. Toward this end, we propose a two-stage solution to address the optimization problem. In the first layer-independent disparity adjustment process, we iteratively adjust the disparity range of each depth layer to satisfy with visual comfort and depth sensation constraints simultaneously. In the following layer-dependent disparity process, disparity adjustment is implemented based on a defined total energy function built with intra-layer data, inter-layer data and just noticeable depth difference terms. Experimental results on perceptually\u00a0\u2026", "Images are subject to blocking artifacts when they are compressed using the JPEG standard. Knowing the extent of blocking artifacts is thus necessary for such applications as automatic quality monitoring and restoration. The current blocking artifacts measures are based on a strong prior that the block boundaries are known in advance, which often does not hold in real-world applications. Therefore, their performances degrade significantly when block boundaries are misaligned. To address the problem, this paper presents a robust no-reference blocking artifacts evaluation metric for JPEG images based on grid strength and regularity (GridSAR). The underlying idea is to extract block grids from a JPEG image and quantify the grid image to evaluate the strength of blocking artifacts. To this end, a grid map of blocking artifacts is first extracted from the image in the spatial domain. Then the blocking artifacts of the\u00a0\u2026", "Videos captured by stationary cameras are usually with a static or gradually changed background. Existing schemes are not able to globally exploit the strong background temporal redundancy. In this paper, motivated by the recent advance on low-rank and sparse decomposition (LRSD), we propose to apply it for the compression of videos captured by fixed cameras. In particular, the LRSD is employed to decompose the input video into the low-rank component, representing the background, and the sparse component, representing the moving objects, which are encoded by different methods. Moreover, we further propose an incremental LRSD (ILRSD) algorithm to reduce the large memory requirement and high computational complexity of the existing LRSD algorithm, which facilitates the process of large-scale video sequences without much performance loss. Experimental results show that the proposed coding\u00a0\u2026", "Peak signal-to-noise ratio (PSNR) is not a good measure of perceived picture quality, especially at low bit rates of coding. This paper proposes a new approach for computing perceptual distortion for visual signal in order to provide an objective measure for perceptual quality at low bit rate coding in typically mobile communications. The regions with three major perceptually disturbing artifacts, namely, damaged edge, blockiness and ringing, are detected as the basis of assessment. The correlation of the metric with human perception has been demonstrated with low bit rate CIF test data.", "Mesh reconstruction from a 3D point cloud is an important topic in the fields of computer graphic, computer vision, and multimedia analysis. In this paper, we propose a voxel structure-based mesh reconstruction framework. It provides the intrinsic metric to improve the accuracy of local region detection. Based on the detected local regions, an initial reconstructed mesh can be obtained. With the mesh optimization in our framework, the initial reconstructed mesh is optimized into an isotropic one with the important geometric features such as external and internal edges. The experimental results indicate that our framework shows great advantages over peer ones in terms of mesh quality, geometric feature keeping, and processing speed. The source code of the proposed method is publicly available 1 .", "In this work, we resolve a big challenge that most current image quality metrics (IQMs) are unavailable across different image contents, especially simultaneously coping with natural scene (NS) images or screen content (SC) images. By comparison with existing works, this paper deploys on-line and off-line data for proposing a unified no-reference (NR) IQM, not only applied to different distortion types and intensities but also to various image contents including classical NS images and prevailing SC images. Our proposed NR IQM is developed with two data-driven learning processes following feature extraction, which is based on scene statistic models, free-energy brain principle, and human visual system (HVS) characteristics. In the first process, the scene statistic models and an image retrieve technique are combined, based on on-line and off-line training instances, to derive a novel loose classifier for retrieving\u00a0\u2026", "Though blind image quality assessment (BIQA) is highly desired in perceptual-oriented image processing systems, it is extremely difficult to design a reliable BIQA method. With the help of the prior knowledge, the human visual system (HVS) hierarchically perceives the quality degradation during the visual recognition. Inspired by this, we suggest different levels of distortion generate individual degradations on hierarchical features, and propose to consider the degradations on both low and high level features for quality prediction. By mimicking the orientation selectivity (OS) mechanism in the primary visual cortex, an OS based local structure is designed for low-level visual information representation. At the meantime, the deep residual network, which possesses multiple levels for feature integration, is employed to extract the deep semantics for high-level visual content representation. By fusing the local structure\u00a0\u2026", "In this paper, we propose a novel image retargeting algorithm to resize images based on the extracted saliency information from the compressed domain. Firstly, we utilize DCT coefficients in JPEG bitstream to perform saliency detection with the consideration of the human visual sensitivity. The obtained saliency information is used to determine the relative visual importance of each 8 x 8 block for the image. Furthermore, we propose a new adaptive block-level seam removal operation for connected blocks to resize the image. Thanks to the directly derived saliency information from the compressed domain, the proposed image retargeting algorithm effectively preserves the objects of attention, efficiently removes the less crucial regions, and therefore significantly outperforms the relevant state-of-the-art algorithms, as demonstrated with the careful analysis and in the extensive experiments.", "An image quality measurement system (10) determines various features of an image that relate to the quality of the image in terms of its appearance. The features include the image's blockiness invisibility (B), the image's colour richness (R) and the image's sharpness (S). These are all obtained without the use of a reference image. The determined features are combined to provide an image quality measure (Q).", "Traditionally, peak signal-to-noise ratio (PSNR) has been used to represent the quality of a compressed video sequence. However, PSNR has been found to correlate poorly with subjective quality ratings, particularly at much lower bit rates and frame rates. This paper proposes an objective video quality metric to automatically measure the perceived quality of a stream of video images based on a combined measure of distortion-invisibility, block-fidelity, and content richness fidelity. The proposed method has been tested on CIF and QCIF video sequences compressed at low bit rates and frame rates and it is shown to give significantly better correlations to human perception than peak signal-to-noise ratio (PSNR).", "Recently, there are lots of visual tracking algorithms proposed to improve the performance of object tracking in video sequences with various real conditions, such as severe occlusion, complicated background, fast motion, and so on. In real visual tracking systems, there are various quality degradation occurring during video acquisition, transmission, and processing. However, most existing studies focus on improving the accuracy of visual tracking while ignoring the performance of tracking algorithms on video sequences with certain quality degradation. In this paper, we investigate the performance evaluation of existing visual tracking algorithms on video sequences with quality degradation. A quality-degraded video database for visual tracking (QDVD-VT), including the reference video sequences and their corresponding distorted versions, is constructed as the benchmarking for robustness analysis of visual\u00a0\u2026", "This paper presents Sparse Depth Odometry (SDO) to incrementally estimate the 3D pose of a depth camera in indoor environments. SDO relies on 3D keypoints extracted on dense depth data and hence can be used to augment the RGB-D camera based visual odometry methods that fail in places where there is no proper illumination. In SDO, our main contribution is the design of the keypoint detection module, which plays a vital role as it condenses the input point cloud to a few keypoints. SDO differs from existing depth alone methods as it does not use the popular signed distance function and can run online, even without a GPU. A new keypoint detection module is proposed via keypoint selection, and is based on extensive theoretical and experimental evaluation. The proposed keypoint detection module comprises of two existing keypoint detectors, namely SURE [1] and NARF [2]. It offers reliable keypoints that\u00a0\u2026", "Although wireless sensor networks (WSNs) are powerful in monitoring physical events, the data collected from a WSN are almost always incomplete if the surveyed physical event spreads over a wide area. The reason for this incompleteness is twofold: i) insufficient network coverage and ii) data aggregation for energy saving. Whereas the existing recovery schemes only tackle the second aspect, we develop Dual-lEvel Compressed Aggregation (DECA) as a novel framework to address both aspects. Specifically, DECA allows a high fidelity recovery of a widespread event, under the situations that the WSN only sparsely covers the event area and that an in-network data aggregation is applied for traffic reduction. Exploiting both the low-rank nature of real-world events and the redundancy in sensory data, DECA combines matrix completion with a fine-tuned compressed sensing technique to conduct a dual-level\u00a0\u2026", "We study the problem of image aesthetic assessment (IAA) and aim to automatically predict the image aesthetic quality in the form of discrete distribution, which is particularly important in IAA due to its nature of having possibly higher diversification of agreement for aesthetics. Previous works show the effectiveness of utilizing object-agnostic attention mechanisms to selectively concentrate on more contributive regions for IAA, e.g., attention is learned to weight pixels of input images when inferring aesthetic values. However, as suggested by some neuropsychology studies, the basic units of human attention are visual objects, i.e., the trace of human attention follows a series of objects. This inspires us to predict contributions of different regions at object level for better aesthetics evaluation. With our framework, region-of-interests (RoIs) are proposed by an object detector, and each RoI is associated with a regional\u00a0\u2026", "In sonar applications, important information such as distributions of minerals, underwater creatures has a high probability of being contained in sonar images. In many underwater applications such as underwater rescue and biometric tracking, it is necessary to send sonar images underwater for further analysis. Due to the bad conditions of underwater acoustic channel and current underwater acoustic communication technologies, sonar images very possibly suffer from several typical types of distortions. As far as we know, limited efforts have been made to gather meaningful sonar image databases and benchmark reliable objective quality model, so far. This paper develops a new objective sonar image quality predictor (SIQP), whose core is the combination of two features specific to a quality measure of sonar images. These two features, which come from statistical and structural information inspired by the\u00a0\u2026", "Positive samples of facial actions are much fewer than negative samples in natural conditions. The highly imbalanced class-distributions may cause very slow rate of convergence of error when using neural networks for facial action recognition. Traditional methods tackle this class-imbalance problem by changing data distributions, which is challenging for preventing the loss of useful information. In this paper we tackle this problem by using very deep (>10 layers) architectures to increase the chance that network training has acceptable rate of convergence using highly imbalanced data sets. Experimental results on EmotioNet Challenge data set show that the error rates of very deep covolutional networks converge to 40% after 90 epochs while shallower networks only converge to 60%. The results also show that very deep network outperforms shallower network by 0.2 on accuracy score. The proposed neural\u00a0\u2026", "Image set compression has recently emerged as an active research topic due to the rapidly increasing demand in cloud storage. In this paper, we propose a novel framework for image set compression based on the rate-distortion optimized sparse coding. Specifically, given a set of similar images, one representative image is first identified according to the similarity among these images, and a dictionary can be learned subsequently in wavelet domain from the training samples collected from the representative image. In order to improve coding efficiency, the dictionary atoms are reordered according to their use frequencies when representing the representative image. As such, the remaining images can be efficiently compressed with sparse coding based on the reordered dictionary that is highly adaptive to the content of the image set. To further improve the efficiency of sparse coding, the number of dictionary\u00a0\u2026", "Most of the existing image reconstruction algorithms are application specific, and have generalization issues due to the need for parameter tuning and an unknown level of signal distortion. Addressing these problems, in this paper, we propose an efficient perceptually motivated and maximum a posterior (MAP)-based generic framework for image reconstruction. This can be applied to several image/video processing applications, where there is a necessity to improve reconstruction accuracy and suppress visible artifacts, such as denoising, deinterlacing, interpolation, de-blocking of Jpeg/Jpeg-2000, and demosaicing. The gradient magnitudes are noise insensitive to a moderate levels of noise and we propose to utilize this property for finding pixels with similar edge semantics in the neighborhood when neighboring pixels are noisy. With this view, we incorporate the gradient magnitude similarity based image quality\u00a0\u2026", "Stereoscopic image retargeting plays an important role in adaptive 3D stereoscopic displays. It aims to fit displays with various resolutions while preserving visually salient content and geometric consistency. We propose a stereoscopic image retargeting method based on stereoscopic visual attention guided seam carving. Firstly, stereoscopic saliency map is generated by combining 2D saliency and depth saliency maps, and significant energy map is generated by considering binocular disparity binocular and binocular just-noticeable-difference (BJND). Then, seam selection is applied to the left image based on stereoscopic saliency and energy maps, and seam replacement is performed for the occluded regions to prevent the geometry inconsistency. Finally, according to the matched left and right seams, the retargeted stereoscopic image is generated. In the experiments, subjective and objective analysis on three\u00a0\u2026", "The piezoelectric response from \u03b2-phase poly(vinylidene fluoride) (PVDF) can potentially be exploited for biomedical application. We hypothesized that \u03b1 and \u03b2-phase PVDF exert direct but different influence on cellular behavior. \u03b1- and \u03b2-phase PVDF films were synthesized through solution casting and characterized with FT-IR, XRD, AFM and PFM to ensure successful fabrication of \u03b1 and \u03b2-phase PVDF films. Cellular evaluation with L929 mouse fibroblasts over one-week was conducted with AlamarBlue\u00ae metabolic assay and PicoGreen\u00ae proliferation assay. Immunostaining of fibronectin investigated the extent and distribution of extracellular matrix deposition. Image saliency analysis quantified differences in cellular distribution on the PVDF films. Our results showed that \u03b2-phase PVDF films with the largest area expressing piezoelectric effect elicited highest cell metabolic activity at day 3 of culture. Increased\u00a0\u2026", "This paper presents the result of a recent large-scale subjective study of image retargeting quality on a collection of images generated by several representative image retargeting methods. Owning to many approaches to image retargeting that are developed, there is a need for a diverse independent public database of the retargeted images and the corresponding subjective scores that is freely available. We build an image retargeting quality database, in which 171 retargeted images (obtained from 57 natural source images of different contents) were generated by several representative image retargeting methods. The perceptual quality of each image is evaluated by at least 30 human subjects and the mean opinion scores (MOS) were recorded. Furthermore, several publicly available quality metrics for the retargeted images are evaluated on the built database. The database is made available [1] to the research\u00a0\u2026", "Phase map of the images captures the most fundamental cognitive features and thus is widely used in various digital image processing tasks. In this paper, we propose the Log Gabor Phase Similarity (LGPS), a novel full reference image quality assessment metrics based on measuring of similarities between phases in log Gabor transform domain. Phase can capture any changes in image details regardless of the fluctuation in contrast, and the similarity between phase maps provides a measure of the perceptual quality of images. An image is firstly decomposed by a filter bank consisting of a pair of log Gabor filters. The phase maps are then computed from the responses of each filter pair. We have developed a window-based similarity metric to evaluate the resemblance between phase maps so as to measure the quality of the image. Experimental results and comparative studies suggest that LGPS can be used to\u00a0\u2026", "In a 3-D video system, automatically predicting the quality of synthesized 3-D video based on the inputs of color and depth videos is an urgent but very difficult task, while the existing full-reference methods usually measure the perceptual quality of the synthesized video. In this paper, a high-efficiency view synthesis quality prediction (HEVSQP) metric for view synthesis is proposed. Based on the derived VSQP model that quantifies the influences of color and depth distortions and their interactions in determining the perceptual quality of 3-D synthesized video, color-involved VSQP and depth-involved VSQP indices are predicted, respectively, and are combined to yield an HEVSQP index. Experimental results on our constructed NBU-3D Synthesized Video Quality Database demonstrate that the proposed HEVSOP has good performance evaluated on the entire synthesized video-quality database, compared with\u00a0\u2026", "We have presented a no-reference quality prediction method for asymmetrically distorted stereoscopic images, which aims to transfer the information from source feature domain to its target quality domain using a label consistent K-singular value decomposition classification framework. To this end, we construct a category-deviation database for dictionary learning that assigns a label for each stereoscopic image to indicate if it is noticeable or unnoticeable by human eyes. Then, by incorporating a category consistent term into the objective function, we learn view-specific feature and quality dictionaries to establish a semantic framework between the source feature domain and the target quality domain. The quality pooling is comparatively simple and only needs to estimate the quality score based on the classification probability. The experimental results demonstrate the effectiveness of our blind metric.", "Image quality assessment (IQA) is a fundamental problem in image processing. While in practice almost all images are represented in the color format, most of the current IQA metrics are designed in gray-scale domain. Color influences the perception of image quality, especially in the case where images are subject to color distortions. With this consideration, this paper presents a novel color image quality index based on Sparse Representation and Reconstruction Residual (SRRR). An overcomplete color dictionary is first trained using natural color images. Then both reference and distorted images are represented using the color dictionary, based on which two feature maps are constructed to measure structure and color distortions in a holistic manner. With the consideration that the feature maps are insensitive to image contrast change, the reconstruction residuals are computed and used as a complementary\u00a0\u2026", "With rapidly advancing computer and network technologies, various visual signals (including image, video, graphics, animation, etc.) are produced, and visual quality of experience (QoE) plays an important role in multimedia applications and services. Visual QoE evaluation is essential not only on its own for testing, optimizing, and inspecting related algorithms, systems and services, but also for shaping and decision-making for virtually all multimedia signal processing and transmission algorithms. It is not an exaggeration to say that how visual signal quality is evaluated shapes the making of almost all multimedia processing algorithms and systems, since the ultimate goal of processing is to achieve the highest possible perceived quality.During the past two decades, the research field of visual quality assessment has experienced significant growth and great progress. With the rapid development of the sensing and\u00a0\u2026", "In this paper, we propose a new computational model of visual attention based on the relevant characteristics of the Human Visual System (HVS) and sparse features. The input image is first divided into small image patches. Then the sparse features of each patch are extracted based on the learned independent components. The human visual acuity is adopted in calculation of the center-surround differences between image patches for saliency extraction. We choose the neighboring patches for center-surround difference calculation based on the relevant characteristics of the HVS. Furthermore, the center-bias factor is adopted to enhance the saliency map. Experimental results show that the proposed saliency detection model achieves better performance than the relevant existing ones on a large public image database with ground truth.", "H.264/AVC FRExt (Fidelity Range Extensions) and Motion JPEG 2000 are the current respective inter-frame and intra-frame coding standards for high resolution (HR) (e.g., 4096\u00d72160) visual signals. It is commonly believed that an inter-frame method could achieve higher coding efficiency compared with an intra-frame one, due to the exploitation of video temporal redundancy. However, Motion JPEG 2000 has been selected as the digital cinema compression standard, and some existing work has demonstrated that JPEG 2000 is more suitable at HR situations. In this paper, we compare the rate\u2013distortion (R\u2013D) performance of these two different schemes and give more insight from both theoretical and experimental point of view. We derive an entropy-based R\u2013D model to analyze the test results and the impact of residual entropy and quantization for inter-frame coding. Several extensions are introduced into H\u00a0\u2026", "The visual quality assessment approaches and their classification are introduced. Recent developments on both image and video quality metrics, as well as several publicly available databases for both images and videos, are reviewed. Then, we conduct experiments on image and video quality databases to compare the performance of some existing state-ofthe-art visual quality metrics. It is shown that multi-metric fusion (MMF) and motion-based video integrity evaluation (MOVIE) are the best methods for image and video quality assessment, respectively. Finally, future trends in visual quality assessment are discussed.", "This paper presents a new and general concept, PQSM (Perceptual Quality Significance Map), to be used in measuring the visual distortion. It makes use of the selectivity characteristic of HVS (Human Visual System) that it pays more attention to certain area/regions of visual signal due to one or more of the following factors: salient features in image/video, cues from domain knowledge, and association of other media (e.g., speech or audio). PQSM is an array whose elements represent the relative perceptual-quality significance levels for the corresponding area/regions for images or video. Due to its generality, PQSM can be incorporated into any visual distortion metrics: to improve effectiveness or/and efficiency of perceptual metrics; or even to enhance a PSNR-based metric. A three-stage PQSM estimation method is also proposed in this paper, with an implementation of motion, texture, luminance, skin-color and\u00a0\u2026", "Reduced-reference (RR) image quality assessment (IQA) aims to use less reference data for quality evaluation. Global features are demanded to effectively express quality degradation caused by distortion. Since the human visual system (HVS) is highly sensitive to structure degradation, we suggest to represent the visual content of an image with several structural patterns. Also, image quality is measured based on the structural degradation on these patterns. Firstly, the spatial correlation of image structure is analyzed with the local binary pattern (LBP), and several representative structural patterns are extracted. Then, the energy changes on these patterns between the reference image and the distorted image are calculated. Finally, the support vector regression (SVR) procedure is adopted for feature pooling, and the quality score of the input image is returned. Experimental results for three publicly available\u00a0\u2026", null, "Glaucoma is an eye disease where a loss of vision occurs as a result of progressive optic nerve damage usually associates with high intraocular pressure. A subtype of glaucoma called primary angle-closure glaucoma (PACG) has been observed to be the result of one or more mechanisms such as Pupil block, Plateau iris, Peripheral iris roll, and Lens in the anterior segment of the eye. Reliable features in anterior segment images are important for determining the specific mechanisms involved in PACG. In this paper, first the discriminant features are selected by several feature selection algorithms in the context of PACG detection based on anterior segment optical coherence tomography (AS-OCT) images, and then a novel criteria is proposed to further select more reliable features. Our approach is based on selecting the top-ranked features in each algorithm and its rank combination for selection of the best\u00a0\u2026", "In this paper, we propose a novel stereoscopic image retargeting algorithm based on 3D visual saliency detection. A new 3D visual attention model is designed based on 2D visual feature detection, depth feature detection and the modeling of various viewing bias in stereo vision. A geometrically consistent seam carving technique is adopted for retargeting stereo image pair. Experimental results demonstrated that both the proposed visual attention model and the proposed retargeting method outperform the state-of-the-art studies.", "In this paper, we propose a novel method for malaria parasite detection based on phase spectrum. The method first obtains the amplitude spectrum and phase spectrum for blood smear images through Quaternion Fourier Transform (QFT). Then it gets the reconstructed image based on Inverse Quaternion Fourier transform (IQFT) on a constant amplitude spectrum and the original phase spectrum. The malaria parasite areas can be detected easily from the reconstructed blood smear images. Extensive experiments have demonstrated the effectiveness of this novel method.", "A point cloud as an information-intensive 3D representation usually requires a large amount of transmission, storage and computing resources, which seriously hinder its usage in many emerging fields. In this paper, we propose a novel point cloud simplification method, Approximate Intrinsic Voxel Structure (AIVS), to meet the diverse demands in real-world application scenarios. The method includes point cloud pre-processing (denoising and down-sampling), AIVS-based realization for isotropic simplification and flexible simplification with intrinsic control of point distance. To demonstrate the effectiveness of the proposed AIVS-based method, we conducted extensive experiments by comparing it with several relevant point cloud simplification methods on three public datasets, including Stanford, SHREC, and RGB-D scene models. The experimental results indicate that AIVS has great advantages over peers in\u00a0\u2026", "The technology of image quality assessment\u00a0(IQA) is urgently demanded in perceptual-orientated image processing systems. Due to the lack of guidance from the reference information, it is challenging for the no-reference\u00a0(NR) IQA to perform consistently with the human perception. It is well known that even though there exists no reference information, the human visual system can still effectively predict the image quality under the guidance of the prior knowledge. Thus, we try to build a visual content based prior knowledge database to guide the NR IQA. Cognitive researches state that the primary visual cortex presents substantially orientation selectivity, within which the structural information is extracted for visual perception. Inspired by this, the correlations among pixels in a local region are firstly analyzed for structure extraction. By mimicking the excitatory/inhibitory interactions among neurons, the relationships\u00a0\u2026", "Most of the existing image blurriness assessment algorithms are proposed based on measuring image edge width, gradient, high-frequency energy, or pixel intensity variation. However, these methods are content sensitive with little consideration of image content variations, which causes variant estimations for images with different contents but same blurriness degrees. In this paper, a content-insensitive blind image blurriness assessment metric is developed utilizing Weibull statistics. Inspired by the property that the statistics of image gradient magnitude (GM) follows Weibull distribution, we parameterize the GM using     (scale parameter) and     (shape parameter) of Weibull distribution. We also adopt skewness (   ) to measure the asymmetry of the GM distribution. In order to reduce the influence of image content and achieve more robust performance, divisive normalization is then incorporated to moderate the  \u00a0\u2026", "Displaying images on different devices, requires resizing of the media. Traditional image resizing methods result in quality degradation. Content-aware retargeting algorithms aim to resize images for displaying them on a new device with the goal of preserving important contents of the image. Quality assessment of retargeted images can be employed to choose among outputs of different retargeting methods or help the optimization of such methods. In this paper we propose a learning based quality assessment method for retargeted images. An optical flow algorithm is used to find the correspondence between regions in the scaled and retargeted images. Three groups of features are defined to cover different aspects of distortions that are important to human observers. Area related features are used to detect how the areas of salient regions are retained and how much geometrical deformities are produced in the\u00a0\u2026", "Though blind image quality assessment (BIQA) is highly demanded for many image processing systems, it is extremely difficult for BIQA to accurately predict the quality without the guide of the reference image. In this paper, we introduce a novel BIQA method with hierarchical feature degradation (HFD). Since the human brain presents hierarchical procedure for visual recognition, we suggest that different levels of distortion generate different degradations on hierarchical features, and propose to consider the degradations on both the low and high level features for quality assessment. Inspired by the orientation selectivity (OS) mechanism in the primary visual cortex, an OS based local visual structure is designed for low-level visual content extraction. Meanwhile, according to the feature integration function of deep neural networks, the deep semantics is extracted with the residual network for high-level visual content representation. Next, by analyzing the degradation on both the local structure and the deep semantics, a HFD based memory (prior knowledge) is learned to represent the generalized quality degradation. Finally, with the guidance of the HFD based memory, a novel HFD-BIQA model is built. Experimental results on the publicly available databases demonstrate the quality prediction accuracy of the proposed HFD-BIQA, and verify that the HFD-BIQA performs highly consistent with the subjective perception.", "Just noticeable difference (JND) reveals the visibility of our human visual system (HVS), below which changes cannot be perceived by the human. Though dozens of JND estimation models have been introduced during the past decade, how to accurately estimate the JND thresholds for different content regions (e.g., edge and texture region) is still an open problem. Research on cognitive science indicates that the HVS is adaptive to extract the visual regularities from an input scene for content perception and understanding. Thus, we analyze the effect of content regularity on visual sensitivity, and suggest that the visual regularity is another important factor that determines the JND threshold. According to the orientation distributions of local regions, the content regularities are firstly calculated. Then, by considering the effect from content regularity, luminance adaptation, and contrast masking, a novel JND model is\u00a0\u2026", "Just noticeable difference (JND) of natural images refers to the maximum pixel intensity change magnitude that typical human visual system (HVS) cannot perceive. Existing efforts on JND estimation mainly dedicate to modeling the diverse masking effects in either/both spatial or/and frequency domains, and then fusing them into an overall JND estimate. In this work, we turn to a dramatically different way to address this problem with a top-down design philosophy. Instead of explicitly formulating and fusing different masking effects in a bottom-up way, the proposed JND estimation model dedicates to first predicting a critical perceptual lossless (CPL) counterpart of the original image and then calculating the difference map between the original image and the predicted CPL image as the JND map. We conduct subjective experiments to determine the critical points of 500 images and find that the distribution of\u00a0\u2026", "Just-Noticeable Difference (JND) is the minimal amount of signal change that the human being is able to perceive. The human has five major sensing organs, namely, eyes, ears, nose, skin and tongue, and therefore JND exists for the corresponding five signal modalities and their derivatives. JND can play an important role in many multimedia applications and services, because these imperfect human perceptual characteristics may be turned into advantages for relevant system design, development and optimization. This paper starts off by giving a general description for JND concepts and the related statistical processes. Then, existing computational models for visual JND, which represent the majority of the related research so far, are to be reviewed systematically, with both handcrafted modeling and machine learning approaches. Furthermore, research attempts will be surveyed for JNDs for audio, smell, haptics\u00a0\u2026", "Quality of experience (QoE) that serves as a direct evaluation of viewing experience from the end users is of vital importance for network optimization, and should be constantly monitored. Unlike existing video-on-demand streaming services, real-time interactivity is critical to the mobile live broadcasting experience for both broadcasters and their audiences. While existing QoE metrics that are validated on limited video contents and synthetic stall patterns have shown effectiveness in their trained QoE benchmarks, a common caveat is that they often encounter challenges in practical live broadcasting scenarios, where one needs to accurately understand the activity in the video with fluctuating QoE and figure out what is going to happen to support the real-time feedback to the broadcaster. In this paper, we propose a temporal relational reasoning guided QoE evaluation approach for mobile live video broadcasting\u00a0\u2026", "During recent years, quality-aware features extracted from natural scene statistics (NSS) models have been used in development of blind image quality assessment (BIQA) algorithms. Generally, the univariate distributions of bandpass coefficients are used to fit a parametric probabilistic model and the model parameters serve as the quality-aware features. However, the inter-location, inter-direction and inter-scale correlations of natural images cannot be well exploited by such NSS models, as it is hard to capture such dependencies using univariate marginal distributions. In this paper, we build a novel NSS model of joint log-contrast distribution to take into account the across space and direction correlations of natural images (inter-scale correlation to be explored as the next step). Furthermore, we provide a new efficient approach to extract quality-aware features as the gradient of log-likelihood on the NSS model\u00a0\u2026", "Visual saliency on stereoscopic 3D (S3D) images has been shown to be heavily influenced by image quality. Hence, this dependency is an important factor in image quality prediction, image restoration and discomfort reduction, but it is still very difficult to predict such a nonlinear relation in images. In addition, most algorithms specialized in detecting visual saliency on pristine images may unsurprisingly fail when facing distorted images. In this paper, we investigate a deep learning scheme named Deep Visual Saliency (DeepVS) to achieve a more accurate and reliable saliency predictor even in the presence of distortions. Since visual saliency is influenced by low-level features (contrast, luminance, and depth information) from a psychophysical point of view, we propose seven low-level features derived from S3D image pairs and utilize them in the context of deep learning to detect visual attention adaptively to\u00a0\u2026", "The recent advances of hardware technology have made the intelligent analysis equipped at the front-end with deep learning more prevailing and practical. To better enable the intelligent sensing at the front-end, instead of compressing and transmitting visual signals or the ultimately utilized top-layer deep learning features, we propose to compactly represent and convey the intermediate-layer deep learning features of high generalization capability, to facilitate the collaborating approach between front and cloud ends. This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis. Moreover, the presented strategy also makes the standardization of deep feature coding more feasible and promising, as a series of tasks can simultaneously benefit from the transmitted intermediate layers. We also present the results for evaluation of lossless deep feature compression with four benchmark data compression methods, which provides meaningful investigations and baselines for future research and standardization activities.", "We propose a method to detect keypoint sets on 3D point clouds. Contrary to the existing norm that each and every keypoint should be distinctive, most of the keypoints detected by the proposed method lie in groups (from now, we refer to \u2018groups\u2019 as \u2018sets\u2019) and these groups (sets) of keypoints are distinctive. It is reliable to have sets of keypoints at high curvature and more informative areas rather than having a single keypoint that might sometimes arise due to noise. The proposed algorithm has two well-defined steps for keypoint sets detection. Firstly, Histogram of Normal Orientations (HoNO), which is calculated at every point in the point cloud is employed to avoid planar regions and successfully detect salient regions. Secondly, the keypoint sets are detected from the salient regions by evaluating the properties of both the HoNO and the neighborhood covariance matrix. Through extensive experiments on publicly\u00a0\u2026", "Research in human visual perception has found that the sense of natural scences cannot be conveyed only through lines and edges. It also needs the knowledge of texture regions within the image, which can be obtained through the analysis of higher derivatives. Inspired by the research from neuroscience that high order derivatives can capture the details of image structure, we propose a novel simple yet effective blind image quality assessment (IQA) metric based on high order derivatives (BHOD). In the proposed metric, we extract multi-scale structural features up to fourth order image derivatives, to obtain the image structural features. Support vector regression (SVR) is used to learn the mapping between feature space and subjective opinion scores. The proposed method is extensively evaluated on three image databases and shows highly competitive performance to state-of-the-art NR-IQA methods.", "In this paper we propose a new general histogram modification framework for contrast enhancement. The proposed model works with a hybrid transformation technique to improve image brightness and contrast based on an optional histogram matching in terms of reassigned probability distribution and S-shaped transfer mapping. Experimental results conducted on natural, dimmed, and tone-mapped images show that the proposed technique creates enhanced images efficiently with equivalent or superior visual quality to those produced by classical and state-of-the-art enhancement approaches.", "We propose to learn a non-intrusive quality assessment metric for enhanced speech signals. High-dimension spectro-temporal features are extracted by the Gabor filter bank for speech signals. To reduce the high-dimension features, we use PCA (Principal Component Analysis) to process these features. After obtaining the feature vector from audio signals, Support Vector Regression (SVR) is used to learn the metric for quality evaluation of enhanced speech signals. Experimental results on NOIZEUS dataset demonstrate that proposed non-intrusive quality assessment metric by using spectro-temporal features can obtain better performance for enhanced speech signals.", "The H.263 video codec is an improved version of H.261 and is a constituent part of the H.324 video-conferencing codec suite. A system implementing a real-time ITU-T H.263 codec using a multi-processor DSP system has been configured by the authors. This paper describes the implementation scheme and additionally, some programming techniques to exploit the internal architecture of the multimedia video processor (MVP), and some strategies introduced to reduce the computational complexity whilst minimizing degradation of picture quality. The effects of these strategies have been tested and the codec performance are reported. The video codec is integrated with the G.723.1, H.223 and H.245 to form a full-duplex video-conferencing system.", "Contrast Sensitivity (CS), Luminance Adaptation (LA) and Contrast Masking (CM) are important contributing factors for Just Noticeable Difference (JND) in images. Most of the existing pixel domain JND algorithms are based only on LA and CM. Research shows that the human vision depends significantly on CS, and an underlying assumption in the existing algorithms is that CS cannot be estimated in the pixel domain JND algorithms. However, in the case of natural images, this assumption is not true. Studies on human vision suggest that CS can be estimated using the Root Mean Square (RMS) contrast in the pixel domain. With this in perspective, we propose the first pixel-based JND algorithm that includes a very important component of the human vision, namely CS by measuring RMS contrast. This RMS contrast is combined with LA and CM to form a comprehensive pixel-domain model to efficiently estimate\u00a0\u2026", "To know what kinds of image features are crucial for image quality assessment (IQA) and how these features affect the human visual system (HVS) is still largely beyond human knowledge. Hence, machine learning (ML) is employed to build IQA by simulating the HVS behavior in IQA processes. Support vector machine/regression (SVM/SVR) is a major member of ML. It has been successfully applied to IQA recently. As to image quality rating, the human\u2019s opinion about it is not always reliable. In fact, the subjects cannot precisely rate the small difference of image quality in subjective testing, resulting in unreliable Mean Opinion Scores (MOSs). However, they can easily identify the better/worse one from two given images, even their qualities do not differ much. In this sense, the human\u2019s opinion on pairwise comparison (PC) of image quality is more reliable than image quality rating. Thus, PC has been exploited in\u00a0\u2026", "While quality assessment is essential for testing, optimizing, benchmarking, monitoring, and inspecting related systems and services, it also plays an essential role in the design of virtually all visual signal processing and communication algorithms, as well as various related decision-making processes. In this paper, we first provide an overview of recently derived quality assessment approaches for traditional visual signals (i.e., 2D images/videos), with highlights for new trends (such as machine learning approaches). On the other hand, with the ongoing development of devices and multimedia services, newly emerged visual signals (e.g., mobile/3D videos) are becoming more and more popular. This work focuses on recent progresses of quality metrics, which have been reviewed for the newly emerged forms of visual signals, which include scalable and mobile videos, High Dynamic Range (HDR) images, image\u00a0\u2026", "In general, low and very-low bitrate video communication systems cannot achieve the full frame rate (25 Hz for PAL or 30 Hz for NSTC), which brings temporal distortion. The influence of reduced frame rate on subjective quality evaluation is a very important topic for both perceptual quality metrics and communication system optimization. In this paper, we present our work on the numerical modeling of the influence. The work includes two parts: first we measured the detectability and annoyance of periodic frame dropping's effect on perceptual visual quality evaluation under different content and frame size conditions. Then, a simple and effective feature is proposed to represent the content of video in temporal quality evaluation. The high Pearson and Spearman correlation results between the MOS and proposed model, as well as the results of other two error metrics, confirm the success of the selected temporal\u00a0\u2026", "As a prevailing task in video surveillance and forensics field, person re-identification (re-ID) aims to match person images captured from non-overlapped cameras. In unconstrained scenarios, person images often suffer from the resolution mismatch problem, i.e., \\emph{Cross-Resolution Person Re-ID}. To overcome this problem, most existing methods restore low resolution (LR) images to high resolution (HR) by super-resolution (SR). However, they only focus on the HR feature extraction and ignore the valid information from original LR images. In this work, we explore the influence of resolutions on feature extraction and develop a novel method for cross-resolution person re-ID called \\emph{\\textbf{M}ulti-Resolution \\textbf{R}epresentations \\textbf{J}oint \\textbf{L}earning} (\\textbf{MRJL}). Our method consists of a Resolution Reconstruction Network (RRN) and a Dual Feature Fusion Network (DFFN). The RRN uses an input image to construct a HR version and a LR version with an encoder and two decoders, while the DFFN adopts a dual-branch structure to generate person representations from multi-resolution images. Comprehensive experiments on five benchmarks verify the superiority of the proposed MRJL over the relevent state-of-the-art methods.", "In this paper, we propose a novel reduced-reference quality assessment metric for image super-resolution (RRIQA-SR) based on the low-resolution (LR) image information. With the pixel correspondence, we predict the perceptual similarity between image patches of LR and SR images by two components: the energy change in low-frequency regions, which can be used to capture the global distortion in SR images, and texture variation in high-frequency regions, which can be used to capture the local distortion in SR images. The overall quality of SR images is estimated by perceptual similarity calculated by energy change and texture variation between local image patches of LR and HR images. Experimental results demonstrate that the proposed method can obtain better performance of quality prediction for SR images than other existing ones, even including some full-reference (FR) metrics.", "The concept of just noticeable difference (JND), which accounts for the visibility threshold (visual redundancy) of the human visual system, is useful in perception-oriented signal processing systems. In this work, we present a comprehensive review of JND estimation technology. First, the visual mechanism and its corresponding computational modules are illustrated. These include luminance adaptation, contrast masking, pattern masking, and the contrast sensitivity function. Next, the existing pixel domain and subband domain JND models are presented and analyzed. Finally, the challenges associated with JND estimation are discussed.", "Organic light-emitting diode (OLED) display has been widely used for mobile devices due to its features of wide color range and high contrast. However, display systems dominate the significant power consumption of mobile devices that limit its usage time. Hence, this paper proposes a visual-attention-based pixel dimming (VAB-PD) technique to achieve power saving for real-time video displaying on OLED mobile devices. First, the proposed method extracts motion vector (MV) array from the bitstream. Next, considering the perception of human vision to moving objects, a saliency map is calculated from the MV array. Finally, a pixel dimming function is designed to reduce the brightness of the pixels lying in the unnoticeable parts of the video frames, thereby elevating the energy efficiency without deteriorating the visual quality. In addition, an absolute category rating 11-grade scale (ACR11) subjective visual quality\u00a0\u2026", "Abstract:Abstract We propose an image quality model based on phase and amplitude differences between a reference and a distorted image. The proposed model is motivated by the fact that polar representations can separate visual information in a more independent and efficient manner than Cartesian representations in the primary visual cortex (V1). We subsequently estimate the model parameters from a large subjective data set using maximum likelihood methods. By comparing the various model hypotheses on the functional form about the phase and amplitude, we find that:(a) discrimination of visual orientation is important for quality assessment and yet a coarse level of such discrimination seems sufficient; and (b) a product-based amplitude\u2013phase combination before pooling is effective, suggesting an interesting viewpoint about the functional structure of the simple cells and complex cells in V1.", "Image distortion can be categorized into two aspects: content-dependent degradation and content-independent one. An existing full-reference image quality assessment (IQA) metric cannot deal with these two different impacts well. Singular value decomposition (SVD) as a useful mathematical tool has been used in various image processing applications. In this paper, SVD is employed to separate the structural (content-dependent) and the content-independent components. For each portion, we design a specific assessment model to tailor for its corresponding distortion properties. The proposed models are then fused to obtain the final quality score. Experimental results with the TID database demonstrate that the proposed metric achieves better performance in comparison with the relevant state-of-the-art quality metrics.", "Discrete cosine transform (DCT) is the orthogonal transform that is most commonly used in image and video compression. The motion-compensation residual (MC-residual) is also compressed with the DCT in most video codecs. However, the MC-residual has different characteristics from a nature image. In this paper, we develop a new orthogonal transform-rotated orthogonal transform (ROT) that can perform better on the MC-residual than the DCT for coding purposes. We derive the proposed ROT based on orthogonal-constrained L1-Norm minimization problem for its sparse property. Using the DCT matrix as the starting point, a better orthogonal transform matrix is derived. In addition, by exploring inter-frame dependency and local motion activity, transmission of substantial side information is avoided. The experiment results confirm that, with small computation overhead, the ROT is adaptive to change of local\u00a0\u2026", "Most existing R-D model based rate control schemes are not applicable in H.264/AVC due to the unique features of H.264/AVC, for example, the QP-dependent rate distortion optimization (RDO); the large number of bits (header bits) used for encoding the header information (except DCT coefficients) of a macroblock (MB); the huge number of zero-coefficient MBs. We present an R-D model based rate control scheme for H.264/AVC. A pre-analysis is conducted for R-D estimation. A rate model is developed to estimate the coefficient bits and the header bits separately. A frame-level bit allocation scheme is proposed. In comparison with JM6.1e, our scheme not only evidently improves the PSNR of decoded video (an average 0.54 dB PSNR gain is achieved for 7 sequences tested) but also meets the target bit rates accurately (within 2%).", "With the rapid growth of in-the-wild videos taken by non-specialists, blind video quality assessment (VQA) has become a challenging and demanding problem. Although lots of efforts have been made to solve this problem, it remains unclear how the human visual system (HVS) relates to the temporal quality of videos. Meanwhile, recent work has found that the frames of natural video transformed into the perceptual domain of the HVS tend to form a straight trajectory of the representations. With the obtained insight that distortion impairs the perceived video quality and results in a curved trajectory of the perceptual representation, we propose a temporal perceptual quality index (TPQI) to measure the temporal distortion by describing the graphic morphology of the representation. Specifically, we first extract the video perceptual representations from the lateral geniculate nucleus (LGN) and primary visual area (V1) of the\u00a0\u2026", "The global and local contexts significantly contribute to the integrity of predictions in Salient Object Detection (SOD). Unfortunately, existing methods still struggle to generate complete predictions with fine details. There are two major problems in conventional approaches: first, for global context, high-level CNN-based encoder features cannot effectively catch long-range dependencies, resulting in incomplete predictions. Second, downsampling the ground truth to fit the size of predictions will introduce inaccuracy as the ground truth details are lost during interpolation or pooling. Thus, in this work, we developed a Transformer-based network and framed a supervised task for a branch to learn the global context information explicitly. Besides, we adopt Pixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the size of ground truth instead of the reverse. Thus details in the ground truth are untouched. In addition, we developed a two-stage Context Refinement Module (CRM) to fuse global context and automatically locate and refine the local details in the predictions. The proposed network can guide and correct itself based on the global and local context generated, thus is named, Self-Refined Transformer (SelfReformer). Extensive experiments and evaluation results on five benchmark datasets demonstrate the outstanding performance of the network, and we achieved the state-of-the-art.", "As an important perceptual characteristic of the Human Visual System (HVS), the Just Noticeable Difference (JND) has been studied for decades with image and video processing (e.g., perceptual visual signal compression). However, there is little exploration on the existence of JND for the Deep Machine Vision (DMV), although the DMV has made great strides in many machine vision tasks. In this paper, we take an initial attempt, and demonstrate that the DMV has the JND, termed as the DMV-JND. We then propose a JND model for the image classification task in the DMV. It has been discovered that the DMV can tolerate distorted images with average PSNR of only 9.56dB (the lower the better), by generating JND via unsupervised learning with the proposed DMV-JND-NET. In particular, a semantic-guided redundancy assessment strategy is designed to restrain the magnitude and spatial distribution of the DMV\u00a0\u2026", "A method is provided for encoding an intra predicted residual block of an image for use in image or video compression. The intra predicted residual block is associated with an intra prediction coding mode. The method includes generating a set of residual error blocks including residual data with different statistical characteristics from the residual data in the intra predicted residual block. Each of the residual error blocks is scanned and entropy coded to produce a first set of bit streams. The lengths of each of the first set of bit streams are recorded. The intra predicted residual block is also scanned and entropy coded to produce a second bit stream. The length of the second bit stream is recorded. Selecting the minimum length bit stream from the first set of bit streams and the second bit stream as the output coded bit stream of the intra predicted residual block.", "Automatically understanding and discriminating different users' liking for an image is a challenging problem. This is because the relationship between image features (even semantic ones extracted by existing tools, viz., faces, objects, and so on) and users' likes is non-linear, influenced by several subtle factors. This paper presents a deep bi-modal knowledge representation of images based on their visual content and associated tags (text). A mapping step between the different levels of visual and textual representations allows for the transfer of semantic knowledge between the two modalities. Feature selection is applied before learning deep representation to identify the important features for a user to like an image. The proposed representation is shown to be effective in discriminating users based on images they like and also in recommending images that a given user likes, outperforming the state-of-the-art\u00a0\u2026", "Saliency detection is widely used to extract the regions of interest in images. Many saliency detection models have been proposed for videos in the uncompressed domain. However, videos are always stored in the compressed domain such as MPEG2, H.264, MPEG4 Visual, etc. In this study, we propose a video saliency detection model based on feature contrast in the compressed domain. Four features of luminance, color, texture and motion are extracted from DCT coefficients and motion vectors in the video bitstream. The static saliency map of video frames is calculated based on the luminance, color and texture features, while the motion saliency map for video frames is computed by motion feature. The final saliency map for video frames is obtained through combining the static saliency map and motion saliency map. Experimental results show good performance of the proposed video saliency detection model\u00a0\u2026", "Accurate estimation of noise level is of fundamental interest in a wide variety of vision and image processing applications as it is critical to the processing techniques that follow. In this paper, a new, effective noise level estimation method is proposed based on the study of singular values of noise-corrupted images. There are two major novel aspects of this work to address the major challenges in noise estimation: 1) the use of the tail of singular values for noise estimation to alleviate the influence of the signal on the data basis for the noise estimation process, 2) the addition of known noise to estimate the content-dependent parameter, so that the proposed scheme is adaptive to visual signal and therefore it enables wider application scope of the proposed scheme. The analysis and experiments results demonstrate that the proposed algorithm can reliably infer noise levels and show robust behavior over a wide range\u00a0\u2026", "We propose a new objective quality metric for evaluating image and video quality in coding applications. In this metric, visual quality scores are given based on a combination of three factors: visually masked error, blurring distortion and structural distortion. Compared with vision-based models, it is easier to calculate and applicable to both image and video processing applications. Good correlation with subjective test scores has been obtained using the proposed metric.", "One of the most critical missions of sonar is to capture deep-sea pictures to depict sea floor and various objects, and provide an immense understanding of biology and geology in deep sea. Due to the poor condition of underwater acoustic channel, the captured sonar images very possibly suffer from several typical types of distortions before finally reaching to users. Unfortunately, very limited efforts have been devoted to collecting meaningful sonar image databases and benchmark reliable objective quality predictors. In this paper, we first generate a sonar image quality database (SIQD), including 840 images. All distorted images were collected without artificially introducing any distortions beyond those occurring during compression and transmission. The subjective quality assessment was conducted for gathering mean opinion score (MOS) to represent the image quality and existence of target (EOT) which\u00a0\u2026", "Wiener filtering, which has been widely used in the field of image restoration, is statistically optimal in the sense of mean square error. The adaptive loop filter in video coding inherits the design of Wiener filters, and has been proved to achieve significant improvement on compression performance by reducing coding artifacts and providing high-quality references for subsequent frames. To further improve the compression performance via filtering technique, we explore the factors that may hinder the potential performance of Wiener-based filters, and propose a near-optimal filter learning scheme for high-efficiency image coding. Based on the analyses, we observe that the foremost factor affecting the performance of Wiener-based filters is the divergence of statistical characteristics of training samples, instead of the filter taps or shapes. In view of this, we propose an iterative training method to derive the near-optimal\u00a0\u2026", "The applicability of Emotion Recognition using machines spans on or many industries. It is due to this thai much resource has been put into researching and developing algorithms that can perform such task more accurately and efficiently. However, despite all the work done and its uses, such technology is not readily available to non-professionals. Most applications in the market tend to be complex. In this paper, we document the development of EmoRec. an application made for iOS that strive to serve this purpose. It achieves this through two folds. Firstly, it is presents an intuitive interface that a layperson can use to predict his or her emotions through an audio input easily. Also, it allows users to record and upload their audio data and labels which can be used for future work in the field of Emotion Recognition. As with the algorithm used for EmoRec. deep learning algorithms work best with a huge dataset. Hence\u00a0\u2026", "Constructing objective assessment model on visual quality is challenging, since it associates closely with many factors in human visual perception, as well as both source coding and transmission. In this paper, a no reference hybrid model for video quality assessment is proposed by employing Partial Least Squares Regression (PLSR). The hybrid model combines both bitstream-based features and network-based features, taking into video quality dependence on visual content and network conditions account. We have conducted subjective tests to validate its quality assessment accuracy. Simulation results in Network Simulator version 2 (NS-2) verify the performance advantage of the hybrid model for SD (standard definition) and HD (high definition) video sequences. Proposed model could be used as a quality monitoring tool for assessing the real-time video quality during wireless transmission\u00a0\u2026", "There are usually two components in computer screen images: textual and pictorial parts. The pictorial part can be compressed efficiently by classical coding approaches (e.g. JPEG, JPEG2000), while the compression of the textual part is still far away from being satisfactory for the reason that the textual content is usually of high-frequency. In this paper, a learning approach is used to construct a tailored dictionary for text representation. Based on the learned dictionary, a novel screen image compression algorithm is proposed through adopting different basis functions for the textual and pictorial components respectively. The screen images are firstly segmented into textual and pictorial parts. Then we employ traditional discrete cosine transformation (DCT) to facilitate the compression of pictorial part, while the learned dictionary is used to represent the textual part in screen images. Experimental results demonstrate\u00a0\u2026", "Compared with spatial counterparts, temporal relationships between frames and their influences on video quality assessment (VQA) are still relatively under-studied in existing works. These relationships lead to two important types of effects for video quality. Firstly, some meaningless temporal variations (such as shaking, flicker, and unsmooth scene transitions) cause temporal distortions that degrade quality of videos. Secondly, the human visual system often has different attention to frames with different contents, resulting in their different importance to the overall video quality. Based on prominent time-series modeling ability of transformers, we propose a novel and effective transformer-based VQA method to tackle these two issues. To better differentiate temporal variations and thus capture the temporal distortions, we design the Spatial-Temporal Distortion Extraction (STDE) module that extracts multi-level spatial\u00a0\u2026", "In this work, we investigate deep learning based solutions to blind quality assessment of stitched panoramic images (SPI). The main problem to tackle is that the ground truth data is usually insufficient. As a result, the learned model can easily overfit data with specific content. Because most distortions of SPIs lie within local regions, the problem cannot be alleviated by commonly-used patch-wise training, which assumes local quality equals global quality. We propose a multi-task learning strategy which encourages learned representation to be less dependent on image content. A siamese network with two weight-shared CNN branches is trained to simultaneously compare the quality of two images of the same scene and predict the quality score of each image. Since two images of the same scene are processed by the same CNN, the CNN tends to find their quality differences instead of content differences under the\u00a0\u2026", "During the past few years, there have been various kinds of content-aware image retargeting methods proposed for image resizing. However, the lack of effective objective retargeting quality metric limits the further development of image retargeting. Different from the traditional image quality assessment, the quality degradation of the retargeted images is mainly caused by the geometric changes due to retargeting. In this paper, we propose a practical approach to reveal the geometric changes during image retargeting, and design an Aspect Ratio Similarity (ARS) metric to predict the visual quality of the retargeted image. The experimental results on the widely used dataset show that the proposed metric outperforms the state of the arts.", "Image set coding improves the compression efficiency by reducing both intra- and inter-image redundancy. The key of success is to select representative image(s) to predict set of similar images. This paper proposes an inter-image redundancy measure for representative image selection in image set compression. In the proposed method, the inter-image redundancy is measured jointly by the extent of similar content (EOS) and the correlation of similar content (COS) shared in two images. We take the covered area of matched SIFT points to measure the EOS, and take the distance of the matched SIFT descriptors to measure the COS. The image with largest redundancy for the set is selected as the representative one to predict other images. Experimental results show that the proposed method can select better representative image, and achieve bitrate saving up to 9.2% and 20.8% compared with state-of-the-art\u00a0\u2026", "Emotional facial expression transfer involves sequence-to-sequence mappings from an neutral facial expression to another emotional facial expression, which is a well-known problem in computer graphics. In the graphics community, current considered methods are typically linear (e.g., methods based on blendshape mapping) and the dynamical aspects of the facial motion itself are not taken into account. This makes it difficult to retarget the facial articulations involved in speech. In this paper, we apply a temporal restricted Boltzmann machines based model to emotional facial expression transfer. The method can encode a complex nonlinear mapping from the motion of one neutral facial expression to another emotional facial expression which captures facial geometry and dynamics of both neutral state and emotional state.", "In this paper, we introduce the discretized-Vapnik-Chervonenkis (VC) dimension for studying the complexity of a real function class, and then analyze properties of real function classes and neural networks. We first prove that a countable traversal set is enough to achieve the VC dimension for a real function class, whereas its classical definition states that the traversal set is the output range of the function class. Based on this result, we propose the discretized-VC dimension defined by using a countable traversal set consisting of rational numbers in the range of a real function class. By using the discretized-VC dimension, we show that if a real function class has a finite VC dimension, only a finite traversal set is needed to achieve the VC dimension. We then point out that the real function classes, which have the infinite VC dimension, can be grouped into two categories: TYPE-A and TYPE-B. Subsequently, based on\u00a0\u2026", "In contemporary society full of stereoscopic images, how to assess visual quality of 3D images has attracted an increasing attention in field of Stereoscopic Image Quality Assessment (SIQA). Compared with 2D-IQA, SIQA is more challenging because some complicated features of Human Visual System (HVS), such as binocular interaction and binocular fusion, must be considered. In this paper, considering both binocular interaction and fusion mechanisms of the HVS, a hierarchical no-reference stereoscopic image quality assessment network (StereoIF-Net) is proposed to simulate the whole quality perception of 3D visual signals in human cortex, including two key modules: BIM and BFM. In particular, Binocular Interaction Modules (BIMs) are constructed to simulate binocular interaction in V2-V5 visual cortex regions, in which a novel cross convolution is designed to explore the interaction details in each region. In\u00a0\u2026", "Depth-Image-Based-Rendering (DIBR) is fundamental in free-viewpoint 3D video, which has been widely used to generate synthesized views from multi-view images. The majority of DIBR algorithms cause disoccluded regions, which are the areas invisible in original views but emerge in synthesized views. The quality of synthesized images is mainly contaminated by distortions in these disoccluded regions. Unfortunately, traditional image quality metrics are not effective for these synthesized images because they are sensitive to geometric distortions. To solve the problem, this paper proposes an objective quality evaluation method for 3D Synthesized images via Disoccluded Region Discovery (SDRD). A self-adaptive scale transform model is first adopted to preprocess the images on account of the impacts of view distance. Then disoccluded regions are detected by comparing the absolute difference between the\u00a0\u2026", "Reduced-reference (RR) image quality assessment (IQA) aims to use less reference data and achieve higher quality prediction accuracy. Recent researches confirm that the human visual system (HVS) is adapted to extract structural information and is sensitive to structure degradation. Therefore, in this paper, we try to represent image contents with several structural patterns, and measure image quality according to the structural degradation on these patterns. The classic local binary patterns (LBPs) are firstly employed to extract image structures and create LBP based structural histogram. And then, the structural degradation is computed as the histogram distance between the reference and distorted images. Experimental results on three large databases demonstrate that the proposed RR IQA method greatly improved the quality prediction accuracy.", "H.264/AVC video coding standard with hierarchical bipredictive picture (HBP) generally outperforms the other prediction structures such as \u2018IPPP\u2026\u2019 and \u2018IBBP\u2026\u2019 through better exploitation of data correlation using the preceding and succeeding reference frames. However, due to the different coding order of frames, the HBP scheme could not fully exploit the data correlations using multiple reference frames for occluded background, repetitive motion, etc. In this paper, we propose a new HBP scheme which uses the most common reference frame in scene (McFIS) as a third reference frame with other two closest bipredictive reference frames assuming that foreground and background areas of the current frame are referenced from the two bipredicted frames and the McFIS respectively. The experimental results confirm that the proposed scheme outperforms two state-of-art algorithms by improving significant image\u00a0\u2026", "For streaming of pre-encoded bitstreams over constant bit rate (CBR) channels, the channel bandwidth, the receiver buffer capacity as well as the latency requirement vary greatly from application to application. In this paper, we attempt to determine the minimum buffer size and the minimum start-up delay required for streaming a pre-encoded bitstream over CBR channels at any specific bit rate. The proposed method employs geometric operations to derive the optimal determination for low or high bit rates and sub-optimal determination for medium bit rates. The algorithm developed requires little extra information from the encoder and is easy to implement. Our algorithm is implemented in a H.264/AVC video encoder and its performance is compared with that of H.264/AVC hypothetical reference decoder. Our approach provides new theoretical insight and an excellent solution for determining the leaky bucket\u00a0\u2026", "This paper is devoted to discriminate pixel difference in the context of visual signal for picture quality prediction, according to human perceptual characteristics. The new quality predictor as the result of the analysis remedies one oversight in all the previous metrics, and proves to be more accurate and consistent in visual quality prediction then the best metrics in the VQEG tests.", "Recently, relying on convolutional neural networks (CNNs), many methods for salient object detection in optical remote-sensing images (ORSI-SOD) are proposed. However, most methods ignore the number of parameters and computational cost brought by CNNs, and only a few pay attention to portability and mobility. To facilitate practical applications, in this article, we propose a novel lightweight network for ORSI-SOD based on semantic matching and edge alignment, termed SeaNet. Specifically, SeaNet includes a lightweight MobileNet-V2 for feature extraction, a dynamic semantic matching module (DSMM) for high-level features, an edge self-alignment module (ESAM) for low-level features, and a portable decoder for inference. First, the high-level features are compressed into semantic kernels. Then, semantic kernels are used to activate salient object locations in two groups of high-level features through dynamic\u00a0\u2026", "Numerous image retargeting algorithms have been proposed to achieve adaptive image resizing during the past years. To compare different image retargeting algorithms, reliable objective image retargeting quality assessment (IRQA) metrics are highly desired. Given that image retargeting usually introduces geometric distortions, this paper presents an objective IRQA metric by measuring both local and global geometric distortions (LGGD). Since human visual system perception is highly dependent on edges and the geometric distortions caused by image retargeting usually cause edge deformation, a sketch token-based local edge descriptor (ST-LED) is introduced to represent geometric-aware features in LGGD. First, ST-LED is first applied on both source and retargeted images for edge pattern representation. Second, pixel-level backward registration is conducted to enable estimating local geometric distortion\u00a0\u2026", "Three-dimensional structure-based localization aims to estimate the six-DOF camera pose of a query image by means of feature matches against a 3D Structure-from-Motion (SfM) point cloud. For city-scale SfM point clouds with tens of millions of points, it becomes more and more difficult to disambiguate matches. Therefore, a 3D structure-based localization method, which can efficiently handle matches with very large outlier ratios, is needed. We propose a two-stage outlier filtering framework for city-scale localization that leverages both visibility and geometry intrinsics of the SfM point clouds. First, we propose a visibility-based outlier filter, which is based on a bipartite visibility graph, to filter outliers on a coarse level. Second, we apply a geometry-based outlier filter to generate a set of fine-grained matches with a novel data-driven geometrical constraint for efficient inlier evaluation. The proposed two-stage outlier\u00a0\u2026", "In this paper, we propose a novel reduced-reference quality assessment metric for image super-resolution (RRIQA-SR) based on the low-resolution (LR) image information. First, we use the Markov Random Field (MRF) to model the pixel correspondence between LR and high-resolution (HR) images. Based on the pixel correspondence, we predict the perceptual similarity between image patches of LR and HR images by two components: the energy change and texture variation. The overall quality of HR images is estimated by the perceptual similarity between local image patches of LR and HR images. Experimental results demonstrate that the proposed method can obtain better performance of quality prediction for HR images than other existing ones, even including some full-reference (FR) metrics.", "Images compressed at low bit rates usually suffer from annoying artifacts due to coarse quantization of transform coefficients. In this paper, we propose a soft-thresholding scheme to reduce compression noise with content-based noise level estimation. In the proposed method, a compressed image is divided into multiple similar image patch groups, and the compression noise is estimated from every group respectively based on coefficient distribution in transform domain. For each group of similar patches, soft-thresholding is applied to the singular values in the singular value decomposition (SVD) of every group of similar patches. The threshold is adaptively determined based on the standard deviation of image signals and compression noise. Finally, quantization constraint is applied to estimated images to avoid over-smoothing. Extensive experimental results show that the proposed method improves the quality of\u00a0\u2026", "Image activity measure (IAM) is a measure of image activity and reflects the image complexity. In this paper, we introduce a simple and efficient segmentation algorithm based on IAM for computer screen image compression. A new multi-stage approach of computing IAM is proposed according to the multi-directional distribution of text edges and contours. Based on the acquired IAM values, the image blocks are classified into two categories: textual and pictorial blocks. A refinement is then applied to rectify a small number of remaining misclassified blocks. The experimental results show that our segmentation algorithm is not only efficient in decomposing screen images, but also significantly improves the performance of the block based screen image compression.", "Image quality assessment (IQA) is useful in many visual processing systems but challenging to perform in line with the human perception. A great deal of recent research effort has been directed towards IQA. In order to overcome the difficulty and infeasibility of subjective tests in many situations, the aim of such effort is to assess visual quality objectively towards better alignment with the perception of the Human Visual system (HVS). In this work, we review and analyze the recent progress in the areas related to IQA, as well as giving our views whenever possible. Following the recent trends, we discuss the engineering approach in more details, explore the related aspects for feature pooling, and present a case study with machine learning.", "This paper proposes a new full-reference objective metric for image quality assessment. The reference and distorted images are decomposed into a number of wavelet subbands, in which mean curvatures and perceived error of the wavelet coefficients of two images are computed and integrated to give overall quality index. Taking structural similarity and error visibility into account, the new method can achieve high consistency with subjective evaluation compared with other metrics. Experimental results have shown the effectiveness of the proposed metric.", "The paper presents a new and general concept, PQSM (perceptual quality significance map), to be used in measuring visual distortion. It makes use of the mechanism that the HVS (human visual system) pays more attention to certain areas of visual signals due to one or more of the following factors: salient features in image/video; cues from domain knowledge; association of other media (e.g., speech or audio). PQSM is a 3D/4D array whose elements represent the relative perceptual-quality significance levels for the corresponding pixels/regions for images or video. Due to its generality, PQSM can be incorporated into any visual distortion metrics; it can improve effectiveness or/and efficiency of perceptual metrics or even enhance a PSNR-based metric. A three-stage PQSM generation method is also proposed, with an implementation of motion, luminance, skin-color and face mapping. Experimental results show\u00a0\u2026", "Multi-view clustering divides similar objects into the same class through using the fused multiview information. Most multi-view clustering methods obtain clustering result by only analyzing structure relationship among samples, ignoring the analysis of intrinsic features of each sample, while a few methods operate the original feature on the corresponding high-dimensional kernel matrices. However, noisy and redundant features of samples are inevitably mixed in original multi-view data or high-dimensional kernel matrices. To address this problem, we propose a novel multiview clustering method, which unifies structure learning and feature learning to a framework. Specifically, we obtain a consensus structure information from multiple views via sparse subspace structure learning with weight tensor nuclear norm constraint. Then our feature learning seeks projection directions to obtain data representation by data\u00a0\u2026", "Combining color (RGB) images with thermal images can facilitate semantic segmentation of poorly lit urban scenes. However, for RGB-thermal (RGB-T) semantic segmentation, most existing models address cross-modal feature fusion by focusing only on exploring the samples while neglecting the connections between different samples. Additionally, although the importance of boundary, binary, and semantic information is considered in the decoding process, the differences and complementarities between different morphological features are usually neglected. In this paper, we propose a novel RGB-T semantic segmentation network, called MMSMCNet, based on modal memory fusion and morphological multiscale assistance to address the aforementioned problems. For this network, in the encoding part, we used SegFormer for feature extraction of bimodal inputs. Next, our modal memory sharing module\u00a0\u2026", "User-generated-content (UGC) videos have dominated the Internet during recent years. While many methods attempt to objectively assess the quality of these UGC videos, the mechanisms of human quality perception in the UGC-VQA problem is still yet to be explored. To better explain the quality perception mechanisms and learn more robust representations, we aim to disentangle the effects of aesthetic quality issues and technical quality issues risen by the complicated video generation processes in the UGC-VQA problem. To overcome the absence of respective supervisions during disentanglement, we propose the Limited View Biased Supervisions (LVBS) scheme where two separate evaluators are trained with decomposed views specifically designed for each issue. Composed of an Aesthetic Quality Evaluator (AQE) and a Technical Quality Evaluator (TQE) under the LVBS scheme, the proposed Disentangled Objective Video Quality Evaluator (DOVER) reach excellent performance (0.91 SRCC for KoNViD-1k, 0.89 SRCC for LSVQ, 0.88 SRCC for YouTube-UGC) in the UGC-VQA problem. More importantly, our blind subjective studies prove that the separate evaluators in DOVER can effectively match human perception on respective disentangled quality issues. Codes and demos are released in https://github.com/teowu/dover.", "Objective quality assessment of 3D point clouds is essential for the development of immersive multimedia systems in real-world applications. Despite the success of perceptual quality evaluation for 2D images and videos, blind/no-reference metrics are still scarce for 3D point clouds with large-scale irregularly distributed 3D points. Therefore, in this paper, we propose an objective point cloud quality index with Structure Guided Resampling (SGR) to automatically evaluate the perceptually visual quality of 3D dense point clouds. The proposed SGR is a general-purpose blind quality assessment method without the assistance of any reference information. Specifically, considering that the human visual system (HVS) is highly sensitive to structure information, we first exploit the unique normal vectors of point clouds to execute regional pre-processing which consists of keypoint resampling and local region construction. Then, we extract three groups of quality-related features, including: 1) geometry density features; 2) color naturalness features; 3) angular consistency features. Both the cognitive peculiarities of the human brain and naturalness regularity are involved in the designed quality-aware features that can capture the most vital aspects of distorted 3D point clouds. Extensive experiments on several publicly available subjective point cloud quality databases validate that our proposed SGR can compete with state-of-the-art full-reference, reduced-reference, and no-reference quality assessment algorithms.", "The goal of Image Quality Assessment (IQA) is to design computational models that can automatically predict the perceived image quality consistent with human subjective ratings. In this paper, we propose a full reference IQA metric gradient weighted structural similarity (GW-SSIM) by incorporating the gradient information to the well-known IQA metric SSIM. Experimental results demonstrate that GW-SSIM can greatly improve the quality prediction accuracy and achieve the best performance among the SSIM-based methods by addressing SSIM's shortcomings. Additionally, incorporating the proposed gradient weighting (GW) map into peak-signal-to-noise ratio (PSNR) also makes it quite competitive to state-of-the-art IQA models, and this is meaningful since PSNR is still a widely adopted metric.", "Since the human visual system (HVS) is the ultimate appreciator of most photorealistically rendered images, rendering process can be accelerated by exploiting the properties of the HVS. According to the concept of entropy masking, the HVS is not sensitive to visual distortions in unstructured visual signals. For structured regions, pixels are highly correlated, while the similarity among pixels in unstructured regions is low. In this paper, we detect unstructured regions by extracting local patches from each pixel and its neighboring pixels, and comparing the similarity between the local patches of the center pixel and the neighboring pixels. We further exploit entropy masking in perceptual rendering, and experimental results demonstrate that the proposed method can accelerate rendering, without degrading the perceived quality of resultant images.", "Visual quality evaluation of compressed Digital Compound Images (DCIs) becomes important in many multi-device communication systems. In this paper, we study subjective quality evaluation for compressed DCIs and investigate whether existing Image Quality Assessment (IQA) metrics are effective to evaluate the visual quality of compressed DCIs. A new Compound Image Quality Assessment Database (CIQAD) is therefore constructed, including 24 reference and 576 compressed DCIs. The subjective scores of these DCIs are obtained via visual judgement of 62 subjects using Paired Comparison (PC) in which the Hodgerank decomposition is adopted to generate uncompleted but near balanced pairs. Fourteen state-of-the-art IQA metrics are adopted to assess quality of images in CIQAD, and experimental results indicate that the existing IQA methods are limited in evaluating visual quality of DCIs. Compression\u00a0\u2026", "Image retargeting techniques aim to obtain retargeted images with different sizes or aspect ratios for various display screens. Various content-aware image retargeting algorithms have been proposed recently. However, there is still no accurate objective metric for visual quality assessment of retargeted images. In this paper, we propose a novel objective metric for assessing visual quality of retargeted images based on perceptual geometric distortion and information loss. The proposed metric measures the geometric distortion of retargeted images by SIFT flow variation. Furthermore, a visual saliency map is derived to characterize human perception of the geometric distortion. On the other hand, the information loss in a retargeted image, which is calculated based on the saliency map, is integrated into the proposed metric. A user study is conducted to evaluate the performance of the proposed metric. Experimental\u00a0\u2026", "This paper presents a novel non-intrusive respiratory monitoring system for detection of life threatening situations in bedridden or bed bound patients, and for monitoring of sleep disorders in elderly or patients. Specifically, we present the subtle design and implementation of a system using Fiber Bragg Grating pressure sensors to monitor the respiratory rate without requiring the patients or elderly to wear any probes. The special packaging design of the sensors allows them to be blended into the natural settings of a normal bed for continuous 24x7 respiratory rate monitoring.", "We present a method for extracting local visual perceptual clues and its application for rate control of videophone, in order to ensure the scarce bits to be assigned for maximum perceptual coding quality. The optimum quantization step is determined with the rate-distortion model considering the local perceptual clues in the visual signal. For extraction of the perceptual clues, luminance adaptation and texture masking are used as the stimulus-driven factors, while skin color serves as the cognition-driven factor in current implementation. Both objective and subjective quality evaluations are given by evaluating the proposed perceptual rate control (PRC) scheme in the H.263 platform, and the evaluations show that the proposed PRC scheme achieves significant quality improvement in the block-based coding for bandwidth-hungry applications.", "The increased resolution of real-world videos presents a dilemma between efficiency and accuracy for deep Video Quality Assessment (VQA). On the one hand, keeping the original resolution will lead to unacceptable computational costs. On the other hand, existing practices, such as resizing or cropping, will change the quality of original videos due to difference in details or loss of contents, and are henceforth harmful to quality assessment. With obtained insight from the studies of spatial-temporal redundancy in the human visual system, visual quality around a neighbourhood has high probability to be similar, and this motivates us to investigate an effective quality-sensitive neighbourhood representative sampling scheme for VQA. In this work, we propose a unified scheme, spatial-temporal grid mini-cube sampling (St-GMS), and the resultant samples are named   fragments  . In St-GMS, full-resolution videos are\u00a0\u2026", "Personalized image aesthetics assessment (IAA) aims to estimate aesthetic experiences subject to the preferences of individual users, contrary to generic IAA that estimates aesthetic experiences subject to average preferences. Most existing personalized IAA methods treat personalized aesthetic experiences as deviations from a generic aesthetic experience, and therefore, personalized IAA models are designed to build upon the prior knowledge on generic IAA. However, we propose that acquiring knowledge on generic IAA is not necessary for building a personalized IAA model. Instead of modeling personalized IAA on the basis of generic IAA, this work proposes to directly estimate personalized aesthetic experiences from the interactions between image contents and user preferences (i.e., preference-content interaction), where interaction-matrices representing preference-content interactions are constructed\u00a0\u2026", "Just Noticeable Difference (JND) has many applications in multimedia signal processing, especially for visual data processing up to date. It's generally defined as the minimum visual content changes that the human can perspective, which has been studied for decades. However, most of the existing methods only focus on the luminance component of JND modelling and simply regard chrominance components as scaled versions of luminance. In this paper, we propose a JND model to generate the JND by taking the characteristics of full RGB channels into account, termed as the RGB-JND. To this end, an RGB-JND-NET is proposed, where the visual content in full RGB channels is used to extract features for JND generation. To supervise the JND generation, an adaptive image quality assessment combination (AIC) is developed. Besides, the RDB-JND-NET also takes the visual attention into account by automatically mining the underlying relationship between visual attention and the JND, which is further used to constrain the JND spatial distribution. To the best of our knowledge, this is the first work on careful investigation of JND modelling for full-color space. Experimental results demonstrate that the RGB-JND-NET model outperforms the relevant state-of-the-art JND models. Besides, the JND of the red and blue channels are larger than that of the green one according to the experimental results of the proposed model, which demonstrates that more changes can be tolerated in the red and blue channels, in line with the well-known fact that the human visual system is more sensitive to the green channel in comparison with the red and blue ones.", "Recently, a new paradigm of transmitting and compressing intermediate deep learning features (i.e., feature maps) for distributed visual analysis systems is emerging. As the fundamental infrastructure in such paradigm, research and standardization for feature maps coding has attracted more and more attention. In this paper, to improve the state-of-the-art hybrid coding framework which integrates the traditional video codecs to compress feature maps, we investigate the data representation procedure in such coding framework. Specifically, we proposed three modes in Repack module to help explore inter-channel redundancy, and we explore the fidelity maintenance ability of two modes in Pre-Quantization modules. It is worth mentioning that the proposed coding modes have been partially adopted in to the ongoing AVS (Audio Video Coding Standard Workgroup) - Visual Feature Coding Standard.", "In the previous years, a lot of saliency detection algorithms have been designed for saliency computation of visual content. Recently, stereoscopic display techniques have developed rapidly, which results in much requirement of stereoscopic saliency detection for emerging stereoscopic applications. Different from 2D saliency prediction, stereoscopic saliency detection methods have to consider depth factor. We design a novel stereoscopic saliency detection algorithm by machine learning technique. First, the features of luminance, color and texture are extracted to calculate the feature contract for predicting feature maps of stereoscopic images. Furthermore, the depth features are extracted for depth feature map computation. Sematic features including the center-bias factor and other top-down cues are also applied as the features in the proposed stereoscopic saliency detection method. Support Vector Regression\u00a0\u2026", "Stereoscopic subtitle insertion is a fundamental and essential element in stereoscopic film and TV industry. However, little work has been dedicated to the optimal region selection for stereoscopic subtitle insertion. In addition, there is no public database reported for the performance evaluation of it. In this paper, we build the first large-scale video database (TJU3D) for stereoscopic video subtitle insertion, which includes 50 video sequences with rich screen scenes. Compared with 2D subtitle region selection, there are several problems we have to consider in stereoscopic subtitle region selection: 1) the subtitle should avoid depth cue collision and occlusion from objects in stereoscopic video sequences; 2) the disparity value of the subtitle must be minimized to reduce visual discomfort; and 3) the temporal coherence constraint must be considered during region selection for subtitles in video sequences. By\u00a0\u2026", "For many computer vision problems, the deep neural networks are trained and validated based on the assumption that the input images are pristine (i.e., artifact-free). However, digital images are subject to a wide range of distortions in real application scenarios, while the practical issues regarding image quality in high level visual information understanding have been largely ignored. In this paper, in view of the fact that most widely deployed deep learning models are susceptible to various image distortions, the distorted images are involved for data augmentation in the deep neural network training process to learn a reliable model for practical applications. In particular, an image quality assessment based label smoothing method, which aims at regularizing the label distribution of training images, is further proposed to tune the objective functions in learning the neural network. Experimental results show that the proposed method is effective in dealing with both low and high quality images in the typical image classification task.", "Background and objectivesAngle closure disease in the eye can be detected using time-domain Anterior Segment Optical Coherence Tomography (AS-OCT). The Anterior Chamber (AC) characteristics can be quantified from AS-OCT image, which is dependent on the image quality at the image acquisition stage. To date, to the best of our knowledge there are no objective or automated subjective measurements to assess the quality of AS-OCT images.MethodsTo address AS-OCT image quality assessment issue, we define a method for objective assessment of AS-OCT images using complex wavelet based local binary pattern features. These features are pooled using the Na\u00efve Bayes classifier to obtain the final quality parameter. To evaluate the proposed method, a subjective assessment has been performed by clinical AS-OCT experts, who graded the quality of AS-OCT images on a scale of good, fair, and poor\u00a0\u2026", "Recommending content to users involves understanding a) what to present and b) how to present them, so as to increase quality of experience (QoE) and thereby, content consumption. This work attempts to address the question of how to present contents in a way so that the user finds it easy to get to desired content. While the process of User Interface (UI) design is dependent on several human factors, there are basic design components and their combination that have to be common to any recommender system user interface. Personalization of the UI design process involves picking the right components and their combination, and presenting a UI to suit the usage behavior of an individual user, so as to enhance the QoE. This work proposes a system that learns from a user's content consumption patterns and makes some recommendations regarding how to present the content for the user (in the context of Video\u00a0\u2026", "Perceptual image and video coders have been reported extensively in the literature with a recent survey listing sixteen representative PVCs (perceptual video coders) for single-view video coding, ten of which conform to H.263, MPEG-4 or H.264/AVC international standard. Successful approaches to perceptual picture coding are classified as perceptual predictive coding, perceptual quantization, rate-perceptual-distortion optimization (RpDO) and perception-based pre-, loop- and post-filtering. This contribution analyzes and examines the performance and tradeoffs of representative perceptual as well as traditional waveform based picture coding methods in terms of their effectiveness in quality constrained coding and RpDO criterion in the context of visual quality controlled or regulated visual communication, broadcasting, entertainment and consumer electronic products, systems and services or applications\u00a0\u2026", "Unlike image quality, video quality is affected by the temporal factor, in addition to the spatial one. In this paper, we investigate into the impact of both the factors on the overall perceived video quality and combine them into a metric. We use machine learning as a tool to study and analyze the relationship between the factors and the overall perceived video quality. It is shown that apart from their individual contributions, the interaction of the two factors also plays a role in determining the overall video quality. We report the experimental results and the related analysis using videos from two publicly available databases.", "The existing high-accuracy environment matting extraction methods usually require the capturing of thousands of sample images and spend several hours in data acquisition. In this paper, a fast environment matting algorithm is proposed to ex tract the environment matte data effectively and efficiently. In particular, we incorporate the recently developed compressive sensing theory to simplify the data acquisition process. More over, taking into account special properties of light refraction and reflection effects of transparent object, we further propose to use hierarchical sampling and group clustering based recovery to accelerate the matte extraction process. Compared with the state-of-the-art approaches, our proposed algorithm significantly accelerates the environment matting extraction process while still achieving high-accuracy results.", "In this paper, we propose a low-complexity video codec based on two-dimensional Singular Value Decomposition (2D-SVD). We exploit the common temporal characteristics of video without resorting to motion estimation. It has been demonstrated that this codec has higher coding efficiency than the relevant existing low complexity codecs. Moreover, the proposed codec performs well to deal with packet loss that is unavoidable in error-prone transmission. Therefore it is with advantages and good potential for wireless video applications such as mobile video calls and wireless surveillance.", "The conventional Intra (I-) frame is used for error propagation prevention, backward/forward play, random access, indexing, etc. This frame is also used as an anchor frame for referencing the subsequence frames. To get better rate-distortion performance a frame should have the following quality to be an ideal I-frame: the best similarity with the frames in a GOP, so that (i) when it is used as a reference frame for a frame in the GOP we need less bits to achieve the desired image quality; (ii) if any frame is missing at the decoding end we can retrieve the missing frame from it. In this paper we will generate a most common frame of a scene (McFIS) in a video sequence using dynamic background modelling and then encode it to replace the conventional I-frame. By using McFIS as an I-frame, we not only gain the above mentioned two benefits but also ensure adaptive GOP for better rate-distortion performance compared to\u00a0\u2026", "In this paper, we develop a new image quality assessment scheme with the use of Singular Value Decomposition (SVD). In a previous work [1], only the singular values out of SVD were used for quality assessment. We show that singular vectors are even more important than singular values for quality assessment. Hence, we use both singular vectors and values for comprehensive image quality evaluation, and demonstrate that the quality prediction is significantly improved. Extensive experimental results are reported as evidence of the efficiency of the proposed scheme, in comparison with the relevant existing methods, for three open and independent datasets with totally 2647 distorted images.", "JPEG 2000 image coding standard aims to achieve high compression performance. As the bit rate decreases, visual degradation caused by information loss on an image becomes more and more visible. We propose an adaptive nonlinear diffusion method to reduce ringing artifacts while preserving and enhancing image edges. The diffusion coefficient is the linear combination of three membership functions that correspond to the different gradient scales. The parameters are tuned using a stochastic optimization technique so that the diffusion coefficient can adapt to the given image data to achieve optimal restoration performance. Experimental results on JPEG 2000 compressed images have shown the effectiveness of the proposed nonlinear diffusion processes for ringing artifacts removal.", "In this paper, just noticeable distortion (JND) profile based upon the human visual system (HVS) has been exploited to guide the motion search and introduce an adaptive filter for residue error after motion compensation, in hybrid video coding (e.g., H.26x and MPEG-x). Because of the importance of accurate JND estimation, a new spatial-domain JND estimator (the nonlinear additivity model for masking-NAMM for short) is to be firstly proposed. The obtained JND profile is then utilized to determine the extent of motion search and whether a residue error after motion compensation needs to be consine-tranformed. Both theoretical analysis and experimental data indicate significant improvement in motion search speedup, perceptual visual quality measure, and most remarkably, objective quality (i.e., PSNR) measure.", "Unsupervised person re-identification (Re-ID) aims at finding the most informative features from unlabeled person datasets. Some recent approaches adopted camera-aware strategies for model training and have thereby achieved highly promising results. However, these methods simultaneously address intra-ID discrepancies of all cameras and require independent learning under each camera, which increases the complexity of algorithm. To resolve this issue, we present a camera contrast learning framework for unsupervised person Re-ID. Our method first proposes a time-based camera contrastive learning module to facilitate model learning. At each iteration, we follow the time contrast principle to select one camera centroid as proxy of each cluster. By enforcing the samples to converge to positive proxies, the correlation between features and cameras can gradually be reduced. Moreover, we design a 3\u00a0\u2026", "Ensemble learning performs better than a single classifier in most tasks due to the diversity among multiple classifiers. However, the enhancement of the diversity is at the expense of reducing the accuracies of individual classifiers in general and, thus, how to balance the diversity and accuracies is crucial for improving the ensemble performance. In this paper, we propose a new ensemble method which exploits the correlation between individual classifiers and their corresponding weights by constructing a joint optimization model to achieve the tradeoff between the diversity and the accuracy. Specifically, the proposed framework can be modeled as a shallow network and efficiently trained by the end-to-end manner. In the proposed ensemble method, not only can a high total classification performance be achieved by the weighted classifiers but also the individual classifier can be updated based on the error of the\u00a0\u2026", "Content-aware retargeting methods are used to adjust images to different resolutions and aspect ratios with low deformation and information loss in salient regions. Effective objective quality assessment of retargeted images can provide a way to improve retargeting methods. The non-uniform geometrical degradations caused by retargeting algorithms make it impossible to use traditional image quality assessment metrics for retargeted images. Although some quality evaluation methods have been proposed till now, the resulted quality scores are not well correlated with the subjective ones. In this paper we propose a pyramidal global-to-local pooling method to combine pixel/block deformation measures. In each level of locality, the Euclidean distance between the retargeted and original image is used as an individual feature. Therefore, in addition to the global summation, assessment of local deformations\u00a0\u2026", "In this paper we propose to blindly evaluate the quality of images synthesized based on a depth image-based rendering (DIBR) procedure. As an important branch in virtual reality (VR), superior DIBR techniques provide free viewpoints in many real applications such as remote surveillance and education, but few efforts have been made to measure the performance of DIBR methods (i.e. the quality of DIBR-synthesized images), especially in the condition of reference unavailable. To this aim, we put forward a new no-reference (NR) image quality assessment (IQA) model via multiscale analysis, dubbed as MSA. The design philosophy of our proposed MSA model is that the DIBR-introduced geometry distortions damage the self-similarity characteristic of natural images and the damage degrees present regular variations at distinct scales. Through systematically incorporating the measurements of the variations\u00a0\u2026", "In this paper, a low-complexity coding approach is explored for depth video under the framework of high efficiency video coding based 3-D video coding standard. Unlike the existing low-complexity coding approaches that optimize motion search and mode decision, the major technical innovation of this paper is to incorporate depth sensitivity fidelity (DSF) into the rate-distortion optimization (RDO) process. Specifically, a quantitative maximum tolerable depth distortion is derived to measure the DSF, and jointly estimate the view synthesis distortion to account for the DSF. Then, a DSF-aware RDO scheme is proposed by developing new quantization parameter and Lagrangian multiplier determination strategies. Extensive experimental results demonstrate that the proposed method can reduce the computational complexity of encoding without significant view synthesis performance loss.", "In this paper, we propose a novel image set compression approach based on sparse coding with an ordered dictionary learned from perceptually informative signals. For a group of similar images, one representative image is first selected and transformed into wavelet domain, and then its AC components are utilized as samples to train an over-complete dictionary. In order to improve compression efficiency, the dictionary atoms are reordered according to their frequency used in sparse approximation of the representative image. In addition, a rate-distortion based sparse coding method is proposed to distribute atoms among different image patches adaptively. Experimental results show that the proposed method outperforms JPEG and JPEG2000 up to 6+ dB and 2+ dB, respectively.", "Research on non-intrusive speech quality assessment (SQA) aims to develop a computational model simulating the human perception of speech signals accurately and automatically without any prior information about the reference clean speech signals. In this paper, we propose to learn a non-intrusive SQA metric based on bag-of-words (BoW) representation of speech signals. In particular, the proposed method treats the whole speech utterance as a text document and extracts perceptual linear prediction (PLP) features of local segments as words. The speech utterance is then represented as a histogram of codewords, with each entry as the probability of a codeword appeared in the utterance. After the BoW representation of speech signals is obtained, support vector regression (SVR) is used to learn the metric for quality evaluation. Experimental results demonstrate that the proposed non-intrusive SQA metric\u00a0\u2026", "A method of performing an image retargeting quality assessment comprising comparing an original image and a retargeted image in a frequency domain, wherein the retargeted image is obtained by performing a retargeting algorithm on the original image. The disclosure also includes an apparatus comprising a processor configured to perform an image retargeting quality assessment, and compare an original image and a retargeted image in a spatial domain, wherein the retargeted image is obtained by performing a retargeting algorithm on the original image, and wherein comparing the original image and the retargeted image in the spatial domain comprises comparing the original image and the retargeted image to determine an amount of shape distortion between the images.", "Selection of relevant features is of fundamental importance in building robust classifiers for computer-aided detection (CAD) of angle closure glaucoma mechanism. Typically one is interested in determining which, of a large number of potentially redundant or noisy features, are most discriminative for classification. The objective of the paper is to exploit machine learning algorithms for automated classification of different angle closure mechanisms based on the quantitative assessment of Anterior Segment Optical Coherence Tomography (AS-OCT). In this paper, we propose an effective combination of Minimum Redundancy Maximum Relevance (MRMR), a mutual information feature selection, with AdaBoost, an adaptive boosting to detect angle closure glaucoma. The proposed method effectively combines the best feature selection and classifier to the problem. A sequential forward search was conducted to\u00a0\u2026", "Although wireless sensor networks (WSNs) are powerful in monitoring physical events, the data collected from a WSN are almost always incomplete if the surveyed physical event spreads over a wide area. The reason for this incompleteness is twofold: i) insufficient network coverage and ii) data aggregation for energy saving. Whereas the existing recovery schemes only tackle the second aspect, we develop Dual-lEvel Compressed Aggregation (DECA) as a novel framework to address both aspects. Specifically, DECA allows a high fidelity recovery of a widespread event, under the situations that the WSN only sparsely covers the event area and that an in-network data aggregation is applied for traffic reduction. Exploiting both the low-rank nature of real-world events and the redundancy in sensory data, DECA combines matrix completion with a fine-tuned compressed sensing technique to conduct a dual-level reconstruction process. We demonstrate that DECA can recover a widespread event with less than 5% of the data, with respect to the dimension of the event, being collected. Performance evaluation based on both synthetic and real data sets confirms the recovery fidelity and energy efficiency of our DECA framework.", "In this paper, we propose a new video quality evaluation method based on multi-feature and radial basis function neural network. Multi-feature is extracted from a degraded image sequence and its reference sequence, including error energy, activity-masking and luminance-masking as well as blockiness and blurring features. Based on these factors we apply a radial basis function neural network as a classifier to give quality assessment scores. After training with the subjective mean opinion scores (MOS) data of VQEG test sequences, the neural network model can be used to evaluate video quality with good correlation performance in terms of accuracy and consistency measurements.", "Existing convolutional neural networks (CNN) based image super-resolution (SR) methods have achieved impressive performance on bicubic kernel, which is not valid to handle unknown degradations in real-world applications. Recent blind SR methods suggest to reconstruct SR images relying on blur kernel estimation. However, their results still remain visible artifacts and detail distortion due to the estimation errors. To alleviate these problems, in this paper, we propose an effective and kernel-free network, namely DSSR, which enables recurrent detail-structure alternative optimization without blur kernel prior incorporation for blind SR. Specifically, in our DSSR, a detail-structure modulation module (DSMM) is built to exploit the interaction and collaboration of image details and structures. The DSMM consists of two components: a detail restoration unit (DRU) and a structure modulation unit (SMU). The former\u00a0\u2026", "Due to occlusion among views and temporal inconsistency in depth video, spatio-temporal distortion occurs in 3D synthesized video with depth image-based rendering. In this paper, we propose a deep Convolutional Neural Network (CNN)-based synthesized video denoising algorithm to reduce temporal flicker distortion and improve perceptual quality of 3D synthesized video. First, we analyze the spatio-temporal distortion, and model eliminating spatio-temporal distortion as a perceptual video denoising problem. Then, a deep learning-based synthesized video denoising network is proposed, in which a CNN-friendly spatio-temporal loss function is derived from a synthesized video quality metric and integrated with a single image denoising network architecture. Finally, specific schemes, i.e., specific Synthesized Video Denoising Networks (SynVD-Nets), and a general scheme, i.e., General SynVD-Net (GSynVD\u00a0\u2026", "Object detection is significant for event analysis in various intelligent multimedia processing systems. Although there have been many studies conducting research in this area, effective and efficient object detection methods for video sequences are still much desired. In this article, we investigate salient object detection in real-time multimedia processing systems. Considering the intrinsic relationship between top-down and bottom-up saliency features, we present a new effective method for video salient object detection based on deep semantic and spatiotemporal cues. After extracting top-down semantic features for object perception by a 2-D convolutional network, we concatenate them with bottom-up spatiotemporal cues for motion perception extracted by a 3-D convolutional network. In order to combine these features effectively, we feed them into a 3-D deconvolutional network for feature-sharing learning\u00a0\u2026", "Computationally modeling users `liking' for image(s) requires understanding how to effectively represent the image so that different factors influencing user `likes' are considered. In this work, an evaluation of the state-of-the-art visual features in multimedia understanding at the task of predicting user `likes' is presented, based on a collection of images crawled from Flickr. Secondly, a probabilistic approach for modeling `likes' based only on tags is proposed. The approach of using both visual and text-based features is shown to improve the state-of-the-art performance by 12%. Analysis of the results indicate that more human-interpretable and semantic representations are important for the task of predicting very subtle response of `likes'.", "Definition and extraction of local features play a very important role in image retrieval (IR), pattern recognition and computer vision. Fast growth of technology today calls for local features to be as compact as possible toward real-time and limited bandwidth applications. In this paper, we study the problem of representing images in a compact way to achieve low bit-rate transmission while maintaining good performance. To be more specific, we propose a novel compact descriptor, dominant SIFT, which only uses 48 bits to describe local features. Importantly, our descriptor is training-free, vocabulary-free and suitable for real-time and mobile applications. We show the effectiveness of the proposed compact descriptor in image retrieval.", "Currently, there are various saliency detection models proposed for saliency prediction in 2D images/video in the previous decades. With the rapid development of stereoscopic display techniques, stereoscopic saliency detection is much desired for the emerging stereoscopic applications. Compared with 2D saliency detection, the depth factor has to be considered in stereoscopic saliency detection. Inspired by the wide applications of machine learning techniques in 2D saliency detection, we propose to use the machine learning technique for stereoscopic saliency detection in this paper. The contrast features from color, luminance and texture in 2D images are adopted in the proposed framework. For the depth factor, we consider both the depth contrast and depth degree in the proposed learned model. Additionally, the center-bias factor is also used as an input feature for learning the model. Experimental results on\u00a0\u2026", "A fast and efficient method [fast efficient blind (FEB)] for no\u2010reference image quality assessment (IQA) is presented. Two new features, log\u2010energy and variance, are proposed in the spatial domain, which make the IQA algorithm faster and more efficient. FEB obviates the training process of distortion images and subjective opinion scores due to the properties of the new features. The experiment shows that the proposed method outperforms conventional methods in terms of both accuracy and execution speed and is also consistent with the subjective assessment of human beings. Owing to the simplicity of the features proposed, FEB can realise real\u2010time IQA completely.", "Conventional content-based image retrieval (CBIR) systems with the Euclidean distance metric in a high-dimensional visual feature space usually cannot achieve satisfactory performance due to the semantic gap. Relevance feedback (RF) has been introduced as a powerful tool to involve the user in the system to improve the performance of CBIR. Despite the success, an on-line learning task can be tedious and boring for the user. Various schemes have been proposed to exploit the RF log data to further enhance the performance of CBIR. In this paper, we propose a semantic subspace learning (SSL) method to exploit the RF log data with contextual information for an image retrieval task. Different from conventional subspace learning approaches, our method can directly learn a semantic concept subspace from the RF log data with contextual information without using any class label information. We show that the\u00a0\u2026", "From the perspective of linear algebra, the performance of super-resolution reconstruction (SR) depends on the conditioning of the linear system characterizing the degradation model. This is analyzed in the Fourier domain using the perturbation theory. By proposing a new SR error bound in terms of the point spread function (PSF), we reveal that the blur function dominates the condition number (CN) of degradation matrix, and the advantage of non-integer magnification factors (MFs) over the integer ones comes from sampling zero crossings of the DFT of the PSF. We also explore the effect of regularization by integrating it into the SR model, and investigate the influence of the optimal regularization parameter. A tighter error bound is derived given the optimal regularization parameter. Two curves of error bounds vs. MFs are presented, and verified by processing real images. It explains that with proper regularization\u00a0\u2026", "As a result of market, technology and standardization drives, products and services based upon image and video, as well as their delivery, have grown at an explosive rate, with either wired or wireless terminals. Visual signal can be acquired, synthesized, enhanced, watermarked, compressed, transmitted, stored, reconstructed, retrieved, authenticated and presented (eg, displayed or printed) for various applications. Obviously there is need for accurate quality assessment for images and video, because:(a) visual quality has to be evaluated at different stages of a system or process;(b) how quality is gauged plays a central role in various decision making processes and therefore shaping many (if not all) visual signal manipulating algorithms. Traditionally, simple and mathematically defined measures (eg, MSE (mean square error), SNR (signal to noise ratio), PSNR (peak signal to noise ratio), or their relatives) have\u00a0\u2026", "In this paper, we propose an efficient spatial domain deringing algorithm using quadtree (QT) decomposition and block-shift filtering (BSF). The ringing artifacts are located through the QT decomposition of an image down to the 4 \u00d7 4 block size. The blocks with suspicious ringing are then replaced with the weighted average of itself and pixel-by-pixel shifted neighboring blocks. The block shifting range is up to 9 \u00d7 9 and only those shifted blocks resembling the center one are involved in the averaging stage. Experimental results show that the proposed deringing algorithm can effectively suppress the ringing artifact while preserving image edges and textures well, and the method substantially outperforms most of the existing deringing algorithms reported in the literature, both subjectively and objectively.", "This paper proposes a new approach for computing perceptual distortion for visual signal in order to provide an objective measure for perceptual quality at low bit rate MPEG-4 and H.26L coding in typically mobile multimedia communications. The regions with three major perceptually disturbing artefacts, namely, damaged edge, blockiness and ringing, are detected as the basis of assessment. The correlation of the metric with human perception has been demonstrated with low bit rate (24/spl sim/384 Kbps) CIF and QCIF test data.", "With rapid development of 3D scanning technology, 3D point cloud based research and applications are becoming more popular. However, major difficulties are still exist which affect the performance of point cloud utilization. Such difficulties include lack of local adjacency information, non-uniform point density, and control of point numbers. In this paper, we propose a two-step intrinsic and isotropic (I&I) resampling framework to address the challenge of these three major difficulties. The efficient intrinsic control provides geodesic measurement for a point cloud to improve local region detection and avoids redundant geodesic calculation. Then the geometrically-optimized resampling uses a geometric update process to optimize a point cloud into an isotropic or adaptively-isotropic one. The point cloud density can be adjusted to global uniform (isotropic) or local uniform with geometric feature keeping (being\u00a0\u2026", "Many stereoscopic image retargeting (SIR) methods have been developed for automatically and intelligently resizing stereoscopic images and we cannot always rely on time-consuming subjective user studies to validate the performance of different SIR methods. It is therefore required to design reliable objective metrics for SIR quality evaluation. This paper extends our previous 2D aspect ratio similarity (ARS) metric to a stereo 3D version termed as  StereoARS  where the key idea is to investigate into retargeting inconsistency between the original stereo correspondences. Our proposed  StereoARS  operates via two stages: monocular quality estimation and binocular inconsistency detection. In the first stage, monocular quality estimation is performed by applying a modified ARS measure on the left and right views separately to quantify the quality degradation within each monocular view. In the second stage\u00a0\u2026", "Video quality assessment (VQA) is much more challenging than image quality assessment, due to the difficulty of modeling temporal influence among frames. Most of the existing VQA methods usually isolate each moment within the video (i.e., it neglects the sequential nature), leading to a large gap from the subjective perception. Recent research on neuroscience suggests a serially dependent perception (SDP) mechanism in the human visual system (HVS). Namely, the HVS tends to incorporate the recent past visual experience to predict the present perception. Inspired by the SDP, we suggest that the HVS prefers stable and continuous degradations in videos due to their predictability, and exhibits less tolerance to interrupted and unpredictable disturbances. Thus, we introduce a novel serial dependence modeling (SDM) framework for full-reference VQA in this paper. Firstly, the instantaneous degradation is\u00a0\u2026", "In this work, we propose a novel graphic saliency detection method to detect visually salient objects in images rendered from 3D geometry models. Different from existing graphic saliency detection methods, which estimate saliency based on pixel-level contrast, the proposed method detects salient objects by computing object-level contrast. Given a rendered image, the proposed method first extracts dominant colors from each object, and represents each object with a dominant color descriptor (DCD). Saliency of each object is then calculated by measuring the contrast between the DCD of the object and the DCDs of its surrounding objects. We also design a new iterative suppression operator to enhance the saliency result. Compared with existing graphic saliency detection methods, the proposed method can obtain much better performance in salient object detection. We further apply the proposed method to\u00a0\u2026", "In this work, we decompose an input video clip into multiple smaller intervals, measure the quality of each interval separately, and apply a fusion approach to integrating these scores into a final one. To give more details, an input video clip is first decomposed into smaller units along the temporal domain, called the temporal decomposition units (TDUs). Next, for each TDU that consists of a small number of frames, we adopt a proper video quality metric (specifically, the MOVIE index in this work) to compute the quality scores of all frames and, based on the sociological findings, choose the worst scores of TDUs for data fusion. Finally, a regression approach is used to fuse selected worst scores from all TDUs to get the ultimate quality score of the input video as a whole. We conduct extensive experiments on the LIVE video database, and show that the proposed approach indeed improves MOVIE and is also competitive\u00a0\u2026", "A Method for encoding a picture with a first region and a second region is described wherein the first region is encoded according to a first encoding scheme, the second region is encoded according to a second encoding scheme and the second encoding scheme is carried out taking a parameter representing the visual perception of the second region into account.", "In H.264/advanced video coding (AVC), lossless coding and lossy coding share the same entropy coding module. However, the entropy coders in the H.264/AVC standard were original designed for lossy video coding and do not yield adequate performance for lossless video coding. In this paper, we analyze the problem with the current lossless coding scheme and propose a mode-dependent template (MD-template) based method for intra lossless coding. By exploring the statistical redundancy of the prediction residual in the H.264/AVC intra prediction modes, more zero coefficients are generated. By designing a new scan order for each MD-template, the scanned coefficients sequence fits the H.264/AVC entropy coders better. A fast implementation algorithm is also designed. With little computation increase, experimental results confirm that the proposed fast algorithm achieves about 7.2% bit saving compared\u00a0\u2026", "Objective video quality assessment (VQA) is the use of computational models to predict the video quality in line with the perception of the human visual system (HVS). It is challenging due to the underlying complexity, and the relatively limited understanding of the HVS and its intricate mechanisms. There are two important issues regarding VQA: (a) the temporal factors apart from the spatial ones also need to be considered, (b) the contribution of each factor and their interaction to the overall video quality needs to be determined. In this paper, we attempt to tackle the first issue by utilizing the variation of spatial quality along the temporal axis. The second issue is addressed by the use of machine learning; we believe this to be more convincing since the relationship between the factors and the overall quality is derived via training with substantial ground truth (i.e. subjective scores). Experiments conducted using two\u00a0\u2026", "In this paper, we investigate into a new lossless compression framework for video. We are to exploit fuller scale of temporal redundancy, and propose an adaptive optimal compression plane (OCP) determination process to be used with the standard intra-frame lossless predictive coding scheme. The essence of the scheme is to perform the prediction on the plane formed by two axes (among X, Y and T) corresponding to signal correlation evaluation, which enables better prediction (therefore better compression). In spite of the simplicity of the proposed method, simulation results show that the rate saving of the resultant system is 27.0% on average with respect to JPEG-LS, for a large number of videos with different visual content.", "Objective video quality metrics reported in the literature have so far been focused on TV signals with large frame sizes, full TV frame rates, and high compressed bit rates. The aim of this paper is to provide an analysis of 4 objective video quality metrics tested on low bit rate videos with small frame sizes, various frame rates, and low bit rates. The video quality metrics used were the NTIA video quality metric, modified Watson's digital video quality metric, video structural similarity metric, and the perceptual video quality metric. The test videos consist of H.264 compressed videos with CIF and QCIF frame sizes, at various bit rates (24 kbps - 384 kbps) and frame rates (7.5 fps - 30 fps). Here, results and analysis derived from the comparison of the metrics are provided.", "This paper presents a new objective visual quality metric based on the measurement of error spread and isotropic local contrast of the image. It is shown that error spread is a reasonable measure for perceived visual quality while the image contrast is a content indicator for different images. The performance of the proposed method is shown to be superior compared to the 9 proponents in VQEG Phase I test dataset and another recently reported method in terms of prediction accuracy, monotonicity and consistency. In addition, the new method is very simple and does not need to tune any parameters.", "Just-noticeable distortion (JND) based on human visual characteristics provides a good means to reflect the tolerance of image distortion with respect to human observers, for both data compression and quality measurement. In this paper, various existing JND models are surveyed and the typical models are benchmarked based on a uniform criterion. A new model is then developed and tested for different types of images. Experiments and the associated subjective tests show the improved performance of the proposed scheme over the existing models for luminance adaptation (especially in dark regions) and masking effect in edge regions.", "In this paper, we propose a new video post-processing algorithm for reducing coding artifacts. The algorithm is based on regularized image restoration technique, in which a nonconvex regularization function is used to enforce smoothness constraints on the spatial image pixels for suppressing coding artifacts while preserving important image attribute. Temporal regularization is carried out along the motion trajectories. We apply half-quadratic regularization and piecewise constraint strategy to simplify the optimization problem. Explicit filtering formulae are obtained and a recursive filtering algorithm is developed. Experimental results demonstrate that the proposed method can significantly improve both subjective and objective quality of MPEG-4 coded video sequences in comparison with the postprocessing technique in MPEG-4.", "A real-time H.263 video codec system is presented is this paper. The TMS320C80 which consists of four parallel DSPs is used in the system. A few strategies are used to reduce the computation complexity and hence increase the frame rate. The parallel implementation of the H.263 algorithm on the parallel DSPs is described and the performance of the real-time codec is discussed.", "In recent video-based point cloud compression (V-PCC), 3D point clouds are projected onto 2D images and compressed by High-Efficiency Video Coding (HEVC). However, HEVC was originally designed for natural visual signals, which is a suboptimal framework for point clouds. Therefore, there are still problems in geometry information compression in V-PCC: (1) The distortion based on the sum of squared error (SSE) in the existing rate-distortion optimization (RDO) is inconsistent with the geometric quality measurement; (2) The existing prediction cannot explore the fixed relationship between the corresponding far layer and near layer depth, which means that the far layer depth can be always not less than the corresponding near layer depth. In this paper, we present an efficient geometry surface coding (EGSC) method for V-PCC to address the problems. Firstly, an error projection (EP) model is designed to\u00a0\u2026", "Due to the high dimensionality of point cloud data and the irregularity and complexity of its geometric structure, effective attribute compression remains a very challenging task. Many recent efforts have focused on transforming point clouds into images and leveraging existing sophisticated image/video codecs to improve attribute coding efficiency. However, how to synthesize coherent and correlation-preserving attribute images is still inadequately addressed by existing studies, which are hindering the exertion of the merits of well-developed compression infrastructure. In this paper, we present a novel image synthesis method for effective point cloud attribute compression. Firstly, the proposed scheme segments a given point cloud into a collection of fine-grained patches by performing geometric structure analysis using heat kernel signature feature descriptor and complex points; Secondly, we transform the obtained\u00a0\u2026", "Image quality assessment (IQA) plays a central role in many image processing algorithms and systems. Although many popular IQA models achieves high performance on existing released databases, they are still not well accepted in practical applications due to the not-always satisfactory accuracy on real-world data and situations. In this paper, we revisit the IQA research, and point out an ignored but interesting problem in IQA: the coarse-grained (i.e., when quality variation is sufficiently big, as the setting of most IQA databases up to date) statistical results evaluated on existing databases mask the fine-grained differentiation. Accordingly, we present a survey on image quality assessment from a new perspective: fine-grained image quality assessment (FG-IQA). Recent FG-IQA research on five major kinds of images is introduced, and some popular IQA methods are analyzed from FG-IQA perspective. The potential\u00a0\u2026", "The recent advances of hardware technology have made the intelligent analysis equipped at the front-end with deep learning more prevailing and practical. To better enable the intelligent sensing at the front-end, instead of compressing and transmitting visual signals or the ultimately utilized top-layer deep learning features, we propose to compactly represent and convey the intermediate-layer deep learning features with high generalization capability, to facilitate the collaborating approach between front and cloud ends. This strategy enables a good balance among the computational load, transmission load and the generalization ability for cloud servers when deploying the deep neural networks for large scale cloud based visual analysis. Moreover, the presented strategy also makes the standardization of deep feature coding more feasible and promising, as a series of tasks can simultaneously benefit from the transmitted intermediate layer features. We also present the results for evaluations of both lossless and lossy deep feature compression, which provide meaningful investigations and baselines for future research and standardization activities.", "Whilst affective responses to various forms and genres of multimedia content have been well researched, precious few studies have investigated the combined impact that multimedia system parameters and human factors have on affect. Consequently, in this paper we explore the role that two primordial dimensions of human factors - personality and culture - in conjunction with system factors - frame rate, resolution, and bit rate - have on user affect and enjoyment of multimedia presentations. To this end, a two-site, cross-cultural study was undertaken, the results of which produced three predictve models. Personality and Culture traits were shown statistically to represent 5.6% of the variance in positive affect, 13.6% in negative affect and 9.3% in enjoyment. The correlation between affect and enjoyment, was significant. Predictive modeling incorporating human factors showed about 8%, 7% and 9% improvement in predicting positive affect, negative affect and enjoyment respectively when compared to models trained only on system factors. Results and analysis indicate the significant role played by human factors in influencing affect that users experience while watching multimedia.", "In-loop filters have been widely utilized in latest video coding standards to improve the video coding efficiency by reducing compression artifacts. However, existing in-loop filters only utilize image local correlations, leading to limited performance improvement. In this paper, we explore a novel adaptive in-loop filter by means of the nonlocal similar content to improve the quality of reconstructed video frames. In our proposed filter, the input video frame is first divided into different image patch groups based on their similarity, and then a soft-thresholding method is applied to the singular values of matrices composed of image patches in every group. Since compression noise is highly correlated with image content, we propose a group-wise threshold estimation method based on image statistical characteristics, coding modes and quantization parameters. To ensure the filtering efficiency, slice level control flags are\u00a0\u2026", "Reduced-reference (RR) image quality assessment (IQA) method aims to accurately measure quality with part of the reference data. The challenge for RR IQA is how to effectively represent the visual content of an image with limited data for quality measurement. Inspired by the orientation selectivity (OS) mechanism in the primary visual cortex, we introduce an OS based visual pattern (OSVP) to extract visual content for RR IQA. The OS arises from the arrangement of the excitatory and inhibitory interactions among connected cortical neurons. Inspired by this, we investigate the correlation among neighbor pixels, and propose the OSVP to represent the visual content of an image. Then, the quality degradation is measured as the changes of OSVP, and a novel RR IQA model is proposed. Experimental results demonstrate that the OSVP based RR IQA model uses limited reference data (9 values) and performs highly\u00a0\u2026", "Quality assessment of digital compound images is a less investigated research topic. In this paper, we present a study for subjective quality assessment of Digital Compound Images (DCIs), and investigate whether existing Image Quality Assessment (IQA) methods are effective to evaluate the quality of distorted DCIs. A new Compound Image Quality Assessment Database (CIQAD) is constructed, including 24 reference DCIs and their 576 distorted versions. The Paired Comparison (PC) method is employed for the subjective viewing, and the Hodgerank decomposition is adopted to generate incomplete but balanced comparison pairs, so as to reduce the execution time while guaranteeing the reliability of the results. In our experiment, correlation of 14 existing IQA methods with the obtained Mean Opinion Score (MOS) values on the CIQAD is calculated, which indicates that the 14 IQA methods are not consistent with\u00a0\u2026", "A model of visual masking, which reveals the visible threshold of human perception, is useful in perceptual based image/video processing. The existing visual masking formulation, which mainly considers luminance contrast, cannot accurately estimate the visible threshold. Recent researches indicate that human perception is highly adaptive to extract orderly structures and is insensitive to disorderly structures. Therefore, we suggest that the structural characteristic is another determining factor for visual masking, and deduce a novel visual masking function based on structural uncertainty. Experimental results demonstrate that the proposed model is more consistent with human perception than the existing visual masking model.", "The nine papers in this special issue report the latest findings in subjective and objective methodologies for audio and visual signal processing.", "In this paper, a perceptual multiview video coding scheme is proposed, based on the synthesized Just Noticeable Distortion (JND) maps. In JND-based perceptual video coding, the residues after intra or inter prediction are tuned according to the corresponding JND thresholds to save the bits without affecting the perceptual quality. In our scheme, to reduce the computational cost of generating the multiview JND maps, only the JND maps of some anchor views are calculated directly. These maps are then used to synthesize the JND maps of other views via the block-based Depth Image Based Rendering (DIBR) method, which can be more than 30 times faster than direct computation with reasonable error. Experimental results show that the proposed scheme can improve the perceptual performance of the JMVC by more than 1 dB in terms of the Peak Signal Perceptual Noise Ratio (PSPNR).", "It is well known that the human visual system (HVS) cannot sense all changes in an image/video due to its underlying physiological and psychological mechanisms. We propose a complete masking estimation model for image/video in this paper. In our model, a very important mechanism of the HVS, visual attention, is incorporated to the existing just noticeable difference (JND) estimation models. A formula for evaluating the weighting map is adopted to deal with the effect of visual attention using foveation technique. Experimental results with noise shaping and subjective viewing confirm the improved accuracy of the proposed model over the existing relevant models.", "Both visual sensitivity and spatial selectivity determine the overall visibility threshold at each pixel in an image, according to the physiological and psychological evidence towards the human visual system (HVS). Visual sensitivity can be decided by an existing estimator for just-noticeable-distortion (JND). A computational model is proposed for incorporating a selectivity measure into the JND profile so that more effective noise shaping is possible in various applications. Experimental results with noise-embedded video sequences confirm that the introduction of spatial selectivity enhances the performance of the JND profile used in noise shaping.", "We explore a new perceptually-adaptive video coding (PVC) scheme in the motion-compensated prediction loop for hybrid video compression, aiming at better perceptual coding quality and operational efficiency. A new just noticeable distortion (JND) estimator for color images/video is first devised in image-domain based on our introduced nonlinear additivity model for masking (NAMM). Secondly, the image-domain JND profile is incorporated into hybrid video encoding via the JND-adaptive motion estimation (ME) and residue filtering. The scheme works with any prevalent video coding standards and various ME strategies. As an example of implementation, it is applied to the MPEG-2 TM5 coder and demonstrated to achieve average improvement of over 18% in ME efficiency, 0.6 dB in perceptual coding quality and most remarkably, 0.17 dB in the objective coding quality measure (PSNR).", null, "We propose IntegratedPIFu, a new pixel-aligned implicit model that builds on the foundation set by PIFuHD. IntegratedPIFu shows how depth and human parsing information can be predicted and capitalized upon in a pixel-aligned implicit model. In addition, IntegratedPIFu introduces depth-oriented sampling, a novel training scheme that improve any pixel-aligned implicit model\u2019s ability to reconstruct important human features without noisy artefacts. Lastly, IntegratedPIFu presents a new architecture that, despite using less model parameters than PIFuHD, is able to improves the structural correctness of reconstructed meshes. Our results show that IntegratedPIFu significantly outperforms existing state-of-the-arts methods on single-view human reconstruction. We provide the code in our supplementary materials. Our code is available at https://github.com/kcyt/IntegratedPIFu.", "Viewing stereoscopic images sometimes causes viewers to feel inconvenience, which is called 3D visual discomfort. Excessive horizontal disparity, misalignment between the left and right views, or depth cues conflicts are some of the important factors involved in 3D visual discomfort. The ability to estimate the degree of 3D visual discomfort can be used to improve the 3D display systems and provide acceptable binocular visual quality. Most of the existing visual discomfort prediction (VDP) approaches extract hand-crafted features based on perceptual modeling and statistical analysis of disparities. We have proposed a simple yet effective VDP model based on unsupervised learning of sparse features which are highly predictive of subjective discomfort levels. These features are extracted from the aggregation of left and right disparity maps. This aggregation effectively highlights the areas with sudden changes and\u00a0\u2026", "With the availability of commodity depth sensors and depth sensing capabilities on mobile devices, there is a need to develop memory efficient and computationally cheap applications. 3D feature descriptor based keypoint matching is the very first step in various computer vision and robotic applications, and is also one of the most memory and computationally demanding steps. In this work, we apply Principal Component Analysis (PCA) to reduce the dimensionality of 3D feature descriptors and show that they retain their descriptiveness, while drastically decreasing their memory and computational requirements. We apply this scalable PCA based dimensionality reduction technique on three state-of-the-art 3D feature descriptors and quantitatively evaluate their performance with varying dimensionality. Experimental results on publicly available Bologna Kinect dataset show that the proposed method reduces the\u00a0\u2026", "In this study, we investigate the robustness of existing visual tracking algorithms with quality-degraded video. A video database including the reference video sequences and their distorted versions is created as the benchmark for robustness analysis of visual tracking algorithms. Ten existing visual tracking algorithms are used to conduct the experiments for robustness analysis based on the benchmark. Our initial investigation demonstrates that all the existing visual tracking algorithms cannot obtain the robust visual tracking results for quality-degraded video sequences. The experimental results in this study show that there is still much room for the design of robust visual tracking algorithms.", "Quality evaluation for multimedia content is a basic and challenging problem in the field of multimedia processing, as well as various practical applications such as process evaluation, implementation, optimization, testing, and monitoring. Generally, the quality of multimedia content is affected by various factors such as acquisition, processing, compression, transmission, output interface, decoding, and other systems [1\u20133]. The perceived quality of impaired multimedia content depends on various factors: the individual interests, quality expectations, and viewing experiences of the user; output interface type and properties; and so on [2\u20135]. Since the Human Visual System (HVS) and the Human Auditory System (HAS) are the ultimate receiver and interpreter of the content, subjective measurement represents the most accurate method and thus serves as the benchmark for objective quality assessment [2\u20134]. Subjective experiments require a number of subjects to watch and/or listen to the test material and rate its quality. The Mean Opinion Score (MOS) is used for the average rating over all subjects for each piece of multimedia content. A detailed discussion of subjective measurements can be found in Chapter 6. Although subjective experiments are accurate for the quality evaluation of multimedia content, they suffer from certain important drawbacks and limitations\u2013they are time consuming, laborious, expensive, and so on [2]. Therefore, many objective metrics have been proposed to evaluate the quality of multimedia content in past decades. Objective metrics try to approximate human perceptions of multimedia quality. Compared with subjective\u00a0\u2026", "In this paper, we propose a novel dense correspondence based prediction approach to reduce the inter-image redundancy for image set compression. Unlike previous methods, we manage to utilize the dense correspondence to predict and parameterize the inter-image relation and then reconstruct a new reference for the subsequent HEVC inter-prediction and encoding. Comparing to relevant state-of-the-art feature-based methods, our method is able to locally approximate the inter-image relation and thus more robust to complex local variations. Experimental results show that our proposed approach achieves better coding gains when the local variations are dominant.", "This paper presents a no-reference (NR) image sharpness algorithm based on natural scene statistics (NSS) in discrete cosine transform (DCT) domain. It relies on the assumption that natural images possess certain statistics that will change with blur distortion. We propose a new image representation, normalized discrete cosine transform (NDCT) coefficients. Both the theoretical analysis and experimental tests have proven that the statistics of NDCT coefficients are highly correlated with the human judgments of image quality. To represent the statistics of natural images, a model is built with a small set of natural images. We define the perceptual sharpness index on normalized discrete cosine transform coefficients (NDCT-PSI) as the difference between the NSS model and the tested image. The NDCT-PSI outperforms recent relevant state-of-the-art algorithms as evaluated on a subject-rated image database. The\u00a0\u2026", "Machine learning (ML) techniques are widely used in recent no-reference visual quality assessment (NR-VQA) metrics by training on subjective image quality databases. In these metrics, the optimization function is constructed based on L 2  norm of the distance between subjective image quality and predicted image quality. There are two problems in these L 2  norm based methods: (1) human's opinion on subjective image quality rating is not reliable at fine-scale level. A small difference between subjective image qualities represented by mean opinion scores (MOSs) of two images may not truly reflect the real quality difference between these two images, but acts as noise. The optimization process should avoid such noise. (2) Generally, human's opinion on pairwise comparison (PC) for image quality is more reliable and believable than MOS. The importance of PC is ignored during the optimization process of\u00a0\u2026", "Modern image quality assessment (IQA) indices, e.g. SSIM and FSIM, are proved to be effective for some image distortion types. However, they do not exploit the characteristics of the human visual system (HVS) explicitly. In this work, we investigate a method to incorporate the human visual saliency (VS) model in these full-reference indices, and call the resulting indices SSIM VS  and FSIM VS , respectively. First, we decompose an image into non-overlapping patches, calculate visual saliency, and assign a parameter ranging from 0 and 1 to each patch. Then, the local SSIM or FSIM values of the patches are weighed by the said parameter. Finally, the weighed similarity of all patches are integrated into one single index for the whole image. Experimental results are given to demonstrate the improved performance of the proposed VS-enhanced indices.", "All existing video coding standards consider a video as a temporal (along T-axis) collection of two dimensional pictures (formed by XY axes) and compress them by exploiting spatial and temporal redundancy in the pictures. A recent optimal compression plane (OCP) determination technique shows that better compression can be achieved by relaxing the physical meaning of axes by exploring information redundancy in a fuller extent where a video is considered as a 3D data cube. Spatial and temporal dimensions are determined based on the statistical redundancy along each axis. Treating a video as a 3D data cube revolutionizes the traditional video features such as background, motion, object, zooming, panning, etc. In this paper we apply dynamic background modeling to the OCP plane to exploit the newly generated background in the video for further improving the coding performance. The experimental\u00a0\u2026", "Contrast masking (CM) on edge and textured regions have to be distinguished since distortions on edge regions are easier to be noticed than that on textured regions. Therefore, how to efficiently estimate the CM on edge and textured regions of an image is a key issue for accurate JND (Just Noticeable Difference) estimation. An enhanced image domain JND estimator is devised in this paper with new model for CM. We use the total variation method to obtain a structural image (which contains edge information) and a textural image (which contains texture information) from the input image, and then evaluate the CM for the two images separately rather than the whole image, and hence edge and texture are better distinguished and the under-estimation of JND on textured regions can be effectively avoided. Experimental results of subjective viewing confirm that the proposed model is capable of determining more\u00a0\u2026", "H.264/AVC FRExt (Fidelity Range Extensions) and Motion JPEG2000 are the latest inter-frame and intra-frame video coding standards, respectively. It is well known that an inter-frame method achieves higher coding efficiency compared with an intra-frame one, and the Motion JPEG2000 has been selected for digital cinema compression. In this paper, we attempt to compare these two different schemes with theoretical and experimental analysis for super-HD (high definition) visual signals. One additional contribution of the paper is that the impact of block partition, motion search range and skipped block size for inter-frame coding is discussed. Based on the analysis, we extend the standard H.264/AVC FRExt by using larger block size and search range. The experimental results show that this extension leads to higher coding efficiency and makes the H.264/AVC FRExt more suitable for super-HD video coding.", "A fast algorithm to extract the visual saliency map from digital video is proposed in this research. Since digital video is stored and/or transmitted in coded form, we utilize the motion vector field (MVF) for foreground/background separation and apply an image based contrast model for further performance improvement. The performance of the proposed algorithm is demonstrated by experimental results.", "In this paper, we present our work on developing perceptual quality metrics for low and very low bit-rate videos. At current stage, we've finished a number of subjective viewing experiments, and several video content-based models are built based on the experiments to measure the perceptual quality of frame-dropped and blurred video sequences. Experimental results show that the models can achieve very good performances.", "This paper addresses the issue of incorporating contrast sensitivity function (CSF) into just-noticeable distortion (JND) estimation for image pixels in spatial domain. Based upon our earlier work on DCT-subband JND estimation, the resultant new pixel-wise JND model fully incorporates the spatial CSF, in addition to considerations of luminance adaptation and local contrast masking effect. Various experiments confirm the improved accuracy of the proposed model over the existing relevant JND estimators", "A real-time full-duplex videoconferencing system using parallel DSPs is described. Effective strategies for H.263 video coding have been developed to improve the efficiency of the system. Further speed-up is achieved by utilising multiple DSP processors. The video codec is integrated with a G.723.1 speech codec to form an H.324 compliant bitstream, using H.223 multiplex and H.245 control protocols.", "Recent learning-based video quality assessment (VQA) algorithms are expensive to implement due to the cost of data collection of human quality opinions, and are less robust across various scenarios due to the biases of these opinions. This motivates our exploration on opinion-unaware (a.k.a zero-shot) VQA approaches. Existing approaches only considers low-level naturalness in spatial or temporal domain, without considering impacts from high-level semantics. In this work, we introduce an explicit semantic affinity index for opinion-unaware VQA using text-prompts in the contrastive language-image pre-training (CLIP) model. We also aggregate it with different traditional low-level naturalness indexes through gaussian normalization and sigmoid rescaling strategies. Composed of aggregated semantic and technical metrics, the proposed Blind Unified Opinion-Unaware Video Quality Index via Semantic and Technical Metric Aggregation (BUONA-VISTA) outperforms existing opinion-unaware VQA methods by at least 20% improvements, and is more robust than opinion-aware approaches.", "The rapid increase in user-generated-content (UGC) videos calls for the development of effective video quality assessment (VQA) algorithms. However, the objective of the UGC-VQA problem is still ambiguous and can be viewed from two perspectives: the technical perspective, measuring the perception of distortions; and the aesthetic perspective, which relates to preference and recommendation on contents. To understand how these two perspectives affect overall subjective opinions in UGC-VQA, we conduct a large-scale subjective study to collect human quality opinions on overall quality of videos as well as perceptions from aesthetic and technical perspectives. The collected Disentangled Video Quality Database (DIVIDE-3k) confirms that human quality opinions on UGC videos are universally and inevitably affected by both aesthetic and technical perspectives. In light of this, we propose the Disentangled Objective Video Quality Evaluator (DOVER) to learn the quality of UGC videos based on the two perspectives. The DOVER proves state-of-the-art performance in UGC-VQA under very high efficiency. With perspective opinions in DIVIDE-3k, we further propose DOVER++, the first approach to provide reliable clear-cut quality evaluations from a single aesthetic or technical perspective.", "Recent image inpainting methods have achieved remarkable improvements by using generative adversarial networks (GAN). Most of them have been designed to produce plausible results from high-level semantic features using only high-resolution (HR) supervision. However, because abundant details are lost in large holes, it is difficult to simultaneously synthesize details while preserving structural coherence in HR space. Besides, the correlations between the inside and outside of the missing region play a critical role in transferring relevant known information to generate semantic-coherent textures, especially in patch matching-based methods. In this work, we present SRInpaintor which inherits the merits of super-resolution (SR) and transformer for high-fidelity image inpainting. The SRInpaintor starts from global structure reasoning with low-resolution (LR) input and progressively refines the local textures in HR\u00a0\u2026", "Blind image quality assessment (BIQA) aims to evaluate the perceptual quality of image with no pristine image for comparison, which attracts extensive attention and is of wide applications. Research on human visual system (HVS) indicates visual perception is classically modeled as a hierarchical process. Meanwhile, empirical evidence shows that different levels of distortion generate individual degradation on hierarchical features. Although previous BIQA methods exploited different levels of feature, the degradations and correlations on hierarchical features are not detailedly analyzed. In this paper, hierarchical degradation is systematically analyzed by quantitative and qualitative experiments using interpretable hierarchical features. Additionally, as there exist strong relationships, these features cannot be simply concatenated for quality prediction. Inspired by the Bayesian brain theory, correlations among\u00a0\u2026", "The image quality assessment (IQA) technique, which aims to perform coherently with subjective perception, is useful in quality-orientated image processing systems. In this paper, we suggest to take the saliency change into account for reduced reference (RR) IQA model. Generally, a saliency region will attract more attention, and our human vision is more sensitive to quality degradation on such region. Inspired by this, saliency values are firstly used to highlight these sensitive regions, and a local saliency weighted histogram (LSWH) based on visual orientation pattern is generated for visual feature extraction. Next, strong distortion may change the saliency from the reference to the distorted images. Thus, the saliency of each visual orientation pattern is measured, and a global saliency based histogram (GSBH) is created. Finally, by combining the LSWH and GSBH, a novel IQA model for reduced reference is\u00a0\u2026", "For robust visual tracking, appearance modeling should be able to well separate the object from its backgrounds, while accurately adapt to its appearance variations. However, most of the existing tracking methods mainly focus on one of the two aspects; or design two different modules to combine them with the price of double computational cost. In this paper, by using pairwise metric learning, we present a novel appearance model for robust visual tracking. Specifically, visual tracking is viewed as a pairwise regression problem, and extreme learning machine (ELM) is utilized to construct the pairwise regression framework. In ELM-based pairwise training, two constraints are enforced: the target observations must have different regression outputs from those background ones; while the various target observations during tracking should have approximate regression outputs. Thus, the discriminative and generative\u00a0\u2026", "A subjective assessment method is described to determine picture quality levels in the supra-threshold region for processed images, with reference to their original counterparts, based on just-noticeable difference (JND) detection experiment. It has been found that the range of JND levels is dependent on picture contents and can be predicted as a function of texture masking factor computed in the pixel domain. The experimental data obtained also reveal that relationship of JND levels in the supra-threshold region and the MSE (mean squared error) can be approximated by a linear function whose slope is modeled as a function of edge and texture contrast masking factors. The model is devised to predict JND levels which provide subjective picture quality rating discernible by human viewers and can be used for visual quality regulated image/video coding, as well as evaluating the capacity of existing objective\u00a0\u2026", "Purpose We hypothesize that patients with primary angle closure (PAC) have common significant facial characteristics that set them apart from an age-matched, sex-matched, and race-matched control population. The primary objective of this study was to test whether a 3-dimensional (3D) camera could pick up these differences in order to differentiate PAC patients from controls. Patients A total of 55 patients with PAC and 38 controls were included in the study. Materials and Methods Cases and controls had their facial photographs taken using a 3-dimensional digital camera (3dMdFace System). The facial features in the captured photographs were analyzed using a computer software (Neural Network Toolbox). A regression formula was devised to show whether these facial parameters could be used to distinguish between patients and controls. Results Thirteen facial parameters were measured, and using a\u00a0\u2026", "Recent advances in image-based localization have made it possible to estimate the camera pose relative to 3D point clouds. In a multimedia cloud computing paradigm, large-scale 3D point clouds can be stored on the server side, and this enables us to implement image-based localization applications on mobile devices toward many cloud-media applications and services. When scaling the 3D point cloud database up to a world-wide level, the prohibitive memory footprint of such large-scale point clouds is a heavy burden to the server side. In this paper, we present a method to compute a compact subset of a 3D point cloud for image-based localization while achieving acceptable localization performance. In our method, we consider both coverage of database images and emphasis on 3D points of high spatial significance, and we show that our method outperforms the state-of-the-art methods on three public\u00a0\u2026", "Existing operational rate-distortion shape coding aims at finding a polygon which can be encoded with the lowest bit rate under a given upper bound on the edge error. However, this upper bound may cause noticeable errors. Therefore, we add an \u2113 2 -norm error regularization term to the objective function, and seek the globally optimal solution using a shortest path algorithm for a weighted directed acyclic graph. Experiments confirm the accuracy and robustness of our method.", "3D triangle meshes are widely used in representation of graphic data, and they are subject to various visual distortions during geometry processing and transmission. In this study, we propose a novel objective quality assessment method for 3D meshes based on curvature information. According to characteristics of the human visual system (HVS), two new components including visual masking modulation and saturation effect modulation are designed for the proposed method. Besides, inspired by the fact that the HVS is sensitive to structural information, we compute the structure distortion for 3D meshes. We test the performance of the proposed method on three publicly available databases of 3D mesh quality evaluation, and compare the performance of the proposed method with the relevant state-of-the-art methods. Experimental results demonstrate that the proposed method can predict consistent results in terms\u00a0\u2026", "Just noticeable difference (JND) reveals the minimum visible threshold of the human visual system (HVS), which is useful in visual redundancy reduction. Existing JND models estimate the visible threshold with luminance adaptation and contrast masking. As a result, the smooth and edge regions are effectively estimated, while the disorderly texture regions are always underestimated. The disorderly texture regions possess a large amount of disorderly structures and the HVS cannot fully perceive them. Therefore, in this work, we suggest to consider the disorder degree of structure for JND threshold estimation. According to the correlation among neighboring pixels, the uncertain information is extracted, and the disorder degree of structure is computed, which we called structural uncertainty. Then, taking the effect of background luminance, contrast, and structural uncertainty into account, a novel JND model is\u00a0\u2026", "The visual quality assessment (VQA) becomes prevailing in the studies of image and video coding. It assesses the quality of image or video more accurately than mean square error (MSE) with respect to the human visual system (HVS). Toward perceptual video coding, MSE is weighted spatially and temporally to simulate the HVS response to visual signal in this paper. Firstly, the image content is depicted by edge strength to compose spatial weighting factors. Secondly, the motion strength calculated from motion vector of each block gives temporal weighting factors. Thirdly, the motion trajectory based saliency map for video signal is integrated as another weighting factor of MSE. The proposed VQM not only efficiently model HVS but also relate to quantization parameter (QP) capable of guiding perceptual video coding. A perceptual rate distortion optimization (RDO) is established on the proposed VQM. The\u00a0\u2026", "Quality assessment is not only essential on its own for testing, optimizing, benchmarking, monitoring and inspecting related systems and services, but also plays an essential role in the design of virtually all visual signal processing and communication algorithms, as well as various related decision making processes. In the paper, we provide an overview of the quality assessment approaches for traditional visual signals, as well as the newly emerged ones, which covers the subjective quality evaluation and objective quality metrics of scalable and mobile videos, high dynamic range (HDR) images, image segmentation results, 3D images/videos, and retargeted images. Also the challenges for designing effective quality metrics and corresponding applications are discussed.", "This chapter introduces basic phenomena, concepts, experiments and technological development for selective visual attention. It also provides relevant references to facilitate model development. The basic phenomena and concepts are given in the first section. Section 1.2 introduces the types of selective visual attention, such as pre-attention and attention, bottom-up attention and top-down attention, attention in parallel and serial processing, and overt and covert attention. Then two phenomena related visual attention models - change blindness and inhibition of return - are discussed in Section 1.3. There is also an overview of different phases of research and development in the related areas, followed by a discussion on the scope of the various chapters in this book.", "Proceedings of the 13th Pacific-Rim conference on Advances in Multimedia Information \nProcessing | Guide Proceedings ACM Digital Library home ACM home Google, Inc. (search) \nAdvanced Search Browse About Sign in Register Advanced Search Journals Magazines \nProceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch \nAdvanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsPCM'12 \nABSTRACT No abstract available. Comments Login options Check if you have access \nthrough your login credentials or your institution to get full access on this article. Sign in Full \nAccess Get this Publication Information Contributors Published in Guide Proceedings cover \nimage PCM'12: Proceedings of the 13th Pacific-Rim conference on Advances in Multimedia \nInformation Processing December 2012 879 pages ISBN:9783642347771 Editors: \u2026", "In this paper, the problem of concealing missing image/video blocks is casted into a framework of Bayesian estimation. The conditional expectation of the missing block vector is taken over a pilot vector of correctly decoded pixels near the missing block. Multiple observations of the missing vector and pilot vectors obtained in a nonlocal manner are used to approximate the expectation. We design a multiscale estimation approach with DCT pyramid to improve estimation efficiency. The DC image of the missing block is recovered first, and then more details related to high frequency AC coefficients are recovered successively. The algorithm is found to be quite competitive among state-of-the-art, and more substantial improvement over existing algorithms on image with heavy loss rate/large block size is observed in our experiments.", "This paper derives the formulae for defocus blur parameter from a single image, based upon the line spread function (LSF). To achieve high accuracy and robustness, the over determining strategies are adopted: 1) a number of LSFs on one edge are extracted; 2) more edges in the images are used. The trust-region method is then employed to obtain the optimal estimation of blur parameter. The experimental results have demonstrated the effectiveness of the proposed method. It can be used for blind image quality evaluation in vision-based applications.", "We propose a computationally simple yet effective image/video deblocking algorithm based on block level postfiltering. Each image block is shifted in the range of half of the block dimension to find the candidate blocks for smoothing. The Mean Absolute Error (MAE) between the central image block and the block indicated by a shifted window is used as a criterion to determine if the corresponding shifted block is used in smoothing. The threshold of the MAE is defined with analyzing the quantization table used in the codec. The proposed algorithm successfully removes the blockiness while preserving the edge features. Another merit of the proposed algorithm is that it is applicable to almost all bit rates without the requirement of detection on blockiness position or estimation on blockiness severity, thus does not impose computational overhead. The experimental results demonstrate that in a wide range of bit rates, the\u00a0\u2026", "In this paper, we present a perceptual quality metric for evaluating image and video quality. The metric is based on a fuzzy inference system with Takagi-Sugeno's inference engine. Three visually important factors including visual masking error, blurring distortion and contrast distortion are used as the inputs to the inference system. Through learning algorithm with the subjective test data, the inference system can predict video quality with high accuracy and monotonicity.", "Affine motion model is widely used in motion segmentation. Accurately estimating and evaluating affine parameters are two key problems in such kind of approaches. This paper tries to address these issues by presenting an effective video object segmentation method based on affine motion similarity measure. The image is firstly segmented into irregularly-shaped intensity homogenous regions. Then, relatively reliable affine parameters for each region are estimated by a robust motion estimator according to the individual coordinate system of each region. Finally, a new motion similarity measure and merge process is applied to obtain meaningful objects. Experimental results demonstrate the effectiveness of the proposed method.", "In this paper, a novel multiple motion object segmentation scheme based on homogenous region merging is presented. This scheme is based on utilization of a robust multiresolution motion estimator that can provide good motion estimates from irregularly-shaped regions. In addition, a novel region-merging criterion and an iterative region-merging process have been proposed. Experimental results demonstrate the effectiveness of the proposed method.", "Recently, person re-identification (re-ID) has gained much attention for its extensive applications in public security. In order to mine more detailed information, some researches have attempted to extract local representations from important regions through human semantic parsing. However, some valuable visual cues beyond human body, such as backpacks and handbags, which can provide complementary information for recognition, may be misclassified as background noise. In addition, when meeting low-quality captured images or severe occlusions, the semantic regions generated by parsing model are not accurate enough, even resulting in disturbance. In this paper, we present complementary networks for person re-ID, which aims to extract more discriminative and robust local representations with complementary clues. Our model contains two branches and one of which aims to mine potential discriminative\u00a0\u2026", "Compared with current RGB or RGB-D saliency detection datasets, those for light field saliency detection often suffer from many defects,  e.g ., insufficient data amount and diversity, incomplete data formats, and rough annotations, thus impeding the prosperity of this field. To settle these issues, we elaborately build a large-scale light field dataset, dubbed  PKU-LF , comprising 5,000 light fields and covering diverse indoor and outdoor scenes. Our PKU-LF provides all-inclusive representation formats of light fields and offers a unified platform for comparing algorithms utilizing different input formats. For sparking new vitality in saliency detection tasks, we present many unexplored scenarios (such as underwater and high-resolution scenes) and the richest annotations (such as scribble annotations, bounding boxes, object-/instance-level annotations, and edge annotations), on which many potential attention modeling\u00a0\u2026", "Deep learning models are found to be vulnerable to adversarial examples, as wrong predictions can be caused by small perturbation in input for deep learning models. Most of the existing works of adversarial image generation try to achieve attacks for most models, while few of them make efforts on guaranteeing the perceptual quality of the adversarial examples. High quality adversarial examples matter for many applications, especially for the privacy preserving. In this work, we develop a framework based on the Minimum Noticeable Difference (MND) concept to generate adversarial privacy preserving images that have minimum perceptual difference from the clean ones but are able to attack deep learning models. To achieve this, an adversarial loss is firstly proposed to make the deep learning models attacked by the adversarial images successfully. Then, a perceptual quality-preserving loss is developed by\u00a0\u2026", "Significant improvement has been made on just noticeable difference (JND) modelling due to the development of deep neural networks, especially for the recently developed unsupervised-JND generation models. However, they have a major drawback that the generated JND is assessed in the real-world signal domain instead of in the perceptual domain in the human brain. There is an obvious difference when JND is assessed in such two domains since the visual signal in the real world is encoded before it is delivered into the brain with the human visual system (HVS). Hence, we propose an HVS-inspired signal degradation network for JND estimation. To achieve this, we carefully analyze the HVS perceptual process in JND subjective viewing to obtain relevant insights, and then design an HVS-inspired signal degradation (HVS-SD) network to represent the signal degradation in the HVS. On the one hand, the well learnt HVS-SD enables us to assess the JND in the perceptual domain. On the other hand, it provides more accurate prior information for better guiding JND generation. Additionally, considering the requirement that reasonable JND should not lead to visual attention shifting, a visual attention loss is proposed to control JND generation. Experimental results demonstrate that the proposed method achieves the SOTA performance for accurately estimating the redundancy of the HVS. Source code will be available at https://github.com/jianjin008/HVS-SD-JND.", "Three-dimensional (3D) point clouds (PCs) and meshes have increasingly become available and indispensable for diversified applications in work and life. In addition, 3D visual data contain information from any viewpoint when needed, introducing new challenges and opportunities. As in the cases of 2D images and videos, computationally modeling saliency and quality for 3D PCs and meshes are important for widespread, economical adaption and optimization. This paper aims to provide a comprehensive overview of the related signal presentation and existing saliency and quality models, with major perspectives from the ultimate users (ie, humans or machines), modeling methodology (with handcrafted features or machine learning), and modeling scope (generic or utility-oriented models). Possible future research directions are also discussed.", "As it is said by Van Gogh, great things are done by a series of small things brought together. Aesthetic experience arises from the aggregation of underlying visual components. However, most existing deep image aesthetic assessment (IAA) methods over-simplify the IAA process by failing to model image aesthetics with clearly-defined visual components as building blocks. As a result, the connection between resulting aesthetic predictions and underlying visual components is mostly invisible and hard to be explicitly controlled, which limits the model in both performance and interpretability. This work aims to model image aesthetics from the level of visual components. Specifically, object-level regions detected by a generic object detector are defined as visual components, namely object-level visual components (OVCs). Then generic features representing OVCs are aggregated for the aesthetic prediction based upon proposed object-level and graph attention mechanisms, which dynamically determines the importance of individual OVCs and relevance between OVC pairs, respectively. Experimental results confirm the superiority of our framework over previous relevant methods in terms of SRCC and PLCC on the aesthetic rating distribution prediction. Besides, quantitative analysis is done towards model interpretation by observing how OVCs contribute to aesthetic predictions, whose results are found to be supported by psychology on aesthetics and photography rules. To the best of our knowledge, this is the first attempt at the interpretation of a deep IAA model.", "Despite demonstrated outstanding effectiveness in various computer vision tasks, Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples. Nowadays, adversarial attacks as well as their defenses w.r.t. DNNs in image domain have been intensively studied, and there are some recent works starting to explore adversarial attacks w.r.t. DNNs in video domain. However, the corresponding defense is rarely studied. In this paper, we propose a new two-stage framework for defending video adversarial attack. It contains two main components, namely self-adaptive Joint Photographic Experts Group (JPEG) compression defense and optical texture based defense (OTD). In self-adaptive JPEG compression defense, we propose to adaptively choose an appropriate JPEG quality based on an estimation of moving foreground object, such that the JPEG compression could depress most impact of\u00a0\u2026", "After the works on High Efficiency Video Coding (HEVC) standard, the standard organizations continued to study the next generation of video coding standard, named Versatile Video Coding (VVC). The compression capacity of the VVC standard is expected to be substantially improved relative to the current HEVC standard by evolving the potential coding tools greatly. Transform is a key technique for compression efficiency, and core experiment 6 (CE6) in JVET is established to explore the transform-related coding tools. In this paper, we propose a novel signal-independent separable transform based on the Karhunen-Lo\u00e8ve transform (KLT) to improve the efficiency of both intra and inter residual coding. In the proposed method, the drawbacks of the traditional KLT are addressed. A group of mode-independent intra transform matrices is calculated from abundant intra residual samples of all intra modes, while the\u00a0\u2026", "To obtain effective pedestrian detection results in surveillance video, there have been many methods proposed to handle the problems from severe occlusion, pose variation, clutter background, and so on. Besides detection accuracy, a robust surveillance video system should be stable to video quality degradation by network transmission, environment variation, and so on. In this paper, we conduct the research on the robustness of pedestrian detection algorithms to video quality degradation. The main contribution of this paper includes the following three aspects. First, a large-scale distorted surveillance video data set ( DSurVD ) is constructed from high-quality video sequences and their corresponding distorted versions. Second, we design a method to evaluate detection stability and a robustness measure called  robustness quadrangle , which can be adopted to the visualize detection accuracy of pedestrian\u00a0\u2026", "For many computer vision problems, deep neural networks are trained and validated based on the assumption that the input images are pristine (i.e., artifact-free). However, digital images are subject to a wide range of distortions in real application scenarios, while the practical issues regarding image quality in high level visual information understanding have been largely ignored. In this paper, in view of the fact that most widely deployed deep learning models are susceptible to various image distortions, distorted images are involved for data augmentation in the deep neural network training process to learn a reliable model for practical applications. In particular, an image quality assessment based label smoothing method, which aims at regularizing the label distribution of training images, is further proposed to tune the objective functions in learning the neural network. Experimental results show that the proposed\u00a0\u2026", "Balancing diversity and accuracy of individuals is crucial for improving the performance of an ensemble system, since they are two important but incompatible factors for ensemble learning. When multiple individuals are combined with the corresponding weights, the diversity should be dominated by individuals and their weights, whereas the weights are normally ignored in the analysis of diversity in most research. Inspired by this, the authors propose a novel ensemble method which seeks an optimal combination to maximise diversity and accuracy of weighted individuals with the constraint on the minimal ensemble error. Furthermore, a new expression is given based on the generated individuals and their weights to exploit the diversity of an ensemble. Experimental results illustrate that the proposed method outperforms relevant existing methods.", "Singing voice offers rich information such as human emotions and moods, it is considered to be a particular musical instrument. Nowadays, there are many situations where the singing voice needs to be evaluated, such as teaching, singing competition, and so on. But not everyone has a professional teacher, a singing evaluation system will be very helpful for ordinary people to learn singing in a proper way. In this paper, a system is developed to analyze the performance of singing voice and rank it according to predictive model trained from the acoustic emotion features and musicians' opinions.", "Automatically understanding and modeling a user\u2019s liking for an image is a challenging problem. This is because the relationship between the images features (even semantic ones extracted by existing tools, viz. faces, objects etc.) and users\u2019 \u2018likes\u2019 is non-linear, influenced by several subtle factors. This work presents a deep bi-modal knowledge representation of images based on their visual content and associated tags (text). A mapping step between the different levels of visual and textual representations allows for the transfer of semantic knowledge between the two modalities. It also includes feature selection before learning deep representation to identify the important features for a user to like an image. Then the proposed representation is shown to be effective in learning a model of users image \u2018likes\u2019 based on a collection of images \u2018liked\u2019 by him. On a collection of images \u2018liked\u2019 by users (from Flickr\u00a0\u2026", "The diversity and versatility of the display devices have imposed new demands on digital image processing. Variant devices of different resolution screens need to display the same image for human visual experience. The retargeting methods are proposed to adapt the source image into arbitrary sizes and simultaneously keep the salient content of the source signal of high visual quality. Therefore, there is a new challenge of objectively evaluating the retargeted image perceptual quality, where variant resolutions may be presented, the objective shape may be distorted, and some content information may be discarded. In this chapter, recent progresses in quality assessment of retargeted images are reviewed. Firstly, we will review and discuss the recently developed retargeting methods for images. Afterwards, the subjective approaches to assess the retargeted image are reviewed, as well as the constructed\u00a0\u2026", "In this work, we firstly identify the shortcomings of the existing work of selective image rendering. In order to remedy the identified problems, we put forward the concept and formulation of a graphical saliency model (GSM) for selective image rendering applications, in which the sampling rate is determined adaptively according to the resultant saliency map under a computation budget. Different from the existing visual attention (VA) models which have been devised for natural image/video processing and applied to image rendering, the GSM considers the characteristics of the rendering process and aims to detect regions which require high computation to be rendered for good use of the said budget. The proposed GSM improves a VA model by incorporating a metric of rendering complexity. Experiment results show that, under a limited computation budget, selective rendering guided by the proposed GSM can\u00a0\u2026", "In this paper, we propose a saliency detection model based on amplitude spectrum. The proposed model first divides the input image into small patches, and then uses the amplitude spectrum of the Quaternion Fourier Transform (QFT) to represent the color, intensity and orientation distributions for each patch. The saliency for each patch is determined by two factors: the difference between amplitude spectrums of the patch and its neighbor patches and the Euclidian distance of the associated patches. The novel saliency measure for image patches by using amplitude spectrum of QFT proves promising, as the experiment results show that this saliency detection model performs better than the relevant existing models.", "We conduct subjective tests to evaluate the performance of scalable video coding with different spatial-domain bit-allocation methods, visual attention models, and motion feature extractors in the literature. For spatial-domain bit allocation, we use the selective enhancement and quality layer assignment methods. For characterizing visual attention, we use the motion attention model and perceptual quality significant map. For motion features, we adopt motion vectors from hierarchical B-picture coding and optical flow. Experimental results show that a more accurate visual attention model leads to better perceptual quality. In cooperation with a visual attention model, the selective enhancement method, compared to the quality layer assignment, achieves better subjective quality when an ROI has enough bit allocation and its texture is not complex. The quality layer assignment method is suitable for region-wise quality\u00a0\u2026", "As an efficient unitary transform, the discrete cosine transform (DCT) has been widely adopted in compression standards. Most compressed images and videos are stored in the DCT format, and from time to time, they need to be resized for various transmission channels and consumer terminals. In this paper, we investigate existing resizing schemes, focusing on the difference of short and long basis vector truncation. A layered resizing scheme is then proposed based on the above analysis, where compressed images are divided into low- and high-frequency layers. The DCT vectors of these two layers are truncated with different word lengths, and then form the elementary layer (EL) and the enhancement layer (EH) of the downsampled image. The EL and EH can be transmitted together or separately according to the bandwidth available. An upsampling scheme is also provided in this paper to recover visual details\u00a0\u2026", "The Region-of-interest (ROI) based video coding within Scalable Video Coding (SVC) can be implemented by making use of Type 2 Flexible Macroblock Ordering (FMO), which marks independent rectangle regions/slices inside a frame by their top-left and bottom-right coordinates. By employing the proposed scheme, a displaying frame can be separated into several independent regions that are assigned with different Signal-Noise-Ratio (SNR), Spatial and Temporal quality. The scheme can be used to ensure the achievement of high sub-jecitve quality or to fulfill some special functionalities. Owning to the fact that the frame is separated into independent regions, and the regions are assigned with big quality differences, false edge (blockiness) may appear around the ROI boundaries, which cannot be automatically removed by the in-loop filters. The annoyance of such artifact depends on the local visual context\u00a0\u2026", "In this paper, we propose a new video quality evaluation method, which measures the perceived error between original and distorted images using wavelet just-noticeable-distortion (JND) profile. The error of the subband coefficient that is below the visual detection threshold is ignored while some errors that take place in the content-activity regions are masked. Luminance masking and temporal contrast masking are integrated to adjust the perceived error. Experiments on VQEG (video quality expert group) test sequences show that the proposed method can achieve good correlation with subjective evaluation.", "An algorithm has been devised for fast, fully automatic and reliable object segmentation from live video for scenarios with static camera. The contributions in this chapter include methods for:(a) adaptive determination of the threshold for change detection;(b) robust stationary background reference frame generation, which when used in change detection can reduce segmentation fault rate and solve the problems of dis-occluded objects appearing as part of segmented moving objects;(c) adaptive reference frame selection to improve segmentation results; and (d) spatial refinement of modified change detection mask by incorporating information from edges, gradients and motion to improve accuracy of segmentation contours. The algorithm is capable of segmenting multiple objects at a speed of 12 QCIF frames per second with a Pentium-4 2.8 GHz personal computer in C coding without resorting to any code\u00a0\u2026", "The perceptual visual quality evaluation of human visual system (HVS) is very complex. It concerns almost all aspects of visual processing in the vision path, from low-level neuron activities to high-level visual perception. Existing perceptual visual quality metrics (VQMs) only considered several of the mechanisms of HVS, and many others are ignored. In this paper, two global modulatory factors, visual attention and motion suppression, are modelled and combined to form a mathematic expression - perceptual quality significant level (PQSL). To a certain extent, it is believed that PQSL values reflect the processing ability of the human brain on local visual content. To evaluate their effects on visual quality evaluation, two VQMs are proposed. One is a MSE-like VQM based in a PQSL-modulated JND profile, which was proposed in (Z. K. Lu et al, Proc. ICASSP'2004, v.3, p.705-708, 2004); the other VQM is based on\u00a0\u2026", "In this paper a new approach to pitch detection for spontaneous speech in noisy environment is proposed. It uses multi-rate lossless FIR filter to build a model which connects acoustic tubes and wavelet transform. An \"on-line\" eigen-structure algorithm is used to control the parameters of the model based on the state description of the physical system and input speech signal. The pitch periods are determined by identifying the \"events\" from the trajectories of modeling parameters. The simulations illustrate that the proposed method features higher accuracy, more robustness to noise and the capacity of \"on-line\" pitch detection for synthetic and natural speech signal in comparison with other existing wavelet-based pitch detection algorithms.", "Morphological openings and closings are basic operators in mathematical morphology. Morphological area openings and area closings have the same functions as standard morphological openings and closings but avoid distorting object boundaries. The conventional implementation of morphological area operators is based on the heap data structure. A new methodology for implementing morphological area operators, which relies on the disjoint set data structure, is proposed in this paper. The computational complexity of our method is O(N) where N is the image size, and not related to the area parameter. Experiments show that the new implementation of grayscale area operators runs faster than the conventional implementations. The computational complexity of binary area operators is also studied in this paper.", "This paper presents a general and comprehensive analysis of major issues and difficulties in real-time DSP implementation of hybrid video coding. Methodologies for MOPS reduction of motion estimation and DCT, code optimization, task division in parallel processing, and codec adaptation are addressed. Performance of realtime H.263 and MPEG-4 video system with these techniques are demonstrated. Video object segmentation and its implementation in an MPEG-4 system are also discussed.", "The proliferation of videos collected during in-the-wild natural settings has pushed the development of effective Video Quality Assessment (VQA) methodologies. Contemporary supervised opinion-driven VQA strategies predominantly hinge on training from expensive human annotations for quality scores, which limited the scale and distribution of VQA datasets and consequently led to unsatisfactory generalization capacity of methods driven by these data. On the other hand, although several handcrafted zero-shot quality indices do not require training from human opinions, they are unable to account for the semantics of videos, rendering them ineffective in comprehending complex authentic distortions (e.g., white balance, exposure) and assessing the quality of semantic content within videos. To address these challenges, we introduce the text-prompted Semantic Affinity Quality Index (SAQI) and its localized version (SAQI-Local) using Contrastive Language-Image Pre-training (CLIP) to ascertain the affinity between textual prompts and visual features, facilitating a comprehensive examination of semantic quality concerns without the reliance on human quality annotations. By amalgamating SAQI with existing low-level metrics, we propose the unified Blind Video Quality Index (BVQI) and its improved version, BVQI-Local, which demonstrates unprecedented performance, surpassing existing zero-shot indices by at least 24\\% on all datasets. Moreover, we devise an efficient fine-tuning scheme for BVQI-Local that jointly optimizes text prompts and final fusion weights, resulting in state-of-the-art performance and superior generalization ability in\u00a0\u2026", "Multi-view clustering can partition data samples into their categories by learning a consensus representation in unsupervised way and has received more and more attention in recent years. However, most existing deep clustering methods learn consensus representation or view-specific representations from multiple views via view-wise aggregation way, where they ignore structure relationship of all samples. In this paper, we propose a novel multi-view clustering network to address these problems, called Global and Cross-view Feature Aggregation for Multi-View Clustering (GCFAggMVC). Specifically, the consensus data presentation from multiple views is obtained via cross-sample and cross-view feature aggregation, which fully explores the complementary of similar samples. Moreover, we align the consensus representation and the view-specific representation by the structure-guided contrastive learning module, which makes the view-specific representations from different samples with high structure relationship similar. The proposed module is a flexible multi-view data representation module, which can be also embedded to the incomplete multi-view data clustering task via plugging our module into other frameworks. Extensive experiments show that the proposed method achieves excellent performance in both complete multi-view data clustering tasks and incomplete multi-view data clustering tasks.", "With the AI of Things (AIoT) development, a huge amount of visual data, e.g., images and videos, are produced in our daily work and life. These visual data are not only used for human viewing or understanding but also for machine analysis or decision-making, e.g., intelligent surveillance, automated vehicles, and many other smart city applications. To this end, a new image codec paradigm for both human and machine uses is proposed in this work. Firstly, the high-level instance segmentation map and the low-level signal features are extracted with neural networks. Then, the instance segmentation map is further represented as a profile with the proposed 16-bit gray-scale representation. After that, both 16-bit gray-scale profile and signal features are encoded with a lossless codec. Meanwhile, an image predictor is designed and trained to achieve the general-quality image reconstruction with the 16-bit gray-scale profile and signal features. Finally, the residual map between the original image and the predicted one is compressed with a lossy codec, used for high-quality image reconstruction. With such designs, on the one hand, we can achieve scalable image compression to meet the requirements of different human consumption; on the other hand, we can directly achieve several machine vision tasks at the decoder side with the decoded 16-bit gray-scale profile, e.g., object classification, detection, and segmentation. Experimental results show that the proposed codec achieves comparable results as most learning-based codecs and outperforms the traditional codecs (e.g., BPG and JPEG2000) in terms of PSNR and MS-SSIM for image\u00a0\u2026", "Nowadays, three-dimensional (3D) point cloud has been an emerging medium to represent real-world scenes and objects. However, there is a considerable proportion of point clouds whose color attribute information is not captured during the acquisition process due to the device or environment limitations. This poses a great challenge for efficient management and utilization of point clouds. To address this problem, we introduce an automatic colorization scheme based on a deep generative network for 3D point clouds. The proposed approach uses the range images of point could geometry and trains a conditional generative adversarial network to predict the color of those images. Later, the color of each pixel in the colorized image is projected back to its corresponding point in the 3D point cloud. The experimental results demonstrate the efficacy of the proposed colorization approach in facilitating users to\u00a0\u2026", "In this paper, we present an audio-visual emotion conversion based on deep learning for 3D talking head. The technology aims at retargeting neutral facial and speech expression into emotional ones. The challenging issues are how to control dynamics and variations of different expressions of both speech and the face. The controllability of facial expressions is achieved by training a parallel neutral and emotional marker-based facial motion capture data using a temporal restricted Boltzmann machine (TRBMs) for emotion transfer, while emotional voice conversion is to use long short term memory recurrent neural networks (LSTM-RNNs). Through the combination of 3D skinning method and 3D motion capture, we can make facial animation model close to physical reality for different expressions of 3D talking head. Results on subjective emotion recognition task show that recognition rates of the synthetic audio\u00a0\u2026", "In 3D video coding systems, depth maps are not displayed to the viewers, but provide the geometric information to generate virtual views. To ensure the quality of virtual views, the rate-distortion optimization (RDO) in depth map coding adopts the virtual view distortion as the distortion item. The virtual view distortion comes from the reconstructed color video distortion and depth distortion. It is usually recognized that the virtual view distortion caused by reconstructed color video distortion is independent of that in depth map coding. Preliminary experiments reveal that the virtual view distortion in depth map coding is also influenced by the reconstructed color video distortion. Therefore, we proposed a novel distortion criterion of depth map coding in which the reconstructed color video distortion is modeled and joins into the virtual view distortion calculation. Correspondingly, the associated Lagrange multiplier is also\u00a0\u2026", "No-reference (NR) image quality assessment (IQA) technology is greatly required in quality-orientated visual signal processing systems. However, without the guidance of the reference information, it is still a great challenge for NR IQA to perform consistent with the subjective perception. Researches on cognitive neuroscience state that the human visual system (HVS) presents substantially orientation selectivity mechanism, within which the visual structures are extracted in the local receptive fields for scene understanding. Inspired by this mechanism, a set of orientation selectivity based visual patterns are designed. By analyzing the quality degradation on those patterns, a novel visual pattern degradation based NR IQA method is proposed. Experimental results on large databases demonstrate that the proposed method outperforms the existing NR IQA methods.", "In applications that require an input point cloud to be matched with a set of database point clouds present on a remote server, it is preferable to compress and transfer 3D feature descriptors online, rather than compressing and transferring the whole input point cloud. This is because the former would require much lesser bandwidth and does not require feature extraction on the server. Existing real valued 3D feature descriptors that offer good keypoint matching performance require higher bandwidth for their transfer over the network. On the other hand, the existing binary 3D feature descriptor requires relatively less bandwidth but offers reduced keypoint matching performance. In this paper, we propose to employ lattice quantization to efficiently compress 3D feature descriptors. These compressed 3D feature descriptors can be directly matched in compressed domain without any need for decompression, hence\u00a0\u2026", "The rate\u2013distortion optimisation criterion of depth maps coding is evaluated by virtual view distortion, which motivates research modelling the distortion and corresponding Lagrange multiplier for depth maps coding. Current studies follow the assumption that the virtual view distortion introduced by texture videos distortion and depth maps distortion can be explored separately. Then the texture\u2010videos\u2010distortion\u2010induced virtual view distortion is considered as a constant value during depth maps coding. A relationship between the optimal Lagrange multiplier of depth maps and texture videos distortion is investigated. Based on this relationship, an efficient Lagrange multiplier selection algorithm is proposed for depth maps coding based on quantisation parameters of texture videos and depth maps. With the proposed Lagrange multiplier, 1.36% bitrate saving with the same virtual view quality can be achieved\u00a0\u2026", "Research on no-reference image quality assessment (IQA) aims to develop a computational model simulating the human perception of image quality accurately and automatically without any prior information about the reference clean image signals. In this paper, we introduce a novel no-reference IQA metric, based on the analysis of structural degradation and luminance changes. Since the human visual system (HVS) is highly sensitive to structural distortion, we encode the image structural information as the local binary pattern (LBP) distribution. Besides, image quality is also affected by luminance changes, which cannot be captured properly by LBP threshold mechanism. Hence, the distribution of normalized luminance magnitudes is also included in the proposed IQA metric. Extensive experiments conducted on two large public image databases have demonstrated the effectiveness and robustness of\u00a0\u2026", "Color-plus-depth 3D video is booming up with the growing popularity of depth-sensing camera. This imposes new challenges on video resizing since the information embedded in the depth and color video should be taken into consideration simultaneously. In this paper, we propose an energy-constrained video retargeting approach for color-plus-depth 3D video. We first build a 3D video visual attention (3DV-VA) computation model by combining gradient, saliency, motion and depth information. Then, an energy function is generated using spatial, temporal and 3DV-VA constraints. Finally, seam selection and carving operation is conducted to generate a retargeted 3D video. Experimental results on two types of 3D video sequences shows that the proposed approach produces better seam carving results than the related existing methods.", "In surveillance systems, pedestrian detection is a fundamental task. To improve the detection accuracy, various approaches have been proposed to address severe occlusion, pose variation, etc. However, apart from the detection accuracy, a robust surveillance system also requires stable detection performance even when the video quality degrades due to the bandwidth limitation and environment variation. To study the robustness of detection algorithms, we introduce the Distorted Surveillance Video Database (DSurVD) which includes four types of common distortions in surveillance video; we benchmark several state-of-the-art pedestrian detection algorithms on this database; miss rate index (MRI) is proposed to evaluate the performance stability of the detectors on distorted videos. Performance-Quality curves of these algorithms regarding to different types of distortion are provided. We also provide discussion on\u00a0\u2026", "Recently, a lot of bilateral and non-local means based reconstruction algorithms are proposed in literature. The success of these filters lies in finding similar patches in the neighbourhood. However, in complex regions and in presence of noise, finding similar patches based on mean square error (MSE) is not reliable. This results into blurred edges and visible patches in the reconstructed image. To address this issue, we propose a new Observation model based and Perceptually Motivated Bilateral Filter (OPBIF) for Image Reconstruction. In which image quality assessment (IQA) matrices are used to find the similarity among the patches. From experimental results, it is validated that the proposed algorithm has the capability of reconstructing sharp edges, as compared to existing non-linear filtering algorithms.", "We propose a cloud based image contrast enhancement framework, in which the context-sensitive and context-free contrast is improved via solving a multi-criteria optimization problem. Specifically, the context-sensitive contrast enhancement is based on the unsharp masking of the input and edge-preserving filtered images, while the context-free contrast enhancement is achieved by the sigmoid transfer mapping. The parameters in the optimization process are determined with the reference to the image that has a similar content and better enhancement quality in the cloud. The image complexity from the free energy based brain theory and the \"surface\" quality statistics is collaboratively optimized to infer the parameters. Experimental results demonstrate that the proposed technique can efficiently create visually-pleasing enhanced images with the guidance image from cloud.", "The just noticeable distortion (JND) map is a useful tool for perceptual video coding. However, direct calculation of the JND map incurs high complexity, and the problem is aggravated in multiview video coding. In this paper, two fast methods are proposed to generate the JND maps of multiview videos. In the first method, the JND maps of some anchor views are used to synthesize the JND maps of other views via the depth image based rendering (DIBR), which can be much faster than direct JND computation. In the second method, the motion and disparity vectors obtained during the video coding are employed to predict the JND maps. If the prediction is not satisfactory, the JND block will be refreshed by calculating the JND directly. This method does not need any camera parameters and depth maps. The performances of the two fast JND map generation methods are evaluated in a perceptual MVC framework\u00a0\u2026", "Occlusion and appearance variation are two common challenges in visual object tracking. Existing methods may not distinguish occlusion from large appearance variation during appearance model updating, as both of them may cause large appearance transformation inside the bounding box. In this paper, we propose an appearance model selection (AMS) based visual tracking algorithm. In the proposed method, the appearance model will be duplicated and one of them stops updating when there is large appearance change inside the bounding box, led by either allowed appearance variation or unexpected occlusion. According to the appearance information of an incoming video frame, the proposed method will choose the best appearance model in the model pool by the model selection mechanism. The proposed method can track the visual targets with appearance variation accurately and avoid error\u00a0\u2026", "In this paper, we propose a novel image quality assessment (IQA) metric based on nonnegative matrix factorization (NM-F). With nonnegativity and parts-based properties, NMF well demonstrates how human brain learns the parts of objects. This makes NMF distinguished from other feature extraction methods like singular value decomposition (SVD), principal components analysis (PCA), etc. Inspired by this, we adopt NMF to extract image features from reference and distorted images. Extreme learning machine (ELM) [10] is then employed for feature pooling to obtain the overall quality score. Compared with other machine learning techniques such as neural networks and support vector machines (SVMs), ELM provides better generalization performance with much faster learning speed and less human intervene. Experimental results with the TID database demonstrate that the proposed metric achieves better\u00a0\u2026", "This chapter is a continuation and an extension of the previous chapter. After the brief introduction of selective visual attention in the previous chapter, we present more details on the related knowledge, theory and experiments for visual attention. Section 2.1 describes the relevant basic properties and structures of the human visual system (HVS). In Sections 2.2 and 2.3, the widely used feature integration theory (FIT), as well as its extension (i.e. the guided search (GS) theory), is to be discussed, with the experimental confirmation available. FIT deals with bottom-up attention, while GS enables a combination of bottom-up and top-down attention. Section 2.4 further discusses the time binding theory for multi-feature integration at the neuronal level. Section 2.5 gives insight into some important issues in visual attention modelling, such as competition, normalization and frequency whitening. The final section covers\u00a0\u2026", "We propose a new no-reference image quality assessment model nPSNR (No-reference PSNR) for JPEG compressed images. The model performs in DCT domain and the DCT coefficient distribution is used. This method estimates the MSQE (mean-squared quantization error) of a decoded image with the distributions of AC coefficients and DC coefficients of the encoded image, for PSNR estimation in DCT domain without reference. We test the proposed model on the selected images from the TID2008 database. Then, the computational scores (nPSNR) are compared with the ground truth values (MOS) based on three commonly-used performance criteria. Experimental results demonstrate that the proposed approach is more consistent with the subjective perception than the state-of-the-art full-reference image quality assessment schemes.", "Content-based image retrieval (CBIR) has attracted substantial attention during the past few years for its potential applications. To bridge the gap between low level visual features and high level semantic concepts, various relevance feedback (RF) or interactive re-ranking (IR) schemes have been designed to improve the performance of a CBIR system. In this paper, we propose a novel subspace learning based IR scheme by using a graph embedding framework, termed Laplacian Regularized Subspace Learning (LRSL). The LRSL method can model both within-class compactness and between-class separation by specially designing an intrinsic graph and a penalty graph in the graph embedding framework, respectively. In addition, LRSL can share the popular assumption of the biased discriminant analysis (BDA) for IR but avoid the singular problem in BDA. Extensive experimental results have shown that the\u00a0\u2026", null, "Support Vector Machine (SVM) based Relevance Feedback (RF) is one of the most popular techniques for Content-Based Image Retrieval (CBIR). However, it is not appropriate to directly use the SVM as a RF scheme since it treats the positive and negative feedbacks equally. Additionally, it does not take into account unlabelled samples although unlabelled samples are very helpful in constructing a good classifier. To explore solutions to these two problems, we propose a Biased Maximum Margin Analysis (BMMA) and a Semi-Supervised Biased Maximum Margin Analysis (SemiBMMA) combined with SVM RF in this paper. Extensive experiments on a large real world image database demonstrate that the proposed scheme can significantly improve the performance of the traditional SVM-based RF for CBIR.", "In this paper, a new full-reference metric for image quality assessment is proposed, which is based on the recent feature similarity (FSIM) index and incorporates proper human visual system (HVS) characteristics. This method improves FSIM by using the CSF (Contrast Sensitivity Function) operator and the contrast masking operator in DCT domain. To test the performance of the proposed metric, we have carried out experiments on LIVE database. Experimental results demonstrate that the improved metric can achieve higher consistency with the subjective evaluation than FSIM and other relevant state-of-the-art image quality assessment metrics.", "This chapter presents a novel metric for image quality assessment from a single image. The key idea is to estimate the point spread function (PSF) from the line spread function (LSF), whereas the LSF is constructed from edge information. It is proven that an edge point corresponds to the local maximal gradient in a blurred image, and therefore edges can be extracted from blurred images by conventional edge detectors. To achieve high accuracy, local Radon transform is implemented and a number of LSFs are extracted from each edge. The experimental results on a variety of synthetic and real blurred images validate the proposed method. To improve the system efficiency, a criterion for edge sharpness is further proposed and only the edge points from sharp edges are selected for extracting the LSF without using all edge information. The effects of nearby edges on the selected edge feature and the\u00a0\u2026", "Resizing compressed images is often needed for different display, storage and transmission. Based on the analysis of energy reserving capacity for different basis vectors, we propose a two-layer downsizing scheme, where the elementary layer represents the low frequency components of the image, while some high frequency details can be recovered from the enhancement layer. The new method provides a solution for low definition image scalable transmission, at the same time, provides a potential tool to rationally recover the high definition information. Experimental results show that the new scheme exhibits better picture quality than some current downsizing approaches, with a comparable computational cost", "An improved edge-adaptive color interpolation method for Bayer pattern images of single-sensor digital cameras is proposed in this paper. We derive a more accurate color edge classifier by using both ultra-channel and inter-channel correlation. In order to reduce the aliasing artifacts, missing channel values are interpolated along the edge direction. Experiment results show that the proposed method can preserve the edge information better, reduce color-bleeding artifacts and achieve higher image quality and signal fidelity against the existing schemes.", null, "Deep learning approaches for Image Aesthetics Assessment (IAA) have shown promising results in recent years, but the internal mechanisms of these models remain unclear. Previous studies have demonstrated that image aesthetics can be predicted using semantic features, such as pre-trained object classification features. However, these semantic features are learned implicitly, and therefore, previous works have not elucidated what the semantic features are representing. In this work, we aim to create a more transparent deep learning framework for IAA by introducing explainable semantic features. To achieve this, we propose Tag-based Content Descriptors (TCDs), where each value in a TCD describes the relevance of an image to a human-readable tag that refers to a specific type of image content. This allows us to build IAA models from explicit descriptions of image contents. We first propose the explicit\u00a0\u2026", "Nowadays, most 3D model quality assessment (3DQA) methods have been aimed at improving performance. However, little attention has been paid to the computational cost and inference time required for practical applications. Model-based 3DQA methods extract features directly from the 3D models, which are characterized by their high degree of complexity. As a result, many researchers are inclined towards utilizing projection-based 3DQA methods. Nevertheless, previous projection-based 3DQA methods directly extract features from multi-projections to ensure quality prediction accuracy, which calls for more resource consumption and inevitably leads to inefficiency. Thus in this paper, we address this challenge by proposing a no-reference (NR) projection-based \\textit{\\underline{G}rid \\underline{M}ini-patch \\underline{S}ampling \\underline{3D} Model \\underline{Q}uality \\underline{A}ssessment (GMS-3DQA)} method. The projection images are rendered from six perpendicular viewpoints of the 3D model to cover sufficient quality information. To reduce redundancy and inference resources, we propose a multi-projection grid mini-patch sampling strategy (MP-GMS), which samples grid mini-patches from the multi-projections and forms the sampled grid mini-patches into one quality mini-patch map (QMM). The Swin-Transformer tiny backbone is then used to extract quality-aware features from the QMMs. The experimental results show that the proposed GMS-3DQA outperforms existing state-of-the-art NR-3DQA methods on the point cloud quality assessment databases. The efficiency analysis reveals that the proposed GMS-3DQA requires far\u00a0\u2026", "With the rapid advancements of the text-to-image generative model, AI-generated images (AGIs) have been widely applied to entertainment, education, social media, etc. However, considering the large quality variance among different AGIs, there is an urgent need for quality models that are consistent with human subjective ratings. To address this issue, we extensively consider various popular AGI models, generated AGI through different prompts and model parameters, and collected subjective scores at the perceptual quality and text-to-image alignment, thus building the most comprehensive AGI subjective quality database AGIQA-3K so far. Furthermore, we conduct a benchmark experiment on this database to evaluate the consistency between the current Image Quality Assessment (IQA) model and human perception, while proposing StairReward that significantly improves the assessment performance of subjective text-to-image alignment. We believe that the fine-grained subjective scores in AGIQA-3K will inspire subsequent AGI quality models to fit human subjective perception mechanisms at both perception and alignment levels and to optimize the generation result of future AGI models. The database is released on \\url{https://github.com/lcysyzxdxc/AGIQA-3k-Database}.", "Image aesthetics assessment (IAA) has attracted growing interest in recent years but is still challenging due to its highly abstract nature. Nowadays, more and more people tend to comment images shared on the social networks, which can provide rich aesthetics-aware semantic information from different aspects. Therefore, user comments of an image can be exploited as supplementary information for enhancing aesthetic representation learning. Previous researches have demonstrated that aesthetic attributes make significant effect on image aesthetic quality and humans\u2019 aesthetic perception. Typically, people are used to give comments on an image from the perspective of aesthetic attributes, based on which the aesthetic quality of images can be inferred. Motivated by this, this paper presents an Attribute-assisted Multimodal Memory Network (AMM-Net) for image aesthetics assessment, which utilizes aesthetic\u00a0\u2026", "We present three novel strategies to incorporate a parametric body model into a pixel-aligned implicit model for single-view clothed human reconstruction. Firstly, we introduce ray-based sampling, a novel technique that transforms a parametric model into a set of highly informative, pixel-aligned 2D feature maps. Next, we propose a new type of feature based on blendweights. Blendweight-based labels serve as soft human parsing labels and help to improve the structural fidelity of reconstructed meshes. Finally, we show how we can extract and capitalize on body part orientation information from a parametric model to further improve reconstruction quality. Together, these three techniques form our S-PIFu framework, which significantly outperforms state-of-the-arts methods in all metrics. Our code is available at https://github. com/kcyt/SPIFu.", "Due to the rapid increase in video traffic and relatively limited delivery infrastructure, end users often experience dynamically varying quality over time when viewing streaming videos. The user quality-of-experience (QoE) must be continuously monitored to deliver an optimized service. However, modern approaches for continuous-time video QoE estimation require densely annotating the continuous-time QoE labels, which is labor-intensive and time-consuming. To cope with such limitations, we propose a novel weakly-supervised domain adaptation approach for continuous-time QoE evaluation, by making use of a small amount of continuously labeled data in the source domain and abundant weakly-labeled data (only containing the retrospective QoE labels) in the target domain. Specifically, given a pair of videos from source and target domains, effective spatiotemporal segment-level feature representation is first\u00a0\u2026", "Existing blind evaluators for screen content images (SCIs) are mainly learning-based and require a number of training images with co-registered human opinion scores. However, the size of existing databases is small, and it is labor-, time-consuming and expensive to largely generate human opinion scores. In this study, we propose a novel blind quality evaluator without training. Specifically, the proposed method first calculates the gradient similarity between a distorted image and its translated versions in four directions to estimate the structural distortion, the most obvious distortion in SCIs. Given that the edge region is easier to be distorted, the inter-scale gradient similarity is then calculated as the weighting map. Finally, the proposed method is derived by incorporating the gradient similarity map with the weighting map. Experimental results demonstrate its effectiveness and efficiency on a public available SCI\u00a0\u2026", "After the works on the state-of-the-art High Efficiency Video Coding (HEVC) standard, the standard organizations continued to study the potential video coding technologies for the next generation of video coding standard, named Versatile Video Coding (VVC). Transform is a key technique for compression efficiency, and core experiment 6 (CE6) is carried out to explore the transform related coding tools. In this paper, we propose a novel separable transform based on Karhunen-Lo\u00e8ve Transform (KLT) to eliminate the horizontal and vertical correlations in the residual samples of intra coding. In the proposed method, the weaknesses of the traditional KLT are addressed. The separable KLT is developed as an alternative transform type in addition to DCT-II, and the transform matrices from 4\u00d74 to 64\u00d764 are trained from intra residual samples. Experimental results show the proposed method can achieve 2.7% bitrate\u00a0\u2026", "With the advent of deep convolutional neural networks (DCNN), the improvements in visual saliency prediction research are impressive. Despite this, it is still needed to fully characterize the multi-scale saliency-influential factors into the current deep saliency framework for further improvement. However, the existing approaches aiming at capturing multi-scale contextual features either suffer from the heavy computation or limited performance gain. To overcome this, a lightweight yet powerful module for fully exploiting multi-scale contextual features is desired. In this paper, we propose a DCNN-based visual saliency prediction model to approach this goal. Our model is inspired by the GoogleNet, which use the inception module to capture multi-scale contextual features at various receptive fields. Specifically, we revise the original inception module to have more powerful multi-scale feature extraction capacity and less\u00a0\u2026", "This proposes a visual-quality guided global backlight dimming (VQG-GBD) algorithm to reduce the power consumption of liquid-crystal display on mobile devices. We build a backlight scaling ratio (BSR) prediction model via visual-quality assessment that not only considers the display contents but also the backlight intensity while measuring video quality. Also, we add visual uncertainty as an indicator to dim the backlight without being noticed by observers. The VQG-GBD includes a training stage and an online stage. For the training stage, first, we collect videos with distinct attributes of brightness and uncertainty. Then, the subjective rating obtains the relationship among the visual quality, BSR, brightness, and visual uncertainty. Finally, we use the trust-region method to build the BSR prediction model. In the online stage, the model is applied to mobile devices for real-time video display and a BSR optimization\u00a0\u2026", "Extracting stable features to enhance object representation has proved to be very effective in improving the performance of object tracking. To achieve this, mining techniques, such as K-means clustering and data associating, are often adopted. However, K-means clustering needs the pre-set number of clusters. Real scenarios (heavy occlusion and so on) often make the tracker lose the target object. To handle these problems, we propose an intraframe clustering and interframe association (ICIA)-based stable feature mining algorithm for object tracking. The value (in HSV space) peak contour is employed to automatically estimate the number of clusters and classify value and saturation colors of the object region to get connected subregions. Every subregion is described with observation and increment models. Multi-feature distances-based subregion association, between the current object template and the current\u00a0\u2026", "In-loop filtering is an important technique in modern video coding standards. In this paper, we propose a transform-domain in-loop filter to further improve the compression performance of high efficiency video coding (HEVC) standard. The proposed method estimates block transform coefficients by adaptively fusing two prediction sources according to their uncertainties respectively. The first prediction is the block transform coefficients of compressed video frames, the uncertainty of which is related to quantization parameters. The second prediction is the weighted average of transform blocks in a neighborhood, and the weights are designed according to block similarity. Its uncertainty is estimated based on the coefficient variance. To optimize the filtering performance, the parameters utilized in the proposed in-loop filter are learned from compressed videos for each quantization parameter offline, and frame level flags\u00a0\u2026", "In this paper, we propose an effective no-reference image quality assessment (IQA) method based on local region statistics (NRLRS). The proposed method is built on the hypothesis that image distortions may alter the local region statistics which can be well characterized by the inter-pixel relationship. Hence, by extracting perceptual features that describe the inter-pixel patterns of a distorted image, we can effectively quantify the impact of image degradation. For this purpose, the perceptual gray-level differences between neighboring pixels are extracted and a Gaussian Mixture Model (GMM) codebook is constructed as the generative model of extracted features. The Fisher vector representation is then derived to describe image as their derivations from the GMM model. Finally, partial least square regression is used to map the Fisher encodings to quality scores. Experimental results indicate that the proposed\u00a0\u2026", "In this work, we apply classifier fusion to tweet polarity identification problem. The task is to predict whether the emotion hidden in a tweet is positive, neutral, or negative. An asymmetric SIMPLS (ASIMPLS) based classifier, which was proved to be able to identify the minority class well in imbalanced classification problems, is implemented. Word embedding is also employed as a new feature. For each word, we obtain three word embedding vectors on positive, neutral, and negative tweet sets respectively. These vectors are used as features in the ASIMPLS classifier. Another three state-of-the-art systems are implemented also, and these four systems are fused together to further boost the performance. The fusion system achieved 59.63% accuracy on the 2016 test set of SemEval2016 Task 4, Subtask A.", "In image / video systems, the contrast adjustment which manages to enhance the visual quality is nowadays an important research topic. Yet very limited efforts have been devoted to the exploration of image quality assessment (IQA) for contrast adjustment. To address the problem, this paper proposes a novel reduced-reference (RR) IQA metric with the integration of bottom-up and top-down strategies. The former one stems from the recently revealed free energy theory which tells that the human visual system always seeks to understand an input image by the uncertainty removal, while the latter one is towards using the symmetric K-L divergence to compare the histogram of the contrast-altered image with that of the reference image. The bottom-up and top-down strategies are lastly combined to derive the Reduced-reference Contrast-altered Image Quality Measure (RCIQM). A comparison with numerous existing\u00a0\u2026", null, "Recently, many saliency detection models use image boundary as an effective prior of image background for saliency extraction. However, these models may fail when the salient object is overlapped with the boundary. In this paper, we propose a novel saliency detection model by computing the contrast between superpixels with background priors and introducing a refinement method to address the problem in existing studies. Firstly, the SLIC (Simple Linear Iterative Clustering) method is used to segment the input image into superpixels. Then, the feature difference is calculated between superpixels based on the color histogram. The initial saliency value of each superpixel is computed as the sum of feature differences between this superpixel and other ones in image boundary. Finally, a saliency map refinement method is used to reassign the saliency value of each image pixel to obtain the final saliency\u00a0\u2026", "Studies on the Human Visual System (HVS) have demonstrated that human eyes are more attentive to spatial or spectral components with irregularities on the scene. This fact was modeled differently in many computational methods such as saliency residual (SR) approach, which tries to find the irregularity of frequency components by subtracting average filtered and original amplitude spectra. However, studies showed that the high frequency components have more effect on the HVS perception. In this paper, we propose a 2D mel-cepstrum based spectral residual saliency detection model (MCSR) to provide perceptually meaningful and more informative saliency map with less redundancy and without down-sampling as in saliency residual approach. Experimental results demonstrate that proposed MCSR model can yield promising results compared to the relevant state of the art models.", null, "In this paper, we propose a novel image retargeting algorithm based on the sensitivity-tuned visual significance map which is composed of a saliency map and a gradient map. We develop a new saliency detection model based on the human visual sensitivity and amplitude spectrum of image patches. We use a coherent normalization based fusion method to combine the saliency map and the gradient map to generate the visual significance map. The seam carving technique is adopted for image retargeting, based on the sensitivity-tuned visual significance map. Experiment results show that the proposed algorithm outperforms the relevant state-of-the-arts image retargeting algorithms significantly.", "Visual attention models generate saliency maps in which attentive regions are more distinctive with respect to remaining parts of the scene. In this work, a new model of orientation conspicuity map (OCM) is presented for the computation of saliency. The proposed method is based on the difference of the Gabor filter outputs with orthogonal orientations because vehicles are the targets for the search tasks in this study. Moreover, as another contribution, selective resolution for the input image, according to the distance of the target in the scene, is also utilized with the proposed scheme for the benefit to target search. Experimental results demonstrate that both the OCM model and selective resolution for input images yield promising results for the target search in cluttered scenes.", "1 The pattern-based video coding (PVC) outperforms the H.264 through better exploitation of block partitioning and partial block skipping. In the PVC scheme the best pattern is determined against the moving regions (MRs) in a macroblock (MB) of the current frame against the co-located MB in the reference frame; motion estimation (ME) and motion compensation (MC) are carried out using the pattern covered MRs, and the rest of the regions are treated as skipped areas. The MRs can be due to the object areas and the uncovered background (UCB) areas. Thus, the ME & MC by the pattern for the MRs of the UCB would not be accurate if there is no similar region in the reference frame. As a result no coding gain can be achieved for the UCB. Recently a dynamic background frame termed as the McFIS (the most common frame of a scene) has been generated using Gaussian mixture models for object detection. In\u00a0\u2026", "An encoded video bitstream is composed of two main components: the coefficient bits representing the discrete cosine transform coefficients, and the header bits representing the header information (e.g., motion vectors, prediction modes, etc.). Compared with previous video standards, the H.264 Advanced Video Coding (AVC) standard has some unique features: (1) the header bits take up a considerable portion of the encoded bitstream; (2) the header bits vary significantly across macroblocks (MBs); and (3) a large number of MBs are quantized to zero and produce zero coefficient bits (zero-coefficient MB). These unique features make most existing rate estimators inaccurate for decision-making processes related to rate-distortion calculation for rate control. This paper analyzes the characteristics of the H.264/AVC bitstream, and reveals that both the header bits and the occurrence of the zero-coefficient MBs are strongly\u00a0\u2026", "In general, an inevitable side effect of the block-based transform coding includes grid noise in the monotone area, staircase noise along the edges, ringing around the strong edges, corner outliers, and edge corruption near the block boundaries. We propose a comprehensive postprocessing method for removing all the blocking-related artifacts in block-based discrete cosine transfer compressed image in the framework of overcomplete wavelet expansion (OWE) proposed by Mallat and Zhong [IEEE Trans. Pattern Anal. Mach. Intell 14(7), 710\u2013732 (1992)], which is translationally invariant and can efficiently characterize signal singularities. We propose to use the wavelet transform modulus maxima extension (WTMME) to represent the image. The WTMME is extracted from the wavelet coefficients of three-level OWE of the blocky image. The artifacts related to blockiness are modeled and detected through multiscale\u00a0\u2026", "In this paper, we address the problem of evaluating perceptual visual quality of compressed video under different settings for mobile communication. A 5-dimensional video feature space is constructed with the codec type, video content, frame size, frame rate and bitrate. The video quality assessment is formulated as a transformation between the multidimensional feature space and the quality space. Subject viewing tests for videos coded with H.263 and H.264 at different bitrates and spatial/temporal resolutions are performed. Based on statistical analysis, we find that the perceptual quality of the decoded video is affected by the encoder type, video content, bitrate, frame rate and frame size, in a descending order for significance. The methodology and findings in this paper designate new researching direction in video quality assessment and can be extended to other applications.", "In this paper, we propose a new image quality evaluation method, which is based on foveation spatial error sensitivity model and wavelet error sensitivity model. The image quality is measured by isotropic contrast and the standard deviation of the errors of the wavelet coefficients in subbands with the weighting factors that are determined by two models. Experiments on a test database comprising JPEG and JPEG 2000 compressed images have shown that the proposed metric can achieve very good correlation with subjective evaluation.", null, "With the fast development of visual noise-shaping related applications (visual compression, error resilience, watermarking, encryption and display), there is an increasingly significant demand on incorporating perceptual characteristics into these applications for improved performance. In this paper, two important mechanisms of the human brain, visual attention and motion suppression, are introduced for visual sensitivity analysis. Based upon the analysis, a new numerical measure for the two mechanisms\u2019s modulatory aftereffects, Perceptual Quality Significance Map (PQSM), is proposed. To a certain extent, the PQSM reflects the processing ability of the human brain on local visual contents statistically. The PQSM is generated with the integration of local perceptual stimuli from color contrast, texture contrast, motion, as well as cognitive features (skin color and face in this study). Experimental results with subjective viewing demonstrate the performance improvement on two PQSM-modulated JND profiles.", null, "Video coding via firmware or software has the advantages of greater flexibility, quicker development cycle and easier updating/reconfiguration; in addition, its efficient and economical realization can be exploited at algorithm and programming/process levels. Issues related to complexity regulation and firmware/software efficiency are discussed for a hybrid video codec. A simple yet effective parametric scheme is proposed for high-level complexity regulation, regardless of the motion estimation (ME) and DCT algorithms adopted. Various methods are also presented for programming and process optimization. Results and performance in real-time codecs confirm the effectiveness of the techniques being presented.", null, "In this paper, two new algorithms have been proposed for performing video object segmentation. In both algorithms, a new method has been proposed for adaptive threshold estimation for the change detection process. The method is able to provide a reliable estimation of the threshold without knowing explicitly which data corresponds to the moving and non-moving elements in the video sequence. In addition, the use of new specially detected \u201cmoving\u201d edgels and also a graph-based edge-linking method that employs both gradient-based and edge-based constraints have also been proposed in order to refine the object segmentation mask. The second algorithm further incorporates an initialisation phase to solve the problems of dis-occluded regions appearing as part of the segmented moving objects. The proposed work has shown a reasonably fast and good video object segmentation technique that operates in realtime and can be applied in applications such as MPEG-4's content-based video coding and home/office/warehouse security.", "In this article, we introduce the methods for image quality assessment. First, the classification of the image quality assessment is provided. Then, the subjective and objective methods for image quality assessment are presented in detail. Besides the quality assessment methods of 2D images, we also give the introduction of the quality assessment methods of specific types of images, such as 3D images, retargeted images, and screen content images. This article provides a general introduction and review for the existing methods of image quality assessment.", "Parallelism has been perceived as the way for a computer vision system to achieve the required speedup in practice with existing algorithms and computing resources. It is known that methods of task division in such systems are algorithm\u2010dependent. This paper investigates the parallel implementation of an object identification system based on an alternating hypothesize\u2010verify\u2010extend strategy with a transputer mesh network. This non\u2010deterministic task is firstly divided on the hypothesis level and then within the processing for each hypothesis. It has been demonstrated that an object with modest complexity can be recognized and located from a cluttering image in a fraction of a second. \u00a9 1997 John Wiley & Sons, Ltd.", "In observing images, the perception of the human visual system (HVS) is affected by both image contents and distortions. Obviously, the visual quality of the same image varies under different distortion types and intensities. Furthermore, the visual masking effects reveal that image content and distortion have a visual interaction, where the HVS presents different visibility of the identical distortion for different image contents. Based upon this, we propose a visual interaction perceptual network that can perceive both content and distortion of an image. The proposed model consists of three sub-modules: content perception module (CPM), distortion perception module (DPM), and visual interaction module (VIM). However, the subjective quality score cannot guide the model to explicitly learn the feature representations of image content and distortion. Thus, we perform a two-stage training procedure. In the first stage, we\u00a0\u2026", "Recently, CNN-based post-processing has shown great potential in Synthesized View Quality Enhancement (SVQE). However, due to the limited receptive field of convolution, it is ineffective in explicitly modeling long-range dependencies, which are critical to eliminate the distortion induced by Depth Image Based Rendering (DIBR) in synthesized views. Although transformers exhibit tremendous success at learning global contextual information, it is weak at extracting local texture information. To take full advantages of the CNN and transformer, we present a novel U-shaped hybrid transformer with asymmetric flow division to collaboratively capture global-local information for SVQE, termed as AFD-former. Specifically, the AFD-former utilizes the Transformer-CNN Block (TCB) as encoder and decoder, in which several Dynamic Hybrid Attention Blocks (DHABs) are designed to simultaneously model long-range\u00a0\u2026", "With the development of 3D digital geometry technology, 3D triangular meshes are becoming more useful and valuable in industrial manufacturing and digital entertainment. A high quality triangular mesh can be used to represent a real world object with geometric and physical characteristics. While anisotropic meshes have advantages of representing shapes with sharp features (such as trimmed surfaces) more efficiently and accurately, isotropic meshes allow more numerically stable computations. When there is no anisotropic mesh requirement, isotropic triangles are always a good choice. In this paper, we propose a remeshing method to convert an input mesh into an adaptively isotropic one based on a curvature smoothed field (CSF). With the help of the CSF, adaptively isotropic remeshing can retain the curvature sensitivity, which enables more geometric features to be kept, and avoid the occurrence of obtuse\u00a0\u2026", "Convolutional Neural Network (CNN)-based image super-resolution (SR) has exhibited impressive success on known degraded low-resolution (LR) images. However, this type of approach is hard to hold its performance in practical scenarios when the degradation process ( i.e.  blur and downsampling) is unknown. Despite existing blind SR methods proposed to solve this problem using blur kernel estimation, the perceptual quality and reconstruction accuracy are still unsatisfactory. In this paper, we analyze the degradation of a high-resolution (HR) image from image intrinsic components according to a degradation-based formulation model. We propose a components decomposition and co-optimization network (CDCN) for blind SR. Firstly, CDCN decomposes the input LR image into structure and detail components in feature space. Then, the mutual collaboration block (MCB) is presented to exploit the\u00a0\u2026", "Perceptual quality assessment of 3D synthesized views is an open research problem in computer vision. Researchers across the globe have developed several algorithms to identify distortions. At the same time, the existing algorithms cannot quantify the context in which these distortions affect the overall perceptual quality. According to the recently proposed 3D view synthesis algorithm, the choice of context region for the disocclusion plays a vital role in predicting the quality of 3D views. The context region taken from the background of a view produces a perceptually better quality of 3D synthesized views than when the context region is taken from the foreground. With this view, the proposed algorithm aims to identify the context region and incorporate this information for the perceptual quality assessment of 3D synthesized views. We observed that the depth energy maps of the 3D synthesized views vary\u00a0\u2026", "In video-based dynamic point cloud compression (V-PCC), 3D point clouds are projected onto 2D images for compressing with the existing video codecs. However, the existing video codecs are originally designed for natural visual signals, and it fails to account for the characteristics of point clouds. Thus, there are still problems in the compression of geometry information generated from the point clouds. Firstly, the distortion model in the existing rate-distortion optimization (RDO) is not consistent with the geometry quality assessment metrics. Secondly, the prediction methods in video codecs fail to account for the fact that the highest depth values of a far layer is greater than or equal to the corresponding lowest depth values of a near layer. This paper proposes an advanced geometry surface coding (AGSC) method for dynamic point clouds (DPC) compression. The proposed method consists of two modules, including an error projection model-based (EPM-based) RDO and an occupancy map-based (OM-based) merge prediction. Firstly, the EPM model is proposed to describe the relationship between the distortion model in the existing video codec and the geometry quality metric. Secondly, the EPM-based RDO method is presented to project the existing distortion model on the plane normal and is simplified to estimate the average normal vectors of coding units (CUs). Finally, we propose the OM-based merge prediction approach, in which the prediction pixels of merge modes are refined based on the occupancy map. Experiments tested on the standard point clouds show that the proposed method achieves an average 9.84\\% bitrate saving for\u00a0\u2026", "As an important perceptual characteristic of the Human Visual System (HVS), the Just Noticeable Difference (JND) has been studied for decades with image/video processing (eg, perceptual image/video coding). However, there is little exploration on the existence of JND for AI, like Deep Machine Vision (DMV), although the DMV has made great strides in many machine vision tasks. In this paper, we take an initial attempt, and demonstrate that DMV does have the JND, termed as DMVJND. Besides, we propose a JND model for the classification task in DMV. It has been discovered that DMV can tolerate distorted images with average PSNR of only 9.56 dB (the lower the better), by generating JND via unsupervised learning with our DMVJND-NET. In particular, a semantic-guided redundancy assessment strategy is designed to constrain the magnitude and spatial distribution of the JND. Experimental results on classification tasks demonstrate that we successfully find and model the JND for deep machine vision. Meanwhile, our DMV-JND paves a possible direction for DMV oriented image/video compression, watermarking, quality assessment, deep neural network security, and so on.", "Existing CNN-based methods for semantic segmentation heavily depend on multi-scale features to meet the requirements of both semantic comprehension and detail preservation. State-of-the-art segmentation networks widely exploit conventional scale-transfer operations, i.e., up-sampling and down-sampling to learn multi-scale features. In this work, we find that these operations lead to scale-confused features and suboptimal performance because they are spatial-invariant and directly transit all feature information cross scales without spatial selection. To address this issue, we propose the Gated Scale-Transfer Operation (GSTO) to properly transit spatial-filtered features to another scale. Specifically, GSTO can work either with or without extra supervision. Unsupervised GSTO is learned from the feature itself while the supervised one is guided by the supervised probability matrix. Both forms of GSTO are\u00a0\u2026", "Every day 1.8+ billion images are being uploaded to Facebook, Instagram, Flickr, Snapchat, and WhatsApp [6]. The exponential growth of visual media has made quality assessment become increasingly important for various applications, from image acquisition, synthesis, restoration, and enhancement, to image search and retrieval, storage, and recognition. There have been two related but different classes of visual quality assessment techniques: image quality assessment (IQA) and image aesthetics assessment (IAA). As perceptual assessment tasks, subjective IQA and IAA share some common underlying factors that affect user judgments. Moreover, they are similar in methodology (especially NR-IQA in-the-wild and IAA). However, the emphasis for each is different: IQA focuses on low-level defects e.g. processing artefacts, noise, and blur, while IAA puts more emphasis on abstract and higher-level concepts that\u00a0\u2026", "The Joint Workshop on Aesthetic and Technical Quality Assessment of Multimedia and Media Analytics for Societal Trends (ATQAM/ MAST) aims to bring together researchers and professionals working in fields ranging from computer vision, multimedia computing, multimodal signal processing to psychology and social sciences. It is divided into two tracks: ATQAM and MAST. ATQAM track: Visual quality assessment techniques can be divided into image and video technical quality assessment (IQA and VQA, or broadly TQA) and aesthetics quality assessment (AQA). While TQA is a long-standing field, having its roots in media compression, AQA is relatively young. Both have received increased attention with developments in deep learning. The topics have mostly been studied separately, even though they deal with similar aspects of the underlying subjective experience of media. The aim is to bring together\u00a0\u2026", "Existing CNN-based methods for pixel labeling heavily depend on multi-scale features to meet the requirements of both semantic comprehension and detail preservation. State-of-the-art pixel labeling neural networks widely exploit conventional scale-transfer operations, i.e., up-sampling and down-sampling to learn multi-scale features. In this work, we find that these operations lead to scale-confused features and suboptimal performance because they are spatial-invariant and directly transit all feature information cross scales without spatial selection. To address this issue, we propose the Gated Scale-Transfer Operation (GSTO) to properly transit spatial-filtered features to another scale. Specifically, GSTO can work either with or without extra supervision. Unsupervised GSTO is learned from the feature itself while the supervised one is guided by the supervised probability matrix. Both forms of GSTO are lightweight and plug-and-play, which can be flexibly integrated into networks or modules for learning better multi-scale features. In particular, by plugging GSTO into HRNet, we get a more powerful backbone (namely GSTO-HRNet) for pixel labeling, and it achieves new state-of-the-art results on the COCO benchmark for human pose estimation and other benchmarks for semantic segmentation including Cityscapes, LIP and Pascal Context, with negligible extra computational cost. Moreover, experiment results demonstrate that GSTO can also significantly boost the performance of multi-scale feature aggregation modules like PPM and ASPP. Code will be made available at https://github.com/VDIGPKU/GSTO.", "In this paper, we propose Deep Holographic Networks (DHN) to learn similarity metrics of videos for multi-label video search. DHN introduces a holographic composition layer to explicitly encode similarity metrics at intermediate layer of the network, instead of conventional deep metric learning approaches driven by ranking losses. The holographic composition layer is parameter-free and enables less memory footprint compared with state-of-the-art. Towards multi-label video search at large scale, we present a new video benchmark built upon the YouTube-8M dataset. Extensive evaluations on this dataset demonstrate that DHN performs better than traditional deep metric learning approaches as well as other compositional networks.", "Spatial Psychovisual Modulation (SPVM) is a novel information display technology which aims to simultaneously generate multiple visual percepts for different viewers on a single display. The SPVM system plays an important role in information security. In a SPVM system, the viewers wearing polarized glasses can see a specific image (called personal view), and meanwhile the viewers not wearing glasses can also see a semantically meaningful image (called shared view). Researches on screen content image (SCI) are very hot recently, which have received a great amount of attention from multiple fields in multimedia signal processing. In this paper, we focus our gaze on how the users\u2019 quality-of-experience on SCIs is influenced under the SPVM display system. To this aim, we implement a comprehensive subjective quality assessment of SCIs by building a database which contains the distorted SCIs\u00a0\u2026", "The computer graphics community has successfully exploited our knowledge about the human visual system and its limitations for years. The perceptual computer graphics domain is a rich research area, and the understanding of human vision mechanisms and their integration in the computer graphics processes are becoming increasingly crucial in most applications. This special issue represents a good sample of the huge diversity of challenges in the perceptual computer graphics area: aesthetics evaluation, user gaze analysis, shape perception, and the wide scope of targeted applications.", "Spatial frequency plays an important role in face perception (Wu et al., 2009). Previous studies (Goffaux & Rossion, 2006) found that low spatial frequency signalled holistic face processing. However, its effect on face adaptation and awareness of emotion is largely unknown. In particular, does spatial frequency affect facial expression adaptation and awareness of emotion? We designed two experiments, the first was to test the adaptation effect by manipulating the spatial frequency of the adapting faces, and the second was to test the disciminability of these faces. First, we manipulated the spatial frequency of a sad real face, and created three faces with normal, high and low spatial frequency. We adapted the subjects to these faces, and tested their facial expression judgement of subsequently presented faces. We found that both the normal-and low-spatial-frequency adapting faces induced significant facial\u00a0\u2026", "In this paper, we introduce a visual pattern degradation based full-reference (FR) image quality assessment (IQA) method. Researches on visual recognition indicate that the human visual system (HVS) is highly adaptive to extract visual structures for scene understanding. Existing structure degradation based IQA methods mainly take local luminance contrast to represent structure, and measure quality as degradation on luminance contrast. In this paper, we suggest that structure includes not only luminance contrast but also orientation information. Therefore, we analyze the orientation characteristic for structure description. Inspired by the orientation selectivity mechanism in the primary visual cortex, we introduce a novel visual pattern to represent the structure of a local region. Then, the quality is measured as the degradations on both luminance contrast and visual pattern. Experimental results on Five benchmark\u00a0\u2026", "In this work, selfies (self-portrait images) of users are used to computationally predict and understand their personality. For users to convey a certain impression with selfie, and for the observers to build a certain impression about the users, many visual cues play a significant role. It is interesting to analyse what these cues are and how they influence our understanding of personality profiles. Selfies of users (from a popular microblogging site, Sina Weibo) were annotated with mid-level cues (such as presence of duckface, if the user is alone, emotional positivity etc.) relevant to portraits (especially selfies). Low-level visual features were used to train models to detect these mid-level cues, which are then used to predict users\u2019 personality (based on Five Factor Model). The mid-level cue detectors are seen to outperform state-of-the-art features for most traits. Using the trained computational models, we then present several insights on how selfies reflect their owners\u2019 personality and how users\u2019 are judged by others based on their selfies.", "Image representation is an elementary problem in any image processing application. The straightforward method is to represent an image by point-to-point. Regarding biological tasks of image processing, such as recognition, retrieval, tracking, and categorizing, such a method would be very uneconomical. The neighboring points are highly correlated with each other in natural images, so there exists a large amount of redundancies in natural images. The biological image processing should compress these redundancies as much as possible, which would significantly benefit the following classification, recognition, or retrieval tasks. To achieve this goal, pictorial information should be processed in such a way that the highest possible proportion of redundant information is filtered out. In this chapter, we first summarize the state-of-the-art processings of image representation by arranging them into basic\u00a0\u2026", "This chapter introduces the basic concepts and methods of machine learning that are related to this book. The classical machine learning methods, like neural network (CNN), support vector machine (SVM), clustering, Bayesian networks, sparse learning, Boosting, and deep learning, are presented in this chapter.", "There have been an abundance of developed image quality assessment (IQA) metrics during the last decade. However, these is not an individual one, whose performance always tops the performance ranking list on all subjective databases and for all distortions. The combination of multiple IQA metrics is expected to be better than each of them individually used. Two metric fusion frameworks are introduced in this chapter. The one introduces a multi-method fusion (MMF), which gives a MMF score by using a nonlinear combination of the scores computed from the multiple IQA metrics for fusion. The combination is achieved by training a set of weights using support vector regression. The other one presents an ensemble-based framework. In this framework, some features are extracted from the existing IQA metrics. These features are trained to be as basic image quality scorers (BIQSs). For copying with\u00a0\u2026", "Target appearance change during tracking is always a challenging problem for visual object tracking. In this paper, we present a novel visual object tracking algorithm based on Structure Complexity Coefficients (SCC) in addressing the motion related appearance change problem fundamentally. Based on our careful analysis, we found that the motion related appearance change is quite related to the SCC of target surface, where the appearance of complex structural regions is easier to change comparing with that of smooth structural regions with target motion. With the proposed SCC, a SCC-GL distance is defined in addressing both the appearance change and occlusion related problems during tracking. Moreover, an Observation Dependent Hidden Markov Model (OD-HMM) framework is designed where the observation dependency between neighboring frames is considered comparing with the\u00a0\u2026", "We propose a new multi-operator retargeting algorithm by using three resizing operators of seam carving, scaling, and cropping iteratively. To determine which operator should be used at each iteration, we adopt structural similarity (SSIM) to evaluate the similarity between the original and retargeted images for the dynamic programming. Since the sizes of original and retargeted images are different, SIFT flow is used for dense correspondence between the original and retargeted images for similarity evaluation. Additionally, visual saliency is used to weight SSIM results based on the characteristics of the Human Visual System (HVS). Experimental results on a public image retargeting database show the promising performance of the proposed multi-operator retargeting algorithm.", "Olfaction-enhanced mulsemedia is also the theme of the invited paper of the Special Issue,\u201cEEG Correlates of Pleasant and Unpleasant Odor Perception\u201d by Kroupi et al. Here, the authors explore the impact that perceiving scents with different degrees of pleasantness has on users, as measured by their electroencephalogram (EEG) activity. Given the fact that there are no primary odors (akin to the primary colours of red, green and blue), it is unsurprising that designing a general odor classifier based on user EEG responses is a challenge; in their work, however, they show that the design of subject-specific odor classifiers, based on EEG responses, is entirely possible.All of the Special Issue papers involve, in one way or another, user studies. Indeed, user acceptance is, one could argue, the key metric for mulsemedia applications\u2019 success and proliferation. Evaluating the mulsemedia experience is the topic of \u201cA Generic Utility Model representing the Quality of Sensory Experience\u201d by Rainer and Timmerer. In it, the authors report on the results of a user study which looked at how sensorial effects, such as light, wind and vibration, can combine in mulsemedia presentations. Employing the MPEG-V standard, to describe and annotate combinations defined in the conducted user study, the authors propose a mulsemedia QoE utility model.", "In this paper, we propose a novel computational model of visual attention based on the relevant characteristics of the Human Visual System (HVS). The input image is firstly divided into small image patches. Then the sparse features for each image patch are extracted based on the learned sparse coding basis. The human visual acuity is adopted in the calculation of the center-surround feature differences for saliency detection. In addition, the neighboring image patches for computing the saliency value of each center image patch are selected based on the characteristics of HVS. Experimental results show that the proposed saliency detection algorithm outperforms other existing schemes tested with a large public image database.", "For high-quality image rendering using Monte Carlo methods, a large number of samples are required to be computed for each pixel. Adaptive sampling aims to decrease the total number of samples by concentrating samples on difficult regions. However, existing adaptive sampling schemes haven't fully exploited the potential of image regions with complex structures to the reduction of sample numbers. To solve this problem, we propose to exploit uncertainty masking in adaptive sampling. Experimental results show that incorporation of uncertainty information leads to significant sample reduction and therefore time-savings.", "In this chapter, we begin to switch our focus from the visual attention modelling of Chapters 3-6 to the applications of these models. In Chapter 7, we first introduce the conventional engineering methods for object detection and recognition in Section 7.1. Then attention modelling combined with object detection and recognition for natural scenes is presented in Section 7.2. Since satellite images are different from natural images, in Section 7.3 we introduce the attention assisted object detection and recognition for satellite images. Section 7.4 presents image retrieval via visual attention. Another application of visual attention is presented finally for robots. This chapter does not try to introduce all aspects and works related to computer vision, image retrieval and robotics based on visual attention, but only demonstrates some typical methods of combining visual attention with conventional engineering methods. Readers\u00a0\u2026", "This chapter mainly discusses the computational models by combining bottom-up and top-down processing. In the combined models, the bottom-up part in almost all models uses all or part of the core of the BS model, and the top-down part often adopts other methods in computer vision, such as neural networks. Seven types of top-down computation are presented in this chapter. The most comprehensive is the population based model in which feature representation is in cell population form, and it is biologically plausible. Many modules are considered in the model such as bottom-up feature extraction, top-down knowledge leaning and storage, feature update by top-down influence, object recognition, inhibition of return (IoR), eye movement map, and so on. This model is first introduced in Section 5.1, then Section 5.2 covers the hierarchical object search model which simulates the human search method from\u00a0\u2026", "The just noticeable distortion (JND) map is a useful tool for perceptual video coding. However, direct calculation of the JND map incurs high complexity, and the problem is aggravated in multiview video coding. In this paper, the motion and disparity vectors obtained during the video coding are employed to predict the JND maps in order to reduce the complexity. The error propagation of the prediction is studied and a JND block refreshing approach is proposed, when the prediction is not satisfactory, to alleviate the influence of the error propagation. The performance of the proposed JND prediction method is evaluated in a perceptual MVC framework, where the prediction residuals are tuned according to the JND thresholds to save the bits without affecting the perceptual quality. Experimental results show that the JND prediction method has better accuracy and lower complexity than an existing JND synthesis\u00a0\u2026", "The Entropy coding module in the H.264 /advanced video coding (AVC) standard was originally designed for lossy video coding and does not yield adequate performance for lossless video coding. In this paper, we analyze the problem with the current lossless coding scheme and therefore propose a Coefficients Reordering based method for H.264/AVC lossless coding. By reordering the position of zigzag-scanned coefficients, the resultant new sequence fits the H.264/AVC entropy coders better. Experimental results confirm that the proposed method achieves about 2.6% and 2.0% bit saving for intra and inter lossless coding compared with the current H.264/AVC fidelity range extensions high profile.", "This paper introduces a novel framework, HodgeRank on Random Graphs, based on paired comparison, for subjective video quality assessment. Two types of random graph models are studied, i.e., Erd\u00f6s-R\u00e9nyi random graphs and random regular graphs. Hodge decomposition of paired comparison data may derive, from incomplete and imbalanced data, quality scores of videos and inconsistency of participants' judgments. We demonstrate the effectiveness of the proposed framework on LIVE video database. Both of the two random designs are promising sampling methods without jeopardizing the accuracy of the results. In particular, due to balanced sampling, random regular graphs may achieve better performances when sampling rates are small. However, when the number of videos is large or when sampling rates are large, their performances are so close that Erd\u00f6s-R\u00e9nyi random graphs, as the simplest independent and identically distributed sampling scheme, could provide good approximations to random regular graphs, as a dependent sampling scheme. In contrast to the traditional deterministic incomplete block designs, our random design is not only suitable for traditional laboratory studies, but also for crowdsourcing experiments on Internet where the raters are distributive and it is hard to control with fixed designs.", "Among the orthogonal transforms used in video and image compression, the Discrete-Cosine-Transform (DCT) is the most commonly used one. In the existing video codecs, the motion-compensation residual (MC-residual) is transformed with the DCT. In this paper, we propose an adaptive orthogonal transform that performs better on the MC-residual than the DCT. We formulate the proposed new transform based on L1-Norm minimization with orthogonal constraints. With the DCT matrix as the starting point, it is guaranteed to derive a better orthogonal transform matrix in terms of L1-Norm minimization. The experimental results confirm that, with little side information, our method leads to higher compression efficiency for the MC-residual. Remarkably, the proposed transform performs better in the high/ complex motion situation.", "We propose a unified deblocking/error-concealment algorithm for simultaneously alleviating blockiness and restoring lost blocks. Blockiness and block loss are two of the most frequently encountered artifacts for block-DCT based visual signal coding and transmission. The problem of error-concealment and deblocking is formulated as block-wise expectation estimation processes conditioned on available pixels in a local neighborhood. A nonparametric kernel regression approach is used for approximating the conditional probability and thus the conditional expectation of each image block. Missing blocks are restored and blockiness are suppressed with a single pass of the algorithm. The algorithm is highly efficient in that only block-wise additions and multiplications in pixel domain within a local neighborhood are involved. Experimental results are provided to justify the effectiveness of the proposed unified error\u00a0\u2026", "Video object segmentation aims to extract different video objects from a video (ie, a sequence of consecutive images). It has attracted vast interests and substantial research effort for the past decade because it is a prerequisite for visual content retrieval (eg, MPEG-7 related schemes), object-based compression and coding (eg, MPEG-4 codecs), object recognition, object tracking, security video surveillance, traffic monitoring for law enforcement, and many other applications. Video object segmentation is a nonstandardized but indispensable component for an MPEG4/7 scheme in order to successfully develop a complete solution. In fact, in order to utilize MPEG-4 object-based video coding, video object segmentation must first be carried out to extract the required video object masks. Video object segmentation is an even more important issue in military applications such as real-time remote missile/vehicle/soldier\u2019s\u00a0\u2026", "Since the coding residues correlates with the coded image, we propose a model for compression residue estimation with the coded image. When used for postprocessing, it results in a LMS (least mean square) process which can be implemented as filtration similar to most existing techniques, but gives more theoretical insight and restores residues without any assumption of the coding distortion (i.e., with higher level in generality). Determination of the critical bit rate is not required since it works for all bitrates, and is applicable to any image, codec and artifacts. The approach avoids the difficulties in image region discrimination and filter switching that are encountered otherwise. The proposed model results in around 0.5 dB for average PSNR improvement consistently over the exhaustive range of bitrates (0.15~4.0 bpp), with more gain at a lower bitrate, for various images.", "Super-resolution (SR) reconstruction is a technique to yield a higher resolution (HR) image from aliasing low resolution (LR) ones. An LR image is upsampled as the initialization, and then iteratively corrected in comparison with the other LR images. As the solution satisfying the SR constraints is non-unique, it is impossible to recover the original HR details completely by SR techniques. The solution reconstructed is sensitive to the starting point, especially when LR observations are insufficient, and may converge to a local optimum point. SR images reconstructed with different initializations may diverge in different ways from the true HR image. The influence of the initial HR estimate has not been sufficiently addressed so far by existing SR methods. We will explore this initial image selection issue to improve the performance of SR reconstruction.", "Many post-processing algorithms can efficiently reduce blocking artifacts but pay less attention to the sharpness of the image edges. In this paper, we propose a new post-processing method based on forward and backward anisotropic diffusion for reducing blocking, ringing effects and sharpening the edges. The proposed algorithm has been applied to the compressed video sequences for picture quality enhancement. Experimental results show that the proposed algorithm can efficiently improve the objective and subjective quality for different bit rate case.", "Most distortion/artifacts in low bit-rate compressed videos are strong and obvious to human eyes, and may even change the spatio-temporal structure of visual objects. Therefore it is important to evaluate the compound perceptual impact of spatial and temporal changes. Recent neurophysiological researches have found that the high-level spatial information (shape and depth, etc.) and temporal information (motion, etc.) are processed separately in two visual pathways in human brains. In this paper, spatial and temporal distortions are evaluated separately in two modules, and are then combined in an integration module. Two subjective viewing experiments are performed. One experiment is to evaluate the subjective quality on temporal distortion only, and second experiment is to evaluate the low bit-rate videos with both spatial and temporal distortions. Experimental results and the associated analysis show that the\u00a0\u2026", "We present a perceptually-adaptive pre-processing scheme for motion-compensated residue, based on just-noticeable-distortion (JND) profile. Human eyes cannot sense any changes below the JND threshold around a pixel due to their underlying spatial/temporal masking properties. From the viewpoint of signal compression, a smaller variance of signal results in less objective distortion of the reconstructed signal for a given bit-rate. In this paper, the JND profile is incorporated into a motion-compensated residue signal preprocessor for variance reduction, aimed at coding quality enhancement. A solution of adaptively determining the parameter for the residue pre-processor is also proposed. Experimental results show that both perceptual quality and objective quality are enhanced in coded video at a given bitrate.", "A perceptual weighting model is proposed for effective rate control so as to enhance the perceptual coding quality of videophone. We exploit two categories of factors affecting the perception of the human visual system, stimulus-driven factors and cognition-driven factors. In order to achieve a simple, but effective, perceptual weighting model, we use luminance adaptation and texture masking as the stimulus-driven factors, while skin color serves as the cognition-driven factor in the videophone application. Both objective and subjective quality evaluations of videophone-like sequences in the H.263 platform validate the effectiveness of our perceptual weighting model.", "Reliable delivery of audio bitstream is vital to ensure the acceptable audio quality perceived by 3G network customers. In other words, an audio coding scheme that is employed must be fairly robust over the error prone channels. Various error resilience techniques can be utilized for the purpose. Due to the fact that some parts of the audio bitstream are less sensitive to transmission errors than others, the Unequal Error Protection (UEP) is used to reduce the redundancy introduced by error resilience requirements. The current UEP scheme with convolutional codes and multi-stage interleaving has an unfortunate tendency to generate burst errors at the decoder output as the noise level is increased. A concatenated system combining Reed-Solomon codes with convolutional codes in UEP scheme is investigated for MPEG Advanced Audio Coding (AAC). Under severe channel conditions with random bit error rates of\u00a0\u2026", "Performance is presented for an object recognition and location method which alternately optimizes two representations of the match between object and image. The two representations are a geometric transform mapping the model onto the image coordinate and a feature pair list indicating the association of matched model/image features. Each optimization process has been designed to be history-free and consequently the system is capable of correcting mistakes formed in an earlier phase when the matching was rough.<>", "Adversarial images are able to fool the Deep Neural Network (DNN) based visual identity recognition systems, with the potential to be widely used in online social media for privacy-preserving purposes, especially in edge-cloud computing. However, most of the current techniques used for adversarial attacks focus on enhancing their ability to attack without making a deliberate, methodical, and well-researched effort to retain the perceptual quality of the resulting adversarial examples. This makes obvious distortion observed in the adversarial examples and affects users\u2019 photo-sharing experience. In this work, we propose a method for generating images inspired by the Human Visual System (HVS) in order to maintain a high level of perceptual quality. Firstly, a novel perceptual loss function is proposed based on Just Noticeable Difference (JND), which considered the loss beyond the JND thresholds. Then, a perturbation adjustment strategy is developed to assign more perturbation to the insensitive color channel according to the sensitivity of the HVS for different colors. Experimental results indicate that our algorithm surpasses the SOTA techniques in both subjective viewing and objective assessment on the VGGFace2 dataset.", "The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess fundamental low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs\u00a0\u2026", "The traditional strategy of acquiring satellite images involves transmitting compressed satellite data to ground stations solely via the downlink, without utilizing the uplink. In this paper, we propose an enhanced remote sensing (RS) image compression approach that utilizes uplink assistance to improve compression efficiency. By leveraging the uplink, historical images from ground stations can serve as reference images for on-orbit compression, effectively eliminating spatio-temporal redundancy in RS images. However, due to radiation variations among RS images captured on different dates, pixel-wise referencing as employed in the prior codec paradigm is insufficient. To address this, we propose a novel dual-end referencing downsampling-based coding (RefDBC) framework. At the encoder, relevance embedding evaluates reconstructability and records information to restore texture details from the reference\u00a0\u2026", "While saliency detection for 3D meshes has been extensively studied in the past decades, only a little work considers color information, and most of existing 3D mesh saliency databases are collected using meshes without color information. The lack of publicly available 3D colored mesh saliency database hinders the research progress in 3D colored mesh saliency detection. In this paper, we established a novel 3D colored mesh saliency database (3DCMS) based on an eye-tracking experiment and investigated subjects' visual attention behavior towards 3D colored meshes. Based on the investigations, a novel 3D colored mesh saliency detection framework is proposed which takes both color and geometric features into consideration. To evaluate the performance of the proposed algorithm, we compare it with several relevant methods and apply it to 3D mesh simplification task. The quantitative and qualitative\u00a0\u2026", "Image Quality Assessment (IQA) constitutes a fundamental task within the field of computer vision, yet it remains an unresolved challenge, owing to the intricate distortion conditions, diverse image contents, and limited availability of data. Recently, the community has witnessed the emergence of numerous large-scale pretrained foundation models, which greatly benefit from dramatically increased data and parameter capacities. However, it remains an open problem whether the scaling law in high-level tasks is also applicable to IQA task which is closely related to low-level clues. In this paper, we demonstrate that with proper injection of local distortion features, a larger pretrained and fixed foundation model performs better in IQA tasks. Specifically, for the lack of local distortion structure and inductive bias of vision transformer (ViT), alongside the large-scale pretrained ViT, we use another pretrained convolution neural network (CNN), which is well known for capturing the local structure, to extract multi-scale image features. Further, we propose a local distortion extractor to obtain local distortion features from the pretrained CNN and a local distortion injector to inject the local distortion features into ViT. By only training the extractor and injector, our method can benefit from the rich knowledge in the powerful foundation models and achieve state-of-the-art performance on popular IQA datasets, indicating that IQA is not only a low-level problem but also benefits from stronger high-level features drawn from large-scale pretrained models.", "Image Quality Assessment (IQA) is a fundamental task in computer vision that has witnessed remarkable progress with deep neural networks. Inspired by the characteristics of the human visual system, existing methods typically use a combination of global and local representations (\\ie, multi-scale features) to achieve superior performance. However, most of them adopt simple linear fusion of multi-scale features, and neglect their possibly complex relationship and interaction. In contrast, humans typically first form a global impression to locate important regions and then focus on local details in those regions. We therefore propose a top-down approach that uses high-level semantics to guide the IQA network to focus on semantically important local distortion regions, named as \\emph{TOPIQ}. Our approach to IQA involves the design of a heuristic coarse-to-fine network (CFANet) that leverages multi-scale features and progressively propagates multi-level semantic information to low-level representations in a top-down manner. A key component of our approach is the proposed cross-scale attention mechanism, which calculates attention maps for lower level features guided by higher level features. This mechanism emphasizes active semantic regions for low-level distortions, thereby improving performance. CFANet can be used for both Full-Reference (FR) and No-Reference (NR) IQA. We use ResNet50 as its backbone and demonstrate that CFANet achieves better or competitive performance on most public FR and NR benchmarks compared with state-of-the-art methods based on vision transformers, while being much more efficient (with only \u00a0\u2026", "Regression-based blind image quality assessment (IQA) models are susceptible to biased training samples, leading to a biased estimation of model parameters. To mitigate this issue, we propose a regression-free framework for image quality evaluation, which is founded upon retrieving similar instances by incorporating semantic and distortion features. The motivation behind this approach is rooted in the observation that the human visual system (HVS) has analogous visual responses to semantically similar image contents degraded by the same distortion. The proposed framework comprises two classification-based modules: semantic-based classification (SC) module and distortion-based classification (DC) module. Given a test image and an IQA database, the SC module retrieves multiple pristine images based on semantic similarity. The DC module then retrieves instances based on distortion similarity from the distorted images that correspond to each retrieved pristine image. Finally, the predicted quality score is derived by aggregating the subjective quality scores of multiple retrieved instances. Experimental results on four benchmark databases validate that the proposed model can remarkably outperform the state-of-the-art regression-based models.", "Traditional person re-identification (re-ID) methods generally rely on inter-camera person images to smooth the domain disparities between cameras. However, collecting and annotating a large number of inter-camera identities is extremely difficult and time-consuming, and this makes it hard to deploy person re-ID systems in new locations. To tackle this challenge, this paper studies the single-camera-training (SCT) setting where every person in the training set only appears in one camera. In this work, we design a novel inter-intra camera identity learning (I 2 CIL) framework to effectively address the SCT person re-ID. Specifically, (i) we design a Dual-Branch Identity Learning (DBIL) network consisting of inter-camera and intra-camera learning branches to learn person ID discriminative information. The former learns camera-irrelevant feature representations by constraining the distance of inter-camera negative\u00a0\u2026", "Text-based person search is an important sub-task in cross-modality image retrieval, aiming to capture interested person images by giving textual descriptions. The huge information differences between image and text modalities make this task challenging. Recent methods take local-aligned feature learning strategy into consideration, but lack sufficient mining of more local information. Accordingly, we explore a Multi-level Part-aware Feature Disentangling (MPFD) framework to more fully extract visual and textual representations from multiple angles. Specifically, we introduce a Textual Part-aware Matching (TPM) module into the existing baseline, to disentangle local features for detailed information from both visual and textual part-aware aspects. Besides, in order to fuse multiple local features and improve discrimination of global features, we propose a Multi-level Feature Integration (MFI) module which is capable\u00a0\u2026", "Digital humans have witnessed extensive applications in various domains, necessitating related quality assessment studies. However, there is a lack of comprehensive digital human quality assessment (DHQA) databases. To address this gap, we propose SJTU-H3D, a subjective quality assessment database specifically designed for full-body digital humans. It comprises 40 high-quality reference digital humans and 1,120 labeled distorted counterparts generated with seven types of distortions. The SJTU-H3D database can serve as a benchmark for DHQA research, allowing evaluation and refinement of processing algorithms. Further, we propose a zero-shot DHQA approach that focuses on no-reference (NR) scenarios to ensure generalization capabilities while mitigating database bias. Our method leverages semantic and distortion features extracted from projections, as well as geometry features derived from the mesh structure of digital humans. Specifically, we employ the Contrastive Language-Image Pre-training (CLIP) model to measure semantic affinity and incorporate the Naturalness Image Quality Evaluator (NIQE) model to capture low-level distortion information. Additionally, we utilize dihedral angles as geometry descriptors to extract mesh features. By aggregating these measures, we introduce the Digital Human Quality Index (DHQI), which demonstrates significant improvements in zero-shot performance. The DHQI can also serve as a robust baseline for DHQA tasks, facilitating advancements in the field. The database and the code are available at https://github.com/zzc-1998/SJTU-H3D.", "Personalized image aesthetics assessment (PIAA) is aimed at modeling the unique aesthetic preferences of individuals, based on which personalized aesthetic scores are predicted. People have different standards for image aesthetics, and accordingly, images rated at the same aesthetic level by different users explicitly reveal their aesthetic preferences. However, previous PIAA models treat each individual as an isolated optimization target, failing to take full advantage of the contrastive information among users. Further, although people's aesthetic preferences are unique, they still share some commonalities, meaning that PIAA models could be built on the basis of generic aesthetics. Motivated by the above facts, this paper presents a Multi-level Transitional Contrast Learning (MTCL) framework for PIAA by transiting features from generic aesthetics to personalized aesthetics via contrastive learning. First, a generic\u00a0\u2026", "Learned image compression (LIC) methods have experienced significant progress during recent years. However, these methods are primarily dedicated to optimizing the rate-distortion (R-D) performance at medium and high bitrates (> 0.1 bits per pixel (bpp)), while research on extremely low bitrates is limited. Besides, existing methods fail to explicitly explore the image structure and texture components crucial for image compression, treating them equally alongside uninformative components in networks. This can cause severe perceptual quality degradation, especially under low-bitrate scenarios. In this work, inspired by the success of pre-trained masked autoencoders (MAE) in many downstream tasks, we propose to rethink its mask sampling strategy from structure and texture perspectives for high redundancy reduction and discriminative feature representation, further unleashing the potential of LIC methods. Therefore, we present a dual-adaptive masking approach (DA-Mask) that samples visible patches based on the structure and texture distributions of original images. We combine DA-Mask and pre-trained MAE in masked image modeling (MIM) as an initial compressor that abstracts informative semantic context and texture representations. Such a pipeline can well cooperate with LIC networks to achieve further secondary compression while preserving promising reconstruction quality. Consequently, we propose a simple yet effective masked compression model (MCM), the first framework that unifies MIM and LIC end-to-end for extremely low-bitrate image compression. Extensive experiments have demonstrated that our approach\u00a0\u2026", "The proliferation of in-the-wild videos has greatly expanded the Video Quality Assessment (VQA) problem. Unlike early definitions that usually focus on limited distortion types, VQA on in-the-wild videos is especially challenging as it could be affected by complicated factors, including various distortions and diverse contents. Though subjective studies have collected overall quality scores for these videos, how the abstract quality scores relate with specific factors is still obscure, hindering VQA methods from more concrete quality evaluations (e.g. sharpness of a video). To solve this problem, we collect over two million opinions on 4,543 in-the-wild videos on 13 dimensions of quality-related factors, including in-capture authentic distortions (e.g. motion blur, noise, flicker), errors introduced by compression and transmission, and higher-level experiences on semantic contents and aesthetic issues (e.g. composition, camera trajectory), to establish the multi-dimensional Maxwell database. Specifically, we ask the subjects to label among a positive, a negative, and a neural choice for each dimension. These explanation-level opinions allow us to measure the relationships between specific quality factors and abstract subjective quality ratings, and to benchmark different categories of VQA algorithms on each dimension, so as to more comprehensively analyze their strengths and weaknesses. Furthermore, we propose the MaxVQA, a language-prompted VQA approach that modifies vision-language foundation model CLIP to better capture important quality issues as observed in our analyses. The MaxVQA can jointly evaluate various specific quality factors\u00a0\u2026", "Blind image quality assessment (BIQA) aims at automatically and accurately forecasting objective scores for visual signals, which has been widely used to monitor product and service quality in low-light applications, covering smartphone photography, video surveillance, autonomous driving, etc. Recent developments in this field are dominated by unimodal solutions inconsistent with human subjective rating patterns, where human visual perception is simultaneously reflected by multiple sensory information (e.g., sight and hearing). In this article, we present a unique blind multimodal quality assessment (BMQA) of low-light images from subjective evaluation to objective score. To investigate the multimodal mechanism, we first establish a multimodal low-light image quality (MLIQ) database with authentic low-light distortions, containing image and audio modality pairs. Further, we specially design the key modules of BMQA, considering multimodal quality representation, latent feature alignment and fusion, and hybrid self-supervised and supervised learning. Extensive experiments show that our BMQA yields state-of-the-art accuracy on the proposed MLIQ benchmark database. In particular, we also build an independent single-image modality Dark-4K database, which is used to verify its applicability and generalization performance in mainstream unimodal applications. Qualitative and quantitative results on Dark-4K show that BMQA achieves superior performance to existing BIQA approaches as long as a pre-trained quality semantic description model is provided. The proposed framework and two databases as well as the collected BIQA methods\u00a0\u2026", "Recently, blind image quality assessment (BIQA) models based on deep neural networks (DNNs) have achieved impressive performance on existing datasets. However, due to the intrinsic imbalance property of the training set, not all distortions or images are handled equally well. Online hard example mining (OHEM) is a promising way to alleviate this issue. Inspired by the recent finding that network pruning disproportionately hampers the model's memorization of a tractable subset, atypical, low-quality, long-tailed samples, that are hard-to-memorize during training and easily \u201cforgotten\u201d during pruning, we propose an effective \u201cplug-and-play\u201d OHEM pipeline, especially for generalizable deep BIQA. Specifically, we train two parallel weight-sharing branches simultaneously, where one is full model and other is a \u201cself-competitor\u201d generated from the full model online by network pruning. Then, we leverage the\u00a0\u2026", "In some aspects, a method for object re-identification may include obtaining a first set of images from a first camera, and a second set of images from at least one second camera; determining a first set of features based on the first set of images, the first set of features lying in a first feature space; and determining a second set of features based on the second set of images, the second set of features lying in a second feature space. The method may additionally include determining a first feature projection matrix and a second feature projection matrix that respectively map the first set of features and the second set of features to a shared feature space; and determining a common dictionary based on the shared feature space.", "Point cloud registration is a popular topic that has been widely used in 3D model reconstruction, location, and retrieval. In this paper, we propose a new registration method, KSS-ICP, to address the rigid registration task in Kendall shape space (KSS) with Iterative Closest Point (ICP). The KSS is a quotient space that removes influences of translations, scales, and rotations for shape feature-based analysis. Such influences can be concluded as the similarity transformations that do not change the shape feature. The point cloud representation in KSS is invariant to similarity transformations. We utilize such property to design the KSS-ICP for point cloud registration. To tackle the difficulty to achieve the KSS representation in general, the proposed KSS-ICP formulates a practical solution that does not require complex feature analysis, data training, and optimization. With a simple implementation, KSS-ICP achieves more\u00a0\u2026", "Recently, with the development of deep learning, a number of Just Noticeable Difference (JND) datasets have been built for JND modeling. However, all the existing JND datasets only label the JND points based on the level of compression distortion. Hence, JND models learned from such datasets can only be used for image/video compression. As known, JND is a major characteristic of the human visual system (HVS), which reflects the maximum visual distortion that the HVS can tolerate. Hence, a generalized JND modeling should take more kinds of distortion types into account. To benefit JND modeling, this work establishes a generalized JND dataset with a coarse-to-fine JND selection, which contains 106 source images and 1,642 JND maps, covering 25 distortion types. To this end, we proposed a coarse JND candidate selection scheme to select the distorted images from the existing Image Quality Assessment (IQA) datasets as JND candidates instead of generating JND maps ourselves. Then, a fine JND selection is carried out on the JND candidates with a crowdsourced subjective assessment.", "A popular track of network compression approach is Quantization aware Training (QAT), which accelerates the forward pass during the neural network training and inference. However, not much prior efforts have been made to quantize and accelerate the backward pass during training, even though that contributes around half of the training time. This can be partly attributed to the fact that errors of low-precision gradients during backward cannot be amortized by the training objective as in the QAT setting. In this work, we propose to solve this problem by incorporating the gradients into the computation graph of the next training iteration via a hypernetwork. Various experiments on CIFAR-10 dataset with different CNN network architectures demonstrate that our hypernetwork-based approach can effectively reduce the negative effect of gradient quantization noise and successfully quantizes the gradients to INT4 with only 0.64 accuracy drop for VGG-16 on CIFAR-10.", "Recently, learned image compression schemes have achieved remarkable improvements in image fidelity (e.g., PSNR and MS-SSIM) compared to conventional hybrid image coding ones due to their high-efficiency non-linear transform, end-to-end optimization frameworks, etc. However, few of them take the Just Noticeable Difference (JND) characteristic of the Human Visual System (HVS) into account and optimize learned image compression towards perceptual quality. To address this issue, a JND-based perceptual quality loss is proposed. Considering that the amounts of distortion in the compressed image at different training epochs under different Quantization Parameters (QPs) are different, we develop a distortion-aware adjustor. After combining them together, we can better assign the distortion in the compressed image with the guidance of JND to preserve the high perceptual quality. All these designs enable the proposed method to be flexibly applied to various learned image compression schemes with high scalability and plug-and-play advantages. Experimental results on the Kodak dataset demonstrate that the proposed method has led to better perceptual quality than the baseline model under the same bit rate.", "In this paper, we propose a novel layer-adaptive weight-pruning approach for Deep Neural Networks (DNNs) that addresses the challenge of optimizing the output distortion minimization while adhering to a target pruning ratio constraint. Our approach takes into account the collective influence of all layers to design a layer-adaptive pruning scheme. We discover and utilize a very important additivity property of output distortion caused by pruning weights on multiple layers. This property enables us to formulate the pruning as a combinatorial optimization problem and efficiently solve it through dynamic programming. By decomposing the problem into sub-problems, we achieve linear time complexity, making our optimization algorithm fast and feasible to run on CPUs. Our extensive experiments demonstrate the superiority of our approach over existing methods on the ImageNet and CIFAR-10 datasets. On CIFAR-10, our method achieves remarkable improvements, outperforming others by up to 1.0% for ResNet-32, 0.5% for VGG-16, and 0.7% for DenseNet-121 in terms of top-1 accuracy. On ImageNet, we achieve up to 4.7% and 4.6% higher top-1 accuracy compared to other methods for VGG-16 and ResNet-50, respectively. These results highlight the effectiveness and practicality of our approach for enhancing DNN performance through layer-adaptive weight pruning. Code will be available on https://github. com/Akimoto-Cris/RD_VIT_PRUNE.", "We propose IntegratedPIFu, a new pixel aligned implicit model that builds on the foundation set by PIFuHD. IntegratedPIFu shows how depth and human parsing information can be predicted and capitalised upon in a pixel-aligned implicit model. In addition, IntegratedPIFu introduces depth oriented sampling, a novel training scheme that improve any pixel aligned implicit model ability to reconstruct important human features without noisy artefacts. Lastly, IntegratedPIFu presents a new architecture that, despite using less model parameters than PIFuHD, is able to improves the structural correctness of reconstructed meshes. Our results show that IntegratedPIFu significantly outperforms existing state of the arts methods on single view human reconstruction. Our code has been made available online.", "Intermediate deep visual feature compression and transmission is an emerging research topic, which enables a good balance among computing load, bandwidth usage and generalization ability for AI-based visual analysis in edge-cloud collaboration. Quantization and the corresponding rate-distortion optimization are the key techniques in deep feature compression. In this paper, by exploring the feature statistics and a greedy iterative algorithm, we propose a channel-wise bit allocation method for deep feature quantization optimizing for network output error. Given the limited rate and computational power, the proposed method can quantize features with small information loss. Moreover, the method also provides the option to handle the trade-offs between computational cost and quantization performance. Experimental results on ResNet and VGGNet features demonstrate the effectiveness of the proposed bit\u00a0\u2026", "More often than not, practical application scenarios call for systems to be capable of dealing with input visual signals with low resolution/quality or environmental illumination. This talk will introduce related recent research in super-resolution reconstruction, signal quality assessment, content enhancement, and person re-identification for low-resolution or varying illumination. We will also discuss possible new research attempts to advance the relevant techniques.", "Recently, various view synthesis distortion estimation models have been studied to better serve 3-D video coding. However, they can hardly model the relationship quantitatively among different levels of depth changes, texture degeneration, and view synthesis distortion (VSD), which is crucial for rate-distortion optimization and rate allocation. In this paper, an auto-weighted layer representation based view synthesis distortion estimation model is developed. Firstly, sub-VSD (S-VSD) is defined according to the level of depth changes and their associated texture degeneration. After that, a set of theoretical derivations demonstrate that the VSD can be approximately decomposed into the S-VSDs multiplied by their associated weights. To obtain the S-VSDs efficiently, a layer-based representation method is developed, where all the pixels with the same level of depth changes are represented with a layer. It enables the\u00a0\u2026", "Salient object detection in optical remote sensing images (ORSI-SOD) has been widely explored for understanding ORSIs. However, previous methods focus mainly on improving the detection accuracy while neglecting the cost in memory and computation, which may hinder their real-world applications. In this article, we propose a novel lightweight ORSI-SOD solution, named CorrNet, to address these issues. In CorrNet, we first lighten the backbone (VGG-16) and build a lightweight subnet for feature extraction. Then, following the coarse-tofine strategy, we generate an initial coarse saliency map from high-level semantic features in a correlation module (CorrM). The coarse saliency map serves as the location guidance for lowlevel features. In CorrM, we mine the object location information between high-level semantic features through the cross-layer correlation operation. Finally, based on low-level detailed features, we refine the coarse saliency map in the refinement subnet equipped with dense lightweight refinement blocks (DLRBs) and produce the final fine saliency map. By reducing the parameters and computations of each component, CorrNet ends up having only 4.09 M parameters and running with 21.09 G FLOPs. Experimental results on two public datasets demonstrate that our lightweight CorrNet achieves competitive or even better performance compared with 26 state-of-the-art methods (including 16 large CNN-based methods and two lightweight methods), and meanwhile enjoys the clear memory and run-time efficiency. The code and results of our method are available at https://github. com/MathLee/CorrNet.", "This protocol presents the Variant Nucleotide Guard (VaNGuard) assay, which is robust towards viral mutations and can be performed on purified RNA or directly on nasopharyngeal (NP) swab samples. The procedure typically comprises three parts, namely sample preparation, RT-LAMP reaction, and Cas12a-based detection via fluorescence or lateral flow assay. Sample preparation from NP swabs involves Proteinase K digestion followed by heat inactivation. Purified RNA or digested NP swab samples are then added as templates into RT-LAMP reactions and incubated at 65\u00baC for 22 minutes. Next, enAsCas12a and ssDNA-probes are added and the reactions are incubated at 60\u00baC for another 5 minutes. End-point fluorescence can be detected by a plate reader or a real-time PCR machine. Alternatively, a lateral flow strip can be inserted into each reaction tube for equipment-free read-out. The VaNGuard assay is a rapid and convenient point-of-care test for SARS-CoV-2 and is applicable to resource poor settings.", "This protocol presents the Variant Nucleotide Guard (VaNGuard) assay, which is robust towards viral mutations and can be performed on purified RNA or directly on nasopharyngeal (NP) swab samples. The procedure typically comprises three parts, namely sample preparation, RT-LAMP reaction, and Cas12a-based detection via fluorescence or lateral flow assay. Sample preparation from NP swabs involves Proteinase K digestion followed by heat inactivation. Purified RNA or digested NP swab samples are then added as templates into RT-LAMP reactions and incubated at 65\u00baC for 22 minutes. Next, enAsCas12a and ssDNA-probes are added and the reactions are incubated at 60\u00baC for another 5 minutes. End-point fluorescence can be detected by a plate reader or a real-time PCR machine. Alternatively, a lateral flow strip can be inserted into each reaction tube for equipment-free read-out. The VaNGuard assay is a rapid and convenient point-of-care test for SARS-CoV-2 and is applicable to resource poor settings.", "In this paper, we propose a new convolutional layer for neural networks on unordered and irregular point set. Most research advanced to date usually face multiple problem related to point cloud density and may require ad-hoc neural network architectures, which overlooks the huge treasure of architectures from computer vision or language processing. To mitigate these shortcomings, we process a point set at its distribution level by introducing statistical convolution (StatsConv). The spotlight feature of StatsConv is that it extracts various statistics to characterize the distribution of the input point set, which makes it highly scalable compared to existing point convolution operators. StatsConv is fundamentally simple, and can be used as a drop-in in any contemporary neural network architecture with negligible changes. Thorough experiments on point cloud classification and segmentation demonstrate the competence\u00a0\u2026", null, "Aesthetics has been an area of intensive interests and continuing exploration for long in multiple disciplines, such as philology, psychology, arts, photography, computer graphics, media, industrial design, and so on. Objective image aesthetic assessment (IAA) is related to three major considerations. First of all, technical quality assessment (TQA) of images still plays an important role in general, because basic visual features (e.g., contrast, brightness, colorfulness and semantic information) definitely influence humans' perception and experience. TQA has been already relatively better developed during the past two decades, so to be successful, IAA needs to focus more on the other two considerations that are special to it. The first consideration special to IAA is generic IAA (GIAA) which deals with aesthetic factors common to a typical human being or user, with examples of Rule of Thirds, symmetry, depth of field\u00a0\u2026", "We propose a generative framework that tackles video frame interpolation. Conventionally, optical flow methods can solve the problem, but the perceptual quality depends on the accuracy of flow estimation. Nevertheless, a merit of traditional methods is that they have a remarkable generalization ability. Recently, deep convolutional neural networks (CNNs) have achieved good performance at the price of computation. However, to deploy a CNN, it is necessary to train it with a large-scale dataset beforehand, not to mention the process of fine tuning and adaptation afterwards. Also, despite the sharp motion results, their perceptual quality does not correlate well with their pixel-to-pixel difference metric performance due to various artifacts created by erroneous warping. In this paper, we take the advantages of both conventional and deep-learning models, and tackle the problem from a different perspective. The\u00a0\u2026", "In this work, we propose a method for the super-resolution of images in the presence of impulse noise. First, the impulse noise locations are identified using a detector and then, an optimization problem is solved to reconstruct the high-resolution image. Further, we propose the concept of image reconstruction using multiple bases and 3D filtering to improve the performance of the proposed method. We call this concept as MB3D. We apply the proposed method along with MB3D on various datasets to test its efficacy. We also perform experiments on some real noisy images.", null, "The latest advanced video coding standard H. 264/AVC [1] improves rate-distortion performance significantly compared to its predecessors (eg, H. 263) and competitors (eg, MPEG-2/4) by introducing a number of innovative ideas in Intra-and Inter-frame coding [2, 3]. Major performance improvement has taken place by means of motion estimation (ME) and motion compensation (MC) using variable block size, sub-pixel search, and multiple reference", "This year we received about 760 paper submissions and accepted 440 of them for presentation in either oral or poster sessions, including the special sessions. In addition, there are about IS0 papers from ICASSP that will also be presented at ICME 2003. The technical program contains all topics of interests related to the field of multimedia, and the quality of papers selected in the program is very good. We have authors from 52 countries all over the world making ICME a truly intemational conference.The selection of outstanding papers for the program is not an easy task. First, multimedia is a diverse area involving several technical disciplines that are very different from each other. Second, it was necessary to mobilize approximately 250 reviewers to accomplish the reviews in time. The technical program team did an excellent job in coordinating this year\u2019s review process! I would like to thank the Technical Program\u00a0\u2026", "\u00a9 2017 Institution of Engineering and Technology (IET). This paper was published in Electronics Letters and is made available as an electronic reprint (preprint) with permission of Institution of Engineering and Technology (IET). The published version is available at:[http://dx. doi. org/10.1049/el. 2017.0795]. One print or electronic copy may be made for personal use only. Systematic or multiple reproduction, distribution to multiple locations via electronic or other means, duplication of any material in this paper for a fee or for commercial purposes, or modification of the content of the paper is prohibited and is subject to penalties under law.", "Extracting stable features to enhance object representation has proved to be very effective in improving the performance of object tracking. To achieve this, mining techniques, such as K-means clustering and data associating, are often adopted. However, K-means clustering needs the pre-set number of clusters. Real scenarios (heavy occlusion and so on) often make the tracker lose the target object. To handle these problems, we propose an intraframe clustering and interframe association (ICIA)-based stable feature mining algorithm for object tracking. The value (in HSV space) peak contour is employed to automatically estimate the number of clusters and classify value and saturation colors of the object region to get connected subregions. Every subregion is described with observation and increment models. Multi-feature distancesbased subregion association, between the current object template and the current\u00a0\u2026", "Interacting with images through social media has become widespread due to ubiquitous Internet access and multimedia enabled devices. Through images, users generally present their daily activities, preferences or interests. This study aims to identify the way and extent to which personality di erences measured as using the Big Five model are related to online image posting and liking. In two experiments, the larger consisting of\u223c 1.5 million Twitter images both posted and liked by\u223c 4,000 users, we extract interpretable semantic concepts using large-scale image content analysis and analyze di erences speci c of each personality trait. Predictive results show that image content can predict personality traits, and that there can be signi cant performance gain by fusing the signal from both posted and liked images.", "In applications that require an input point cloud to be matched with a set of database point clouds present on a remote server, it is preferable to compress and transfer 3D feature descriptors online, rather than compressing and transferring the whole input point cloud. This is because the former would require much lesser bandwidth and does not require feature extraction on the server. Existing real valued 3D feature descriptors that offer good keypoint matching performance require higher bandwidth for their transfer over the network. On the other hand, the existing binary 3D feature descriptor requires relatively less bandwidth but offers reduced keypoint matching performance. In this paper, we propose to employ lattice quantization to efficiently compress 3D feature descriptors. These compressed 3D feature descriptors can be directly matched in compressed domain without any need for decompression, hence drastically reducing the memory footprint and computational requirements. We also propose double stage lattice quantization to achieve even more compression in the case of SHOT 3D feature descriptor. We provide a spectrum of possible bit rates and achievable keypoint matching performance for three state-of-the-art 3D feature descriptors. Experimental evaluation on publicly available benchmark dataset highlights that the compressed 3D feature descriptors require much lesser bandwidth and yet offer good keypoint matching performance. The source code is made publicly available for the benefit of the community.", null, null, "There has been increasing interest in visual quality assessment (VQA) during recent years. Of all these VQA methods, machine learning (ML) based ones became more and more popular. In this book, ML-based VQA and related issues have been extensively investigated. Chapters                    1                                    \u2013                   2                                     present the fundamental knowledge of VQA and ML. In Chap.                    3                                    , ML was exploited for image feature selection and image feature learning. Chapter                    4                                     presents two ML-based frameworks for pooling image features of an image into a number score. In Chap.                    5                                    , two metric fusion frameworks designed to combine multiple existing metrics into a better one, were developed by the aid of ML tools.", "In learning-based image quality assessment, images are represented by features with low dimension much less than the size of image. The features can be obtained by the aid of priori knowledge that people have gained; for example, the aforementioned basic and advantage features. There is also increasing interest in learning-based features which are co-trained along with the learning tasks. For example, the so-called \u201cdeep learning\u201d techniques are studied extensively recently to learn a task-oriented feature. Feature extraction and selection are performed to construct more efficient features of image by compressing the length of feature vectors in order to reduce computational complexity and, more importantly, to avoid overfitting risk as the small number of samples are used in training process. After feature extraction and selection, we need to map image features onto image quality value which is a real\u00a0\u2026", "Local structure, e.g., local binary pattern (LBP), is widely used in texture classification. However, LBP is too sensitive to disturbance. In this paper, we introduce a novel structure for texture classification. Researches on cognitive neuroscience indicate that the primary visual cortex presents remarkable orientation selectivity for visual information extraction. Inspired by this, we investigate the orientation similarities among neighbor pixels, and propose an orientation selectivity based pattern for local structure description. Experimental results on texture classification demonstrate that the proposed structure descriptor is quite robust to disturbance.", null, "Coding artifacts are annoying in highly compressed signals. Most of the existing artifact reduction methods are designed for one specific type of artifacts, codecs, and bitrates, which are complex and exclusive for one type of artifact reduction. Since both the compressed image/video and the coding error contain information of the original signal, they are highly correlated. Therefore, we try to recover some lost data based on the correlation between the compressed signal and the coding error, and introduce a novel and universal artifact reduction method. Firstly, according to the spatial correlation among pixels, a pixel-adaptive anisotropic filter is designed to reconstruct the distorted signal. Next, a globally optimal filter is designed to further recover the coding loss. Experimental results demonstrate that within an extensive range of bitrates, the proposed method achieves about 0.8\u00a0dB, 0.45\u00a0dB, 0.3\u00a0dB, and 0.2\u00a0dB on\u00a0\u2026", "It is now widely acknowledged that the adoption of new multimodal media necessitates mechanisms in order to assess and evaluate perceived quality. Indeed, the problem of understanding and enhancing Quality of Experience (QoE) in complex, distributed and diverse environments has been and is continuing to be the subject of intense research investigation. Considerable efforts have been devoted to assessing QoE via objective or subjective means for new and emerging multimedia services over modern fixed/mobile devices, such as: IPTV (Internet Protocol television), HDTV/UHV (high/ultra high definition television), 3DTV (3D television), video call, 3D smartphone, mobile graphics and gaming. The combination of multiple media in a single application (multimodality) and the need to keep multiple media in synch across multiple parties means that the problem of capturing and controlling QoE becomes multidimensional and cannot be addressed through existing methods. The Special Issue covers different aspects of QoE. Computer graphics, 3D video and multimodal applications are emerging as new media and multimedia. The first paper in this special issue is entitled \u2018\u2018Saliency detection in computer rendered images based on object-level contrast\u2019\u2019by Lu Dong, Weisi Lin, Yuming Fang, Shiqian Wu and Hock Soon Seah. They propose a novel graphic saliency detection method to detect visually salient objects in images rendered from 3D geometry models. By computing object-level contrast, the boundaries of salient objects can be detected accurately and thus, the salient objects can be detected effectively. The second paper is entitled \u2018\u2018A\u00a0\u2026", null, null, "Enriching the abstract tags for social images is important for keyword-based social image search and retrieval. The paper entitled \u201cAutomatic Abstract Tag Detection for Social Image Tag Refinement and Enrichment\u201d by Xia et al. constructs the concept ontology with three-level semantics to detect the candidates of abstract tags. Based on the concept ontology, new tags can also be added to enrich the tags of social images. The proposed methods were compared with other existing approaches to demonstrate its effectiveness.Multimedia event detection is an important research topic due to its application in video indexing and retrieval. In the paper entitled \u201cMultimedia Event Detection Using Segmentbased Approach for Motion Feature,\u201d Phan et al. propose a new approach for multimedia event detection by partitioning each video into segments for feature extraction and classification. The experimental results on the\u00a0\u2026", "In this work, we propose a method to detect visually salient objects in computer synthesized images from 3D meshes. Different from existing detection methods on graphic saliency which compute saliency based on pixel-level contrast, the proposed method computes saliency by measuring object-level contrast of each object to the other objects in a rendered image. Given a synthesized image, the proposed method first extracts dominant colors from each object, and represents each object with the dominant color descriptor (DCD). Saliency is measured as the contrast between the DCD of the object and the DCDs of its surrounding objects. We evaluate the proposed method on a data set of computer rendered images, and the results show that the proposed method obtains much better performance compared with existing related methods.", "A model of visual masking, which reveals the visible threshold of human perception, is useful in perceptual based image/video processing. The existing visual masking formulation, which mainly considers luminance contrast, cannot accurately estimate the visible threshold. Recent researches indicate that human perception is highly adaptive to extract orderly structures and is insensitive to disorderly structures. Therefore, we suggest that the structural characteristic is another determining factor for visual masking, and deduce a novel visual masking function based on structural uncertainty. Experimental results demonstrate that the proposed model is more consistent with human perception than the existing visual masking model.", "\u6b63 Improving the quality and experience perceived by the user is fundamental when developing multimedia technologies, products, and services. Quality of experience (QoE) involves subjective perception, user behavior and needs, appropriateness, context, and usability of delivered content. Modeling QoE is critical for enhancing QoE in various multimedia applications.", "This chapter introduces applications of visual attention models in image processing related areas: just noticeable difference (JND) modelling, visual quality assessment, image and video coding, visual signal retargeting and compressive sensing. Section 8.1 illustrates the combination of visual attention and the JND model towards a complete visibility threshold model which can be used in a wide spectrum of uses. The application of attention models in quality assessment (QA) of images and video is presented in Section 8.2, and a typical QA index, SSIM, is weighted by visual attention in different ways. The use of attention models to explore visual redundancy for better image coding is introduced in Section 8.3. The applications for image retargeting and compressive sensing are presented in Sections 8.4 and 8.5, respectively.", "At the beginning of this chapter, visual attention is briefly introduced from both biological and engineering perspectives. As the emphasis of this book, saliency map computational models lying in the intersection area for biology, psychology and engineering are highlighted again. Section 9.1 summarizes the content of the whole book, and presents the connection between chapters and sections. In Section 9.2, several critical issues of visual attention modelling are discussed, while some final conclusions are given in Section 9.3.", "The piezoelectric response from \u03b2-phase poly (vinylidene fluoride)(PVDF) can potentially be exploited for biomedical application. We hypothesized that \u03b1 and \u03b2-phase PVDF exert direct but different influence on cellular behavior. \u03b1-and \u03b2-phase PVDF films were synthesized through solution casting and characterized with FT-IR, XRD, AFM and PFM to ensure successful fabrication of \u03b1 and \u03b2phase PVDF films. Cellular evaluation with L929 mouse fibroblasts over one-week was conducted with AlamarBlue\u00ae metabolic assay and PicoGreen\u00ae proliferation assay. Immunostaining of fibronectin investigated the extent and distribution of extracellular matrix deposition. Image saliency analysis quantified differences in cellular distribution on the PVDF films. Our results showed that \u03b2-phase PVDF films with the largest area expressing piezoelectric effect elicited highest cell metabolic activity at day 3 of culture. Increased\u00a0\u2026", "This chapter assesses the performance of the saliency detection models described in the previous chapters. As with many other cases in engineering, a developed visual attention model needs to be critically benchmarked against other models, and then fully tested before being used in particular applications and situations. A number of qualitative and quantitative evaluation methods, as well as related ground-truth databases, are introduced in this chapter. Common benchmarks include simple man-made visual patterns, human-labelled images and eye tracking data, which are first given in Sections 6.1-6.3. The quantifying estimation of performance of the computational models is listed in Sections 6.4-6.6. The most commonly used criteria are PPV, TPR, F-measure, ROC and AUC, as introduced in Section 6.4. The statistical criteria for both static and dynamic scene - NNS and KL distance - are presented in Section\u00a0\u2026", "This chapter describes the computational visual attention models in the spatial domain, based on the bottom-up mechanism. Although there have been a large number of bottom-up computational models in the spatial domain since 1998, this chapter only discusses a few typical computational models: baseline saliency (BS) model, models based on neural networks and models based on statistical signal processing theory, such as information theory (the AIM model), decision-theory (the DISC model) natural statistical (the SUN model) and Bayesian theory (the surprise detection model). Section 3.1 introduces the major parts of the BS system, while Section 3.2 addresses the issues related to visual attention for video. These two sections aim to give the reader the most important ideas for modelling bottom-up visual attention in the spatial domain. Section 3.3 presents more details and variations of the BS model, to\u00a0\u2026", null, "Although wireless sensor networks (WSNs) are powerful in monitoring physical events, the data collected from a WSN are almost always incomplete if the surveyed physical event spreads over a wide area. The reason for this incompleteness is twofold: i) insufficient network coverage and ii) data aggregation for energy saving. Whereas the existing recovery schemes only tackle the second aspect, we develop Dual-lEvel Compressed Aggregation (DECA) as a novel framework to address both aspects. Specifically, DECA allows a high fidelity recovery of a widespread event, under the situations that the WSN only sparsely covers the event area and that an in-network data aggregation is applied for traffic reduction. Exploiting both the low-rank nature of real-world events and the redundancy in sensory data, DECA combines matrix completion with a fine-tuned compressed sensing technique to conduct a dual-level reconstruction process. We demonstrate that DECA can recover a widespread event with less than 5% of the data (with respect to the dimension of the event) being collected. Performance evaluation based on both synthetic and real data sets confirms the recovery fidelity and energy efficiency of our DECA framework.", "This paper presents an overview of state-of-the-art technologies for perceptual processing of digital pictures, as well as a discussion of the issues related to their implementation, optimization and testing. The paper begins with a brief description of the main computational modules that are used as part of a perceptual-based visual signal processing framework. Then, a number of perceptual-based visual processing techniques and applications to which perceptual models are presented, including image/video compression, visual signal quality evaluation, and computer graphics. The most significant research efforts are highlighted for each topic, and a number of issues and views are put forward regarding the related research and opportunities.", "In this work, a sample rate estimator is proposed for selective graphics rendering so that the samples are allocated such that the best perceived image quality can be achieved under a given sample budget. In the proposed estimator, the sample rate of a pixel is decided by not only the visual attention (VA) level of the region to which the pixel belongs but also the required rendering complexity (RC) level of the pixel. The VA and RC values are determined based on the phase-spectrum of the Fourier transform (PFT). Compared with existing sample rate estimators for selective rendering that only consider the VA information, the proposed estimator helps to produce synthesized images with higher perceived quality using the same number of samples.", "In this paper, we propose a new psychovisual quality metric of images based on recent developments in brain theory and neuroscience, particularly the free-energy principle. The perception and understanding of an image is modeled as an active inference process, in which the brain tries to explain the scene using an internal generative model. The psychovisual quality is thus closely related to how accurately visual sensory data can be explained by the generative model, and the upper bound of the discrepancy between the image signal and its best internal description is given by the free energy of the cognition process. Therefore, the perceptual quality of an image can be quantified using the free energy. Constructively, we develop a reduced-reference free-energy-based distortion metric (FEDM) and a no-reference free-energy-based quality metric (NFEQM). The FEDM and the NFEQM are nearly invariant to many global systematic deviations in geometry and illumination that hardly affect visual quality, for which existing image quality metrics wrongly predict severe quality degradation. Although with very limited or even without information on the reference image, the FEDM and the NFEQM are highly competitive compared with the full-reference SSIM image quality metric on images in the popular LIVE database. Moreover, FEDM and NFEQM can measure correctly the visual quality of some model-based image processing algorithms, for which the competing metrics often contradict with viewers' opinions.", "This paper presents a framework for video coding, which is compatible with the existing H.264/AVC standard (for preprocessing). Since the video sequences are of 3D data matrices and the traditional XY image frame plane is not always the best for video coding in terms of rate-distortion (RD) performance, optimal compression plane (OCP) determination models are developed to improve the coding efficiency. We design coding plane level RD cost function (in analog to the one in H.264 macroblock level) to measure the RD performance of each coding plane. We jointly consider the available computational resource and the RD performance in order to determine an adaptive OCP. The performance of the proposed coding framework is assessed with a number video sequences. Extensive experimental results show that the new coding framework can achieve better RD performance at approximately the same\u00a0\u2026", "Better image coding quality can be achieved by down sampling the visual content prior to compression and restore the full resolution content at the decoder when a higher compression rate is required. In this paper, a perception based adaptive sampling for low bit rate coding method is proposed. Sampling directions/ratios and the corresponding quantization parameters are determined adaptively, with the consideration of not only the local visual content but also the human attention model. Experiments show that the proposed method can yield more desirable compressed visual content than the other sampling based coding methods (and also in comparison with coding methods without down sampling) and automate the switching of the sampling modes for variable bit rate application.", "The visual saliency map represents the most attractive regions in video. Automatic saliency map determination is important in mobile video applications such as autofocusing in video capturing. It is well known that motion plays a critical role in visual attention modeling. Motion in video consists of camera's motion and foreground target's motion. In determining the visual saliency map, we are concerned with the foreground target's motion. To achieve this, we evaluate the camera/global motion and then identify the moving target from the background. Specifically, we propose a three-step procedure for visual saliency map computation: 1) motion vector (MV) field filtering, 2) background extraction and 3) contrast map computation. In the first step, the mean value of the MV field is treated as the camera's motion. As a result, the MV of the background can be detected and eliminated, and the saliency map can be roughly\u00a0\u2026", "Al-Fahoum and Reza [1] characterized the blocking artifacts in block based DCT (BDCT) compressed image into five types: grid noise, staircase noise, ringing artifacts, corner outliers and the corruption of edges. Most of the comprehensive deblocking algorithms lack a unified framework, and different artifacts are processed with independent ad hoc schemes. In this paper, we propose a comprehensive postprocessing method for removing all the blocking-related artifacts in the framework of overcomplete wavelet expansion (OWE). We use the wavelet transform modulus maxima extension (WTMME) and angle extracted from the wavelet coefficients of 3-level OWE to represent the image. Both the WTMME and the angle image are reconstructed accordingly using inter-/ intra-band correlation to suppress the influence of the distortions. Simulation and comparative study have demonstrated the effectiveness of the\u00a0\u2026", "Super-resolution reconstruction (SR) has been widely used to produce a high resolution (HR) image from several low resolution (LR) ones. In current methods, a LR image is selected as benchmark and upsampled as the initial SR estimate. This SR estimate is then degraded and compared with the adjacent LR frames for correction. Considering LR images captured from the same HR image with different translation at different instants, SR outputs by different benchmark selection should be identical, so tighter constraints can be designed to limit SR indetermination and to produce better SR images. In this paper, we propose a novel SR framework and prove its efficiency statistically using the unbiased estimation. Experimental results indicate that the proposed algorithm outperforms some existing approaches in both subjective and objective terms.", "In this paper, we propose a new deblocking method for DCT-based compressed images using anisotropic diffusion. An adaptive anisotropic coefficient, derived from a neuro-fuzzy network, is introduced. The optimal diffusion parameters are estimated using a stochastic optimization technique, which ensures that heavy diffusion performs across the block boundaries and in flat regions while image edges are well preserved. Experimental results have shown that the proposed adaptive diffusion method can significantly suppress coding artifacts and improve the visual quality of the compressed image.", null, "Captured images may suffer from multiple kinds of distortions, eg, rain, haze, and real-world sensory noise, which severely degrades image qualities and affects subsequent computer vision tasks. It is of great significance to design a general image restoration architecture that can work effectively on different kinds of degradation. In this paper, we propose a general image restoration architecture to exploit the global modeling capability of transformer and local modeling capability of convolutions, which integrates multi-dimensional dynamic attention and transformer into a U-Net architecture. Specifically, only CNNs are employed in the encoder-decoder, and transformers are deployed in the latent layer to fully exploit multi-scale structural differences. Furthermore, instead of learning static convolutional kernels for different inputs, the inut-independent multi-dimensional dynamic attention is designed for convolutional kernels along diverse dimensions in the encoder-decoder by cascading spatial-wise, channel-wise, and filter-wise attentions, which has the potential ability of revealing the degradation patterns. Ablation experiments indicate that it is effective that CNNs are deployed in the encoder-decoder while the transformers in the latent layer. Substantial experiments demonstrate that, compared with extensive image restoration networks, the proposed general image restoration architecture achieves superior performance with lower or comparable computational complexity on five image restoration tasks, such as image deraining, deblurring, denoising, dehazing, and enhancement. The source code will be available at https://github. com/House-yuyu\u00a0\u2026", "In Fig. 1, we show S-PIFu\u2019s results when given images of test subjects who wear large clothings (eg jackets/coats). Images of these test subjects have pixels that belong to human subject but not to the SMPL-X body, and yet S-PIFu is able reconstruct the human subjects accurately.Pixels that belong to human subject but not to the SMPL-X body act as a natural regularizer that prevents S-PIFu from being overly reliant on estimated SMPL-X meshes to reconstruct clothed human meshes. This happens because these pixels only have valid values for the RGB channels and not the channels of our 2D feature maps (ie C, B, and N. Recall that C refers to coordinate information, B refers to blendweights-based labels, and N refers to body part orientation information).", "In this paper, we propose Deep Holographic Networks (DHN) to learn similarity metrics of videos for multi-label video search. DHN introduces a holographic composition layer to explicitly encode similarity metrics at intermediate layer of the network, instead of conventional deep metric learning approaches driven by ranking losses. The holographic composition layer is parameter-free and enables less memory footprint, compared with state-of-the-art. Towards multi-label video search at large scale, we present a new video benchmark built upon the YouTube-8M dataset. Extensive evaluations on this dataset demonstrate that DHN performs better than traditional deep metric learning approaches as well as other compositional networks.", null, null, null, null, null, null, null, "This proposal encompasses an objective video quality measurement method to automatically measure the perceived quality of a stream of video images. The method is based on a combined measure of distortioninvisibility, block-fidelity, and content richness fidelity.There is a need for automatic and objective video quality measurement method that is able to emulate the human vision to detect the perceived quality of a video stream. Traditionally, video quality is performed via a subjective test where a large number of human subjects are used to gauge the quality of a video but this process is not only time-consuming but and tedious and expensive to perform.", null, null, null, null, "Digital images are acquired, synthesized, enhanced, watermarked, compressed, transmitted, stored, reconstructed, evaluated, authenticated, displayed, or printed before being presented to the human visual system (HVS). In various image processing tasks, visual signal is processed for quality improvement, compact signal representation or efficient data protection. It is well known that the HVS cannot sense all changes in an image due to the underlying physiological and psychological mechanisms.Just-noticeable difference (JND) refers to the visibility threshold below which any change cannot be detected by the HVS. Its determination is a challenging task, because it is related to the HVS characteristics, as well as the cognitive process in the human brain, and is adaptive to the contents of the visual signal under consideration.", null, "The human visual system is highly adaptive to extract structure information for scene perception, and structure character is widely used in perception-oriented image processing works. However, the existing structure descriptors mainly describe the luminance contrast of a local region, but cannot effectively represent the spatial correlation of structure. In this paper, we introduce a novel structure descriptor according to the orientation selectivity mechanism in the primary visual cortex. Researches on cognitive neuroscience indicate that the arrangement of excitatory and inhibitory cortex cells arise orientation selectivity in a local receptive field, within which the primary visual cortex performs visual information extraction for scene understanding. Inspired by the orientation selectivity mechanism, we compute the correlations among pixels in a local region based on the similarities of their preferred orientation. By imitating the arrangement of the excitatory/inhibitory cells, the correlations between a central pixel and its local neighbors are binarized, and the spatial correlation is represented with a set of binary values, which is named as the orientation selectivity based pattern. Then, taking both the gradient magnitude and the orientation selectivity based pattern into account, a rotation invariant structure descriptor is introduced. The proposed structure descriptor is applied in texture classification and reduced reference image quality assessment, as two different application domains to verify its generality and robustness. Experimental results demonstrate that the orientation selectivity based structure descriptor is robust to disturbance, and can effectively\u00a0\u2026", "Committees and welcome Page 1 GC13 WS - QoEMC: Globecom 2013 Workshop - Quality of \nExperience for Multimedia Communications - Committees and Welcome Technical Program \nCommittee Savvas Argyropoulos StreamOwl Greece Luigi Atzori University of Cagliari Italy Ivan \nBajic Simon Fraser University Canada Georgios Baltoglou KTH - Royal Institute of Technology \nSweden Paolo Bellavista University of Bologna Italy Matthieu Bloch Georgia Institute of \nTechnology France Periklis Chatzimisios Alexander TEI of Thessaloniki Greece Chang Wen \nChen State University of New York at Buffalo USA Tasos Dagiuklas Hellenic Open University \nGreece Carl Debono University of Malta Malta Chenwei Deng Beijing Institute of Technology \nPR China Sebastian Egger AIT Austrian Institute of Technology GmbH Austria Markus \nFiedler Blekinge Institute of Technology Sweden Alessandro Floris University of Cagliari Italy \u2026", "In this paper, a new method for perceptual video quality evaluation is proposed. The method is based on multi-feature of visual perception and various coding artifacts. A radial basis function neural network is used to give discrimination. The proposed method has been tested using full set of 50 Hz VQEG test data, and the results show that a good correlation coefficient of more than 0.90 with the subjective mean opinion score (MOS) is achieved.", "Visual Processing Driven by Perceptual Quality Gauge: A Perspective Page 1 Visual Processing \nDriven by Perceptual Quality Gauge: A Perspective Weisi Lin, Zhongkang Lu, Susanto \nRahardja, EePing Ong and Susu Yao Media Processing Department Institute for Infocomm \nResearch, Singapore Page 2 Outline of Presentation \u2022 Review on \u2013 perceptual visual quality \ngauges \u2013 perceptual image/video processing \u2022 Some of our recent research attempts \u2013 visual \nquality evaluation \u2013 perceptual signal maniputions \u2022 Concluding remarks Page 3 Facts about \nVisual Quality Evaluation as a standalone metric: \u2022image evaluation \u2022algorithm benchmarking \nas an embedded module: shaping algorithms/systems The HVS: ultimate appreciator of \nmost images PSNR/MSE/MAE: not matching the HVS perception Perceptual metrics so far: \n\u2022much research interest (VQEG, IEEE G-2.1.6, many others) \u2022a difficult odyssey \u2022existence of \u2026", "This contribution consists of two parts. One is on the geometrical relationship between two successive spatial resolutions, which is based on the proposal from Sharp [1]. The other is on the prediction between syntaxes in picture parameter set and slice header that are related to ROI [2].", "In this paper, we propose a visual quality evaluation method based on visible error between original and distorted images using wavelet just-noticeabledistortion (JND) profile. The error below visual thresholds in each frequency bands is ignored while some errors that take place in the content-activity regions are masked. By measuring the visible differences of two inputs and visual masking a quantitative image quality evaluation formula is obtained. Experiments on VQEG test sequences have shown that the proposed method can achieve very good correlation with subjective quality scores.", "Human eyes are highly selective sensors. The selectivity depends on not only local contents, but also global cognitive (high-level) contents. The global selectivity, or visual attention, reflects on the resource allocation competition on both low-level and high-level visual contents. It is believed that because the computational resources in the human brain are limited, visual attention is a mechanism developed in brains to effectively allocate resources on visual field. More resources are allocated to important areas than less important areas. One of the most important aftereffects of visual attention is its modulatory effect on visual sensitivity and visual quality evaluation. The noise or distortions in attentional (important) areas are easier to be detected or bring more annoyance to human eyes. Perceptually, a perfect compression technique should follow the mechanisms of the human visual system. It should have the ability to\u00a0\u2026", "This paper presents a new visual quality metric for assessing compressed images and videos. The proposed method is based on the measurement of error spread and isotropic local contrast of the image. The perceived quality index is proportional to the error spread while inversely proportional to the image contrast. The effectiveness of the new metric has been verified by the experiments on JPEG and JPEG2000 compressed images test set and H. 264 compressed test video sequences and it is shown to give very good correlation with subjective scores.", "In this paper, a visual perception model is applied to a layered image compression system based on Wyner-Ziv coding. In order to achieve better perceptual quality at limited bit rate, we consider the perception-based quantizer using the combination of stimulus-driven model and cognition-driven model. Experimental results have shown that the system with the perception-based quantizer achieves significant improvement in visual quality.", "Visual signal is acquired, synthesized, enhanced, watermarked, compressed, transmitted, stored, reconstructed, evaluated, authenticated, displayed, or printed before being presented to the human visual system (HVS). It is well known that the HVS cannot sense all changes in an image due to its underlying physiological and psychological mechanisms, and therefore advantageous to incorporate knowledge of the HVS visibility thresholds into visual processing algorithms/systems, since the HVS is the ultimate receiver of the majority of processed images and video. With perceptual knowledge, the scarce system resources (computing power, bandwidth, memory space, display/printing resolution, and so on) can be allocated to achieve the maximum perceptual significance, accessory information(eg, for watermarking, authentication, and error protection) can be concealed in the regions with the least HVS sensitivity to the incurred changes, and visual quality of processed images can be evaluated for better alignment with the human perception. Incorporating the HVS visibility thresholds appropriately can play an important role in shaping and optimizing many image processing algorithms.Just-noticeable difference (JND) refers to the visibility threshold below which any change cannot be detected by the HVS [1-3]. Its determination in general is complex and challenging, because this is related to the HVS characteristics, as well as some recognition process in the human brain, and is adaptive to the contents of the visual signal under consideration. Other affecting factors include viewing conditions (such as viewing distance, ambient lighting, the context\u00a0\u2026", "Contrast masking (CM) on edge and textured regions have to be distinguished since distortions on edge regions are easier to be noticed than that on textured regions. Therefore, how to efficiently estimate the CM on edge and textured regions of an image is a key issue for accurate JND (Just Noticeable Difference) estimation. An enhanced image domain JND estimator is devised in this paper with new model for CM. We use the total variation method to obtain a structural image (which contains edge information) and a textural image (which contains texture information) from the input image, and then evaluate the CM for the two images separately rather than the whole image, and hence edge and texture are better distinguished and the under-estimation of JND on textured regions can be effectively avoided. Experimental results of subjective viewing confirm that the proposed model is capable of determining more accurate visibility thresholds.", "A simple yet effective parametric scheme is introduced for general, successive and flexible regulation of computational complexity in hybrid video encoding. It provides high-level adjustment with or without incorporating other fast motion estimation and DCT algorithms. It can serve as a straightforward means to map or re-map a coding process to a processor (with certain MIPS) or an ASIC design (eg, operated at a clock frequency constrained by the power consumption requirement). The same parameters facilitate automatic, adaptive and dynamic load balancing for optimum use of multiprocessor resources. The complexity level can be determined in the system design phase or the course of processing. When implemented with practical real-time video codecs, the scheme performs effectively and efficiently in complexity/load management and gracefully in picture quality variation."]}, "collaboration_network": {"target": [], "target_id": [], "type": [], "location": [], "year": [], "title": [], "link": []}, "published_by_year": {"Year": ["1992", "1993", "1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Publications": [1, 1, 0, 0, 0, 5, 1, 2, 0, 5, 2, 18, 15, 20, 14, 14, 19, 9, 19, 39, 38, 40, 39, 74, 65, 49, 19, 33, 12, 30, 32, 39, 30]}, "citations_by_year": {"Year": ["1992", "1993", "1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [0, 0, 0, 0, 0, 1, 5, 6, 5, 2, 8, 7, 43, 53, 95, 122, 193, 226, 347, 453, 535, 811, 1054, 1447, 1912, 2327, 2874, 3048, 2757, 2854, 3315, 2623, 130]}, "all_time_h_index": 85, "all_time_i10_index": 375, "all_time_i20_index": 292, "h_index_by_year": {"Year": [1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 3, 3, 5, 6, 7, 11, 13, 16, 18, 21, 25, 27, 31, 37, 42, 49, 55, 64, 69, 74, 80, 85]}, "h_index_by_publication_year": {"Publication Year": [1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 1, 0, 0, 0, 3, 1, 2, 0, 2, 2, 10, 8, 14, 10, 7, 10, 6, 11, 22, 18, 14, 18, 29, 33, 24, 12, 18, 6, 15, 11, 6]}, "avg_citations_by_publication_year": {"Publication Year": [1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [1.0, 29.0, 0.0, 0.0, 0.0, 7.2, 7.0, 3.0, 0.0, 3.0, 15.0, 26.666666666666668, 25.333333333333332, 79.85, 46.0, 15.785714285714286, 44.68421052631579, 18.0, 43.0, 86.76923076923077, 55.73684210526316, 35.45, 48.333333333333336, 42.74324324324324, 56.53846153846154, 55.3469387755102, 48.10526315789474, 32.39393939393939, 23.916666666666668, 25.8, 11.09375, 2.4358974358974357]}, "h_index_by_years_from_publication_year": {"Publication Year": [1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1992, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1993, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 5, 5, 6, 6, 6, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 2, 2, 3, 3, 6, 6, 6, 6, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 2, 5, 7, 10, 10, 11, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1, 2, 5, 6, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 2, 2, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 3, 6, 7, 8, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 2, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 1, 4, 5, 7, 8, 9, 9, 10, 11, 11, 11, 11, 11, 11, 2, 5, 10, 13, 15, 17, 17, 19, 19, 20, 21, 21, 22, 2, 7, 10, 12, 14, 16, 17, 17, 17, 17, 17, 18, 2, 7, 10, 13, 13, 13, 13, 13, 14, 14, 14, 3, 8, 11, 13, 15, 17, 17, 17, 18, 18, 4, 10, 15, 21, 24, 25, 27, 28, 29, 6, 13, 20, 25, 29, 32, 33, 33, 5, 11, 17, 19, 22, 23, 24, 3, 7, 9, 11, 12, 12, 3, 8, 13, 16, 18, 3, 5, 6, 6, 6, 11, 15, 6, 11, 6]}}