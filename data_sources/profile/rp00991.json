{"full_name": "Chia Liang Tien", "email": "asltchia@ntu.edu.sg", "google_scholar": "https://scholar.google.com/citations?hl=en&user=Eeolw80AAAAJ", "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp00991", "designation": "Associate Professor, School of Computer Science and Engineering", "image_path": "./profile_img/chia_liang_tien.jpg", "biography": "Liang-Tien, Chia received his B.Sc degree in Electrical and Electronics Engineering and PhD from Loughborough University (of Technology) in 1990 and 1994 respectively. During this period, he was awarded the University Traveling Prize for his academic achievements and he was a recipient of the Overseas Research Scholarship Award. He is currently an Associate Professor in the School of Computer Engineering, Nanyang Technological University, Singapore. He was the Director, Centre for Multimedia and Network Communications from 2002-2007 and is currently Head, Division of Computer Communications. Liang-Tien, Chia\u2019s research interests can be broadly categorized into two main areas, Internet related research with emphasis on the Semantic Web and Multimedia Understanding for Information Management through media analysis, annotation and adaptation. Related topics include multimedia storage & retrieval, multimedia processing, multimodal data fusion and multimedia adaptation/transmission.  He is involved in a number of funded research projects and he serves as a member on numerous conference program committees and reviews for some international journals and he has published over 100 referred international conference and journal papers. He was awarded the A*Star Overseas Attachment Programme and spent one month in Microsoft Research Asia. Liang-Tien, Chia is a council member of the Infocomm Technology Standards Committee and serves as the technical chairman of ITSC Plugfest 2006.  As a Singaporean, he is an active National Serviceman serving as a Signal Officer in the Singapore Armed Forces.", "orcid": null, "other_websites": ["http://cemnet.ntu.edu.sg/home/asltchia/index.html"], "bachelor_degree": null, "masters": null, "phd": null, "name": "Chia Liang Tien", "id": "rp00991", "publications": {"Publication Year": ["2010", "2010", "2012", "2013", "2004", "2012", "2004", "2008", "2005", "2005", "2012", "2006", "2010", "2017", "2010", "2005", "2011", "2005", "2013", "2010", "2005", "2005", "2011", "2006", "2005", "2005", "2009", "2004", "2004", "2009", "2014", "2010", "2021", "2005", "2008", "2005", "2004", "2008", "2005", "2008", "2004", "2011", "2006", "2006", "2010", "2010", "2012", "2012", "2004", "2004", "2003", "1994", "2007", "2004", "1998", "2007", "2007", "2005", "2013", "2010", "2008", "2016", "2010", "2009", "2009", "2010", "2005", "2008", "2004", "2009", "2006", "2006", "2004", "2017", "2012", "2009", "2006", "2011", "2010", "2007", "2007", "2005", "2004", "2013", "2010", "2009", "2009", "2008", "2007", "2004", "2016", "2008", "2005", "2005", "2005", "2004", "1997", "2007", "2007", "2006", "2006", "2012", "2009", "2009", "2007", "2007", "2006", "2006", "2005", "2005", "2012", "2012", "2011", "2010", "2010", "Unknown", "2010", "2007", "2005", "2005", "2004", "1994", "2019", "2019", "2011", "2010", "2007", "2006", "2006", "2005", "2005", "2005", "2004", "2004", "2014", "2011", "2008", "2007", "2007", "2007", "2007", "2006", "2004", "2004", "2004", "2001", "2000", "1999", "1997", "1996", "1994", "1994", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "Title": ["Local features are not lonely\u2013Laplacian sparse coding for image classification", "Kernel sparse representation for image classification and face recognition", "Laplacian sparse coding, hypergraph laplacian sparse coding, and applications", "Region-based saliency detection and its application in object recognition", "DAML-QoS ontology for web services", "Sparse representation with kernels", "HMM-based audio keyword generation", "Motion context: A new representation for human action recognition", "Salient region detection using weighted feature maps based on the human visual attention model", "Affective content analysis in comedy and horror videos by audio emotional event detection", "Fourier transform-based scalable image quality measure", "Does ontology help in image retrieval? A comparison between keyword, text ontology and multi-modality ontology approaches", "Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map", "Convolutional networks for voting-based anomaly classification in metal surface inspection", "Image-to-class distance metric learning for image classification", "Semantics in service discovery and QoS measurement", "Multi-layer group sparse coding\u2014For concurrent image classification and annotation", "Robust subspace analysis for detecting visual attention regions in images", "Regularized feature reconstruction for spatio-temporal saliency detection", "Improved saliency detection based on superpixel clustering and saliency propagation", "A new motion histogram to index motion content in video segments", "Adaptive hierarchical multi-class SVM classifier for texture-based image classification", "Nonintrusive quality assessment of noise suppressed speech with mel-filtered energies and support vector regression", "Multimodal semantic analysis and annotation for basketball video", "Adaptive local context suppression of multiple cues for salient visual attention detection", "Web services discovery with daml-qos ontology", "LT codes decoding: Design and analysis", "QoS-aware and federated enhancement for UDDI", "Region-of-interest based image resolution adaptation for mpeg-21 digital item", "Coherent phrase model for efficient image near-duplicate retrieval", "Concurrent single-label image classification and annotation via efficient multi-layer group sparse coding", "Cross-media retrieval using query dependent search methods", "Deep residual pooling network for texture recognition", "Service discovery and measurement based on DAML-QoS ontology", "Image retrieval with a multi-modality ontology", "QoS measurement issues with DAML-QoS ontology", "Ontology for nature-scene image retrieval", "Ontology enhanced web image retrieval: aided by wikipedia & spreading activation theory", "Semantic video indexing and summarization using subtitles", "Detection of visual attention regions in images using robust subspace analysis", "Audio keyword generation for sports video analysis", "Improved learning of I2C distance and accelerating the neighborhood search for image classification", "FRACTURE mining: Mining frequently and concurrently mutating structures from historical XML documents", "Affective content detection in sitcom using subtitle and audio", "Automatic image tagging via category label and web data", "Wikipedia2Onto: Building concept ontology automatically, experimenting with web image retrieval", "Spatiotemporal saliency detection via sparse representation", "Learning class-to-image distance via large margin and l1-norm regularization", "Mining maximal frequently changing subtree patterns from XML Documents", "Mining association rules from structural deltas of historical xml documents", "A unified approach to detection of shot boundaries and subshots in compressed video", "On the treatment of video cell loss in the transmission of motion-JPEG and JPEG images", "Semantic retrieval with enhanced matchmaking and multi-modality ontology", "A motion based scene tree for browsing and retrieval of compressed videos", "Information technology and the Internet: the Singapore experience", "Secure multi-path in sensor networks", "Efficient sampling of training set in large and noisy multimedia data", "Mining positive and negative association rules from XML query patterns for caching", "Learning image-to-class distance metric for image classification", "Scene classification using multiple features in a two-stage probabilistic classification framework", "Study on the distribution of DCT residues and its application to RD analysis of video coding", "Anomaly region detection and localization in metal surface inspection", "Multi-label learning by image-to-class distance for scene classification and image annotation", "Learning instance-to-class distance for human action recognition", "A latent model for visual disambiguation of keyword-based image search", "Web image concept annotation with better understanding of tags and visual features", "Global motion compensated key frame extraction from compressed videos", "Image retrieval++\u2014web image retrieval with an enhanced multi-modality ontology", "Looking at mapping, indexing & querying of MPEG-7 descriptors in RDBMS with SM3", "Attention-from-motion: A factorization approach for detecting attention objects in motion", "Event on demand with MPEG-21 video adaptation system", "A motion-based scene tree for compressed video content management", "MPEG-21 digital item adaptation by applying perceived motion energy to H. 264 video", "Phase fourier reconstruction for anomaly detection on metal surface using salient irregularity", "Video saliency detection with robust temporal alignment and local-global spatial contrast", "Concept model-based unsupervised web image re-ranking", "Dynamic programming-based reverse frame selection for VBR video delivery under constrained resources", "Stratification-based keyframe cliques for effective and efficient video representation", "Stratification-based keyframe cliques for removal of near-duplicates in video search results", "Mapping, indexing and querying of MPEG-7 descriptors in RDBMS with IXMDB", "NBgossip-neighborhood gossip with network coding based message aggregation", "Automatic generation of MPEG-7 compliant XML document for motion trajectory descriptor in sports video", "Automatic extraction of motion trajectories in compressed sports videos", "Background subtraction via coherent trajectory decomposition", "Wikipedia-assisted concept thesaurus for better web media understanding", "A Bayesian approach integrating regional and global features for image semantic learning", "Wikipedia2Onto---Adding Wikipedia Semantics to Web Image Retrieval", "Nbgossip: An energy-efficient gossip algorithm for wireless sensor networks", "Click4BuildingID@ NTU: Click for Building Identification with GPS-enabled Camera Cell Phone", "Salient object extraction combining visual attention and edge information", "Using material classification methods for steel surface defect inspection", "Image near-duplicate retrieval using local dependencies in spatial-scale space", "Attention region selection with information from professional digital camera", "Semantic analysis of basketball video using motion information", "SM3+: An XML database solution for the management of MPEG-7 descriptions", "Coefficient thresholding and optimized selection of the Lagrangian multiplier for nonreference frames in H. 264 video coding", "Characterization and source modeling of MPEG-2 VBR video source", "Image classification using tensor representation", "Scale adaptive visual attention detection by subspace analysis", "An event-driven sports video adaptation for the MPEG-21 DIA framework", "A repository adapter for resource management information", "Content on demand video adaptation based on MPEG-21 digital item adaptation", "Hierarchicalword image representation for parts-based object recognition", "Understanding tag-cloud and visual features for better annotation of concepts in NUS-WIDE dataset", "Advances in Multimedia Modeling: 13th International Multimedia Modeling Conference, MMM 2007, Singapore, January 9-12, 2007, Proceedings, Part II", "Codebook+: a new module for creating discriminative codebooks", "Efficient data reduction in multimedia data", "Web image clustering with reduced keywords and weighted bipartite spectral graph partitioning", "ARIRS: association rule based image retrieval system", "Efficient sampling: Application to image data", "A non-parametric visual-sense model of images\u2014extending the cluster hypothesis beyond text", "Topic based query suggestions for video search", "Exploiting local dependencies with spatial-scale space (s-cube) for near-duplicate retrieval", "Image retargeting in compressed domain", "Faceted topic retrieval of news video using joint topic modeling of visual features and speech transcripts", "In Process", "Learning to combine multi-resolution spatially-weighted co-occurrence matrices for image representation", "QoS-Aware Web Services Discovery with Federated Support for UDDI", "Easier sampling for audio event identification", "Enhancement layer rate control for high bitrate SNR scalable video coding", "VRules: an effective association-based classifier for videos", "An experimental integrated workstation for teleconferencing", "Salient Textural Anomaly Proposals and Classification for Metal Surface Anomalies", "Texture Recognition on Metal Surface using Order-Less Scale Invariant GLAC", "Salient region detection by jointly modeling distinctness and redundancy of image content", "Discovering class-specific informative patches and its application in landmark charaterization", "A ZGPCA Algorithm for Subspace Estimation", "An improved distortion model for rate control of DCT-based video coding", "Multimedia web services for an object tracking and highlighting application", "Semantic knowledge building for image database by analyzing web page contents", "Region-based image retrieval with scale and orientation invariant features", "JPEG2000 image adaptation for MPEG-21 digital items", "Optimal bit allocation for FGS coding", "Motion Histogram: A New Motion Feature to Index Motion Content in Video Segment.", "2014 Index IEEE Transactions on Multimedia Vol. 16", "Spatially-coherent pyramid matching based on max-pooling", "\u6682\u7f3a", "Gossip-based Computation of Average: a Closest Point Search Approach", "Discriminative Signatures for Image Classification", "MobileMaps@ SG-Mappedia Version 1.1", "Web Services Discovery and QoS-Aware Extension", "Image model based on salient regions and its applications", "Optimum bit allocation for FGS video coding", "Video Indexing Using Motion Correlograms", "Ontologies, Databases, and Applications of Semantics (ODBASE) 2004 International Conference (continued)-Multimedia-Ontology for Nature-Scene Image Retrieval", "The transmission of MPEG\u20102 VBR video under usage parameter control", "A web-based artistes management system: experiences and effects", "Artistes Management System", "An architecture for a modular videoconferencing terminal supporting video cell loss", "A framework for handling MPEG data streams over a Local Area Network", "Error concealment algorithms for an ATM videoconferencing system", "An investigation into the requirements for an efficient image transmission system over an ATM network", "cell loss", "Available online at", "TAN, Ah-hwee. Wikipedia2Onto: Building concept ontology automatically, experimenting with web image retrieval.(2010)", "CCI//or PC/DerS", "A new motion histogram to index motion content", "Semantic Knowledge Building for Image Database by Analyzing Web Page", "Image Attention", "145 Mining user hidden semantics from image content for image retrieval", "Integration of Synthetic and Natural Audio/Video Registration Based on Scene Recognition and Natural Features Tracking Techniques for Wide-Area Augmented Reality", "PET\u2013DEVICE++(Push-pull Extraction Tool for DistributEd audioVIsual Content tErminal)", "OPTIMIZATION-BASED MULTIPLE MPEG-7 DESCRIPTORS FOR IMAGE RETRIEVAL", "ROBUST VIDEO MOTION ANALYSIS IN FREQUENCY DOMAIN"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:Tiz5es2fbqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:B3FOqHPlNUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:BrmTIyaxlBUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:tOudhMTPpwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:WbkHhVStYXYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:tkaPQYYpVKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:kRWSkSYxWN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:CHSYGLWDkRkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:LPZeul_q3PIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:tS2w5q8j5-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:8AbLer7MMksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Y0pCki6q_DkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:bnK-pcrLprsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ufrVoPGSRksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:5Ul4iDaHHb8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:xtRiw3GOFMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:l7t_Zn2s7bgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:sSrBHYA8nusC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:1yQoGdGgb4wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:LkGwnXOMwfcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:MXK_kJrjxJIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:K3LRdlH-MEoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ns9cj8rnVeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:70eg2SAEIzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Mojj43d5GZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:D_sINldO8mEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:AXPGKjj_ei8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:08ZZubdj9fEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:bFI3QPDXJZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:5ugPr518TE4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:NMxIlDl6LWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:YOwf2qJgpHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:eMMeJKvmdy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:dhFuZR0502QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:VOx2b1Wkg3QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:k_IJM867U9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:XiVPGOgt02cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:geHnlv5EZngC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:738O_yMBCRsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:vRqMK49ujn8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:p2g8aNsByqUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:_xSYboBqXhAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:e5wmG9Sq2KIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:_Ybze24A_UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:UxriW0iASnsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:fPk4N6BV_jEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:RHpTSmoSYBkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:f2IySw72cVMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:SeFeTyx0c_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:_Qo2XoVZTnwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:ye4kPcJQO24C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:LjlpjdlvIbIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:4OULZ7Gr8RgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:1sJd4Hv_s6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:2P1L_qKh6hAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:35N4QoGY0k4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:hMod-77fHWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:HE397vMXCloC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:u9iWguZQMMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:dshw04ExmUIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:a0OBvERweLwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:nb7KW1ujOQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:EUQCXRtRnyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:fQNAKQ3IYiAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:4fKUyHm3Qg0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:NaGl4SEjCO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:7T2F9Uy0os0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:b0M2c_1WBrUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:pqnbT2bcN3wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:3s1wT3WcHBgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:UHK10RUVsp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:dQ2og3OwTAUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:hkOj_22Ku90C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:WqliGbK-hY8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:_B80troHkn4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:eflP2zaiRacC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:eJXPG6dFmWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:KxtntwgDAa4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:rO6llkc54NcC"], "Topic": ["Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others"], "# of Citations": [545, 443, 401, 364, 345, 238, 211, 207, 157, 131, 106, 107, 98, 90, 88, 91, 78, 69, 61, 60, 58, 55, 46, 46, 47, 42, 45, 44, 38, 39, 40, 38, 36, 37, 36, 32, 33, 28, 28, 24, 24, 22, 23, 22, 20, 20, 18, 17, 17, 17, 17, 18, 17, 17, 17, 15, 15, 14, 14, 14, 14, 13, 13, 13, 13, 12, 11, 11, 11, 10, 10, 10, 10, 9, 9, 9, 6, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 6, 6, 5, 6, 6, 6, 6, 5, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["Sparse coding which encodes the original signal in a sparse signal space, has shown its state-of-the-art performance in the visual codebook generation and feature quantization process of BoW based image representation. However, in the feature quantization process of sparse coding, some similar local features may be quantized into different visual words of the codebook due to the sensitiveness of quantization. In this paper, to alleviate the impact of this problem, we propose a Laplacian sparse coding method, which will exploit the dependence among the local features. Specifically, we propose to use histogram intersection based kNN method to construct a Laplacian matrix, which can well characterize the similarity of local features. In addition, we incorporate this Laplacian matrix into the objective function of sparse coding to preserve the consistence in sparse representation of similar local features\u00a0\u2026", "Recent research has shown the effectiveness of using sparse coding(Sc) to solve many computer vision problems. Motivated by the fact that kernel trick can capture the nonlinear similarity of features, which may reduce the feature quantization error and boost the sparse coding performance, we propose Kernel Sparse Representation(KSR). KSR is essentially the sparse coding technique in a high dimensional feature space mapped by implicit mapping function. We apply KSR to both image classification and face recognition. By incorporating KSR into Spatial Pyramid Matching(SPM), we propose KSRSPM for image classification. KSRSPM can further reduce the information loss in feature quantization step compared with Spatial Pyramid Matching using Sparse Coding(ScSPM). KSRSPM can be both regarded as the generalization of Efficient Match Kernel(EMK) and an extension of ScSPM. Compared with\u00a0\u2026", "Sparse coding exhibits good performance in many computer vision applications. However, due to the overcomplete codebook and the independent coding process, the locality and the similarity among the instances to be encoded are lost. To preserve such locality and similarity information, we propose a Laplacian sparse coding (LSc) framework. By incorporating the similarity preserving term into the objective of sparse coding, our proposed Laplacian sparse coding can alleviate the instability of sparse codes. Furthermore, we propose a Hypergraph Laplacian sparse coding (HLSc), which extends our Laplacian sparse coding to the case where the similarity among the instances defined by a hypergraph. Specifically, this HLSc captures the similarity among the instances within the same hyperedge simultaneously, and also makes the sparse codes of them be similar to each other. Both Laplacian sparse coding and\u00a0\u2026", "The objective of this paper is twofold. First, we introduce an effective region-based solution for saliency detection. Then, we apply the achieved saliency map to better encode the image features for solving object recognition task. To find the perceptually and semantically meaningful salient regions, we extract superpixels based on an adaptive mean shift algorithm as the basic elements for saliency detection. The saliency of each superpixel is measured by using its spatial compactness, which is calculated according to the results of Gaussian mixture model (GMM) clustering. To propagate saliency between similar clusters, we adopt a modified PageRank algorithm to refine the saliency map. Our method not only improves saliency detection through large salient region detection and noise tolerance in messy background, but also generates saliency maps with a well-defined object shape. Experimental results\u00a0\u2026", "As more and more Web services are deployed, Web service's discovery mechanisms become essential. Similar services can have quite different QoS levels. For service selection and management purpose, it is necessary to explicitly, precisely, and unambiguously specify various constraints and QoS metrics for Web services descriptions. This paper provides a novel DAML-QoS ontology as a complement for DAML-S ontology to provide a better QoS metrics model. Three layers are defined together with clear role descriptions for developments. Cardinality constraints are utilized to describe the QoS property constraints. Basic profile is presented for general Web service's description and the speed startup of ontology definition. Matchmaking algorithm for QoS property constraints is presented and different matching degrees are described. When incorporated with DAML-S, multiple service levels can be described\u00a0\u2026", "Recent research has shown the initial success of sparse coding (Sc) in solving many computer vision tasks. Motivated by the fact that kernel trick can capture the nonlinear similarity of features, which helps in finding a sparse representation of nonlinear features, we propose kernel sparse representation (KSR). Essentially, KSR is a sparse coding technique in a high dimensional feature space mapped by an implicit mapping function. We apply KSR to feature coding in image classification, face recognition, and kernel matrix approximation. More specifically, by incorporating KSR into spatial pyramid matching (SPM), we develop KSRSPM, which achieves a good performance for image classification. Moreover, KSR-based feature coding can be shown as a generalization of efficient match kernel and an extension of Sc-based SPM. We further show that our proposed KSR using a histogram intersection kernel (HIK) can\u00a0\u2026", "With the exponential growth in the production creation of multimedia data, there is an increasing need for video semantic analysis. Audio, as a significant part of video, provides important cues to human perception when humans are browsing and understanding video contents. To detect semantic content by useful audio information, we introduce audio keywords which are sets of specific audio sounds related to semantic events. In our previous work, we designed a hierarchical Support Vector Machine (SVM) classifier for audio keyword identification. However, a weakness of our previous work is that audio signals are artificially segmented into 20 ms frames for frame-based SVM identification without any contextual information. In this paper, we propose a classification method based on Hidden Markov Modal (HMM) for audio keyword identification as an improved work instead of using hierarchical SVM\u00a0\u2026", "One of the key challenges in human action recognition from video sequences is how to model an action sufficiently. Therefore, in this paper we propose a novel motion-based representation called Motion Context (MC), which is insensitive to the scale and direction of an action, by employing image representation techniques. A MC captures the distribution of the motion words (MWs) over relative locations in a local region of the motion image (MI) around a reference point and thus summarizes the local motion information in a rich 3D MC descriptor. In this way, any human action can be represented as a 3D descriptor by summing up all the MC descriptors of this action. For action recognition, we propose 4 different recognition configurations: MW+pLSA, MW+SVM, MC+w 3-pLSA (a new direct graphical model by extending pLSA), and MC+SVM. We test our approach on two human action video\u00a0\u2026", "Detection of salient regions in images is useful for object based image retrieval and browsing applications. This task can be done using methods based on the human visual attention model [1], where feature maps corresponding to color, intensity and orientation capture the corresponding salient regions. In this paper, we propose a strategy for combining the salient regions from the individual feature maps based on a new Composite Saliency Indicator (CSI) which measures the contribution of each feature map to saliency. The method also carries out a dynamic weighting of individual feature maps. The experiment results indicate that this combination strategy reflects the salient regions in an image more accurately.", "We study the problem of affective content analysis. In this paper, we think of affective contents as those video/audio segments, which may cause an audience's strong reactions or special emotional experiences, such as laughing or fear. Those emotional factors are related to the users' attention, evaluation, and memories of the content. The modeling of affective effects depends on the video genres. In this work, we focus on comedy and horror films to extract the affective content by detecting a set of so-called audio emotional events (AEE) such as laughing, horror sounds, etc. Those AEE can be modeled by various audio processing techniques, and they can directly reflect an audience's emotion. We use the AEE as a clue to locate corresponding video segments. Domain knowledge is more or less employed at this stage. Our experimental dataset consists of 40-minutes comedy video and 40-minutes horror film. An\u00a0\u2026", "We present a new image quality assessment algorithm based on the phase and magnitude of the 2-D discrete Fourier transform. The basic idea is to compare the phase and magnitude of the reference and distorted images to compute the quality score. However, it is well known that the human visual system's sensitivity to different frequency components is not the same. We accommodate this fact via a simple yet effective strategy of non-uniform binning of the frequency components. This process also leads to reduced space representation of the image thereby enabling the reduced-reference (RR) prospects of the proposed scheme. We employ linear regression to integrate the effects of the changes in phase and magnitude. In this way, the required weights are determined via proper training and hence more convincing and effective. Last, using the fact that phase usually conveys more information than magnitude\u00a0\u2026", "Ontologies are effective for representing domain concepts and relations in a form of semantic network. Many efforts have been made to import ontology into information matchmaking and retrieval. This trend is further accelerated by the convergence of various high-level concepts and low-level features supported by ontologies. In this paper we propose a comparison between traditional keyword based image retrieval and the promising ontology based image retrieval. To be complete, we construct the ontologies not only on text annotation, but also on a combination of text annotation and image feature. The experiments are conducted on a medium-sized data set including about 4000 images. The result proved the efficacy of utilizing both text and image features in a multi modality ontology to improve the image retrieval.", "A framework is presented for estimating the pose of a camera based on images extracted from a single omnidirectional image of an urban scene, given a 2D map with building outlines with no 3D geometric information nor appearance data. The framework attempts to identify vertical corner edges of buildings in the query image, which we term VCLH, as well as the neighboring plane normals, through vanishing point analysis. A bottom-up process further groups VCLH into elemental planes and subsequently into 3D structural fragments modulo a similarity transformation. A geometric hashing lookup allows us to rapidly establish multiple candidate correspondences between the structural fragments and the 2D map building contours. A voting-based camera pose estimation method is then employed to recover the correspondences admitting a camera pose solution with high consensus. In a dataset that is even\u00a0\u2026", "Automated Visual Inspection (AVI) systems for metal surface inspection is increasingly used in industries to aid human visual inspectors for classification of possible anomalies. For classification, the challenge lies in having a small and specific dataset that may easily result in over-fitting. As a solution, we propose to use deep Convolutional Neural Networks (ConvNets) learnt from the large ImageNet dataset [9] for image representations via transfer learning. Since a small dataset cannot be used to fine-tune a ConvNet due to overfitting, we also propose a Majority Voting Mechanism (MVM), which fuses the features extracted from the last three layers of ConvNets using Support Vector Machine (SVM) classifiers. This classification framework is effective where no prior knowledge of the best performing ConvNet layers is needed. This also allows flexibility in the choice of ConvNet used for feature extraction. The\u00a0\u2026", "Image-To-Class (I2C) distance is first used in Naive-Bayes Nearest-Neighbor (NBNN) classifier for image classification and has successfully handled datasets with large intra-class variances. However, the performance of this distance relies heavily on the large number of local features in the training set and test image, which need heavy computation cost for nearest-neighbor (NN) search in the testing phase. If using small number of local features for accelerating the NN search, the performance will be poor. In this paper, we propose a large margin framework to improve the discrimination of I2C distance especially for small number of local features by learning Per-Class Mahalanobis metrics. Our I2C distance is adaptive to different class by combining with the learned metric for each class. These multiple Per-Class metrics are learned simultaneously by forming a convex optimization problem with the\u00a0\u2026", "For the vision of ubiquitous, easily invokable Web services to become a reality, it will be necessary for service requesters to compare the quality-of-service (QoS) aspects of various services. So standardized QoS information must become a part of the service discovery process. This article surveys the current research on QoS and service discovery, including ontologies such as OWL-S and DAML-S. The authors then propose their own ontology, DAML-QoS, which provides detailed QoS information in a DAML format.", "We present a multi-layer group sparse coding framework for concurrent image classification and annotation. By leveraging the dependency between image class label and tags, we introduce a multi-layer group sparse structure of the reconstruction coefficients. Such structure fully encodes the mutual dependency between the class label, which describes the image content as a whole, and tags, which describe the components of the image content. Then we propose a multi-layer group based tag propagation method, which combines the class label and subgroups of instances with similar tag distribution to annotate test images. Moreover, we extend our multi-layer group sparse coding in the Reproducing Kernel Hilbert Space (RKHS) which captures the nonlinearity of features, and further improves performances of image classification and annotation. Experimental results on the LabelMe, UIUC-Sport and NUS-WIDE\u00a0\u2026", "Detecting visually attentive regions of an image is a challenging but useful issue in many multimedia applications. In this paper, we describe a method to extract visual attentive regions in images using subspace estimation and analysis techniques. The image is represented in a 2D space using polar transformation of its features so that each region in the image lies in a 1D linear subspace. A new subspace estimation algorithm based on Generalized Principal Component Analysis (GPCA) is proposed. The robustness of subspace estimation is improved by using weighted least square approximation where weights are calculated from the distribution of K nearest neighbors to reduce the sensitivity of outliers. Then a new region attention measure is defined to calculate the visual attention of each region by considering both feature contrast and geometric properties of the regions. The method has been shown to be\u00a0\u2026", "Multimedia applications such as image or video retrieval, copy detection, and so forth can benefit from saliency detection, which is essentially a method to identify areas in images and videos that capture the attention of the human visual system. In this paper, we propose a new spatio-temporal saliency detection framework on the basis of regularized feature reconstruction. Specifically, for video saliency detection, both the temporal and spatial saliency detection are considered. For temporal saliency, we model the movement of the target patch as a reconstruction process using the patches in neighboring frames. A Laplacian smoothing term is introduced to model the coherent motion trajectories. With psychological findings that abrupt stimulus could cause a rapid and involuntary deployment of attention, our temporal model combines the reconstruction error, regularizer, and local trajectory contrast to measure the\u00a0\u2026", "Saliency detection is useful for high level applications such as adaptive compression, image retargeting, object recognition, etc. In this paper, we introduce an effective region-based solution for saliency detection. We first use the adaptive mean shift algorithm to extract superpixels from the input image, then apply Gaussian Mixture Model (GMM) to cluster superpixels based on their color similarity, and finally calculate the saliency value for each cluster using compactness metric together with modified PageRank propagation. This solution is able to represent the image in a perceptually meaningful way and is robust to over-segmentation. It highlights salient regions with full resolution, well-defined boundary. Experimental results show that both the adaptive mean shift and the modified PageRank algorithm contribute substantially to the saliency detection result. In addition, the ROC analysis demonstrates that our\u00a0\u2026", "A new motion feature for video indexing is proposed in this paper. The motion content of the video at pixel level, is represented as a Pixel Change Ratio Map (PCRM). The PCRM enables us to capture the intensity of motion in a video sequence. It also indicates the spatial location and size of the moving object. The proposed motion feature is the motion histogram which is a non-uniformly quantized histogram of the PCRM. We demonstrate the usefulness of the motion histogram with three applications, viz., video retrieval, video clustering and video classification.", "In this paper, we present a new classification scheme based on support vector machines (SVM) and a new texture feature, called texture correlogram, for high-level image classification. Originally, SVM classifier is designed for solving only binary classification problem. In order to deal with multiple classes, we present a new method to dynamically build up a hierarchical structure from the training dataset. The texture correlogram is designed to capture spatial distribution information. Experimental results demonstrate that the proposed classification scheme and texture feature are effective for high-level image classification task and the proposed classification scheme is more efficient than the other schemes while achieving almost the same classification accuracy. Another advantage of the proposed scheme is that the underlying hierarchical structure of the SVM classification tree manifests the interclass relationships\u00a0\u2026", "Objective speech quality assessment is a challenging task which aims to emulate human judgment in the complex and time consuming task of subjective assessment. It is difficult to perform in line with the human perception due the complex and nonlinear nature of the human auditory system. The challenge lies in representing speech signals using appropriate features and subsequently mapping these features into a quality score. This paper proposes a nonintrusive metric for the quality assessment of noise-suppressed speech. The originality of the proposed approach lies primarily in the use of Mel filter bank energies (FBEs) as features and the use of support vector regression (SVR) for feature mapping. We utilize the sensitivity of FBEs to noise in order to obtain an effective representation of speech towards quality assessment. In addition, the use of SVR exploits the advantages of kernels which allow the\u00a0\u2026", "This paper presents a new multiple-modality method for extracting semantic information from basketball video. The visual, motion, and audio information are extracted from video to first generate some low-level video segmentation and classification. Domain knowledge is further exploited for detecting interesting events in the basketball video. For video, both visual and motion prediction information are utilized for shot and scene boundary detection algorithm; this will be followed by scene classification. For audio, audio keysounds are sets of specific audio sounds related to semantic events and a classification method based on hidden Markov model (HMM) is used for audio keysound identification. Subsequently, by analyzing the multimodal information, the positions of potential semantic events, such as \"foul\" and \"shot at the basket,\" are located with additional domain knowledge. Finally, a video annotation\u00a0\u2026", "Visual attention is obtained through determination of contrasts of low level features or attention cues like intensity, color etc. We propose a new texture attention cue that is shown to be more effective for images where the salient object regions and background have similar visual characteristics. Current visual attention models do not consider local contextual information to highlight attention regions. We also propose a feature combination strategy by suppressing saliency based on context information that is effective in determining the true attention region. We compare our approach with other visual attention models using a novel average discrimination ratio measure.", "As more and more Web services are deployed, Web service\u2019s discovery mechanisms become essential. Similar services from different sources often exhibit quite different Quality of Service (QoS) levels. For service selection and management purpose, it is necessary to explicitly, precisely, and flexibly specify various constraints and QoS metrics for Web services descriptions. This paper provides a novel ontology as a complement to DARPA Agent Markup Language\u2014Service (DAML-S) Ontology to provide a better QoS metrics model. Three layers are defined, together with clear role descriptions for development. Cardinality constraints are utilized to describe the QoS property constraints. A matchmaking algorithm with multiple matching degrees is provided accordingly. When incorporated with DAML-S, the multiple service level objectives can be described by assigning multiple QoS profiles to one service profile. A\u00a0\u2026", "LT codes provide an efficient way to transfer information over erasure channels. Past research has illustrated that LT codes can perform well for a large number of input symbols. However, it is shown that LT codes have poor performance when the number of input symbols is small. We notice that the poor performance is due to the design of the LT decoding process. In this respect, we present a decoding algorithm called full rank decoding that extends the decodability of LT codes by usingWiedemann algorithm.We provide a detailed mathematical analysis on the rank of the random coefficient matrix to evaluate the probability of successful decoding for our proposed algorithm. Our studies show that our proposed method reduces the overhead significantly in the cases of small number of input symbols yet preserves the simplicity of the original LT decoding process.", "With more and more Web services appearing on the Web, Web service\u2019s discovery mechanisms become essential. UDDI is an online registry standard to facilitate the discovery of business partners and services. Service requesters can choose good performance Web services by manual test and comparison. However, this method of choosing is inefficient and costly in terms of time and money. The ability to predict the quality of service is missing in UDDI. Furthermore, UDDI registries in federated organizations should be able to share more about the service requester\u2019s connection information to make better predictions for services\u2019 performances. To address these problems, we propose UX (UDDI eXtension), a system that facilitates requesters to discover services with good performance. In each enterprise domain, the requesters\u2019 QoS feedback is received and then stored in a local UX server. By sharing these\u00a0\u2026", "The upcoming MPEG-21 standard proposes a general framework for augmented use of multimedia services in different network environments, for various users with various terminal devices. In the context of image adaptation, terminals with different screen size limitation require the multimedia adaptation engine to adapt image resources intelligently. Saliency map based visual attention analysis provides some intelligence for finding the attention area within the image. In this paper, we improved the standard MPEG-21 metadata driven adaptation engine by using enhanced saliency map based visual attention model which provides a mean to intelligently adapt JPEG2000 image resolution for different terminal devices with varying screen size according to human visual attention.", "This paper presents an efficient and effective solution for retrieving image near-duplicate (IND) from image database. We introduce the coherent phrase model which incorporates the coherency of local regions to reduce the quantization error of the bag-of-words (BoW) model. In this model, local regions are characterized by  visual phrase  of multiple descriptors instead of visual word of single descriptor. We propose two types of visual phrase to encode the coherency in feature and spatial domain, respectively. The proposed model reduces the number of false matches by using this coherency and generates sparse representations of images. Compared to other method, the local coherencies among multiple descriptors of every region improve the performance and preserve the efficiency for IND retrieval. The proposed method is evaluated on several benchmark datasets for IND retrieval. Compared to the state-of-the\u00a0\u2026", "We present a multi-layer group sparse coding framework for concurrent single-label image classification and annotation. By leveraging the dependency between image class label and tags, we introduce a multi-layer group sparse structure of the reconstruction coefficients. Such structure fully encodes the mutual dependency between the class label, which describes image content as a whole, and tags, which describe the components of the image content. Therefore we propose a multi-layer group based tag propagation method, which combines the class label and subgroups of instances with similar tag distribution to annotate test images. To make our model more suitable for nonlinear separable features, we also extend our multi-layer group sparse coding in the Reproducing Kernel Hilbert Space (RKHS), which further improves performances of image classification and annotation. Moreover, we also integrate our\u00a0\u2026", "The content-based cross-media retrieval is a new type of multimedia retrieval in which the media types of query examples and the returned results can be different. In order to learn the semantic correlations among multimedia objects of different modalities, the heterogeneous multimedia objects are analyzed in the form of multimedia document (MMD), which is a set of multimedia objects that are of different media types but carry the same semantics. We first construct an MMD semi-semantic graph (MMDSSG) by jointly analyzing the heterogeneous multimedia data. After that, cross-media indexing space (CMIS) is constructed. For each query, the optimal dimension of CMIS is automatically determined and the cross-media retrieval is performed on a per-query basis. By doing this, the most appropriate retrieval approach for each query is selected, i.e. different search methods are used for different queries. The query\u00a0\u2026", "Current deep learning-based texture recognition methods extract spatial orderless features from pre-trained deep learning models that are trained on large-scale image datasets. These methods either produce high dimensional features or have multiple steps like dictionary learning, feature encoding and dimension reduction. In this paper, we propose a novel end-to-end learning framework that not only overcomes these limitations, but also demonstrates faster learning. The proposed framework incorporates a residual pooling layer consisting of a residual encoding module and an aggregation module. The residual encoder preserves the spatial information for improved feature learning and the aggregation module generates orderless feature for classification through a simple averaging. The feature has the lowest dimension among previous deep texture recognition approaches, yet it achieves state-of-the-art\u00a0\u2026", "As more and more Web services are deployed, Web service's discovery mechanisms become essential. Similar services can have quite different QoS behaviors. For service selection and management purpose, it is necessary to clearly specify QoS constraints and metrics definitions for Web services. We investigate on the semantic QoS specification and introduce our design principles on it. Based on the specification refinement and conformance, we introduce the QoS matchmaking algorithm with multiple matching degrees. The matchmaking prototype is designed to prove the feasibility. Well-defined Metrics can be further utilized by measurement organizations to monitor and evaluate the promised service level objectives.", "Ontologies represent domain concepts and relations in a form of semantic network. Many research works use ontologies in the information matchmaking and retrieval. This trend is further accelerated by the convergence of various information sources supported by ontologies. In this paper, we propose a novel multi-modality ontology model that integrates both the low-level image features and the high-level text information to represent image contents for image retrieval. By embedding this ontology into an image retrieval system, we are able to realize intelligent image retrieval with high precision. Moreover, benefiting from the soft-coded ontology model, this system has good flexibility and can be easily extended to the larger domains. Currently, our experiment is conducted on the animal domain canine. An ontology has been built based on the low-level features and the domain knowledge of canine. A\u00a0\u2026", "With the industry's efforts in promoting the use of Web services, a huge number of Web services are being developed for the Web. To improve the service provider's competition advantage, it is necessary to support QoS-aware service discovery mechanism as well as the corresponding measurement system. In this paper we present a QoS measurement framework based on the DAML-QoS ontology, together with the measurement code generator. The DAML-QoS ontology is designed to specify QoS constraints for Web services. It works as a complement to DAML-S to provide QoS-aware service discovery. This paper focuses on its measurement framework to check the service provider's runtime compliance with the advertised service level. The code generator helps to generate the measurement code according to the DAML-QoS specification. This ontology level specification tries to reduce the service's integration\u00a0\u2026", "This paper presents a framework for building an ontology to provide semantic interpretations in image contents. The novelty of this framework comes from building a MPEG-7 ontology for semantic representations of multimedia contents, and from integrating such ontology into an image retrieval system to enable fast, efficient image query and retrieval. The prototype system demonstrated the feasibility of embedding such ontology into an image retrieval system. Its main objective has been achieved by retrieving nature scene images using human readable keywords. Based on the experimental results, we believe that using our \u2018bridging\u2019 technique, the high-level non-machine readable human concepts can be seamlessly mapped to low-level machine processable data. This helps to improve the efficiency of our CBIR system compared to conventional methods.", "Ontology, as an effective approach to bridge the semantic gap in various domains, has attracted a lot of interests from multimedia researchers. Among the numerous possibilities enabled by ontology, we are particularly interested in exploiting ontology for a better understanding of media task (particularly, images) on the World Wide Web. To achieve our goal, two open issues are inevitably involved: 1) How to avoid the tedious manual work for ontology construction? 2) What are the effective inference models when using an ontology? Recent works[11, 16] about ontology learned from Wikipedia has been reported in conferences targeting the areas of knowledge management and artificial intelligent. There are also reports of different inference models being investigated [5, 13, 15]. However, so far there has not been any comprehensive solution. In this paper, we look at these challenges and attempt to provide a\u00a0\u2026", "How to build semantic index for multimedia data is an important and challenging problem for multimedia information systems. In this paper, we present a novel approach to build a semantic video index for digital videos by analyzing the subtitle files of DVD/DivX videos. The proposed approach for building semantic video index consists of 3 stages, viz., script extraction, script partition and script vector representation. First, the scripts are extracted from the subtitle files that are available in the DVD/DivX videos. Then, the extracted scripts are partitioned into segments. Finally, the partitioned script segments are converted into a tfidf vector based representation, which acts as the semantic index. The efficiency of the semantic index is demonstrated through video retrieval and summarization applications. Experimental results demonstrate that the proposed approach is very promising.", "In this paper, we describe a new framework to extract visual attention regions in images using robust subspace estimation and analysis techniques. We use simple features like hue and intensity endowed with scale adaptivity in order to represent smooth and textured areas in an image. A polar transformation maps homogeneity in the features into a linear subspace that also encodes spatial information of a region. A new subspace estimation algorithm based on the Generalized Principal Component Analysis (GPCA) is proposed to estimate multiple linear subspaces. Sensitivity to outliers is achieved by weighted least squares estimate of the subspaces in which weights calculated from the distribution of K nearest neighbors are assigned to data points. Iterative refinement of the weights is proposed to handle the issue of estimation bias when the number of data points in each subspace is very different. A new region\u00a0\u2026", "Semantic sports video analysis has attracted many research interests and audio cues have been shown to play an important role in semantics inference. To facilitate event detection using audio information, we have introduced the concept of audio keyword (e.g. excited/plain commentator speech, excited/plain audience sound, etc.) to describe the game-specific sound associated with an event. In our previous work, we have designed a hierarchical Support Vector Machine (SVM) classifier for audio keyword identification. However, there are two inherent weaknesses: 1) a frame-based SVM classifier does not incorporate any contextual information; 2) a robust recognizer relies on large amounts of training data in the case of different sports games videos. In this demo, we present a flexible Hidden Markov Model (HMM)-based audio keyword generation system. This is motivated by the successful story of applying HMM\u00a0\u2026", "Image-to-class (I2C) distance is a novel measure for image classification and has successfully handled datasets with large intra-class variances. However, due to the lack of a training phase, the performance of this distance is easily affected by irrelevant local features that may hurt the classification accuracy. Besides, the success of this I2C distance relies heavily on the large number of local features in the training set, which requires expensive computation cost for classifying test images. On the other hand, if there are small number of local features in the training set, it may result in poor performance.In this paper, we propose a distance learning method to improve the classification accuracy of this I2C distance as well as two strategies for accelerating its NN search. We first propose a large margin optimization framework to learn the I2C distance function, which is modeled as a weighted combination of the distance\u00a0\u2026", "In the past few years, the fast proliferation of available XML documents has stimulated a great deal of interest in discovering hidden and nontrivial knowledge from XML repositories. However, to the best of our knowledge, none of existing work on XML mining has taken into account of the dynamic nature of XML documents as online information. The present article proposes a novel type of frequent pattern, namely, FRequently And Concurrently muTating substructUREs (FRACTURE), that is mined from the evolution of an XML document. A discovered FRACTURE is a set of substructures of an XML document that frequently change together. Knowledge obtained from FRACTURE is useful in applications such as XML indexing, XML clustering etc. In order to keep the result patterns concise and explicit, we further formulate the problem of maximal FRACTURE mining. Two algorithms, which employ the level-wise and\u00a0\u2026", "From a personalized media point of view, many users favor a flexible tool to quickly browse the affective content in a video. Such affective content may cause audiences' strong reactions or special emotional experiences, such as anger, sadness, fear, joy and love. This paper attempts to extract affective content for digital videos by analyzing the subtitle files of DVD/DivX videos and utilize audio event to assist affective content detection. Firstly, videos are segmented by dialogue script partition. Compared to traditional video shot, video segmented by scripts is not affected by camera changes and shooting angles and easy to include video segments with compact content. Secondly, emotion-related vocabularies in video script are detected to locate affective video content. Using script to directly access video content avoids complex video analysis. Thirdly, audio event detection is utilized to assist affective content\u00a0\u2026", "Image tagging is an important technique for the image content understanding and text based image processing. Given a selection of images, how to tag these images efficiently and effectively is an interesting problem. In this paper, a novel semi-auto image tagging technique is proposed: By assigning each image a category label first, our method can automatically recommend those promising tags to each image by utilizing existing vast web data. The main contributions of our paper can be highlighted as follows: (i) By assigning each image a category label, our method can automatically recommend other tags to the image, thus reducing the human annotation efforts. Meanwhile, our method guarantee tags' diversity due to abundant web data. (ii) We use sparse coding to automatically select those semantically related images for tag propagation. (iii) Local & global ranking agglomeration will make our method robust\u00a0\u2026", "Given its effectiveness to better understand data, ontology has been used in various domains including artificial intelligence, biomedical informatics and library science. What we have tried to promote is the use of ontology to better understand media (in particular, images) on the World Wide Web. This paper describes our preliminary attempt to construct a large-scale multi-modality ontology, called AutoMMOnto, for web image classification. Particularly, to enable the automation of text ontology construction, we take advantage of both structural and content features of Wikipedia and formalize real world objects in terms of concepts and relationships. For visual part, we train classifiers according to both global and local features, and generate middle-level concepts from the training images. A variant of the association rule mining algorithm is further developed to refine the built ontology. Our experimental results show that our method allows automatic construction of large-scale multi-modality ontology with high accuracy from challenging web image data set", "Multimedia applications like retrieval, copy detection etc. can gain from saliency detection, which is essentially a method to identify areas in images and videos that capture the attention of the human visual system. In this paper, we propose a new spatiotemporal saliency framework for videos based on sparse representation. For temporal saliency, we model the movement of the target patch as a reconstruction process, and the overlapping patches in neighboring frames are used to reconstruct the target patch. The learned coefficients encode the positions of the matched patches, which are able to represent the motion trajectory of the target patch. We also introduce a smoothing term into our sparse coding framework to learn coherent motion trajectories. Based on the psychological findings that abrupt stimulus could cause a rapid and involuntary deployment of attention, our temporal model combines the\u00a0\u2026", "Image-to-Class (I2C) distance has demonstrated its effectiveness for object recognition in several single-label datasets. However, for the multi-label problem, where an image may contain several regions belonging to different classes, this distance may not work well since it cannot discriminate local features from different regions in the test image and all local features have to be counted in the I2C distance calculation. In this paper, we propose to use Class-to-Image (C2I) distance and show that this distance performs better than I2C distance for multi-label image classification. However, since the number of local features in a class is huge compared to that in an image, the calculation of C2I distance is much more expensive than I2C distance. Moreover, the label information of training images can be used to help select relevant local features for each class and further improve the recognition performance\u00a0\u2026", "Due to the dynamic nature of online information, XML documents typically evolve over time. The change of the data values or structures of an XML document may exhibit some particular patterns. In this paper, we focus on the sequence of changes to the structures of an XML document to find out which subtrees in the XML structure frequently change together, which we call Frequently Changing Subtree Patterns (FCSP). In order to keep the discovered patterns more concise, we further define the problem of mining maximal FCSPs. An algorithm derived from the FP-growth is developed to mine the set of maximal FCSPs. Experiment results show that our algorithm is substantially faster than the naive algorithm and it scales well with respect to the size of the XML structure.", "Previous work on XML association rule mining focuses on mining from the data existing in XML documents at a certain time point. However, due to the dynamic nature of online information, an XML document typically evolves over time. Knowledge obtained from mining the evolvement of an XML document would be useful in a wide range of applications, such as XML indexing, XML clustering. In this paper, we propose to mine a novel type of association rules from a sequence of changes to XML structure, which we call XML Structural Delta Association Rule (XSD-AR). We formulate the problem of XSD-AR mining by considering both the frequency and the degree of changes to XML structure. An algorithm, which is derived from the FP-growth, and its optimizing strategy are developed for the problem. Preliminary experiment results show that our algorithm is efficient and scalable at discovering a complete set\u00a0\u2026", "This paper describes a method to partition a video sequence into shots and subshots. By subshots, we mean one or a combination of the three camera motions of pan, tilt and zoom. The proposed technique detects both hard cuts and gradual transitions in MPEG compressed video using a single technique. We also present a motion estimation algorithm to compute the dominant motion represented by an affine model. The motion information is used to refine the location of dissolves as well as to subdivide the shot into subshots, thus providing a characterization of camera motion. We consider the dissimilarity between the I-, P- and B-frames with respect to the type of macroblocks used for encoding. Unlike previous algorithms reported, our method requires minimal decompression of the video sequence and uses very loose thresholds. The algorithm is evaluated on several types of video sequences to demonstrate its\u00a0\u2026", "Transmission of video information over computer networks poses many problems that must be solved. In most cases an absolute guarantee that video cells will not be lost under any circumstances cannot be given, and the video service must accept this possibility.This paper addresses the problem of reconstructing lost sections of an image without the need for retransmission of the missing data. This is important where no reference or previous frame information is available for substitution. Furthermore, concealment of the lost information is performed within the transform domain for a JPEG coded image. Identification of the most suitable components from the neighbouring DCT coded blocks to be used in the reconstruction process has enabled both an improvement in reconstructed image quality and a reduction in processing when compared with an earlier algorithm.", "This paper describes a semantic retrieval system that allows matchmaking with ranked output and the use of multi-modality ontology to retrieve animal images. Our multi-modality ontology, which integrates both image features and text information, is extended to provide a ranking mechanism. Ranking is calculated from correlation in each modality and is used to refine the semantic matchmaking result. To benchmark our results, we use the top 200 images of Google Image Search for each category to do the experimental comparison. Google Image Search claims to be the most comprehensive on the Web, with billions of images indexed and available for viewing. For different categories of animals in the canine family, we found averages of about 60% of top 200 images are correct images. Google returns even more false results outside this range. Therefore any bigger image set will become meaningless in our\u00a0\u2026", "This paper describes a fully automatic content-based approach for browsing and retrieval of MPEG-2 compressed video. The first step of the approach is the detection of shot boundaries based on motion vectors available from the compressed video stream. The next step involves the construction of a scene tree from the shots obtained earlier. The scene tree is shown to capture some semantic information as well as to provide a construct for hierarchical browsing of compressed videos. Finally, we build a new model for video similarity based on global as well as local motion associated with each node in the scene tree. To this end, we propose new approaches to camera motion and object motion estimation. The experimental results demonstrate that the integration of the above techniques results in an efficient framework for browsing and searching large video databases.", "Singapore, a small nation with limited resources, has in three decades transformed itself from a British colony to one of the seven Asian economic tigers registering high economic growth. The borderless world of cyberspace has spawn a new digital global economy. To spur Singapore on to new economic growth, the government has identified information technology (IT) as the strategic tool. This paper presents the IT experience of Singapore in the 1980s and the latest IT plan (IT2000) for the 1990s and beyond to build a national information infrastructure (NII) to ensure Singapore's continuous growth in the future. The initiatives and incentives taken by the government to promote the use of the NII and snapshots of the progress of the NII deployment to date are discussed. In particular, the paper focuses on the evolution of Internet in Singapore; it highlights its phenomenal growth and how this growth has\u00a0\u2026", "Wireless sensor network has been identified as being useful in a variety of domains including the battlefield and perimeter defense. These mission critical applications raise the concern for security in sensor network. Typical security problems identified include passive information gathering, subversion of a node, legitimate addition of a node to an existing sensor network, and so forth [1]. Under traditional routing concept, information security is usually ensured through high-level security protocols, but limited computational power and memory space in sensor nodes make traditional cryptographical techniques cumbersome to be implemented in sensor networks.", "As the amount of multimedia data is increasing day-by-day thanks to less expensive storage devices and increasing numbers of information sources, machine learning algorithms are faced with large-sized and noisy datasets. Fortunately, the use of a good sampling set for training influences the final results significantly. But using a simple random sample (SRS) may not obtain satisfactory results because such a sample may not adequately represent the large and noisy dataset due to its blind approach in selecting samples. The difficulty is particularly apparent for huge datasets where, due to memory constraints, only very small sample sizes are used. This is typically the case for multimedia applications, where data size is usually very large. In this article we propose a new and efficient method to sample of large and noisy multimedia data. The proposed method is based on a simple distance measure that compares\u00a0\u2026", "Recently, several approaches that mine frequent XML query patterns and cache their results have been proposed to improve query response time. However, frequent XML query patterns mined by these approaches ignore the temporal sequence between user queries. In this paper, we take into account the temporal features of user queries to discover association rules, which indicate that when a user inquires some information from the XML document, she/he will probably inquire some other information subsequently. We cluster XML queries according to their semantics first and then mine association rules between the clusters. Moreover, not only positive but also negative association rules are discovered to design the appropriate cache replacement strategy. The experimental results showed that our approach considerably improved the caching performance by significantly reducing the query response time.", "Image-To-Class (I2C) distance is a novel distance used for image classification and has successfully handled datasets with large intra-class variances. However, it uses Euclidean distance for measuring the distance between local features in different classes, which may not be the optimal distance metric in real image classification problems. In this article, we propose a distance metric learning method to improve the performance of I2C distance by learning per-class Mahalanobis metrics in a large margin framework. Our I2C distance is adaptive to different classes by combining with the learned metric for each class. These multiple per-class metrics are learned simultaneously by forming a convex optimization problem with the constraints that the I2C distance from each training image to its belonging class should be less than the distances to other classes by a large margin. A subgradient descent method is applied\u00a0\u2026", "Information in the frequency domain is useful in image classification. For natural scene classification, Oliva and Torralba proposed a global feature by sampling the power spectrum of the filtered image. In this paper, we present a hybrid global feature for scene classification. To capture the textural characteristics of the image in the frequency domain, we propose two feature extraction strategies based on gray-level co-occurrence matrices. Both contain statistics of the co-occurrence matrix, but the first one is of a much higher dimension than the second. We demonstrate that the proposed feature is a helpful supplement to the energy feature in terms of increased classification accuracy for real scene images. In order to combine these two kinds of features and further improve the classification accuracy, a posterior probability based two-stage classification method is proposed in which a linear combination of the\u00a0\u2026", "Quantization errors in discrete-cosine-transform (DCT) video compression are known as DCT residues. Knowledge on their distribution is essential in understanding rate-distortion (R-D) behaviors of generic video coding. Traditional R-D analysis adopted a simplified distortion model. Those distortion models took only quantization parameter into account. They lack adaptability to variation of video sources, as the distribution of coding errors also depends on the statistics of video source. Another common approach models the distribution of DCT residues by fitting experimental data from coded pictures to conjectured statistical distributions, but it did not provide insights into what gives rise to the distribution of DCT residues. This paper intends to quantify the distribution of DCT residues with respect to video source and with respect to the quantization strategy by understanding the quantization of DCT frequency\u00a0\u2026", "Visual inspection and identification of anomalies are important steps in the manufacturing process. A data set containing images of metallic components at different orientations and lighting conditions have been used for this experiment. Our aim is to find the anomaly in an image by subtracting it from an equivalent reference image with no anomalies. This is challenging due to the availability of multiple and slightly changing viewpoints for the reference image. The anomalies can only be detected in the residue obtained by using an identical reference image with no anomalies in the subtraction process. The proposed system is based on finding the best reference from a pool of similar images. A method for ranking the images based on similarity is also introduced. The proposed method scales well for high contrast images with dark colored anomalies.", "In multi-label learning, an image containing multiple objects can be assigned to multiple labels, which makes it more challenging than traditional multi-class classification task where an image is assigned to only one label. In this paper, we propose a multi-label learning framework based on Image-to-Class (I2C) distance, which is recently shown useful for image classification. We adjust this I2C distance to cater for the multi-label problem by learning a weight attached to each local feature patch and formulating it into a large margin optimization problem. For each image, we constrain its weighted I2C distance to the relevant class to be much less than its distance to other irrelevant class, by the use of a margin in the optimization problem. Label ranks are generated under this learned I2C distance framework for a query image. Thereafter, we employ the label correlation information to split the label rank for predicting\u00a0\u2026", "In this paper, we propose a large margin framework to learn the local instance-to-class distance function using local patch-based feature vectors, which satisfies the property that distance from instance to its own class should be less than the distance to other class. This instance-to-class distance is modeled as the weighted combination of the distance from every patch in test image to its nearest patch in training class, where the weight is learned through the above learning phase. We evaluate the proposed method on human action datasets and compare with related methods. It is shown that the proposed method achieves promising performance and improves the efficiency.", "The problem of polysemy in keyword-based image search arises mainly from the inherent ambiguity in user queries. We propose a latent model based approach that resolves user search ambiguity by allowing sense specific diversity in search results. Given a query keyword and the images retrieved by issuing the query to an image search engine, we first learn a latent visual sense model of these polysemous images. Next, we use Wikipedia to disambiguate the word sense of the original query, and issue these Wiki-senses as new queries to retrieve sense specific images. A sense-specific image classifier is then learnt by combining information from the latent visual sense model, and used to cluster and re-rank the polysemous images from the original query keyword into its specific senses. Results on a ground truth of 17K image set returned by 10 keyword searches and their 62 word senses provides empirical indications that our method can improve upon existing keyword based search engines. Our method learns the visual word sense models in a totally unsupervised manner, effectively filters out irrelevant images, and is able to mine the long tail of image search.", "This paper focuses on improving the semi-manual method for web image concept annotation. By sufficiently studying the characteristics of tag and visual feature, we propose the Grouping-Based-Precision & Recall-Aided (GBPRA) feature selection strategy for concept annotation. Specifically, for visual features, we construct a more robust middle level feature by concatenating the k-NN results for each type of visual feature. For tag, we construct a concept-tag co-occurrence matrix, based on which the probability of an image belonging to certain concept can be calculated. By understanding the tags\u2019 quality and groupings\u2019 semantic depth, we propose a grouping based feature selection method; by studying the tags\u2019 distribution, we adopt Precision and Recall as a complementary indicator for feature selection. In this way, the advantages of both tags and visual features are boosted. Experimental results show our\u00a0\u2026", "A key frame extraction approach, based on change detection of DC images extracted from compressed video, is proposed in this paper. We define a simple pixel change map that captures additional information in a frame with respect to its adjacent frames. Since global motion contributes to pixel changes, falsely indicating the presence of key frames, it is compensated by adaptively filtering the pixel change map using a modified version of the least mean square (LMS) algorithm. The prediction errors thus obtained are used to subsequently select the key frames. The key frames are selected so that the cumulative prediction error is partitioned into equal amounts in each segment. The entire procedure is computationally simple and flexible. Experimental results illustrate the good performance of the proposed algorithm.", "In this paper we present an enhanced multi-modality ontology-based approach for web image retrieval step by step. Several ontology-based approaches have been made in the field of multimedia retrieval. Our multi-modality approach is one of the earliest attempts to integrate information from different modalities and apply the model in a complex domain. In order to develop the model, we need to answer the following questions: (1) how to find the proper structure and construct an ontology which can integrate information from different modalities; (2) how to quantify the matching degree (concept similarity) and provide an independent ranking mechanism; (3) how to ensure the scalability of this approach when applied to large domains. The first question has been answered by our multi-modality ontology which has been discussed in Wang et\u00a0al. (Does ontology help in image retrieval? In: Asia-Pacific\u00a0\u2026", "MPEG-7 documents, which are primarily for multimedia information exchange, are also data-centric XML documents. Due to its advantages, the relational DBMS is the best choice for storing such XML documents. Storing XML data in relational DBMS can be classified into two classes of storage model: structure-mapping and model-mapping. However, the structure-mapping model cannot support complex Xpath-based query efficiently and model mapping approach lacks the flexible capability in representing all kinds of datatypes. In this paper, we present a new storage approach, called SM3. As an XML document, MPEG-7 document can be viewed as XML tree. Such a tree graph, where the internal nodes are element type with element contents, represents the structure of document and can be viewed as nodes which are meaningful only for document traversal. The leaf node, which is a single-valued attribute or\u00a0\u2026", "This paper introduces the notion of attention-from-motion in which the objective is to identify, from an image sequence, only those object in motions that capture visual attention (VA). Following the important concept in film production, viz, the tracking shot, we define the attention object in motion (AOM) as those that are tracked by the camera. Three components are proposed to form an attention-from-motion framework: (i) a new factorization form of the measurement matrix to describe dynamic geometry of moving object observed by moving camera; (ii) determination of single AOM based on the analysis of certain structure on the motion matrix; (iii) an iterative framework for detecting multiple AOMs. The proposed analysis of structure from factorization enables the detection of AOMs even when only partial data is available due to occlusion and over-segmentation. Without recovering the motion of either object or camera\u00a0\u2026", "In this paper, we present an event-on-demand (EoD)video adaptation system. The proposed system supports users in deciding their events of interest and considers network conditions to adapt video source by event selection and frame dropping.Firstly, events are detected by audio/video analysis and annotated by the description schemes (DSs)provided by MPEG-7 Multimedia Description Schemes (MDSs). And then, to achieve a generic adaptation solution, the adaptation is developed following MPEG-21 Digital Item Adaptation (DIA)framework. We look at early release of the MPEG-21 Reference Software on XML generation and develop our own system for EoD video adaptation in three steps:1) the event information is parsed from MPEG-7 annotation XML file together with bitstream to generate generic Bitstream Syntax Description (gBSD). 2) Users' preference, Network Characteristic and Adaptation QoS (AQoS\u00a0\u2026", "This paper describes a fully automatic content-based approach for browsing and retrieval of MPEG-2 compressed video. The first step of the approach is the detection of shot boundaries based on motion vectors available from the compressed video stream. The next step involves the construction of a scene tree from the shots obtained earlier. The scene tree is shown to capture some semantic information as well as provide a construct for hierarchical browsing of compressed videos. Finally, we build a new model for video similarity based on global as well as local motion associated with each node in the scene tree. To this end, we propose new approaches to camera motion and object motion estimation. The experimental results demonstrate that the integration of the above techniques results in an efficient framework for browsing and searching large video databases.", "The new MPEG-21 standard defines a multimedia framework to enable transparent and augmented use of multimedia resources across heterogeneous networks and devices used by different communities. In this paper, we incorporated the perceived motion energy (PME) model into the proposed MPEG-21 digital item adaptation framework for frame dropping in H.264 encoded video adaptation. There are two advantages of this work, one is the use of PME model to reduce the viewer's perceived motion jitter due to frame dropping to a minimum. The other is the adaptation nodes can easily apply frame dropping operations without knowledge of detailed encoding syntax of H.264 videos.", "In this paper, we propose a Phase Fourier Reconstruction (PFR) approach for anomaly detection on metal surfaces using salient irregularities. To get salient irregularity with images captured from an automatic visual inspection (AVI) system using different lighting settings, we first trained a classifier for image selection as only dark images are utilized for anomaly detection. By doing so, surface details, part design, and boundaries between foreground/background become indistinct, but anomaly regions are highlighted because of diffuse reflection caused by rough surfaces. Then PFR is applied so that regular patterns and homogeneous regions are further de-emphasized, and simultaneously, anomaly areas are distinct and located. Different from existing phase-based methods which require substantial texture information, our PFR works on both textual and non-textual images. Unlike existing template\u00a0\u2026", "Video saliency detection, the task to detect attractive content in a video, has broad applications in multimedia understanding and retrieval. In this paper, we propose a new framework for spatiotemporal saliency detection. To better estimate the salient motion in temporal domain, we take advantage of robust alignment by sparse and low-rank decomposition to jointly estimate the salient foreground motion and the camera motion. Consecutive frames are transformed and aligned, and then decomposed to a low-rank matrix representing the background and a sparse matrix indicating the objects with salient motion. In the spatial domain, we address several problems of local center-surround contrast based model, and demonstrate how to utilize global information and prior knowledge to improve spatial saliency detection. Individual component evaluation demonstrates the effectiveness of our temporal and spatial methods\u00a0\u2026", "Current large scale image retrieval engines rely heavily on the surrounding text information, which inevitably includes some irrelevant images in the retrieval results due to the noisy environment. To improve the retrieval performance, we propose an unsupervised web image re-ranking method by incorporating images' visual information. Our method can automatically select a set of representative images from the original image pool as concept model, which is highly related to the query concept and critically important for the re-ranking result. With a similarity graph constructed by top results given by text based retrieval, we utilize Normalized Cut to select the part with the highest similarity density as concept model. We re-rank the rest images according to their similarities to the concept model. The advantages of our method are (i): Our method is unsupervised, and it doesn't need any pre-prepared query/training image\u00a0\u2026", "In this paper, we investigate optimal frame-selection algorithms based on dynamic programming for delivering stored variable bit rate (VBR) video under both bandwidth and buffer size constraints. Our objective is to find a feasible set of frames that can maximize the video's accumulated motion values without violating any constraint. It is well known that dynamic programming has high complexity. In this research, we propose to eliminate nonoptimal intermediate frame states, which can effectively reduce the complexity of dynamic programming. Moreover, we propose a reverse frame selection (RFS) algorithm, where the selection starts from the last frame and ends at the first frame. Compared with the conventional dynamic programming-based forward frame selection, the RFS is able to find all of the optimal results for different preloads in one round. We further extend the RFS scheme to solve the problem of frame\u00a0\u2026", "As there is an exponential increase of web videos, it is time-consuming to get a query result from the tremendous data. An effective and efficient video management system is in urgent need. To increase the efficiency of video retrieval and storage, the most widely used methods are indexing schemes, such as locality sensitive hashing (LSH). However, it is more essential to represent the video itself compactly. In this paper, we propose a strategy to generate stratification-based keyframe cliques (SKCs) for video description, which are more compact and informative than frames or keyframes. The new representations are scalable for different retrieval tasks due to the ranking of SKCs. To further accelerate the retrieval speed, only top SKCs will be used; meanwhile, the searching results are still satisfactory. Experiments are conducted on TRECVID dataset as well as web video dataset. Results show that our proposed\u00a0\u2026", "The current volume of videos available for distribution or viewing on the internet is increasing exponentially, there is an urgent need for designing effective and efficient video management systems. However, due to the tremendous amounts of video data, it is highly likely that any large scale video systems will provide query results with near-duplicates videos in the return list of videos. In this paper, we introduce our method of identification and removal of near-duplicates in video search results via matching strata of keyframes. To be exact, we detect the near duplicate keyframes in each video separately. Then we partition these keyframes into summarized groups by our quasi-clique based partition. Experiments on the Trecvid dataset confirmed our initial view that a significant number of keyframes from videos in the Trecvid corpus are near-duplicates.Based on the summarized clique representation of each video, we\u00a0\u2026", "MPEG-7 is a promising standard for the description of multimedia content. A number of applications based on MPEG-7 media descriptions have been set up for research, commercial and industrial applications. Therefore, an efficient storage solution for large amounts of MPEG-7 descriptions is certainly desirable. As a kind of data-centric XML documents, MPEG-7 descriptions can be stored in the relational DBMS for efficient and effective management. The approaches of storing XML data in relational DBMS can be classified into two classes of storage model: schema-conscious and schema-oblivious. The schema-conscious model, however, cannot support complex XPath-based queries efficiently and the schema-oblivious approach lacks the flexibility in typed representation and access. Although the leading database systems have provided functionality for the XML document management, none of them can reach\u00a0\u2026", "Gossip based algorithms for information dissemination have recently received significant attention for sensor and ad hoc network applications due to their simplicity and robustness. However, a common drawback of many gossip based protocols is the waste of energy in passing redundant information over the network. Thus gossip algorithms need to be re-engineered in order to be applicable for energy constrained networks. In this paper, we consider a scenario where each node in the network holds a piece of information (message) at the beginning, and the objective is to simultaneously disseminate all information (messages) among all nodes quickly and cheaply. To provide a practical solution to this problem for ad hoc and sensor networks, NBgossip algorithm is proposed, which is based on network coding and neighborhood gossip. In NBgossip, nodes do not simply forward messages they received, instead\u00a0\u2026", "The MPEG-7 standard is a step towards standardizing the description of multimedia content so that quick and efficient identification of relevant content can be facilitated, together with efficient management of information. The description definition language (DDL) is a schema language to represent valid MPEG-7 descriptors and description schemes. MPEG-7 instances are XML documents that conform to a particular MPEG-7 schema, as expressed in the DDL and that describe audiovisual content. In this paper, we pick one of the visual descriptors related to motion in a video sequence, viz., motion trajectory. It describes the displacements of objects in time, where an object is defined as a spatiotemporal region or set of spatiotemporal regions. We present a method of automatically extracting trajectories from video sequences and generating an XML document that conforms to the MPEG-7 schema. We use\u00a0\u2026", "This paper presents an algorithm for automatically extracting significant motion trajectories in sports videos. Our approach consists of four stages: global motion estimation, motion blob detection, trajectory evolution and trajectory refinement. Global motion is estimated from the motion vectors in the compressed video using an iterative algorithm with robust outlier rejection. A statistical hypothesis test is carried out within the Block Rejection Map(BRM), which is the by-product of the global motion estimation, for the detection of motion blobs. Trajectory evolution is the process in which the motion blobs are either appended to an existing trajectory or are considered to be the beginning of a new trajectory based on its distance to an adaptive trajectory description. Finally, the extracted motion trajectories are refined using a Kalman filter. Experimental results on both indoor and outdoor sports videos demonstrate the\u00a0\u2026", "Background subtraction, the task to detect moving objects in a scene, is an important step in video analysis. In this paper, we propose an efficient background subtraction method based on coherent trajectory decomposition. We assume that the trajectories from background lie in a low-rank subspace, and foreground trajectories are sparse outliers in this background subspace. Meanwhile, the Markov Random Field (MRF) is used to encode the spatial coherency and trajectory consistency. With the low-rank decomposition and the MRF, our method can better handle videos with moving camera and obtain coherent foreground. Experimental results on a video dataset show our method achieves very competitive performance.", "Concept ontology has been used in the area of artificial intelligence, biomedical informatics and library science and it has been shown as an effective approach to better understand data in the respective domains. One main difficulty that hedge against the development of ontology approaches is the extra work required in ontology construction and annotation. With the emergent lexical dictionaries and encyclopedias such as WordNet, Wikipedia, innovations from different directions have been proposed to automatically extract concept ontologies. Unfortunately, many of the proposed ontologies are not fully exploited according to the general human knowledge. We study the various knowledge sources and aim to build a construct scalable concept thesaurus suitable for better understanding of media in the World Wide Web from Wikipedia. With its wide concept coverage, finely organized categories, diverse concept\u00a0\u2026", "In content-based image retrieval, the ldquosemantic gaprdquo between visual image features and user semantics makes it hard to predict abstract image categories from low-level features. We present a hybrid system that integrates global features (G-features) and region features (R-features) for predicting image semantics. As an intermediary between image features and categories, we introduce the notion of mid-level concepts, which enables us to predict an image's category in three steps. First, a G-prediction system uses G-features to predict the probability of each category for an image. Simultaneously, a R-prediction system analyzes R-features to identify the probabilities of mid-level concepts in that image. Finally, our hybrid H-prediction system based on a Bayesian network reconciles the predictions from both R-prediction and G-prediction to produce the final classifications. Results of experimental\u00a0\u2026", "This paper describes our preliminary attempt to automatically construct large-scale multi-modality ontology for web image classification. For text part we take advantage of both structural and content features of Wikipedia, and formalize real world objects in terms of concepts and relationships. For visual part we train classifiers according to both global and local features, and generate middle-level concepts from the training result. A variant of the association rule mining algorithm is further developed to refine the built ontology. Through experiment we prove that our method allow automatic construction of large-scale multi-modality ontology with high accuracy from challenging web image data set.", "Gossip-based algorithms for information dissemination have recently received significant attention for sensor and ad hoc network applications because of their simplicity and robustness. However, a common drawback of many gossip-based protocols is the waste of energy in passing redundant information over the network. Thus gossip algorithms need to be re-engineered in order to become applicable to energy constrained networks. In this paper, we consider a scenario where each node in the network holds a piece of information (message) at the beginning, and the objective is to simultaneously disseminate all information (messages) among all nodes quickly and cheaply. To provide a practical solution to this problem for ad hoc and sensor networks, NBgossip algorithm is proposed, which is based on network coding and neighborhood gossip. In NBgossip, nodes do not simply forward messages they\u00a0\u2026", "A working prototype of a building identification service which can be used on any camera cell phones equipped with GPS capability has been developed. Users can simply snap photos of architectures and send them, together with the corresponding GPS coordinates, via MMS to a remote server. The server will match the photos with the stored, GPS-tagged images using a combination of scale saliency algorithm for feature matching and earth movers distance measure for scene matching. The estimated location and other information are then sent back to the users via MMS. This prototype will have better accuracy than systems which rely solely on photo recognition given the exploitation of GPS information. Moreover, it is computationally lighter since the recognition engine only needs to compare stored images which lie within the GPS coordinates error range. It is relatively inexpensive as no special phones or\u00a0\u2026", "Object extraction is very useful in many object related image processing applications such as object recognition, object-based image retrieval. In this technical report, we introduce a new scheme for salient object extraction combing visual attention and edge information. The new scheme tides over the defect of visual attention model. On one hand, by combining edge information, the salient region detection can automatically grow or shrink to fit the potential object to improve viewer experience for image adaptation. On the other hand, visual attention cue helps to cluster the salient object edges which can be used to analysis object shape/contour for object-oriented processing. Visual attention provides a mechanism to locate the visual salient part of the image, which reduces the computation complexity of global image-level processing. But due to the lack of object edge information, the visual saliency map of the\u00a0\u2026", "Steel surface defect inspection is an important industrial application of machine vision that can be used to inspect the quality of many products. Most researchers in this field usually start from a task and design task-specific methods for such task. However, most surface defects can be viewed as surface areas that have abnormal material characteristics and material classification is an extensively studied field in computer vision. It will be very interesting to explore how the state-of-the-art method designed for general material classification works for domain-specific applications like defect inspection and how to improve it so that it can be adapted to real industrial applications where training samples are usually difficult to collect. In this paper, we will show our finding that the state-of-the-art general method for material classification can achieve better performance than the current best tailored method for steel surface\u00a0\u2026", "This paper presents an efficient and effective solution for retrieving Image Near-Duplicate (IND). Different from traditional methods, we analyze the local dependencies among region descriptors in a spatial-scale space. Such local dependencies in spatial-scale space(LDSS) encodes not only visual appearance but also the spatial and scale co-occurrence of them. The local dependencies are integrated over all spatial locations and multiple scales to form the image representation, which is invariant to spatial transformation and scale change. We evaluate our proposed LDSS method for IND retrieval using an existing benchmark as well as a new dataset extracted from the keyframes of TRECVID corpus. Compared to the state-of-the-art results, local dependencies in spatial-scale space(LDSS) approach has been shown to significantly improve the accuracy of IND retrieval.", "The attentive region extraction is a challenging issue for semantic interpretation of image and video content. The successful attentive region extraction greatly facilitates image classification, adaptation, compression and retrieval. Different from the traditional visual attention detection models, we propose a new attentive region extraction method based on out-of-focus blurring (OFB) technique used by professional photographers. Firstly, we combine metadata in Exchangeable Image File Format (EXIF) with visual features to quickly select professional photographs from image database. After that, an algorithm is implemented to automatically extract the attentive region from these photographs. This algorithm measures the saliency for individual pixels based on edge distribution of the images. The experimental results on OFB images have proved that our approach is able to overcome the contrast map selection problem\u00a0\u2026", "This paper presents a new method for extracting semantic information from basketball video. Our approach consists of three stages: shot and scene boundary detection, scene classification and semantic video analysis for event detection. The scene boundary detection algorithm is based on both visual and motion prediction information. After the shot and scene boundary detection, a set of visual and motion features are extracted from scene or shot. The motion features, describing the total motion, camera motion and object motion within the scene respectively, are computed from the motion vector of the compressed video using an iterative algorithm with robust outlier rejection. Finally, the extracted features are used to differentiate offensive/defensive activities in the scenes. By analyzing the offensive/defensive activities, the positions of potential semantic events, such as foul and goal, are located\u00a0\u2026", "MPEG-7 is a promising standard for the description of multimedia content. A lot of applications based on MPEG-7 media descriptions have been set up. Therefore, an efficient storage solution for large amounts of MPEG-7 descriptions are certainly desirable. MPEG-7 documents are also data-centric XML documents. Due to many advantages, the relational DBMS is the best choice for storing such XML documents. However, the existing RDBMS-based XML storage solutions can not reach all the critical requirements for MPEG-7 descriptions management. In this paper, we analyse the problems when using existing RDBMS-based XML storage approaches to store MPEG-7 documents and then present a new storage approach, called SM3+ that integrates the advantages of existing XML storage models and avoid the main drawbacks from them. Its features can reach the most critical requirements for MPEG-7\u00a0\u2026", "A new strategy to select the Lagrangian multiplier for nonreference frames is proposed, with the objective to maximize the average PSNR. The selection is based on the observation that nonreference frames should be optimized using a Lagrangian multiplier equivalent to the negative slope of the global R-D curve. This curve is not known a priori and a simple approximation is presented. Further, a new criterion for transform coefficient thresholding is proposed based on the current frame type and Lagrangian optimization. The new scheme is shown to improve the PSNR between 0.35 and 1.12 dB compared to the H.264 Test Model. The average improvement for all sequences is 0.63 dB, or equivalently a bit rate reduction of 11%.", "This paper presents the results of our study on the statistical characterization of MPEG-2 VBR video stream and the modeling of such source. The frame trace, marginal distribution of the frame size and the high autocorrelation existing in the continuous frames are analyzed. An ARMA process is used to model MPEG-2 video source. The different frame types of MPEG-2 video are considered together by performing a normalization transformation during modeling. The performance of the model is examined and the results show that the model is good in fitting the histogram and preserving the property of autocorrelation. We also indicate that the proper order of the ARMA model depends on the frame pattern of the MPEG source.", "We propose a new approach to exploit the different discriminability of image features at different scales simultaneously. By modifying the Bag-of-words model, we represent an image as a matrix whose elements are the occurrences of a set of codewords within different scale ranges. In this way, we can represent an image collection using a 3rd-order tensor. Then a new classification method, tensor-pLSA, which is an extension of Probabilistic Latent Semantic Analysis (pLSA), is introduced to classify these images based on this tensor representation. Finally, we compare the tensor representation with the original matrix representation to show the effectiveness of our approach.", "We describe a method to extract visual attention regions in images by robust subspace analysis from simple feature like intensity endowed with scale adaptivity in order to represent textured areas in an image. The scale adaptive descriptor is mapped onto clusters in linear spaces. A new subspace estimation algorithm based on the Generalized Principal Component Analysis (GPCA) is proposed to estimate multiple linear subspaces. The visual attention of each region is calculated using a new region attention measure that considers feature contrast and spatial geometric properties. Compared with existing visual attention detection methods, the proposed method directly measures global visual attention at the region level as opposed to pixel level.", "We present an event-driven video adaptation system in this paper. Events are detected by audio/video analysis and annotated by the description schemes (DSs) provided by MPEG-7 multimedia description schemes (MDSs). And then, adaptation take account of users' preference of events and network characteristic to adapt video by event selection and frame dropping as following three steps: 1) the event information is parsed from MPEG-7 annotation XML file together with bitstream to generate generic bitstream syntax description (gBSD), 2) users' preference, network characteristic and adaptation QoS (AQoS) are considered for making adaptation decision, 3) adaptation engine automatically parses adaptation decisions and gBSD to achieve adaptation. Different from most existing adaptation work, the system adapts video by interesting events according to users' preference. To achieve a generic adaptation\u00a0\u2026", "Integrated network management frameworks for self-managing systems in a grid environment consisting of disparate applications, devices and subsystems require the use of a common definition of these managed resources. The common information model (CIM) provides such a standard for their description. However, the CIM specification lacks formalism which limits its use in knowledge aggregation and reasoning. This paper discusses the design of a repository adapter for resource information modeled in CIM. The adapter translates CIM constructs to an ontology-based language, the Data Centre Markup Language (DCML), thereby formalizing the model. Issues encountered during this process are identified and areas for future work are discussed.", "One of the major objectives in multimedia research is to provide pervasive access and personalized use of multimedia information. Pervasive access of video data implies the access of cognitive and affective aspects of video content. Personalized use requires the services satisfy individual user's needs on video content. This article attempts to provide a content-on-demand (CoD) video adaptation solution by considering users' preference on cognitive content and affective content for video media in general, sports video and movies in particular. In this article, CoD video adaptation system is developed to support users' decision in selecting their content of interest and adaptively deliver video source by selecting relevant content and dropping frames while considering network conditions. First, video contents are annotated by the description schemes (DSs) provided by MPEG-7 multimedia description schemes (MDSs). Then, to achieve a generic adaptation solution, the adaptation is developed following MPEG-21 Digital Item Adaptation (DIA) framework. We study the MPEG-21 reference software on XML generation and develop our own system for CoD video adaptation in three steps: (1) the content information is parsed from MPEG-7 annotation XML file together with bitstream to generate generic Bitstream Syntax Description (gBSD); (2) Users' preference, network characteristic and adaptation QoS (AQoS) are considered for making adaptation decision; (3) adaptation engine automatically parses adaptation decisions and gBSD to achieve adaptation. Unlike most existing adaptation work, the system adapts the content of interest in the video stream\u00a0\u2026", "Many multimedia applications can benefit from recognizing image content. It requires a robust and discriminative representation of objects, especially in the situation of only a few training samples available. In this paper, we present a new approach to integrate the advantages of bag-of-words model and part-based model for image recognition. Each image is encoded as a Hierarchical Word Image (HWI), which contains not only visual appearance but also spatial information. The object parts are then located and represented in HWI. Finally, the part-based Star Model (SM) is used to learn the object model and recognize the test images. It is shown that our proposed approach can detect more accurate part candidates and significantly improve the performance of original part-based model for object recognition.", "Large-scale dataset construction will require a significant large amount of well labeled ground truth. For the NUS-WIDE dataset, a less labor-intensive annotation process was used and this paper will focuses on improving the semi-manual annotation method used. For the NUS-WIDE dataset, improving the average accuracy for top retrievals of individual concepts will effectively improve the results of the semi-manual annotation method. For web images, both tags and visual feature play important roles in predicting the concept of the image. For visual features, we have adopted an adaptive feature selection method to construct a middle level feature by concatenating the k-NN results for each type of visual feature. This middle feature is more robust than the average combination of single features, and we have shown it achieves good performance for the concept prediction. For Tag cloud, we construct a concept-tag co\u00a0\u2026", "The two volume set LNCS 4351 and LNCS 4352 constitutes the refereed proceedings of the 13th International Multimedia Modeling Conference, MMM 2007, held in Singapore in January 2007. Based on rigorous reviewing, the program committee selected 123 carefully revised full papers of the main technical sessions and 33 revised full papers of four special sessions from a total of 392 submissions for presentation in two volumes.", "In this paper, we introduce a new module,  Codebook +, into a classical framework which combines bag-of-words image representation with probabilistic latent semantic analysis (pLSA) for unsupervised object categorization. This new module makes the framework less sensitive to the image sampling methods as well as improves its performance. In this module, we create a new codebook based on the discriminability of each codeword in the original codebook for different categories. In our experiments, we compare the classification results of the framework with and without  Codebook + using five different image sampling methods.", "As the amount of multimedia data is increasing day-by-day thanks to cheaper storage devices and increasing number of information sources, the machine learning algorithms are faced with large-sized datasets. When original data is huge in size small sample sizes are preferred for various applications. This is typically the case for multimedia applications. But using a simple random sample may not obtain satisfactory results because such a sample may not adequately represent the entire data set due to random fluctuations in the sampling process. The difficulty is particularly apparent when small sample sizes are needed. Fortunately the use of a good sampling set for training can improve the final results significantly. In KDD\u201903 we proposed EASE that outputs a sample based on its \u2018closeness\u2019 to the original sample. Reported results show that EASE outperforms simple random sampling (SRS). In this paper\u00a0\u2026", "There has been recent work done in the area of search result organization for image retrieval. The main aim is to cluster the search results into semantically meaningful groups. A number of works benefited from the use of the bipartite spectral graph partitioning method [3][4]. However, the previous works mentioned use a set of keywords for each corresponding image. This will cause the bipartite spectral graph to have a high number of vertices and thus high in complexity. There is also a lack of understanding of the weights used in this method. In this paper we propose a two level reduced keywords approach for the bipartite spectral graph to reduce the complexity of bipartite spectral graph. We also propose weights for the bipartite spectral graph by using hierarchical term frequency-inverse document frequency (tf-idf). Experimental data show that this weighted bipartite spectral graph performs better than the\u00a0\u2026", "A new image retrieval system based on association rule (ARIRS) is described in this paper. Association rules have been used in application such as market basket analysis to capture relationships presented among items in large data sets. It is shown that association rules are able to find the frequent item pairs. In the image retrieval system, user\u2019s interaction is a very important part. But it is not easy for the user to specify their query. Relevance feedback is a commonly used technique for the image retrieval system to interactively interpret the user\u2019s desire. Here, we present a new scheme for mining the relevance feedback using association rules. The proposed scheme automatically establish the semantic association among images by mining the previous users\u2019 browsing and relevance feedback. Then images are retrieved based on their semantic association. Experimental results show promising performance of the proposed scheme.", "Sampling is an important preprocessing algorithm that is used to mine large data efficiently. Although a simple random sample often works fine for reasonable sample size, accuracy falls sharply with reduced sample size. In kdd\u201903 we proposed ease that outputs a sample based on its \u2018closeness\u2019 to the original sample. Reported results show that ease outperforms simple random sampling (srs). In this paper we propose easier that extends ease in two ways. 1) ease is a halving algorithm, i.e., to achieve the required sample ratio it starts from a suitable initial large sample and iteratively halves. easier, on the other hand, does away with the repeated halving by directly obtaining the required sample ratio in one iteration. 2) ease was shown to work on ibm quest dataset which is a categorical count dataset. easier, in addition, is shown to work on continuous data such as Color Structure Descriptor of images. Two\u00a0\u2026", "The main challenge of a search engine is to find information that are relevant and appropriate. However, this can become difficult when queries are issued using ambiguous words. Rijsbergen first hypothesized a clustering approach for web pages wherein closely associated pages are treated as a semantic group with the same relevance to the query\u00a0(Rijsbergen 1979). In this paper, we extend Rijsbergen\u2019s cluster hypothesis to multimedia content such as images. Given a user query, the polysemy in the return image set is related to the many possible meanings of the query. We develop a method to cluster the polysemous images into their semantic categories. The resulting clusters can be seen as the visual senses of the query, which collectively embody the visual interpretations of the query. At the heart of our method is a non-parametric Bayesian approach that exploits the complementary text and visual\u00a0\u2026", "Query suggestion is an assistive technology mechanism commonly used in search engines to enable a user to formulate their search queries by predicting or completing the next few query words that the user is likely to type. In most implementations, the suggestions are mined from query log and use some simple measure of query similarity such as query frequency or lexicographical matching. In this paper, we propose an alternative method of presenting query suggestions by their thematic topics. Our method adopts a document-centric approach to mine topics in the corpus, and does not require the availability of a query log. The heart of our algorithm is a probabilistic topic model that assumes that topics are multinomial distributions of words, and jointly learns the co-occurrence of textual words and the visual information in the video stream. Empirical results show that this alternate way of organizing query\u00a0\u2026", "Image Near-Duplicate (IND) plays an important part in many real-world multimedia applications. At the same time, both the accuracy and speed are key problems in INDs. This paper presents an efficient and effective solution for retrieving Image Near-Duplicate. Different from previous methods, we analyze the local dependencies among the descriptors in the spatial-scale space (S-Cube). Such local dependencies in spatial-scale space (S-Cube) encodes not only visual appearance but also the spatial and scale co-occurrence of them. The local dependencies are exploited over the cube-space of neighboring spatial locations and multiple adjacent scales to form the new image representation, which is invariant to spatial transformation and scale change.To speed up the retrieval process, the SuperNodes are built to incorporate the neighbor information. We evaluate our proposed spatial-scale (S-Cube) method for\u00a0\u2026", "A simple algorithm for image retargeting in the compressed domain is proposed. Most existing retargeting algorithms work directly in the spatial domain of the raw image. Here, we work on the DCT coefficients of a JPEG-compressed image to generate a gradient map that serves as an importance map to help identify those parts in the image that need to be retained during the retargeting process. Each 8\u00d78 block of DCT coefficients is scaled based on the least importance value. Retargeting can be done both in the horizontal and vertical directions with the same framework. We also illustrate image enlargement using the same method. Experimental results show that the proposed algorithm produces less distortion in the retargeted image compared to some other algorithms reported recently.", "Because of the inherent ambiguity in user queries, an important task of modern retrieval systems is faceted topic retrieval (FTR), which relates to the goal of returning diverse or novel information elucidating the wide range of topics or facets of the query need. We introduce a generative model for hypothesizing facets in the (news) video domain by combining the complementary information in the visual keyframes and the speech transcripts. We evaluate the efficacy of our multimodal model on the standard TRECVID-2005 video corpus annotated with facets. We find that: (1) the joint modeling of the visual and text (speech transcripts) information can achieve significant F-score improvement over a text-alone system; (2) our model compares favorably with standard diverse ranking algorithms such as the MMR. Our FTR model has been implemented on a news search prototype that is undergoing commercial trial.", null, "Bag-of-Words is widely used to describe images for image classification. However, this approach is limited because the spatial relation over visual words is not well exploited and also it is difficult to generate a single comprehensive vocabulary. In this paper, we propose novel effective schemes to handle these two issues. First, we propose a structure propagation technique to build more reasonable co-occurrence matrices of visual words to exploit the spatial information, which assigns a higher weight to the co-occurrence over two patches that lie in the same object part. Second, we build the multiple-histogram representation over hierarchical vocabularies to avoid the ambiguity of single vocabulary, and particularly present a learning approach to combine the multiple histograms to integrate both within-vocabulary and cross-vocabulary information. We evaluate our proposed method using the Princeton sports event\u00a0\u2026", "Web services\u2019 discovery mechanism is one of the most important research areas in Web services because of the dynamic nature of Web services. In practice, UDDI takes an important role in service discovery since it is an online registry standard to facilitate the discovery of business partners and services. However, QoS related information is not naturally supported in UDDI. Service requesters can only choose good performance Web services by manual test and comparison. In addition, discovery among private UDDI registries in a federation is not naturally supported. To address these problems, we propose UDDI extension (UX), an enhancement for UDDI that facilitates requesters to discover services with QoS awareness. In this system the service requester invokes and generates feedback reports, which are received and stored in local domain\u2019s UX server for future usage. By sharing these experiences from those\u00a0\u2026", "An audio event refers to some specific audio sound which plays important role for video content analysis. In our previous work [3], we have established audio event identification as an audio classification task. Due to the large size of audio database, representative samples are necessary for training the classifier. However, the commonly used random selection of training samples is often not adequate in selecting representative samples. In this paper we present EASIER sampling algorithm to select those data which more efficiently represent audio data characters for audio event identifier training. EASIER deterministically produces a subsample whose \u201cdistance\u201d from the complete database is minimal. Experiments in the context of audio event identification show that EASIER outperforms simple random sampling significantly.", "Signal-to-noise ratio (SNR) scalability has been incorporated into the MPEG-2 video-coding standard to allow for the delivery of two services with the same spatial and temporal resolution but different levels of quality. Scalable video coding has many advantages, such as its capability of coping with bandwidth variations, high flexibility and so on. However, few accurate rate control schemes for enhancement layer coding have been proposed. In this paper, we will present a novel enhancement layer rate control scheme for MPEG-2 SNR scalable video coding. First, we will address the current necessity and problem of rate control for layered coding. Then through analyzing characteristics of compressed data in the enhancement layer, we will derive our rate control model. The proposed rate control model is applied to a drift-free SNR scalable encoder and we show that it performs well for coding of the enhancement\u00a0\u2026", "Video classification is an important step towards multimedia understanding. Most state-of-the-art approaches which apply HMM to capture the temporal information of videos have the limitation by assuming that the current state of a video depends only on the immediate previous state. Nevertheless, this assumption may not hold for videos of various categories. In this paper, we present an effective video classifier which employs the association rule mining technique to discover the actual dependence relationship between video states. The discriminatory state transition patterns mined from different video categories are then used to perform classification. Besides capturing the association between states in the time space, we also capture the association between low-level features in spatial dimension to further distinguish the semantics of videos. Experimental results show that the performance of our association rule\u00a0\u2026", "This paper gives a brief description of an integrated workstaton based on the IBM PC for use as a videoconferencing terminal for communication over an integrated services digital network using asynchronous transfer mode. The MASTER (Multimedia AidS for TeleconfERencing) project, funded largely by the Science and Engineering Research Council, was set up to investigate a low-cost teleconferencing environment suitable for operation across an ISDN. When this project was first conceived telephone-based teleconferencing systems was already in use, but were still quite expensive to install. It was felt that technological advances were making possible the reception, and play-back, of pictures and sound on ordinary desk-top computers, which would provide the basis for low-cost teleconferencing systems. Towards the end of this project, these advances have received recognition, first, through the production of\u00a0\u2026", "With a drastic growth in the development of Automated Visual Inspection (AVI) systems in the industries, the capabilities of such applications to aid human inspectors for anomaly localization and identification have also increased. However, some issues with anomaly detection and classification in AVI systems are that such anomalies are rare in occurrence and exhibit behaviours that are unique to the application. Hence, these anomaly datasets are small and imbalanced, and a robust framework is required for such datasets. This paper proposes a Salient Textural Anomaly Proposal (STAP) framework to generate and classify salient textural proposals of regions of anomalies on metal surfaces. These anomalies have both salient and texture characteristics that are dependent on the properties of the metal surface. Furthermore, when observed across different lighting conditions, the anomalies in this AVI anomaly\u00a0\u2026", "Inspection of metal surface textures using computer vision and machine learning techniques plays an important role in Automated Visual Inspection (AVI) systems. Texture recognition on metal surface is challenging because the characteristics of each texture type are dependent on the properties of the metal surface when captured under different lighting conditions. Since these textures have no obvious repetitive patterns like general textures, this results in high intra-class diversities. Prior knowledge has shown that surface properties such as surface curvature and depth are discriminant to different texture types on metal surface. Since scale, shapes and location of textures within the same type are not fixed, scale property and spatial ordering information are less important for differentiating between texture types. There-fore, surface property, scale invariance and order-less property should be considered when\u00a0\u2026", "Salient region detection in images is a challenging task, despite its usefulness in many applications. By modeling an image as a collection of clusters, we design a unified clustering framework for salient region detection in this paper. In contrast to existing methods, this framework not only models content distinctness from the intrinsic properties of clusters, but also models content redundancy from the removed content during the retargeting process. The cluster saliency is initialized from both distinctness and redundancy and then propagated among different clusters by applying a clustering assumption between clusters and their saliency. The novel saliency propagation improves the robustness to clustering parameters as well as retargeting errors. The power of the proposed method is carefully verified on a standard dataset of 5000 real images with rectangle annotations as well as a subset with accurate\u00a0\u2026", "Discovering class-specific informative regions for a given concept with a few images is an interesting but very challenging task, due to occlusion, scale changes of objects, as well as different views under varying lighting conditions. This paper proposes a new perspective to discover the informative regions by using several images. To achieve this, we introduce a new representation of image: Ordered-BoW Image (BoWI), whose elements summarizes information of the patch centered at the element in original image. Because of its \u201cstructured pixels\u201d, BoWI is robust and informative enough for an object class representation. Histogram-based Multi-Ranking Amalgamation Strategy (MRAS) is adopted to explore the most informative patches for an object in BoWI. Experiments on Landmark-National Icon data set that our approach is robust to occlusion, scale and illumination, and achieves promising\u00a0\u2026", "We propose a new algorithm called the ZGPCA algorithm for subspace estimation based on the GPCA (Generalized Principal Component Analysis) algorithm. It is formulated within an FIR filter framework so that the norm vectors of the subspaces correspond to filter coefficients. It is shown that such an approach leads to a more accurate and computationally efficient method compared to the GPCA algorithm. We extend the ZGPCA algorithm to make it recursive so that subspaces with possibly different dimensions can be obtained. We also propose a new distance measure that can be used for k-means clustering of sample points within a subspace. Experimental results on synthetic data and applications on face clustering and sports video clustering show good performance of the proposed algorithm.", "This paper presents a rate control algorithm for the dominant discrete cosine transform (DCT)-based video coding. It is developed based on a more accurate rate-distortion (R-D) model, specifically, a new distortion-quantization (D-Q) model. Different from previous work that employs a uniform D-Q model or an empirical distortion model, our work proposes an accurate distortion model, which can quantitatively describe the relationship of distortion with respect to video source information and the selected quantization resolution. Based on understanding the distribution of source frequency coefficients and the quantization theory, our distortion model is proposed. This distortion model is combined with the classical R-D theory to generate a new rate model. Finally, the proposed model is implemented on an MPEG-4 encoder to perform rate control for a low-delay visual communication system. We also compare the\u00a0\u2026", "Over the years, multimedia applications are getting increasingly more complex and large in scale. Multimedia Web Service is identified as one of the possible solutions to meet the challenges. The advantages of using Web Services are ease of application development, adaptive to changes, fault tolerance and etc. In the paper, a sample tracking application will be discussed and developed using multimedia Web Services (multimediaWS) approach. Throughout the paper, we will suggest some general rules on designing the multimediaWS as well as evaluate the pros and cons of using multimediaWS for multimedia application.", "In this paper, we present a method of semantic knowledge building for image database by extracting semantic meanings from Web page contents. The novelty of our method is that it is able to effectively extract media with a high degree of relevancy to a specific topic by incorporating word similarity and ontologies. The method is implemented in our Web image crawler and analysis system (WICAS). The system downloads Web pages and media automatically and further analyzes the semantic meanings of page contents to build up semantic knowledge for media entities. Subsequently, our system accepts high-level query terms and returns relevant media efficiently. Our experiment results show that with this new method of high-level content abstraction, media retrieval accuracy can be improved tremendously over traditional methods", "In this paper, we address the problem of image retrieval when the query is in the form of scaled and rotated regions of images in the database. The solution lies in identifying points that are invariant to scaling and rotation and determining a robust distance measure that returns images that contain the query regions. We use the Harris-Laplacian detector to detect the interest points which are then matched with similar points in the image database using a novel fuzzy distance measure. Images with closely matching interest points are further refined using a cross-correlation measure that results in the final set of retrieval images. Experimental results show the effectiveness of the proposal image retrieval strategy.", "MPEG-21 user cases bring out a scenario of Universal Multimedia Access which is becoming the reality: people use different devices such as desktop PC, personal digital assistant as well as smartphone to access multimedia information. Viewing images on mobile devices is more and more popular than before. However, due to the screen size limitation, the experience of viewing large image on small screen devices is awkward. In this paper, an enhanced JPEG2000 image adaptation system is proposed for MPEG-21 digital item adaptation. The image is adapted considering both visual attentive region(s) of image and terminal screen size. Through the subjective testing, the system has been approved to be a solution of efficiently displaying large images in different devices.", "This letter proposes a new bit allocation scheme for Fine-Granular-Scalability (FGS) video coding. Different from traditional bit allocation schemes, we focus on understanding the relationship between rate-distortion (RD) analysis and non-zero binary-scaled coefficients (NZBC) in the bitplane (BP) coding. An optimal strategy for NZBC coding is then derived and experimental results are given for a comparison of the new scheme with uniform bit allocation.Introduction: The FGS video coding technique [1] has been proposed as a new flexible and simple framework for scalable coding, whose major difference from conventional SNR scalability is the adoption of BP coding for discrete cosine transform (DCT) coefficients instead of the conventional quantization of DCT coefficients. When we deliver the FGS-coded bitstreams through a bandwidth-constrained environment, there is a need to regulate the bitstream and accordingly perform an efficient bit allocation. Recent methods [2, 3] on bit allocation for FGS coding either employ the classical RD model used in general transform coders to express the coding behaviors of FGS layer approximately, or extract the RD relationship from experimental video data; but do not give an analytic RD model that combines the characteristics of FGS coding. In this letter, we will propose a new bit allocation scheme for FGS coding after analyzing the relationship of RD behaviors with respect to NZBC. We observe that there is a linear relationship between the output rate and the percentage of encoded NZBC in each BP, and the distortion (D) is directly determined by the coded NZBC. We derive an optimal", "In this paper, we introduce a new feature, motion histogram, for motion based video indexing. First, the motion content of the video clip is summarized in the Pixel Change Ratio Map (PCRM). Then, the histogram feature with nonuniform quantization is extracted from the PCRM to index the digital video. The nonuniform quantization levels are found by a greed algorithm. Finally, the motion histogram features are used for video clips retrieval and classification. Experimental results demonstrate the effectiveness of the feature to index the motion content of the video segment.", "2014 60-67 Hsieh, C.-C., see Hsiao, C.-C., TMM Jan. 2014 60-67 Hsu, C.-H., see Chen, K.-T., TMM Feb. 2014 480-495 Hsu, C.-T., see Tang, NC, TMM Jan. 2014 47-59 Hsu, H.-J., see Chen, K.-T., TMM Feb. 2014 480-495 Hsu, P.-C., see Chen, J.-Y., TMM Feb. 2014 337-345 Hsu, WH, see Su, Y.-C., TMM Oct. 2014 1645-1653 Hu, D., see Wu, J., TMM Jan. 2014 147-158 Hu, H., see Jin, Y., TMM Oct. 2014 1739-1751 Hu, K., see Zhu, X., TMM Nov. 2014 1888-1904 Hu, R., see Jiang, J., TMM Aug. 2014 1268-1281 Hu, W., see Liu, XL, TMM Nov. 2014 2038-2051 Hu, Z., see Liu, S., TMM Jan. 2014 253-265", "This paper presents a method of max-pooling spatially-coherent pyramid matching (MpScPM). Higher-layer representations are generated from lower-layer subregions, by a biologically-inspired max pooling strategy. Second, instead of reshaping the pyramid representation into a vector (used in generic SPM), the layer and location information of each subregion are kept and weak geometrical correspondences between matched subregions are explored to enhance our pyramid matching method. To enhance the possibility of finding the best matches at different scales and locations, cross-layer region similarities are computed, while the correspondences (either spatial neighbors or adjacent layers) are also incorporated. We evaluate our proposed MpScPM method on several existing benchmark datasets and it achieves excellent performances.", "Gossip-based algorithms for information dissemination have recently received significant attention for sensor and ad hoc network applications because of their simplicity and robustness. However, a common drawback of many gossip-based protocols is the waste of energy in passing redundant information over the network. Thus gossip algorithms need to be re-engineered in order to become applicable to energy constrained networks. In this paper, we consider a scenario where each node in the network holds a piece of information (message) at the beginning, and the objective is to simultaneously disseminate all information (messages) among all nodes quickly and cheaply. To provide a practical solution to this problem for ad hoc and sensor networks,{NBgossip} algorithm is proposed, which is based on network coding and neighborhood gossip. In NBgossip, nodes do not simply forward messages they receive\u00a0\u2026", "Due to simplicity and robustness, gossip based algorithms for data aggregation have recently received significant attention for applications in ad hoc and wireless sensor networks. Nodes in such networks operate under limited communication, computation, and energy resources. However, a common drawback of many gossip based protocols is the waste of energy in passing around redundant information multiple times. Thus gossip algorithms need to be re-designed in order to be applicable for energy constraint networks. In this paper, we study the averaging problem under the gossip constraint. In a network of n nodes, each node u i  holds a value x i  at the beginning and the objective is to compute the global average of these values in a distributed manner, while consuming least amount of energy. By formulating the problem as a closest point search in a n- dimensional cube, we demonstrate that the true\u00a0\u2026", "Bag-of-words representation has shown to be a powerful technique for image classification. In this paper, we propose a new approach to discover the discriminability of each visual word (image feature) in the codebook for each image category. A general linear model (GLM) is employed to construct new histograms of the images which are the basis for image classification. We also discuss the relations between our approach and boosting approaches and non-negative matrix factorization (NMF).", "Technology has always been moving. Throughout the decades, improvements in various technological areas have led to a greater sense of convenience to ordinary people, whether it is cutting down time in accessing normal-to-day activities or getting privileged services. One of the technological areas that had been moving very rapidly is that of mobile computing. The common mobile device now has the mobility, provides entertainment via multimedia, connects to the Internet and is powered by intelligent and powerful chips. This paper will touch on an idea that is currently in the works, an integration of a recent technology that has netizens talking all over the world; Google Maps, that provide street and satellite images via the internet to the PC and Wikipedia 's user content support idea, the biggest free-content encyclopedia on the Internet. We will hit on how it is able to integrate such a technology with the idea of\u00a0\u2026", "Web services are self-contained, self-describing modular applications. Different from traditional distributed computing, Web services are more dynamic on its ser-vice discovery and run-time binding mechanism. As big numbers of Web services appear on the Web, Web services discovery mechanism becomes essential. This chapter provides an in-depth discussion on works about Web services discovery. We first present some basis knowledge for the Web services discovery. After that we introduce some value-added services for the Web services discovery, such as the quality of service (QoS)-aware services discovery and semantics-aware service discovery. Since nonfunctional attributes, especially the QoS information, are quite important for mission critical tasks, we finally present our Semantic Web-based solution for QoS-aware service discovery and measurement. It complements Web ontology language\u00a0\u2026", "Detection, representation, and training are the three main issues that need to be resolved in an object recognition or classification system. One possible method is using collection of regions to represent object categories where each region has a distinctive feature. In this paper we present a region-based image model which learn and classify objects by training the image model with variant of the objects within the same category. Each object category is represented by a constellation of representative parts. These regions are detected by salient region detector over suitable scales. The standard disjunction rule is applied to construct the image model. During the learning procedure the distance between any two regions is calculated and accumulated as a measure which is inversely proportional to the probability of a match. The regions with large distances are removed from the image model iteratively. Finally, a\u00a0\u2026", "This paper proposes a new bit allocation scheme for fine-granular-scalability (FGS) video coding, through which we can achieve better video quality. Different from traditional rate-distortion (R-D) optimization schemes, we consider the characteristics of the bit-plane (BF) coding method. To be specific, we first find the approximate linear relationship between the bit rate of the FGS-layer and the percentage of nonzero binary-scaled coefficients (NZBC) in each BF; second, with mathematical justification, we derive an optimal strategy by analyzing the overall distortion with respect to NZBC. Finally, we perform our optimum bit allocation (OBA) on a FGS coder. Experimental results prove that our scheme can achieve smooth video quality with a higher average PSNR gain compared with uniform bit allocation (UBA). And for certain frames with lower PSNR, it has a gain of up to 3 dB. It is highly source-independent and more\u00a0\u2026", "In this report, we present a new motion feature, motion correlograms, to index the motion content in videos. First, the motion content of the video is summarized as a Pixel Change Ratio Map (PCRM). The PCRM captures the intensity and duration of the motion in a video, which are the two most important factors according to human\u2019s perception of the motion content. The PCRM also indicates the spatial location and size of the moving object (s). Then after a non-uniform quantization of the PCRM, the motion correlograms are extracted as the index to motion content. Experimental results on video retrieval and video classification demonstrate that the proposed motion correlogram outperforms not only the traditional motion vector based feature, but also the our previously proposed PCRM based motion histogram features.", null, "This paper studies the transmission of MPEG\u20102 VBR video over ATM network under usage parameter control. The idea is to seek a compromise between the network utilization and the quality of video service by applying UPC\u2010based rate control strategies to the video source. A modified leaky bucket algorithm is proposed to calculate the constraints on the bit\u2010rate guaranteeing conformance to peak cell rate, sustainable cell rate and burst tolerance usage parameters. Two rate control strategies, one for real\u2010time generated video coding and the other for pre\u2010compressed video, are proposed for MPEG\u20102 VBR video. The rate control strategies control the video source to generate traffic conforming to the constraints on the bit rate. The experimental results show that both the UPC\u2010based rate control strategies can provide lossless transmission from the source perspective as well as to reduce the burstiness of the traffic\u00a0\u2026", "Intranet is increasingly being used asthe transport mechanism for informationsystems in corporations. This paper documents the design and development of an Intranet-based system which automates the profiles management, schedules and booking of the artistes in the broadcasting industry according to the changing production schedules. This distributed web database application has direct application to the broadcasting industry and can be deployed in general for any applications, which require scheduling of similar resources. Tasks performed include the booking of artistes, the capturing and maintenance ofartiste schedules, information and dossiers. The latter is captured in both the English and Chinese languages to cater to the multilingual requirements of the industry. The paper also documents the experiences gained and the results obtained in the deployment of the first intranet-based system in the organisation. It has minimised scheduling errors, improved the production planning process and increased both the individual and organisation productivity.", "The Artistes Management System (AMS) is an intranet application designed and developed for the broadcasting industry. AMS captures all the artistes' information and maintains artistes' dossiers, both in English as well as in Chinese. It also automates the artistes' booking system and captures and maintains the artistes' schedules. Other key functions of AMS include the generation of statistical reports and the maintenance of the system's user profiles and access levels. The system entails a minimum installation of additional software on client workstations, as it is Web-based and employs a central server with a multiple-client concept.", "A configurable architecture for a low-cost videoconferencing terminal usable on an ATM network has been developed which provides a high degree of performance in a modular manner. Considerable research has been conducted in order to provide support for the loss of any compressed video which is transferred between such terminals. This work enables lost video to be concealed using a simple processing algorithm operating only on the received data. Additional features, such as multiparty conferencing support, can be added if required to adapt the terminal to both the users' needs and the underlying network capabilities. An example terminal was designed and built to demonstrate the architecture. This was based on a PC with additional hardware support for video and networking functions.", "This paper describes the design and implementation of a video on demand service over a local area network. The implementation allows for the playback of different types of MPEG streams: video only and combined audio plus video streams over the network.", "The paper describes videoconferencing workstation using ATM for the transmission of all data and Motion-JPEG for compression of the video. Of particular interest is the work on algorithms to help conceal the errors that arise from transmission over an ATM network. Several methods are discussed and the results of some subjective assessments of typical images are presented.< >", "This thesis looks into the problems arising in an image transmission system when transmitting over an A TM network. Two main areas were investigated: (i) an alternative coding technique to reduce the bit rate required; and (ii) concealment of errors due to cell loss, with emphasis on processing in the transform domain of DCT-based images. [Continues.]", "A configurable architecture for a low-cost videoconferencing terminal usable on an ATM network has been developed which provides a high degree of performance in a modular manner. Considerable research has been conducted in order to provide support for the loss of any compressed video which is transferred between such terminals. This work enables lost video to be concealed using a simple processing algorithm operating only on the received data. Additional features, such as multiparty conferencing support, can be added if required to adapt the terminal to both the users' needs and the underlying network capabilities. An example terminal was designed and built to demonstrate the architecture. This was based on a PC with additional hardware support for video and networking functions.", null, "Real-world images always involve pictures with various backgrounds, object aspects, poses and appearances. Taking the animal classes in Figure 1 as an example, human-beings can easily differentiate the four classes. However, computers are not able to identify the difference in the same way. The varied background environment of the same Arctic Fox class can introduce great variance in global image features, while the subtle fur color difference between Arctic Fox and Fennec Fox makes it difficult to classify from local image features. It is also hard to identify the different distribution of colors over Maned Wolf or Dhole from spatial features. On the other hand, cues from the text on the corresponding web page could make a substantial contribution to the performance of image classification. For example, even a single keyword Kashmir could indicate the Dhole class, as Kashmir is the habitat of Dhole. Similar useful relationships which help to narrow down the final concepts include name, diet, and distribution relationships. Therefore, an effective way is to combine the images features with the text information for image retrieval, where ontology is utilized for this purpose. Ontology, which clearly defines concepts and their relationships in a domain, has been widely used in many information retrieval fields, including document indexing, ie, extracting semantic contents from a set of text document, image retrieval and classification, ie, using concepts either from image features or surrounding text for content representation, and video retrieval, ie, using text in video captions for semantic concept detection. Note that most of the approaches involve\u00a0\u2026", "\uae30\ud55c\ub2e4. 16. \uadf8\ub9bc\uc740 \ud2b8\ub808\uc774\uc2f1 \ud398\uc774\ud37c\ub098 \ubc31\uc9c0\uc5d0 \uba39\uc774\ub098 \ub808 E \ub808\ub9c1\uc73c\ub85c \uc120\uba85\ud558\uac8c \uadf8\ub9ac\ub3c4, \ud06c\uae30\ub294 \uac00\uae09\uc801 \ubc18\ub2e8 (\uac00\ub85c 7cm) \uc73c\ub85c \ud558\uace0 \uc6d0\uace0\uac00 \uc774 \ud06c\uae30\ubcf4\ub2e4 \ud074 \ub54c\ub294 \ucd95\uc18c\ub420 \uac83\uc744\uace0\ub824\ud558\uc5ec \uae00\uc790\uc758 \ud06c\uae30\ub97c \uc54c\ub9de\uac8c \uc120\uc815\ud55c\ub2e4. \uc804\ub2e8 (\uac00", "10 A new motion feature for video indexing is proposed in this paper. The motion content of the video at pixel level, is 11 represented as a Pixel Change Ratio Map (PCRM). The PCRM enables us to capture the intensity of motion in a video 12 sequence. It also indicates the spatial location and size of the moving object. The proposed motion feature is the motion 13 histogram which is a non-uniformly quantized histogram of the PCRM. We demonstrate the usefulness of the motion 14 histogram with three applications, viz., video retrieval, video clustering and video classification. 15\u00a9 2004 Elsevier BV All rights reserved.", "In this paper, we present a method of semantic knowledge building for image database by extracting semantic meanings from Web page contents. The novelty of our method is that it is able to effectively extract media with a high degree of relevancy to a specific topic by incorporating word similarity and ontologies. The method is implemented in our Web image crawler and analysis system (WICAS). The system downloads Web pages and media automatically and further analyzes the semantic meanings of page contents to build up semantic knowledge for media entities. Subsequently, our system accepts high-level query terms and returns relevant media efficiently. Our experiment results show that with this new method of high-level content abstraction, media retrieval accuracy can be improved tremendously over traditional methods.", "Image Attention Page 1 1 Robust Subspace Analysis for Detecting Visual Attention Regions in \nImages Yiqun Hu, Deepu Rajan and Liang-Tien Chia Center of Multimedia and Network \nTechnology School of Computer Engineering Nanyang Tehnological University Image \nAttention \u220e Visual Attention \u2751 A mechanism of HVS to limit processing to important information \n\u220e Application \u2751 Image Browsing & Retrieval \u2751 Adaptive Content Delivery \u2751 Object Recognition \n\u2751 Mobile Robotics \u2751 Gaming Page 2 2 Different Perspectives \u220e Spatial-based Image Attention \n\u2751 Attention is deployed in different spatial locations \u2751 Before grouping, not information of \nobject \u220e Object-based Image Attention \u2751 Attention is deployed on the level of object; \u2751 Focus \nare shift between different object Space-based Attention Method \u220e Biological-based model \n\u2751 Based on the attention framework of Freman (Freman85) \u2751 \u201cCenter-surround\u201d contrast, \u2026", null, "Presents the table of contents for this issue of the periodical.", "BackgroundThe motivation for this project comes from the desire to provide for PET-DEVICE, for the moment, let\u2019s just call it a Distributed Multimedia Retrieval System with an acceptable level of content interpretation. Current retrieval systems are mainly either on audio, image or video and you will hardly find one that will return you a meaningful result set consisting of multiple media types. The failure to retrieve a meaningful result set could be due to the level of content interpretation that is currently available for video and audio sequences plus the fact that there are no common descriptors that exist for different types of media. Current retrieval is usually from a single database source with limited media resources and unlikely to have access to multimedia databases across geographically different locations.", "In content-based image retrieval, combination of multiple features within a single model has been investigated as a promising technique to increase the retrieval efficiency. MPEG-7 standards provide standard descriptions for multimedia content. In this paper, a descriptor-weighting scheme for combining multiple MPEG-7 visual descriptors is discussed. The descriptors and relationships among them will be combined with optimal weights. We derive the weighting parameters of different descriptors based on optimized technique. An optimal model is build to find a set of optimal weights for a set of descriptors. Explicit solutions can be derived by Lagrange multipliers, which are optimal and easy to calculate. The calculation procedure is fully automatic and no manually work is needed. Experiments show that better retrieval results can be achieved than using single descriptor. It is also better than simple average\u00a0\u2026", "In this paper, we present a new approach for video motion analysis in frequency domain. First, the 3D video signal (xyt) is transformed into the frequency domain by 3D Fourier transform. After applying 3D Fourier transform on the video signal, we derive that the non-zeros coefficients should lie on one plane (coplanarity) under the condition that the video motion is translational. We call this plane video motion plane. Furthermore, the normal of the video motion plane gives the velocity of the motion. Therefore, the video motion estimation problem turns out to be a plane fitting problem in the 3D frequency domain with the nonzero Fourier coefficients. Robust M-estimator is used to fit the plane and give the estimated velocity. Experimental results on both synthetic and real video sequences illustrate robustness against noise and good performance of the proposed approach. keywords"]}, "collaboration_network": {"target": ["Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Shenghua Gao", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Yiqun Hu", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Ivor W. Tsang, IEEE Fellow", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Min Xu", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Ah-Hwee Tan", "Zhengxiang Wang", "Zhengxiang Wang", "Zhengxiang Wang", "Zhengxiang Wang", "Zhengxiang Wang", "Zhengxiang Wang", "Zhengxiang Wang", "Zhengxiang Wang", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Sourav S Bhowmick", "Sourav S Bhowmick", "Sourav S Bhowmick", "Sourav S Bhowmick", "Sourav S Bhowmick", "Sourav S Bhowmick", "Sourav S Bhowmick", "Sourav S Bhowmick", "Ziming Zhang", "Ziming Zhang", "Ziming Zhang", "Ziming Zhang", "Feng Lu", "Feng Lu", "Feng Lu", "Feng Lu", "Feng Lu", "Feng Lu", "Feng Lu", "Xing Xie \u8c22\u5e78", "Xing Xie \u8c22\u5e78", "Xing Xie \u8c22\u5e78", "Tzu-Yi HUNG", "Tzu-Yi HUNG", "Tzu-Yi HUNG", "Tzu-Yi HUNG", "Tat-Jen Cham", "Tat-Jen Cham", "Tat-Jen Cham", "Ling Chen", "Ling Chen", "Ling Chen", "Ling Chen", "Ling Chen", "Changsheng Xu", "Changsheng Xu", "Sriram Vaikundam", "Sriram Vaikundam", "Sriram Vaikundam", "Lin Weisi", "Lin Weisi", "Sabu Emmanuel", "Sabu Emmanuel", "Ian McLoughlin", "Ian McLoughlin", "Joo Hwee Lim", "Joo Hwee Lim", "Joo Hwee Lim", "Joo Hwee Lim", "Joo Hwee Lim", "Qi Tian"], "target_id": ["1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "fe-1v0MAAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "gIHCye8AAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "rJMOlVsAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "Ac6VCMkAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "G0hdDqYAAAAJ", "43G9x0YAAAAJ", "43G9x0YAAAAJ", "43G9x0YAAAAJ", "43G9x0YAAAAJ", "43G9x0YAAAAJ", "43G9x0YAAAAJ", "43G9x0YAAAAJ", "43G9x0YAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "o0F3sqEAAAAJ", "o0F3sqEAAAAJ", "o0F3sqEAAAAJ", "o0F3sqEAAAAJ", "o0F3sqEAAAAJ", "o0F3sqEAAAAJ", "o0F3sqEAAAAJ", "o0F3sqEAAAAJ", "2yqx3oIAAAAJ", "2yqx3oIAAAAJ", "2yqx3oIAAAAJ", "2yqx3oIAAAAJ", "ySTrnjMAAAAJ", "ySTrnjMAAAAJ", "ySTrnjMAAAAJ", "ySTrnjMAAAAJ", "ySTrnjMAAAAJ", "ySTrnjMAAAAJ", "ySTrnjMAAAAJ", "5EQfAFIAAAAJ", "5EQfAFIAAAAJ", "5EQfAFIAAAAJ", "W_GZKeYAAAAJ", "W_GZKeYAAAAJ", "W_GZKeYAAAAJ", "W_GZKeYAAAAJ", "Lx3X7W0AAAAJ", "Lx3X7W0AAAAJ", "Lx3X7W0AAAAJ", "L5aYWQcAAAAJ", "L5aYWQcAAAAJ", "L5aYWQcAAAAJ", "L5aYWQcAAAAJ", "L5aYWQcAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "91NBEiUAAAAJ", "91NBEiUAAAAJ", "91NBEiUAAAAJ", "D_S41X4AAAAJ", "D_S41X4AAAAJ", "4q2qmJsAAAAJ", "4q2qmJsAAAAJ", "mcnKgPoAAAAJ", "mcnKgPoAAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "HJt0niEAAAAJ"], "type": ["NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "NTU", "NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU"], "location": ["Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "ShanghaiTech University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Worcester Polytechnic Institute", "Worcester Polytechnic Institute", "Worcester Polytechnic Institute", "Worcester Polytechnic Institute", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Microsoft", "Microsoft", "Microsoft", "Unknown", "Unknown", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "University of Technology Sydney", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Unknown", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR"], "year": [2006, 2005, 2010, 2005, 2004, 2007, 2013, 2005, 2010, 2003, 2004, 2021, 2013, 2005, 2004, 2005, 2006, 2010, 2007, 2011, "Unknown", 2006, 2009, "Unknown", 2012, "Unknown", 2006, 2008, 2005, 2006, 2004, 2012, 2007, 2009, 2007, 2005, 2005, 2005, 2004, 2005, "Unknown", 2004, 2005, 2005, 2006, 2005, "Unknown", "Unknown", 2012, 2010, 2009, 2013, 2012, 2010, 2013, 2009, 2010, 2014, 2010, 2012, 2012, 2013, 2011, 2010, 2009, 2010, 2005, 2007, 2009, 2005, 2008, 2008, 2010, 2011, "Unknown", 2009, 2011, 2006, 2013, 2008, 2005, 2006, 2004, 2009, 2004, 2005, "Unknown", 2011, 2010, 2012, "Unknown", 2013, 2010, 2014, 2012, 2010, 2011, 2010, 2007, 2005, 2012, 2004, 2004, 2006, 2006, 2006, 2005, 2006, 2006, 2012, 2009, 2008, 2009, 2009, 2010, 2010, 2009, 2012, "Unknown", 2009, "Unknown", 2010, 2012, 2011, 2013, 2010, 2010, 2004, 2006, 2006, 2009, 2007, 2004, 2004, 2005, 2004, 2005, 2004, 2006, 2007, 2007, 2007, 2007, 2008, 2006, 2008, 2007, 2008, 2007, 2007, 2009, 2009, 2005, 2004, 2016, 2017, 2017, 2016, 2007, 2010, "Unknown", 2004, 2004, 2005, 2004, 2006, 2004, 2004, 2017, 2017, 2016, 2012, 2011, 2012, 2011, 2012, 2011, 2012, 2009, 2009, 2010, 2012, 2004], "title": ["A motion-based scene tree for compressed video content management", "ARIRS: association rule based image retrieval system", "Scene classification using multiple features in a two-stage probabilistic classification framework", "Salient region detection using weighted feature maps based on the human visual attention model", "Video Indexing Using Motion Correlograms", "Scale adaptive visual attention detection by subspace analysis", "Background subtraction via coherent trajectory decomposition", "A new motion histogram to index motion content in video segments", "Image retargeting in compressed domain", "A unified approach to detection of shot boundaries and subshots in compressed video", "A motion based scene tree for browsing and retrieval of compressed videos", "Deep residual pooling network for texture recognition", "Regularized feature reconstruction for spatio-temporal saliency detection", "Adaptive local context suppression of multiple cues for salient visual attention detection", "Automatic extraction of motion trajectories in compressed sports videos", "Region-based image retrieval with scale and orientation invariant features", "Dynamic programming-based reverse frame selection for VBR video delivery under constrained resources", "Improved saliency detection based on superpixel clustering and saliency propagation", "Click4BuildingID@ NTU: Click for Building Identification with GPS-enabled Camera Cell Phone", "Salient region detection by jointly modeling distinctness and redundancy of image content", "145 Mining user hidden semantics from image content for image retrieval", "Multimodal semantic analysis and annotation for basketball video", "Coherent phrase model for efficient image near-duplicate retrieval", "ROBUST VIDEO MOTION ANALYSIS IN FREQUENCY DOMAIN", "Video saliency detection with robust temporal alignment and local-global spatial contrast", "OPTIMIZATION-BASED MULTIPLE MPEG-7 DESCRIPTORS FOR IMAGE RETRIEVAL", "Event on demand with MPEG-21 video adaptation system", "Detection of visual attention regions in images using robust subspace analysis", "Robust subspace analysis for detecting visual attention regions in images", "An event-driven sports video adaptation for the MPEG-21 DIA framework", "Region-of-interest based image resolution adaptation for mpeg-21 digital item", "Spatiotemporal saliency detection via sparse representation", "Advances in Multimedia Modeling: 13th International Multimedia Modeling Conference, MMM 2007, Singapore, January 9-12, 2007, Proceedings, Part II", "Attention-from-motion: A factorization approach for detecting attention objects in motion", "A ZGPCA Algorithm for Subspace Estimation", "Adaptive hierarchical multi-class SVM classifier for texture-based image classification", "Automatic generation of MPEG-7 compliant XML document for motion trajectory descriptor in sports video", "Semantic video indexing and summarization using subtitles", "Motion Histogram: A New Motion Feature to Index Motion Content in Video Segment.", "Global motion compensated key frame extraction from compressed videos", "A new motion histogram to index motion content", "Salient object extraction combining visual attention and edge information", "Attention region selection with information from professional digital camera", "JPEG2000 image adaptation for MPEG-21 digital items", "Affective content detection in sitcom using subtitle and audio", "Semantic analysis of basketball video using motion information", "Image Attention", "Integration of Synthetic and Natural Audio/Video Registration Based on Scene Recognition and Natural Features Tracking Techniques for Wide-Area Augmented Reality", "Laplacian sparse coding, hypergraph laplacian sparse coding, and applications", "Wikipedia-assisted concept thesaurus for better web media understanding", "Understanding tag-cloud and visual features for better annotation of concepts in NUS-WIDE dataset", "Regularized feature reconstruction for spatio-temporal saliency detection", "Learning class-to-image distance via large margin and l1-norm regularization", "Kernel sparse representation for image classification and face recognition", "Region-based saliency detection and its application in object recognition", "Concept model-based unsupervised web image re-ranking", "Web image concept annotation with better understanding of tags and visual features", "Concurrent single-label image classification and annotation via efficient multi-layer group sparse coding", "Automatic image tagging via category label and web data", "Sparse representation with kernels", "Spatiotemporal saliency detection via sparse representation", "Background subtraction via coherent trajectory decomposition", "Multi-layer group sparse coding\u2014For concurrent image classification and annotation", "Local features are not lonely\u2013Laplacian sparse coding for image classification", "Hierarchicalword image representation for parts-based object recognition", "Image-to-class distance metric learning for image classification", "Salient region detection using weighted feature maps based on the human visual attention model", "Scale adaptive visual attention detection by subspace analysis", "Learning instance-to-class distance for human action recognition", "Adaptive local context suppression of multiple cues for salient visual attention detection", "Image near-duplicate retrieval using local dependencies in spatial-scale space", "Motion context: A new representation for human action recognition", "Improved saliency detection based on superpixel clustering and saliency propagation", "Salient region detection by jointly modeling distinctness and redundancy of image content", "145 Mining user hidden semantics from image content for image retrieval", "Coherent phrase model for efficient image near-duplicate retrieval", "Improved learning of I2C distance and accelerating the neighborhood search for image classification", "Event on demand with MPEG-21 video adaptation system", "Learning image-to-class distance metric for image classification", "Detection of visual attention regions in images using robust subspace analysis", "Robust subspace analysis for detecting visual attention regions in images", "An event-driven sports video adaptation for the MPEG-21 DIA framework", "Region-of-interest based image resolution adaptation for mpeg-21 digital item", "Attention-from-motion: A factorization approach for detecting attention objects in motion", "Salient object extraction combining visual attention and edge information", "JPEG2000 image adaptation for MPEG-21 digital items", "Image Attention", "Exploiting local dependencies with spatial-scale space (s-cube) for near-duplicate retrieval", "Multi-label learning by image-to-class distance for scene classification and image annotation", "Laplacian sparse coding, hypergraph laplacian sparse coding, and applications", "In Process", "Region-based saliency detection and its application in object recognition", "Automatic image tagging via category label and web data", "Concurrent single-label image classification and annotation via efficient multi-layer group sparse coding", "Sparse representation with kernels", "Local features are not lonely\u2013Laplacian sparse coding for image classification", "Multi-layer group sparse coding\u2014For concurrent image classification and annotation", "Kernel sparse representation for image classification and face recognition", "Efficient sampling of training set in large and noisy multimedia data", "Affective content analysis in comedy and horror videos by audio emotional event detection", "Content on demand video adaptation based on MPEG-21 digital item adaptation", "Audio keyword generation for sports video analysis", "HMM-based audio keyword generation", "Event on demand with MPEG-21 video adaptation system", "Efficient data reduction in multimedia data", "Affective content detection in sitcom using subtitle and audio", "Easier sampling for audio event identification", "An event-driven sports video adaptation for the MPEG-21 DIA framework", "Multimodal semantic analysis and annotation for basketball video", "Topic based query suggestions for video search", "Wikipedia2Onto---Adding Wikipedia Semantics to Web Image Retrieval", "Ontology enhanced web image retrieval: aided by wikipedia & spreading activation theory", "A latent model for visual disambiguation of keyword-based image search", "A Bayesian approach integrating regional and global features for image semantic learning", "Faceted topic retrieval of news video using joint topic modeling of visual features and speech transcripts", "Wikipedia2Onto: Building concept ontology automatically, experimenting with web image retrieval", "Coherent phrase model for efficient image near-duplicate retrieval", "A non-parametric visual-sense model of images\u2014extending the cluster hypothesis beyond text", "Integration of Synthetic and Natural Audio/Video Registration Based on Scene Recognition and Natural Features Tracking Techniques for Wide-Area Augmented Reality", "Learning instance-to-class distance for human action recognition", "In Process", "Image-to-class distance metric learning for image classification", "Learning class-to-image distance via large margin and l1-norm regularization", "Improved learning of I2C distance and accelerating the neighborhood search for image classification", "Learning image-to-class distance metric for image classification", "Automatic image tagging via category label and web data", "Multi-label learning by image-to-class distance for scene classification and image annotation", "HMM-based audio keyword generation", "Dynamic programming-based reverse frame selection for VBR video delivery under constrained resources", "An event-driven sports video adaptation for the MPEG-21 DIA framework", "LT codes decoding: Design and analysis", "Advances in Multimedia Modeling: 13th International Multimedia Modeling Conference, MMM 2007, Singapore, January 9-12, 2007, Proceedings, Part II", "Mining maximal frequently changing subtree patterns from XML Documents", "VRules: an effective association-based classifier for videos", "SM3+: An XML database solution for the management of MPEG-7 descriptions", "Looking at mapping, indexing & querying of MPEG-7 descriptors in RDBMS with SM3", "Mining positive and negative association rules from XML query patterns for caching", "Mining association rules from structural deltas of historical xml documents", "FRACTURE mining: Mining frequently and concurrently mutating structures from historical XML documents", "Mapping, indexing and querying of MPEG-7 descriptors in RDBMS with IXMDB", "Discriminative Signatures for Image Classification", "Codebook+: a new module for creating discriminative codebooks", "Image classification using tensor representation", "Motion context: A new representation for human action recognition", "Multimedia web services for an object tracking and highlighting application", "\u6682\u7f3a", "NBgossip-neighborhood gossip with network coding based message aggregation", "Nbgossip: An energy-efficient gossip algorithm for wireless sensor networks", "Secure multi-path in sensor networks", "Gossip-based Computation of Average: a Closest Point Search Approach", "LT codes decoding: Design and analysis", "Coherent phrase model for efficient image near-duplicate retrieval", "Salient region detection using weighted feature maps based on the human visual attention model", "Salient object extraction combining visual attention and edge information", "Using material classification methods for steel surface defect inspection", "Phase fourier reconstruction for anomaly detection on metal surface using salient irregularity", "Convolutional networks for voting-based anomaly classification in metal surface inspection", "Anomaly region detection and localization in metal surface inspection", "Click4BuildingID@ NTU: Click for Building Identification with GPS-enabled Camera Cell Phone", "Estimating camera pose from a single urban ground-view omnidirectional image and a 2D building outline map", "CCI//or PC/DerS", "Mining maximal frequently changing subtree patterns from XML Documents", "VRules: an effective association-based classifier for videos", "Mining positive and negative association rules from XML query patterns for caching", "Mining association rules from structural deltas of historical xml documents", "FRACTURE mining: Mining frequently and concurrently mutating structures from historical XML documents", "Audio keyword generation for sports video analysis", "HMM-based audio keyword generation", "Phase fourier reconstruction for anomaly detection on metal surface using salient irregularity", "Convolutional networks for voting-based anomaly classification in metal surface inspection", "Anomaly region detection and localization in metal surface inspection", "Fourier transform-based scalable image quality measure", "Nonintrusive quality assessment of noise suppressed speech with mel-filtered energies and support vector regression", "Fourier transform-based scalable image quality measure", "Nonintrusive quality assessment of noise suppressed speech with mel-filtered energies and support vector regression", "Fourier transform-based scalable image quality measure", "Nonintrusive quality assessment of noise suppressed speech with mel-filtered energies and support vector regression", "Topic based query suggestions for video search", "A latent model for visual disambiguation of keyword-based image search", "A Bayesian approach integrating regional and global features for image semantic learning", "Faceted topic retrieval of news video using joint topic modeling of visual features and speech transcripts", "A non-parametric visual-sense model of images\u2014extending the cluster hypothesis beyond text", "HMM-based audio keyword generation"], "link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ns9cj8rnVeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:5ugPr518TE4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:1yQoGdGgb4wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:kRWSkSYxWN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:8AbLer7MMksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:CHSYGLWDkRkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:rO6llkc54NcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:08ZZubdj9fEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:KxtntwgDAa4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:XiVPGOgt02cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:l7t_Zn2s7bgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:vRqMK49ujn8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:1sJd4Hv_s6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:LPZeul_q3PIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:hMod-77fHWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:eflP2zaiRacC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:Tiz5es2fbqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:kRWSkSYxWN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:sSrBHYA8nusC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:bnK-pcrLprsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:B3FOqHPlNUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:l7t_Zn2s7bgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:5ugPr518TE4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:8AbLer7MMksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:CHSYGLWDkRkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:xtRiw3GOFMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:K3LRdlH-MEoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:XiVPGOgt02cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:_xSYboBqXhAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:Tiz5es2fbqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:_Ybze24A_UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:bnK-pcrLprsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:B3FOqHPlNUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:BrmTIyaxlBUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:738O_yMBCRsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:RHpTSmoSYBkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:XiVPGOgt02cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:NMxIlDl6LWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:e5wmG9Sq2KIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:p2g8aNsByqUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:eflP2zaiRacC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:_Ybze24A_UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:sSrBHYA8nusC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:xtRiw3GOFMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:K3LRdlH-MEoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:XiVPGOgt02cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:vRqMK49ujn8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:SeFeTyx0c_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:dhFuZR0502QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:a0OBvERweLwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:k_IJM867U9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:YOwf2qJgpHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:dshw04ExmUIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:eMMeJKvmdy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:AXPGKjj_ei8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:tkaPQYYpVKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Mojj43d5GZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:WqliGbK-hY8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:SeFeTyx0c_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:AXPGKjj_ei8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:tkaPQYYpVKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:Mojj43d5GZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:tOudhMTPpwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:tS2w5q8j5-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:tOudhMTPpwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:tS2w5q8j5-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:tOudhMTPpwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:tS2w5q8j5-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=20&pagesize=80&citation_for_view=Eeolw80AAAAJ:NMxIlDl6LWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:e5wmG9Sq2KIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&cstart=100&pagesize=100&citation_for_view=Eeolw80AAAAJ:p2g8aNsByqUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Eeolw80AAAAJ&citation_for_view=Eeolw80AAAAJ:UeHWp8X0CEIC"]}, "published_by_year": {"Year": ["1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Publications": [4, 0, 1, 2, 1, 1, 1, 1, 0, 1, 20, 24, 14, 16, 9, 10, 17, 7, 9, 4, 2, 0, 2, 2, 0, 2, 0, 1, 0, 0, 13]}, "citations_by_year": {"Year": ["1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [3, 1, 1, 3, 7, 2, 3, 5, 3, 4, 10, 50, 111, 152, 166, 224, 246, 279, 335, 446, 495, 499, 510, 414, 349, 310, 239, 231, 200, 147, 14]}, "all_time_h_index": 35, "all_time_i10_index": 73, "all_time_i20_index": 45, "h_index_by_year": {"Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 5, 8, 10, 13, 15, 16, 18, 20, 23, 26, 28, 30, 31, 31, 32, 32, 34, 34, 35]}, "h_index_by_publication_year": {"Publication Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 10, 12, 6, 6, 7, 7, 11, 4, 6, 4, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0]}, "avg_citations_by_publication_year": {"Publication Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [5.0, 0.0, 0.0, 3.0, 17.0, 0.0, 0.0, 0.0, 0.0, 17.0, 39.45, 33.791666666666664, 17.285714285714285, 5.6875, 37.0, 15.1, 80.58823529411765, 22.428571428571427, 88.77777777777777, 111.5, 20.0, 0.0, 9.5, 49.5, 0.0, 1.0, 0.0, 36.0, 0.0, 0.0]}, "h_index_by_years_from_publication_year": {"Publication Year": [1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 5, 6, 7, 7, 8, 8, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 2, 5, 7, 8, 9, 9, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 2, 2, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 2, 4, 5, 5, 5, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 2, 3, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 2, 5, 6, 7, 9, 9, 10, 10, 10, 10, 10, 10, 10, 11, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 4, 5, 5, 6, 6, 6, 6, 6, 6, 6, 1, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]}}