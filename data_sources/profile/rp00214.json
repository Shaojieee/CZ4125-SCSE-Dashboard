{"full_name": "Zinovi Rabinovich", "email": "zinovi@ntu.edu.sg", "google_scholar": "https://scholar.google.com/citations?hl=en&user=JwJRnmAAAAAJ", "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp00214", "designation": "Assistant Professor, School of Computer Science and Engineering", "image_path": "./profile_img/zinovi_rabinovich.jpg", "biography": "Zinovi Rabinovich is an Assistant Professor at the School of Computer Science and Engineering of the Nanyang Technological University since February 2017. Prior to that he has spent 5 years as a Senior Algorithms Engineer at the Mobileye Vision Technologies Ltd, a leader in the Advanced Driver Assistant Systems (ADAS) enabling software and hardware, with an outlook at autonomous driving. Between years 2007 and 2012 he was a Postdoctoral Fellow at the University of Sothampton, UK,  as a member of the \"Market-Based Control\" project,  and then a Postdoctoral Researcher in a joint research by the GM Advanced Research Center and the Bar-Ilan University, Israel. His education record includes PhD and MSc in Computer Science, and a joint Computer Science and Mathematics BSc degrees, all from the Hebrew University of Jerusalem, Israel. His research interests include multi-agent systems, artificial intelligence, modelling of manipulative interaction,  perceptual control and emergent behaviour. Common themes of his publications are social behaviour analysis, persuasion, and control via environment response modifications. His research has been presented at leading international conference on Artificial Intelligence, such as  IJCAI, AAAI and AAMAS, included in high ranking journals, such as  Artificial Intelligence, and Robotics and Autonomous Systems, and appeared as a subject of multiple Tutorials for research students.His service to the research community includes participation in program committees of top conferences (IJCAI, AAAI, AAMAS) , workshop advisory boards, and others.", "orcid": null, "other_websites": ["zinovi.net"], "bachelor_degree": null, "masters": null, "phd": null, "name": "Zinovi Rabinovich", "id": "rp00214", "publications": {"Publication Year": ["2015", "2003", "2015", "2020", "2015", "2014", "2012", "2013", "2015", "2011", "2021", "2009", "2019", "2017", "2015", "2021", "2021", "2021", "2020", "2005", "2015", "2015", "2015", "2004", "2021", "2017", "2016", "2015", "2010", "2016", "2015", "2008", "2016", "2006", "2006", "2017", "2017", "2002", "2020", "2007", "2019", "2021", "2010", "2005", "2014", "2008", "2022", "2020", "2006", "2006", "2023", "2020", "2013", "2008", "2008", "2005", "2002", "2023", "2022", "2022", "2022", "2019", "2019", "2019", "2018", "2012", "2009", "2008", "2007", "2005", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "Title": ["Exploring information asymmetry in two-stage security games", "The complexity of multiagent systems: The price of silence", "Information disclosure as a means to security", "Learning efficient multi-agent communication: An information bottleneck approach", "On the convergence of iterative voting: how restrictive should restricted dynamics be?", "Strategic information disclosure to people with multiple alternatives", "Strategic advice provision in repeated human-agent interactions", "Computing pure Bayesian-Nash equilibria in games with finite actions and continuous types", "Analysis of equilibria in iterative voting schemes", "Giving advice to people in path selection problems", "RMIX: Learning risk-sensitive policies for cooperative reinforcement learning agents", "Generalised fictitious play for a continuum of anonymous players", "Imitative follower deception in stackelberg games", "Doodle poll games", "Security games with information leakage: Modeling and computation", "Mis-spoke or mis-lead: Achieving robustness in multi-agent communicative reinforcement learning", "Transferable environment poisoning: Training-time attack on reinforcement learning", "I2HRL: interactive influence-based hierarchical reinforcement learning", "New algorithms for continuous distributed constraint optimization problems", "Multiagent coordination by extended Markov tracking", "Strategic candidacy games with lazy candidates", "Convergence to equilibria in strategic candidacy", "Lie on the fly: Manipulative voters under iterative voting centre", "Extended Markov Tracking with an application to control", "Protecting elections by recounting ballots", "Real candidacy games: a new model for strategic candidacy", "Trembling Hand Equilibria of Plurality Voting", "Beyond plurality: Truth-bias in binary scoring rules", "Cultivating desired behaviour: Policy teaching via environment-dynamics tweaks", "Nonmyopic voting dynamics: An optimistic approach", "Farsighted voting dynamics", "Approximating mixed Nash equilibria using smooth fictitious play in simultaneous auctions", "Between fairness and a mistrial: Consensus under a deadline", "On the response of EMT-based control to interacting targets and models", "Behaviosites: A novel paradigm for affecting distributed behavior", "Haste makes waste: a case to favour voting bots", "Distant Truth: Bias Under Vote Distortion Costs.", "Non-approximability of Centralized Control", "Manipulating elections by selecting issues", "Dynamics based control with an application to area-sweeping problems", "Lie on the fly: Strategic voting in an iterative preference elicitation process", "Reaching consensus under a deadline", "A hybrid controller based on the egocentric perceptual principle", "Robot-control based on Extended Markov Tracking: Initial experiments", "Faustian Dynamics in Sarkar's Social Cycle", "Dynamics based control with psrs", "Spiking pitch black: Poisoning an unknown environment to attack unknown reinforcement learners", "RMIX: Risk-sensitive multi-agent reinforcement learning", "Behaviosites: Manipulation of multiagent system behavior through parasitic infection", "Dynamics based control: Structure", "Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning", "Teaching multiple learning agents by environment-dynamics tweaks", "Ishikawa play.", "Coordination and multi-tasking using EMT", "Approximating mixed Nash equilibria using smooth fictitious play in simultaneous auctions (short paper)", "Dynamics based control: An introduction", "Non-approximability of centralized control in distributed systems", "Policy Resilience to Environment Poisoning Attacks on Reinforcement Learning", "Towards Skill and Population Curriculum for MARL", "Multi-Agent Multi-Game Entity Transformer", "Off-Beat Multi-Agent Reinforcement Learning", "Stategic Candidacy with Keen Candidates", "Reaching Consensus Under a Deadline", "These Polar Twins: Opinion Dynamics of Intervals", "Voting application: where we will go for dinner?", "Strategic behaviour under constrained autonomy", "Extended Markov Tracking with Ensemble Actions", "HANS-HUJI\u2019s Autonomous Navigation System", "Dynamics Based Control and Continual Planning", "Simulation of Cooperative Behavioral Trends by Local Interaction Rules", "King\u2019s Research Portal", "Poisoning the Well: Can We Simultaneously Attack a Group of Learning Agents?", "Lie on the Fly: Strategic Voting in an Iterative Preference Elicitation Process", "Neither here nor there", "Truly Black-box Attack on Reinforcement Learning via Environment Poisoning", "Truth-Bias Complexity in the Veto Voting Rule", "Flexagons and Prolog", "Strategic and Tactical Planning", "Decision-Making in Extended Multiagent Interactions", "Dynamics Based Control", "A Strategic/Tactical Architecture for Planning in Dynamic Environments"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:ns9cj8rnVeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:70eg2SAEIzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:2P1L_qKh6hAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:NaGl4SEjCO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:vV6vV6tmYwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Y0pCki6q_DkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:SeFeTyx0c_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:M05iB0D1s5AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:pqnbT2bcN3wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ufrVoPGSRksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:LkGwnXOMwfcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:_Qo2XoVZTnwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:pyW8ca7W8N0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:fPk4N6BV_jEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:a0OBvERweLwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:MXK_kJrjxJIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:UebtZRa9Y70C"], "Topic": ["Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others"], "# of Citations": [94, 92, 87, 66, 64, 60, 54, 52, 48, 40, 34, 30, 28, 23, 22, 20, 20, 19, 19, 16, 14, 14, 15, 14, 12, 10, 11, 10, 10, 9, 9, 8, 8, 7, 8, 7, 7, 7, 6, 4, 5, 3, 4, 4, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["Stackelberg security games have been widely deployed to protect real-word assets. The main solution concept there is the Strong Stackelberg Equilibrium (SSE), which optimizes the defender's random allocation of limited security resources. However, solely deploying the SSE mixed strategy has limitations. In the extreme case, there are security games where the defender is able to defend all the assets``almost perfectly\" at the SSE, but she still sustains significant loss. In this paper, we propose an approach for improving the defender's utility in such scenarios. Perhaps surprisingly, our approach is to strategically reveal to the attacker information about the sampled pure strategy. Specifically, we propose a two-stage security game model, where in the first stage the defender allocates resources and the attacker selects a target to attack, and in the second stage the defender strategically reveals local information about that target, potentially deterring the attacker's attack plan. We then study how the defender can play optimally in both stages. We show, theoretically and experimentally, that the two-stage security game model allows the defender to gain strictly better utility than SSE.", "In this work, we suggest representing multiagent systems using computational models, choosing, specifically, Multi-Prover Interactive Protocols to represent agent systems and the interactions occurring within them. This approach enables us to analyze complexity issues related to multiagent systems. We focus here on the complexity of coordination and study the possible sources of this complexity. We show that there are complexity bounds that cannot be lowered even when approximation techniques are applied.", "In this paper we present a novel Stackelberg-type model of security domains: Security Assets aSsignment with Information disclosure (SASI). The model combines both the features of the Stackelberg Security Games (SSGs) model and of the Bayesian Persuasion (BP) model. More specifically, SASI includes: a) an uncontrolled, exogenous security state that serves as the Defender\u2019s private information; b) multiple security assets with non-accumulating, targetlocal defence capability; c) a pro-active, verifiable and public, unidirectional information disclosure channel from the Defender to the Attacker. We show that SASI with a non-degenerate information disclosure can be arbitrarily more efficient, than a \u201csilent\u201d Stackelberg assets allocation. We also provide a linear program reformulation of SASI that can be solved in polynomial time in SASI parameters. Furthermore, we show that it is possible to remove one of SASI parameters and, rather than require it as an input, recover it by computation. As a result, SASI becomes highly scalable.", "We consider the problem of the limited-bandwidth communication for multi-agent reinforcement learning, where agents cooperate with the assistance of a communication protocol and a scheduler. The protocol and scheduler jointly determine which agent is communicating what message and to whom. Under the limited bandwidth constraint, a communication protocol is required to generate informative messages. Meanwhile, an unnecessary communication connection should not be established because it occupies limited resources in vain. In this paper, we develop an Informative Multi-Agent Communication (IMAC) method to learn efficient communication protocols as well as scheduling. First, from the perspective of communication theory, we prove that the limited bandwidth constraint requires low-entropy messages throughout the transmission. Then inspired by the information bottleneck principle, we learn a valuable and compact communication protocol and a weight-based scheduler. To demonstrate the efficiency of our method, we conduct extensive experiments in various cooperative and competitive multi-agent tasks with different numbers of agents and different bandwidths. We show that IMAC converges faster and leads to efficient communication among agents under the limited bandwidth as compared to many baseline methods.", "We study convergence properties of iterative voting procedures. Such procedures are defined by a voting rule and a (restricted) iterative process, where at each step one agent can modify his vote towards a better outcome for himself. It is already known that if the iteration dynamics (the manner in which voters are allowed to modify their votes) are unrestricted, then the voting process may not converge. For most common voting rules this may be observed even under the best response dynamics limitation. It is therefore important to investigate whether and which natural restrictions on the dynamics of iterative voting procedures can guarantee convergence. To this end, we provide two general conditions on the dynamics based on iterative myopic improvements, each of which is sufficient for convergence. We then identify several classes of voting rules (including Positional Scoring Rules, Maximin, Copeland and Bucklin), along with their corresponding iterative processes, for which at least one of these conditions hold.", "In this article, we study automated agents that are designed to encourage humans to take some actions over others by strategically disclosing key pieces of information. To this end, we utilize the framework of persuasion games\u2014a branch of game theory that deals with asymmetric interactions where one player (Sender) possesses more information about the world, but it is only the other player (Receiver) who can take an action. In particular, we use an extended persuasion model, where the Sender\u2019s information is imperfect and the Receiver has more than two alternative actions available. We design a computational algorithm that, from the Sender\u2019s standpoint, calculates the optimal information disclosure rule. The algorithm is parameterized by the Receiver\u2019s decision model (i.e., what choice he will make based on the information disclosed by the Sender) and can be retuned accordingly. We then provide an\u00a0\u2026", "This paper addresses the problem of automated advice provision in settings that involve repeated interactions between people and computer agents. This problem arises in many real world applications such as route selection systems and office assistants. To succeed in such settings agents must reason about how their actions in the present influence people's future actions. This work models such settings as a family of repeated bilateral games of incomplete information called``choice selection processes'', in which players may share certain goals, but are essentially self-interested. The paper describes several possible models of human behavior that were inspired by behavioral economic theories of people's play in repeated interactions. These models were incorporated into several agent designs to repeatedly generate offers to people playing the game. These agents were evaluated in extensive empirical investigations including hundreds of subjects that interacted with computers in different choice selections processes. The results revealed that an agent that combined a hyperbolic discounting model of human behavior with a social utility function was able to outperform alternative agent designs, including an agent that approximated the optimal strategy using continuous MDPs and an agent using epsilon-greedy strategies to describe people's behavior. We show that this approach was able to generalize to new people as well as choice selection processes that were not used for training. Our results demonstrate that combining computational approaches with behavioral economics models of people in repeated interactions facilitates the\u00a0\u2026", "We extend the well-known fictitious play (FP) algorithm to compute pure-strategy Bayesian-Nash equilibria in private-value games of incomplete information with finite actions and continuous types (G-FACTs). We prove that, if the frequency distribution of actions (fictitious play beliefs) converges, then there exists a pure-strategy equilibrium strategy that is consistent with it. We furthermore develop an algorithm to convert the converged distribution of actions into an equilibrium strategy for a wide class of games where utility functions are linear in type. This algorithm can also be used to compute pure \u03f5-Nash equilibria when distributions are not fully converged. We then apply our algorithm to find equilibria in an important and previously unsolved game: simultaneous sealed-bid, second-price auctions where various types of items (e.g., substitutes or complements) are sold. Finally, we provide an analytical\u00a0\u2026", "Following recent studies of iterative voting and its effects on plurality vote outcomes, we provide characterisations and complexity results for three models of iterative voting under the plurality rule. Our focus is on providing a better understanding regarding the set of equilibria attainable by iterative voting processes. We start with the basic model of plurality voting. We first establish some useful properties of equilibria, reachable by iterative voting, which enable us to show that deciding whether a given profile is an iteratively reachable equilibrium is NP-complete. We then proceed to combine iterative voting with the concept of truth bias, a model where voters prefer to be truthful when they cannot affect the outcome. We fully characterise the set of attainable truth-biased equilibria, and show that it is possible to determine all such equilibria in polynomial time. Finally, we also examine the model of lazy voters, in which a voter may choose to abstain from the election. We establish convergence of the iterative process, albeit not necessarily to a Nash equilibrium. As in the case with truth bias, we also provide a polynomial time algorithm to find all the attainable equilibria.", "We present a novel computational method for advicegeneration in path selection problems which are difficult for people to solve. The advisor agent\u2019s interests may conflict with the interests of the people who receive the advice. Such optimization settings arise in many human-computer applications in which agents and people are self-interested but also share certain goals, such as automatic route-selection systems that also reason about environmental costs. This paper presents an agent that clusters people into one of several types, based on how their path selection behavior adheres to the paths preferred by the agent and are not necessarily preferred by the people. It predicts the likelihood that people deviate from these suggested paths and uses a decision theoretic approach to suggest modified paths to people that will maximize the agent\u2019s expected benefit. This technique was evaluated empirically in an extensive study involving hundreds of human subjects solving the path selection problem in mazes. Results showed that the agent was able to outperform alternative methods that solely considered the benefit to the agent or the person, or did not provide any advice.", "Current value-based multi-agent reinforcement learning methods optimize individual Q values to guide individuals' behaviours via centralized training with decentralized execution (CTDE). However, such expected, ie, risk-neutral, Q value is not sufficient even with CTDE due to the randomness of rewards and the uncertainty in environments, which causes the failure of these methods to train coordinating agents in complex environments. To address these issues, we propose RMIX, a novel cooperative MARL method with the Conditional Value at Risk (CVaR) measure over the learned distributions of individuals' Q values. Specifically, we first learn the return distributions of individuals to analytically calculate CVaR for decentralized execution. Then, to handle the temporal nature of the stochastic outcomes during executions, we propose a dynamic risk level predictor for risk level tuning. Finally, we optimize the CVaR policies with CVaR values used to estimate the target in TD error during centralized training and the CVaR values are used as auxiliary local rewards to update the local distribution via Quantile Regression loss. Empirically, we show that our method outperforms many state-of-the-art methods on various multi-agent risk-sensitive navigation scenarios and challenging StarCraft II cooperative tasks, demonstrating enhanced coordination and revealing improved sample efficiency.", "Recently, efficient approximation algorithms for finding Nash equilibria have been developed for the interesting class of anonymous games, where a player's utility does not depend on the identity of its opponents. In this paper, we tackle the problem of computing equilibria in such games with\u00a0 continuous player types, extending the framework to encompass settings with imperfect information. In particular, given the existence result for pure Bayes-Nash equilibria in these games, we generalise the fictitious play algorithm by developing a novel procedure for finding a best response strategy, which is specifically designed to deal with continuous and, therefore, infinite type spaces. We then combine the best response computation with the general fictitious play structure to obtain an equilibrium. To illustrate the power of this approach, we apply our algorithm to the domain of simultaneous auctions with continuous private values and discrete bids, in which the algorithm shows quick convergence.", "Information uncertainty is one of the major challenges facing applications of game theory. In the context of Stackelberg games, various approaches have been proposed to deal with the leader's incomplete knowledge about the follower's payoffs, typically by gathering information from the leader's interaction with the follower. Unfortunately, these approaches rely crucially on the assumption that the follower will not strategically exploit this information asymmetry, i.e., the follower behaves truthfully during the interaction according to their actual payoffs. As we show in this paper, the follower may have strong incentives to deceitfully imitate the behavior of a different follower type and, in doing this, benefit significantly from inducing the leader into choosing a highly suboptimal strategy. This raises a fundamental question: how to design a leader strategy in the presence of a deceitful follower? To answer this question, we put\u00a0\u2026", "In Doodle polls, each voter approves a subset of the available alternatives according to his preferences. While such polls can be captured by the standard models of Approval voting, Zou et al. [18] analyse real-life Doodle poll data and conclude that poll participants' behaviour seems to be affected by considerations other than their intrinsic preferences over the alternatives. To capture this phenomenon, they propose a model of social voting, where voters approve their top alternatives as well as additional 'safe' choices so as to appear cooperative. The predictions of this model turn out to be consistent with the real-life data. However, Zou et al. do not attempt to rationalise the voters' behaviour in the context of social voting: They explicitly describe the voters' strategies rather than explain how these strategies arise from voters' preferences. In this paper, we complement the work of Zou et al. by putting forward a model in which the behaviour described by Zou et al. arises as an equilibrium strategy. In our model, a voter derives a bonus from approving each additional alternative, up to a certain cap. We show that trembling hand perfect Nash equilibria of our model behave consistently with the model of Zou et al. Importantly, placing a cap on the total bonus is an essential component of our model: in the absence of the cap, all Nash equilibria are very far from the behaviour observed in Doodle polls.", "Most models of Stackelberg security games assume that the attacker only knows the defender's mixed strategy, but is not able to observe (even partially) the instantiated pure strategy. Such partial observation of the deployed pure strategy -- an issue we refer to as information leakage -- is a significant concern in practical applications. While previous research on patrolling games has considered the attacker's real-time surveillance, our settings, therefore models and techniques, are fundamentally different. More specifically, after describing the information leakage model, we start with an LP formulation to compute the defender's optimal strategy in the presence of leakage. Perhaps surprisingly, we show that a key subproblem to solve this LP (more precisely, the defender oracle) is NP-hard even for the simplest of security game models. We then approach the problem from three possible directions: efficient algorithms for restricted cases, approximation algorithms, and heuristic algorithms for sampling that improves upon the status quo. Our experiments confirm the necessity of handling information leakage and the advantage of our algorithms.", "Recent studies in multi-agent communicative reinforcement learning (MACRL) have demonstrated that multi-agent coordination can be greatly improved by allowing communication between agents. Meanwhile, adversarial machine learning (ML) has shown that ML models are vulnerable to attacks. Despite the increasing concern about the robustness of ML algorithms, how to achieve robust communication in multi-agent reinforcement learning has been largely neglected. In this paper, we systematically explore the problem of adversarial communication in MACRL. Our main contributions are threefold. First, we propose an effective method to perform attacks in MACRL, by learning a model to generate optimal malicious messages. Second, we develop a defence method based on message reconstruction, to maintain multi-agent coordination under message attacks. Third, we formulate the adversarial communication problem as a two-player zero-sum game and propose a game-theoretical method R-MACRL to improve the worst-case defending performance. Empirical results demonstrate that many state-of-the-art MACRL methods are vulnerable to message attacks, and our method can significantly improve their robustness.", "Studying adversarial attacks on Reinforcement Learning (RL) agents has become a key aspect of developing robust, RL-based solutions. Test-time attacks, which target the post-learning performance of an RL agent\u2019s policy, have been well studied in both white-and black-box settings. More recently, however, state-of-the-art works have shifted to investigate training-time attacks on RL agents, ie, forcing the learning process towards a target policy designed by the attacker. Alas, these SOTA works continue to rely on white-box settings and/or use a reward-poisoning approach. In contrast, this paper studies environment-dynamics poisoning attacks at training time. Furthermore, while environment-dynamics poisoning presumes a transfer-learning capable agent, it also allows us to expand our approach to black-box attacks. Our overall framework, inspired by hierarchical RL, seeks the minimal environment-dynamics manipulation that will prompt the momentary policy of the agent to change in a desired manner. We show the attack efficiency by comparing it with the reward-poisoning approach, and empirically demonstrate the transferability of the environment-poisoning attack strategy. Finally, we seek to exploit the transferability of the attack strategy to handle black-box settings.", "Hierarchical reinforcement learning (HRL) is a promising approach to solve tasks with long time horizons and sparse rewards. It is often implemented as a high-level policy assigning subgoals to a low-level policy. However, it suffers the high-level non-stationarity problem since the lowlevel policy is constantly changing. The nonstationarity also leads to the data efficiency problem: policies need more data at non-stationary states to stabilize training. To address these issues, we propose a novel HRL method: Interactive Influence-based Hierarchical Reinforcement Learning (I2HRL). First, inspired by agent modeling, we enable the interaction between the lowlevel and high-level policies, ie, the low-level policy sends its policy representation to the highlevel policy. The high-level policy makes decisions conditioned on the received low-level policy representation as well as the state of the environment. Second, we stabilize the training of the high-level policy via an information-theoretic regularization with minimal dependence on the changing low-level policy. Third, we propose the influence-based exploration to more frequently visit the non-stationary states where more transition data is needed. We experimentally validate the effectiveness of the proposed solution in several tasks in MuJoCo domains by demonstrating that our approach can significantly boost the learning performance and accelerate learning compared with stateof-the-art HRL methods.", "Distributed Constraint Optimization Problems (DCOPs) are a powerful tool to model multi-agent coordination problems that are distributed by nature. The formulation is suitable for problems where variables are discrete and constraint utilities are represented in tabular form. However, many real-world applications have variables that are continuous and tabular forms thus cannot accurately represent constraint utilities. To overcome this limitation, researchers have proposed the Continuous DCOP (C-DCOP) model, which are DCOPs with continuous variables. But existing approaches usually come with some restrictions on the form of constraint utilities and are without quality guarantees. Therefore, in this paper, we (i) propose an exact algorithm to solve a specific subclass of C-DCOPs;(ii) propose an approximation method with quality guarantees to solve general C-DCOPs;(iii) propose additional C-DCOP algorithms that are more scalable; and (iv) empirically show that our algorithms outperform existing state-of-the-art C-DCOP algorithms when given the same communication limitations.", "We present here Extended Markov Tracking (EMT), a computationally tractable method for the online estimation of Markovian system dynamics, along with experimental support for its successful contribution to a specific control architecture. The control architecture leverages EMT to simultaneously track and correct system dynamics.Using a widespread extension of the Markovian environment model to multiagent systems, we provide an application of EMT-based control to multiagent coordination. The resulting coordinated action algorithm, in contrast to alternative approaches, does not eliminate interference among agents, but rather exploits it for purposes of synchronization and implicit information transfer. This information transfer enables the algorithm to be computationally tractable. Experiments are presented that demonstrate the effectiveness of EMT-based control for multiagent coordination in stochastic\u00a0\u2026", "In strategic candidacy games, both voters and candidates have preferences over the set of candidates, and candidates may strategically withdraw from the election in order to manipulate the outcome according to their preferences. In this work, we extend the standard model of strategic candidacy games by observing that candidates may find it costly to run an electoral campaign and may therefore prefer to withdraw if their presence has no effect on the election outcome. We study the Nash equilibria and outcomes of natural best-response dynamics in the resulting class of games, both from a normative and from a computational perspective, and compare them with the Nash equilibria of the standard model.", "We study equilibrium dynamics in candidacy games, in which candidates may strategically decide to enter the election or withdraw their candidacy, following their own preferences over possible outcomes. Focusing on games under Plurality, we extend the standard model to allow for situations where voters may refuse to return their votes to those candidates who had previously left the election, should they decide to run again. We show that if at the time when a candidate withdraws his candidacy, with some positive probability each voter takes this candidate out of his future consideration, the process converges with probability 1. This is in sharp contrast with the original model where the very existence of a Nash equilibrium is not guaranteed. We then consider the two extreme cases of this setting, where voters may block a withdrawn candidate with probabilities 0 or 1. In these scenarios, we study the complexity of reaching equilibria from a given initial point, converging to an equilibrium with a predermined winner or to an equilibrium with a given set of running candidates. Except for one easy case, we show that these problems are NP-complete, even when the initial point is fixed to a natural\u2014truthful\u2014state where all potential candidates stand for election.", null, "We present here a tracking method for Markovian system dynamics which operates in an on-line fashion, during interaction with the system. Existence of a simple numerical solution of the technique is demonstrated. We also demonstrate the application of the tracking method in a novel continual planning architecture, together with experimental data to support the validity of the presented tracking approach.", "Complexity of voting manipulation is a prominent topic in computational social choice. In this work, we consider a two-stage voting manipulation scenario. First, a malicious party (an attacker) attempts to manipulate the election outcome in favor of a preferred candidate by changing the vote counts in some of the voting districts. Afterwards, another party (a defender), which cares about the voters' wishes, demands a recount in a subset of the manipulated districts, restoring their vote counts to their original values. We investigate the resulting Stackelberg game for the case where votes are aggregated using two variants of the Plurality rule, and obtain an almost complete picture of the complexity landscape, both from the attacker's and from the defender's perspective.", "We introduce Real Candidacy Games (RCGs)\u2014a novel strategic candidacy model, where candidates have a continuous range of positions that affect their attractiveness for voters. We also allow candidates to have their own non-trivial preferences over the candidate set. We study RCGs with restricted and unrestricted positioning strategies to establish conditions for Nash Equilibrium (NE) existence. That is, we investigate under what voting rules and tiebreaking schemes, a stable candidate positioning exists. While for several voting rule classes (eg, Condorcet-Consistent) we obtain positive results, we also show that for some scoring rules there are examples without a NE for an arbitrarily large number of voters.", "Trembling hand (TH) equilibria were introduced by Selten in 1975. Intuitively, these are Nash equilibria that remain stable when players assume that there is a small probability that other players will choose off-equilibrium strategies. This concept is useful for equilibrium refinement, i.e., selecting the most plausible Nash equilibria when the set of all Nash equilibria can be very large, as is the case, for instance, for Plurality voting with strategic voters. In this paper, we analyze TH equilibria of Plurality voting. We provide an efficient algorithm for computing a TH best response and establish many useful properties of TH equilibria in Plurality voting games. On the negative side, we provide an example of a Plurality voting game with no TH equilibria, and show that it is NP-hard to check whether a given Plurality voting game admits a TH equilibrium where a specific candidate is among the election winners.", "It is well known that standard game-theoretic approaches to voting mechanisms lead to a multitude of Nash Equilibria (NE), many of which are counter-intuitive. We focus on truth-biased voters, a model recently proposed to avoid such issues. The model introduces an incentive for voters to be truthful when their vote is not pivotal. This is a powerful refinement, and recent simulations reveal that the surviving equilibria tend to have desirable properties. However, truth-bias has been studied only within the context of plurality, which is an extreme example of k-approval rules with . We undertake an equilibrium analysis of the complete range of k-approval. Our analysis begins with the veto rule, the other extreme point of k-approval, where each ballot approves all candidates but one. We identify several crucial properties of pure NE for truth-biased veto. These properties show a clear distinction from\u00a0\u2026", "In this paper we study, for the first time explicitly, the implications of endowing an interested party (i.e. a teacher) with the ability to modify the underlying dynamics of the environment, in order to encourage an agent to learn to follow a specific policy. We introduce a cost function which can be used by the teacher to balance the modifications it makes to the underlying environment dynamics, with the learner's performance compared to some ideal, desired, policy. We formulate teacher's problem of determining optimal environment changes as a planning and control problem, and empirically validate the effectiveness of our model.", "Iterative voting has presented, in the past few years, a voting model in which a player participates in an election poll, and can change his vote at any time to influence the result. Several extensions for this model have been considered, including some attempts to handle the uncertainty that players may face. However, all those extensions retained the myopic assumption\u2014that is, players change their vote only when they believe that their move will have an immediate effect on the outcome. In this paper, we address this assumption by allowing for certain non-myopic dynamics. Specifically, the outlook is optimistic to a certain extent, a horizon, as players change their vote if they believe that if some other players also move, the outcome can change. We show that players with the same horizon of optimism would converge to a Nash equilibrium under Plurality, and for Veto, even players with varying horizons of optimism always converge. However, such non-myopic behavior is not necessarily a positive feature\u2014as we demonstrate, in some cases it is better for the player to stick to myopic moves.", "Iterative voting has presented, in the past few years, a voting model in which a player is presented with an election poll, and changes their vote to influence the result immediately. Several extensions have been presented for this model, including some attempts to handle the uncertainty facing the players, but all of them retained the myopic assumption\u2013players change their vote only when they believe they might be changing the outcome by their move. This paper tackles this assumption by bounding the farsightedness of the players. Players will change their vote if they believe that if a certain number of other voters will change as well, the outcome might change. We show that players with the same farsightedness will converge to a Nash equilibrium with plurality, and with veto, even players with varying farsightedness degree will always converge. However, we show farsightedness is not necessarily a positive feature\u2014in several cases it is better for the player to be myopic.", "We investigate equilibrium strategies for bidding agents that participate in multiple, simultaneous second-price auctions with perfect substitutes. For this setting, previous research has shown that it is a best response for a bidder to participate in as many such auctions as there are available, provided that other bidders only participate in a single auction. In contrast, in this paper we consider equilibrium behaviour where all bidders participate in multiple auctions. For this new setting we consider mixed-strategy Nash equilibria where bidders can bid high in one auction and low in all others. By discretising the bid space, we are able to use smooth fictitious play to compute approximate solutions. Specifically, we find that the results do indeed converge to -Nash mixed equilibria and, therefore, we are able to locate equilibrium strategies in such complex games where no known solutions previously existed.", "Jury trial is, perhaps, the most prominent example of seeking a consensus. The process is particularly difficult if the judge places a deadline by which the jury must reach a unanimous decision, otherwise declaring a mistrial. A mistrial is commonly perceived to be worse than any decision the jury might render. As a result, while each juror has her own idea about the fairness of each possible trial outcome, she may eventually choose to vote for a less fair outcome, rather than cause a mistrial by breaking unanimity.In this paper we propose a model for the above scenario\u2014Consensus Under a Deadline (CUD)\u2014based on a time-bounded iterative voting process. We provide some theoretical features of CUDs, particularly focusing on convergence guarantees and the quality of the final decision. An extensive experimental study demonstrates the more subtle features of CUDs, eg, the difference between two simple types of juror behaviour, lazy vs. proactive voters.", "A novel control mechanism was recently introduced based on Extended Markov Tracking (EMT) [9, 10]. In this paper, we present a study of its response to multiple interacting control goals. We show a simple extension that can be integrated into EMT-based control, and which provides it with the ability to handle several behavioral targets. Experimental support for the validity of this extension is provided. We also describe an experiment with a simulated robot, where EMT-based controllers interact and interfere indirectly via the environment. Experiments support the resilience of multiagent EMT-based team control to potential conflicts that may appear within a team.", "We present the Behaviosite paradigm, a new approach to affecting the behavior of distributed agents in a multiagent system, which is inspired by biological parasites with behavior manipulation properties. Behaviosites are special kinds of agents that \"infect\" a system composed of agents operating in that environment. The behaviosites facilitate behavioral changes in agents to achieve altered, potentially improved, performance of the overall system. Behaviosites need to be designed so that they are intimately familiar with the internal workings of the environment and of the agents operating within it, and behaviosites apply this knowledge for their manipulation, using various infection and manipulation strategies. To demonstrate and test this paradigm, we implemented a version of the El Farol problem, using behaviosites.", "Voting is a common way to reach a group decision. When possible, voters will attempt to vote strategically, in order to optimize their satisfaction from the outcome. Previous research has modelled how rational voter agents (bots) vote to maximize their personal utility in an iterative voting process that has a deadline (a timeout). However, it remains an open question whether human beings behave rationally when faced with the same settings. The focus of this paper is therefore to examine how the deadline factor affects manipulative behavior in real-world scenarios were humans are required to reach a decision before a deadline. An On-line platform was built to enable voting games by all types of users: agents (bots), humans, and mixed games with both humans and agents. We compare the results of human behavior and bot behavior and conclude that it might be wise to allow bots to make (certain) decisions on our\u00a0\u2026", "In recent years, there has been increasing interest within the computational social choice community regarding models where voters are biased towards specific behaviors or have secondary preferences. An important representative example of this approach is the model of truth bias, where voters prefer to be honest about their preferences, unless they are pivotal. This model has been demonstrated to be an effective tool in controlling the set of pure Nash equilibria in a voting game, which otherwise lacks predictive power. However, in the models that have been used thus far, the bias is binary, ie, the final utility of a voter depends on whether he cast a truthful vote or not, independently of the type of lie.In this paper, we introduce a more robust framework, and eliminate this limitation, by investigating truth-biased voters with variable bias strength. Namely, we assume that even when voters face incentives to lie towards a better outcome, the ballot distortion from their truthful preference incurs a cost, measured by a distance function. We study various such distance-based cost functions and explore their effect on the set of Nash equilibria of the underlying game. Intuitively, one might expect that such distance metrics may induce similar behavior. To our surprise, we show that the presented metrics exhibit quite different equilibrium behavior.", "The complexity of finding an optimal decentralized policy for a cooperative multi-agent system is known to be non-deterministically exponential. Agents in such a system try to maximize a single global utility function, and do not compete; nevertheless, computing the optimal strategies for these agents is intractable. In this paper, we relax the requirement of finding an optimal decentralized policy for a cooperative multiagent system. We consider, instead, the complexity of finding decentralized policy solutions whose values are-distant from the optimal solution value, ie, sub-optimal solutions that may have errors. Among the major contributions of this research is showing that the apparently easier problem of finding an approximate solution to decentralized control remains as hard as the optimal version of the problem.This paper, by formally analyzing the complexity of finding optimal and sub-optimal joint policies for cooperative decentralized systems, sheds new light on the theoretical foundations of multiagent systems. In particular, we have adopted from complexity theory the concept of multiprover interactive protocols (MIPs), which can serve as computational models for certain kinds of multi-agent systems. This is a new direction not yet studied within the distributed artificial intelligence community, one that provides new opportunities for the theoretical analysis of multiagent systems.", "Constructive election control considers the problem of an adversary who seeks to sway the outcome of an electoral process in order to ensure that their favored candidate wins. In this thesis, we first consider the computational problem of constructive election control via issue selection. In this problem, a party decides which political issues to focus on to ensure victory for the favored candidate. We also consider a variation in which the goal is to maximize the number of voters supporting the favored candidate. We present strong negative results, showing, for example, that the latter problem is inapproximable for any constant factor. On the positive side, we show that when issues are binary, the problem becomes tractable in several cases, and admits a 2-approximation in the two-candidate case. Finally, we develop integer programming and heuristic methods for these problems.", "In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments. Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics. We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC. EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments. We exploit this efficiency in a set of experiments that applied multi-target EMT to a class of area-sweeping problems (searching for moving targets). We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.", "A voting center is in charge of collecting and aggregating voter preferences. In an iterative process, the center sends comparison queries to voters, requesting them to submit their preference between two items. Voters might discuss the candidates among themselves, figuring out during the elicitation process which candidates stand a chance of winning and which do not. Consequently, strategic voters might attempt to manipulate by deviating from their true preferences and instead submit a different response in order to attempt to maximize their profit. We provide a practical algorithm for strategic voters which computes the best manipulative vote and maximizes the voter\u2019s selfish outcome when such a vote exists. We also provide a careful voting center which is aware of the possible manipulations and avoids manipulative queries when possible. In an empirical study on four real world domains, we show that in\u00a0\u2026", "Group decisions are often complicated by a deadline. For example, in committee hiring decisions the deadline might be the next start of a budget, or the beginning of a semester. It may be that if no candidate is supported by a strong majority, the default is to hire no one - an option that may cost dearly. As a result, committee members might prefer to agree on a reasonable, if not necessarily the best, candidate, to avoid unfilled positions. In this paper we propose a model for the above scenario\u2014Consensus Under a Deadline (CUD)\u2014based on a time-bounded iterative voting process. We provide convergence guarantees and an analysis of the quality of the final decision. An extensive experimental study demonstrates more subtle features of CUDs, e.g., the difference between two simple types of committee member behavior, lazy vs.\u00a0proactive voters. Finally, a user study examines the differences between the\u00a0\u2026", "In this paper we extend the control methodology based on Extended Markov Tracking (EMT) by providing the control algorithm with capabilities to calibrate and even partially reconstruct the environment\u2019s model. This enables us to resolve the problem of performance deterioration due to model incoherence, a problem faced in all model-based control methods. The new algorithm, Ensemble Actions EMT (EA-EMT), utilises the initial environment model as a library of state transition functions and applies a variation of prediction with experts to assemble and calibrate a revised model. By so doing, this is the first hybrid control algorithm that enables on-line adaptation within the egocentric control framework which dictates the control of an agent\u2019s perceptions, rather than an agent\u2019s environment state. In our experiments, we performed a range of tests with increasing model incoherence induced by three types of\u00a0\u2026", "Extended Markov Tracking (EMT) is a computationally tractable method for the online estimation of Markovian system dynamics. In this paper, we present our initial experimentation with EMT-based control applied to robotic motion. In our experiments, a robot uses a predetermined mapping of the world onto an abstract model, over which EMT Control is applied; this dictates the choice of an abstract action, which in turn is mapped back into actual robot operation. Simulations in which a robot was constrained to follow a target show that although the abstract model was (intentionally) only weakly coherent with the real dynamics of the robot\u2019s world, EMT Control was able to provide reasonable performance. We also demonstrate that EMT-provided data provides sensible information for action model calibration. We do so by constructing a calibration scheme based on a training technique and simple data statistics. The scheme is then validated by carrying out additional robot motion control simulations, using the calibrated abstract model.", "Recently Bai and Lagunoff [1] have studied the question of Faustian Dynamics (FD) of policy and political power using a formal game theoretic framework. Specifically, they studied the conflict between implementing a (personally) optimal policy and maintaining political power. However, these works assumed that the policy makers come from the same population that empowers them. In contrast, in this paper we study a society that has a political class, hence policy makers are detached from the general population. Specifically, we study a society where members of the political class compete via pre-election propaganda campaigns\u2013a competition form characteristic to modern democracies. We assume that the society is characterised by an inherent cyclical Faustian dynamics, such as the Sarkar Cycle, and concentrate on the strategic behaviour of the political class members. We show that their propaganda over\u00a0\u2026", "We present an extension of the Dynamics Based Control (DBC) paradigm to environment models based on Predictive State Representations (PSRs). We show an approximate greedy version of the DBC for PSR model, EMT-PSR, and demonstrate how this algorithm can be applied to solve several control problems. We then provide some classifications and requirements of PSR environment models that are necessary for the EMT-PSR algorithm to operate.", "As reinforcement learning (RL) systems are deployed in various safety-critical applications, it is imperative to understand how vulnerable they are to adversarial attacks. Of these, an environmentpoisoning attack (EPA) is considered particularly insidious, since environment hyper-parameters are significant factors in determining an RL policy, yet prone to be accessed by third parties. The success of EPAs relies on comprehensive prior knowledge of the attacked RL system, including RL agent\u2019s learning mechanism and/or its environment model. Unfortunately, such an assumption of prior knowledge creates an unrealistic attack, one that poses limited threat to real-world RL systems. In this paper, we propose a Double-Black-Box EPA framework, only assuming the attacker\u2019s ability to alter environment hyperparameters. Considering that environment alteration comes at a cost, we seek minimal poisoning in an unknown environment and aim to force a black-box RL agent to learn an attacker-designed policy. To this end, we incorporate an inference module in our framework to capture the internal information of an unknown RL system and, accordingly, learn an adaptive strategy based on an approximation of our attack objective. We empirically show the threat posed by our attack to both tabular-RL and deep-RL algorithms, in both discrete and continuous environments.", "Centralized training with decentralized execution (CTDE) has become an important paradigm in multi-agent reinforcement learning (MARL). Current CTDE-based methods rely on restrictive decompositions of the centralized value function across agents, which decomposes the global Q-value into individual Q values to guide individuals' behaviours. However, such expected, i.e., risk-neutral, Q value decomposition is not sufficient even with CTDE due to the randomness of rewards and the uncertainty in environments, which causes the failure of these methods to train coordinating agents in complex environments. To address these issues, we propose RMIX, a novel cooperative MARL method with the Conditional Value at Risk (CVaR) measure over the learned distributions of individuals' Q values. Our main contributions are in three folds: (i) We first learn the return distributions of individuals to analytically calculate CVaR for decentralized execution; (ii) We then propose a dynamic risk level predictor for CVaR calculation to handle the temporal nature of the stochastic outcomes during executions; (iii) We finally propose risk-sensitive Bellman equation along with Individual-Global-MAX (IGM) for MARL training. Empirically, we show that our method significantly outperforms state-of-the-art methods on many challenging StarCraft II tasks, demonstrating significantly enhanced coordination and high sample efficiency.", "In this paper we present the Behaviosite Paradigm, a new approach to coordination and control of distributed agents in a multiagent system, inspired by biological parasites with behavior manipulation properties. Behaviosites are code modules that \u201cinfect\u201d a system, attaching themselves to agents and altering the sensory activity and actions of those agents. These behavioral changes can be used to achieve altered, potentially improved, performance of the overall system; thus, Behaviosites provide a mechanism for distributed control over a distributed system. Behaviosites need to be designed so that they are intimately familiar with the internal workings of the environment and of the agents operating within it. To demonstrate our approach, we use behaviosites to control the behavior of a swarm of simple agents. With a relatively low infection rate, a few behaviosites can engender desired behavior over the swarm as a whole: keeping it in one place, leading it through checkpoints, or moving the swarm from one stable equilibrium to another. We contrast behaviosites as a distributed swarm control mechanism with alternatives, such as the use of group leaders, herders, or social norms.", "In this paper we introduce a novel approach to continual planning and control, called Dynamics Based Control (DBC). The approach is similar in spirit to the Actor-Critic [6] approach to learning and estimation-based differential regulators of classical control theory [13]. However, DBC is not a learning algorithm, nor can it be subsumed within models of standard control theory. We provide a general framework for applying DBC to discrete Markovian environments, and discuss the key differences between it and a popular alternative for this type of environment\u2014Partially Observable Markov Decision Processes (POMDPs). We then show how a recently developed control scheme based on Extended Markov Tracking (EMT)[9, 10] can be seen as a suboptimal algorithm within the DBC framework, and discuss EMT\u2019s limitations relative to the general DBC approach.", "Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments.", "In recent years, inducing a desired behaviour in learning agents by exercising some degree of influence over their experiences has received extensive treatment. While there are several offshoots of this problem, it is commonly viewed within the framework of Curriculum Learning: devising a sequence of tasks that gradually induce the desired behaviour. Majority of these methods assume value alignment between the teacher and the learner, though some notable exceptions exist. One such exception is the method of Behaviour Cultivation (BC) that induces a desired behaviour by tweaking the environment dynamics. BC does not assume value alignment, and has been shown to be indispensable, ie, not reproducible by other teaching methods. Unfortunately, classical BC is an open loop method, ie, blind to the progress of the learner, and lacks the ability to teach group of agents. In this paper, we combine the Behaviour Cultivation core with the recent advances of Curriculum MDPs. This allows us to address several shortcomings of the classical BC, while preserving its strengths, such as the freedom from the teacher-learner value alignment. Our model exploits the knowledge of the learner population adaptation process to induce and proliferate a desired behaviour throughout the population. We term our model BC-MDP, and experimentally show its effectiveness, and retention of key positive features of both BC and Curriculum-MDP.", "Bayes-Nash Equilibrium (BNE) is at the root of many significant applications of modern game theory to multiagent systems, ranging from airport security scheduling, to network analysis, to mechanism design in e-commerce. However, the computational complexity of calculating BNEs makes the process prohibitively costly, and the process does not scale well. On the other hand, finding BNEs by simulating the repeated interaction of adaptive players has been demonstrated to succeed even in very complex domains. Unfortunately, adaptive algorithms that iteratively shift strategy towards an equilibrium (eg, the Fictitious Play algorithm) do not provide stable performance across all classes of games. Therefore, active research into these stability issues, and the design of new algorithms for interactive BNE calculation, remain highly important. In this paper we present a variation to the Ishikawa Iteration to calculate a Bayes-Nash Equilibrium. We demonstrate that the Ishikawa algorithm can take an interactive form, which we term Ishikawa Play (I-Play), and apply it in repeated games. Our experimental data shows that variations of the I-Play algorithm are effective in self-play (converge to a BNE), and outperform the Fictitious Play algorithm, while maintaining low computational costs per game cycle.", "We introduce a multi-model variant of the EMT-based control algorithm. The new algorithm, MM-EMT, is capable of balancing several control tasks expressed using separate dynamic models with a common action space. Such multiple models are common in both single-agent environments, when the agent has multiple tasks to achieve, and in team activities, when agent actions affect both the local agent\u2019s task as well as the overall team\u2019s coordination. To demonstrate the behaviour that MM-EMT engenders, several experimental setups were devised. Simulation results support the effectiveness of the approach, which in the multiagent scenario is expressed in the MM-EMT algorithm\u2019s ability to balance local and team-coordinated motion requirements.", "We investigate equilibrium strategies for bidding agents that participate in multiple, simultaneous second-price auctions with perfect substitutes. For this setting, previous research has shown that it is a best response for a bidder to participate in as many such auctions as there are available, provided that other bidders only participate in a single auction. In contrast, in this paper we consider equilibrium behaviour where all bidders participate in multiple auctions. For this new setting we consider mixed-strategy Nash equilibria where bidders can bid high in one auction and low in all others. By discretising the bid space, we are able to use smooth fictitious play to compute approximate solutions. Specifically, we find that the results do indeed converge to \u01eb-Nash mixed equilibria and, therefore, we are able to locate equilibrium strategies in such complex games where no known solutions previously existed.", "In this paper we introduce a novel approach to continual planning and control, called Dynamics Based Control (DBC). The approach is similar in spirit to the Actor-Critic [6] approach to learning and estimation-based differential regulators of classical control theory [12]. However, DBC is not a learning algorithm, nor can it be subsumed within models of standard control theory. We provide a general framework for applying DBC to discrete Markovian environments, and discuss the key differences between it and a popular alternative for this type of environment\u2014Partially Observable Markov Decision Processes (POMDPs). We then show how a recently developed control scheme based on Extended Markov Tracking (EMT)[9, 10] can be seen as a suboptimal algorithm within the DBC framework, and discuss EMT\u2019s limitations relative to the general DBC approach.", null, "This paper investigates policy resilience to training-environment poisoning attacks on reinforcement learning (RL) policies, with the goal of recovering the deployment performance of a poisoned RL policy. Due to the fact that the policy resilience is an add-on concern to RL algorithms, it should be resource-efficient, time-conserving, and widely applicable without compromising the performance of RL algorithms. This paper proposes such a policy-resilience mechanism based on an idea of knowledge sharing. We summarize the policy resilience as three stages: preparation, diagnosis, recovery. Specifically, we design the mechanism as a federated architecture coupled with a meta-learning manner, pursuing an efficient extraction and sharing of the environment knowledge. With the shared knowledge, a poisoned agent can quickly identify the deployment condition and accordingly recover its policy performance. We empirically evaluate the resilience mechanism for both model-based and model-free RL algorithms, showing its effectiveness and efficiency in restoring the deployment performance of a poisoned policy.", "Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolve them is automated curriculum learning (ACL), where a student (curriculum learner) train on tasks of increasing difficulty controlled by a teacher (curriculum generator). Unfortunately, in spite of its success, ACL's applicability is restricted due to: (1) lack of a general student framework to deal with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity in the teacher's task due to the ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Curriculum Oriented Skills and Tactics (COST), adapting curriculum learning to multi-agent coordination. To be specific, we endow the student with population-invariant communication and a hierarchical skill set. Thus, the student can learn cooperation and behavior skills from distinct tasks with a varying number of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies. As a result, a team of agents can change its size while retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem, and provide a corresponding regret bound. Empirical results show that our method improves scalability, sample efficiency, and generalization in MPE and Google Research Football. The source code and the video can be found at https://sites.google\u00a0\u2026", "Building large-scale generalist pre-trained models for many tasks is becoming an emerging and potential direction in reinforcement learning (RL). Research such as Gato and Multi-Game Decision Transformer have displayed outstanding performance and generalization capabilities on many games and domains. However, there exists a research blank about developing highly capable and generalist models in multi-agent RL (MARL), which can substantially accelerate progress towards general AI. To fill this gap, we propose Multi-Agent multi-Game ENtity TrAnsformer (MAGENTA) from the entity perspective as an orthogonal research to previous time-sequential modeling. Specifically, to deal with different state/observation spaces in different games, we analogize games as languages, thus training different \"tokenizers\" for various games. The feature inputs are split according to different entities and tokenized in the same continuous space. Then, two types of transformer-based model are proposed as permutation-invariant architectures to deal with various numbers of entities and capture the attention over different entities. MAGENTA is trained on Honor of Kings, Starcraft II micromanagement, and Neural MMO with a single set of transformer weights. Extensive experiments show that MAGENTA can play games across various categories with arbitrary numbers of agents and increase the efficiency of fine-tuning in new games and scenarios by 50\\%-100\\%. See our project page at \\url{https://sites.google.com/view/rl-magenta}.", "We investigate model-free multi-agent reinforcement learning (MARL) in environments where off-beat actions are prevalent, i.e., all actions have pre-set execution durations. During execution durations, the environment changes are influenced by, but not synchronised with, action execution. Such a setting is ubiquitous in many real-world problems. However, most MARL methods assume actions are executed immediately after inference, which is often unrealistic and can lead to catastrophic failure for multi-agent coordination with off-beat actions. In order to fill this gap, we develop an algorithmic framework for MARL with off-beat actions. We then propose a novel episodic memory, LeGEM, for model-free MARL algorithms. LeGEM builds agents' episodic memories by utilizing agents' individual experiences. It boosts multi-agent learning by addressing the challenging temporal credit assignment problem raised by the off-beat actions via our novel reward redistribution scheme, alleviating the issue of non-Markovian reward. We evaluate LeGEM on various multi-agent scenarios with off-beat actions, including Stag-Hunter Game, Quarry Game, Afforestation Game, and StarCraft II micromanagement tasks. Empirical results show that LeGEM significantly boosts multi-agent coordination and achieves leading performance and improved sample efficiency.", "In strategic candidacy games, both voters and candidates have preferences over the set of candidates, and candidates make strategic decisions about whether to run an electoral campaign or withdraw from the election, in order to manipulate the outcome according to their preferences. In this work, we extend the standard model of strategic candidacy games to scenarios where candidates may find it harmful for their reputation to withdraw from the election and would only do so if their withdrawal changes the election outcome for the better; otherwise, they would be keen to run the campaign. We study the existence and the quality of Nash equilibria in the resulting class of games, both analytically and empirically, and compare them with the Nash equilibria of the standard model. Our results demonstrate that while in the worst case there may be none or multiple, bad quality equilibria, on average, these games have a unique, optimal equilibrium state.", "Committee decisions are complicated by a deadline, e.g., the next start of a budget, or the beginning of a semester. In committee hiring decisions, it may be that if no candidate is supported by a strong majority, the default is to hire no one - an option that may cost dearly. As a result, committee members might prefer to agree on a reasonable, if not necessarily the best, candidate, to avoid unfilled positions. In this paper, we propose a model for the above scenario - Consensus Under a Deadline (CUD)- based on a time-bounded iterative voting process. We provide convergence guarantees and an analysis of the quality of the final decision. An extensive experimental study demonstrates more subtle features of CUDs, e.g., the difference between two simple types of committee member behavior, lazy vs.~proactive voters. Finally, a user study examines the differences between the behavior of rational voting bots and real voters, concluding that it may often be best to have bots play on the voters' behalf.", "A common assumption in opinion dynamics is that an individual\u2019s opinion is an atomic point in the opinion space, or a central point accompanied by a certain degree of uncertainty or indifference. While this is satisfactory in many domains, we propose an extension where an individual\u2019s preference is an interval in the space of opinions with two characteristic endpoints. These endpoints represent extreme expressions of the same opinion and can be influenced by different sources. For instance, an individual\u2019s political opinion, as seen through the lense of the expressed range of political stances, may be influenced by their family on one endpoint, and by their peers on the other endpoint. In this paper, we introduce a general model for capturing this type of interval opinion dynamics and examine several empirical features of this model in the presence of polarizing extremists.", "A voting system is a set of rules that determines how elections are conducted and how the results are determined. The design of voting rules has been deeply studied for centuries by great minds in the scientific field called social choice theory.   Computational social choice (COMSOC) is a new field that was formed in the early 2000s. It combines the scientific field of social choice theory with theoretical computer science. However, there were some predecessors before the field is formed. In 1962, David Gale and Lloyd Shapley presented a proof for finding stable matchings between two sets of elements that have preferences over each other. The proof utilizes computational complexity in the algorithm. In the late 1980s, John Bartholdi, Craig Tovey, and Michael Trick proposed ways to fight against strategic manipulations in elections using computational complexity. Additionally, computers are helpful to organize large scale political elections. The interdisciplinary view of computer science and social choice theory has created a whole new field called Computational Social Choice.  This project aims to explore advantages and disadvantages of different voting systems created by theorists in Computational Social Choice, by means of simulation and experimentation, i. e., allowing users to create an election using those voting systems. In this project, we explore and implement three different voting rules: Plurality Voting, Approval Voting, and Copeland Rule.  A web-based approach is chosen to simulate and implement the application. The reason is for choosing a web-based approach is due to its flexibility and modularity of the application structure. As\u00a0\u2026", "In this paper we investigate the strategic behaviour of a self-preserving assistant robot (SPAR) with constrained autonomy. Specifically, we consider an asymmetric repeated game for two players (the User and the SPAR), where at each stage of the game the User delegates her task to the SPAR. The task has several (uncertain) methods of execution, and the User can either explicitly state a particular method or leave the choice to the SPAR. In addition to this execution asymmetry (the User chooses the method, but the SPAR executes), the game also possesses information asymmetry. Specifically, the SPAR is assumed to know exactly the costs that each execution method will incur at the current stage of the game, while the User can only observe the cost of the chosen method postexecution. In this paper, we concentrate on formalising and generating a behavioural strategy for the SPAR, while assuming that the User can be modelled as an algorithm that solves a multi-armed bandit problem. In particular, we show that the SPAR\u2019s problem can be captured by a particular instance of expected average cost MDPs (EAC-MDP). In addition, we provide an approximate solution for the SPAR\u2019s EAC-MDP and prove some preliminary experimental results of its efficacy.", "In this paper we extend the control methodology based on Extended Markov Tracking (EMT) by providing the control algorithm with capabilities to calibrate and even partially reconstruct the environment's model. This enables us to resolve the problem of performance deterioration due to model incoherence, a negative problem in all model based control methods. The new algorithm, Ensemble Actions EMT (EA-EMT), utilises the initial environment model as a library of state transition functions and applies a variation of prediction with experts to assemble and calibrate a revised model. By so doing, this is the first control algorithm that enables on-line adaptation within the Dynamics Based Control (DBC) framework. In our experiments, we performed a range of tests with increasing model incoherence induced by three types of exogenous environment perturbations: catastrophic, periodic and deviating. The results show that EA-EMT resolved model incoherence and significantly outperformed the best currently available DBC solution by up to .", "Building autonomous robots that can operate in various scenarios has long been a goal of research in Artificial Intelligence. Recent progress has been made on space exploration systems, and systems for autonomous driving. In line with this work, we present HANS, an autonomous mobile robot that navigates the Givat Ram campus of the Hebrew University. We have constructed a wheel-based platform that holds various sensors. Our system\u2019s hardware includes a GPS, compass, and digital wheel encoders for localizing the robot within the campus area. Sonar is used for fast obstacle avoidance. It also employs a video camera for vision-based path detection. HANS\u2019software uses a wide variety of probabilistic methods and machine learning techniques, ranging from particle filters for robot localization to Gaussian Mixture Models and Expectation Maximization for its vision system. Global path planning is performed on a GPS coordinate database donated by MAPA Ltd., and local path planning is implemented using the A* algorithm on a local grid map constructed by the robot\u2019s sensors.", "Human society has long tried to control its environment, and this has also been one of the more prominent tasks of, and applications for, artificial intelligence (AI) systems. With the rise of the concept of an agent within AI, it became of even greater importance to endow these agents with the capability to control their environments. Given real-world limitations on agents, however, it became crucial to develop control mechanisms that go beyond standard control theory, incorporating bounded reasoning into these AI systems, recognising and utilising the agent\u2019s limitations regarding computational effort and complexity. This thesis introduces Dynamics Based Control (DBC)\u2014a novel framework for continual planning and control in stochastic environments. While it can be related to the principles of model-following and perceptual control (and in fact uses these principles as a part of its philosophical intuition), DBC directly targets system dynamics, rather than the system state. DBC views the sensory subsystem of an agent as a continual environment dynamics estimation and identification algorithm, and concentrates on the sensory subsystem as its control subject. As the sensory system limits the agent\u2019s ability to decipher the world, it makes little sense to attempt to invest more effort into controlling an agent\u2019s surroundings than can actually be detected. Thus the Dynamics Based Control (DBC) framework, following the perceptual control principle, has us design an agent\u2019s behaviour not to explicitly enforce preferred environment circumstances, but rather to create conditions within the environment that would be recognised by the sensory system as the\u00a0\u2026", "We explore the creation of cooperative behavioral trends in a group of agents, within the framework of an artificial physics simulation. Local interaction rules, that have \u201cselfish\u201d interpretations, were specifically designed to engender cooperative behavior. Although we conclude that it is necessary to further refine our specific set of interaction rules, our results support the view that it is indeed possible to simulate sophisticated cooperative behavior via a set of selfish local interactions.", "Trembling hand (TH) equilibria were introduced by Selten in 1975. Intuitively, these are Nash equilibria that remain stable when players assume that there is a small probability that other players will choose off-equilibrium strategies. This concept is useful for equilibrium refinement, ie, selecting the most plausible Nash equilibria when the set of all Nash equilibria can be very large, as is the case, for instance, for Plurality voting with strategic voters. In this paper, we analyze TH equilibria of Plurality voting. We provide an efficient algorithm for computing a TH best response and establish many useful properties of TH equilibria in Plurality voting games. On the negative side, we provide an example of a Plurality voting game with no TH equilibria, and show that it is NP-hard to check whether a given Plurality voting game admits a TH equilibrium where a specific candidate is among the election winners.", "As Reinforcement Learning (RL) solutions are becoming ubiquitous, so is the study of potential threats to their training and deployment. While single-learner training-time attacks, capable of\" preprogramming\" behavioral triggers into a strategy, receive increasing attention, attacks on collections of learning agents have been largely overlooked. We remedy the situation by developing a constructive training-time attack on a population of learning agents and make the attack agnostic to the size of the population. The attack constitutes a sequence of environment (re) parameterizations (poisonings), generated to overcome individual differences between agents and lead the entire population to the same target behavior while minimizing effective environment modulation. Our method is demonstrated on populations of independent learners in\" ghost\" environments (learners do not interact or perceive each other) as well as environments with mutual awareness, with or without individual learning. From the attack perspective, we pursue an ultra-blackbox setting, ie, crosspolicy traces of the victim learners are the only input both for attack conditioning and attack evaluation during the attacker\u2019s training. To manage the resulting uncertainty in population behavior, we deploy a novel Wasserstein distance-based Gaussian embedding of detected behaviors within the population of victim learners. To align with prior works on environment poisoning, our experiments are based on a 3D Grid World domain and show: a) feasibility, ie, despite the uncertainty, the attack forces a population-wide adoption of target behavior; b) efficacy, ie, the attack is size-agnostic and\u00a0\u2026", "A voting center is in charge of collecting and aggregating voter preferences. In an iterative process, the center sends comparison queries to voters, requesting them to submit their preference between two items. Voters might discuss the candidates among themselves, figuring out during the elicitation process which candidates stand a chance of winning and which do not. Consequently, strategic voters might attempt to manipulate by deviating from their true preferences and instead submit a different response in order to attempt to maximize their profit. We provide a practical algorithm for strategic voters which computes the best manipulative vote and maximizes the voter\u2019s selfish outcome when such a vote exists. We also provide a careful voting center which is aware of the possible manipulations and avoids manipulative queries when possible. In an empirical study on four real world domains, we show that in practice manipulation occurs in a low percentage of settings and has a low impact on the final outcome. The careful voting center reduces manipulation even further, thus allowing for a non-distorted group decision process to take place. We thus provide a core technology study of a voting process that can be adopted in opinion or information aggregation systems and in crowdsourcing applications, eg, peer grading in Massive Open Online Courses (MOOCs).", "Representation of a multi-issue opinion (or valuation) as a point in a metric space is commonplace in many areas, including economics, social choice and social networking. This representation has naturally been extended to include indifference sets (eg when certain variations in a product features are irrelevant to its use) and uncertainty ranges (ie a point opinion is a sample from a distribution over the metric space). In this paper, we suggest that a non-atomic sub-set of points in a metric space can represent a richer form of an opinion. More specifically, we suggest that an opinion captures how changes in the valuation of one issue relate to the change in valuations of other issues, and what are the acceptable bounds of those changes. As an instance of this approach, we consider non-atomic opinions represented by intervals in a simplex, and experimentally demonstrate the impact of the opinions\u2019 non-atomic nature on social network opinion dynamics, as well as implications to the distortion measure of voting rules.", "Transferable environment-poisoning attack (TEPA) has been proposed recently to force a Reinforcement Learning (RL) agent to learn a target policy designed by an attacker. Comparing with the majority of training-time attack approaches, TEPA is more realistic because of its effectiveness for black-box RL agents. However, the knowledge of the victim\u2019s environment dynamics model is at the heart of TEPA\u2019s foundation, which constraints its scalability to sophisticated real-world scenarios. In this paper, we study the training-time attack on RL in truly black-box settings where both RL agent and environment dynamics are unknown. We leverage latent representation of the RL agent\u2019s stochastic process (ie, the combination of policy and dynamics) to train an adaptive attack strategy. In the preliminary experiment, we show that our attack strategy performs successfully in an unknown environment, and is generally effective for different black-box RL agents.", "We study a recently proposed model for the game-theoretic analysis of voting mechanisms. It is well known that standard approaches can lead to a multitude of Nash Equilibria (NE), many of which are counter-intuitive. Instead, we focus on truth-biased voters, a promising model proposed to avoid such issues. The model introduces an incentive for voters to be truthful when their vote is not pivotal, ie, when they cannot change the outcome by a unilateral deviation. This is a powerful refinement and recent simulations reveal that the surviving equilibria tend to have desirable properties. However, truth bias has thus far been studied mainly within the context of plurality elections. In this work, we undertake an equilibrium analysis of the veto rule under the truth bias assumption. We identify several crucial properties of pure NE for this voting rule under truth-bias, which show a clear distinction from the non-biased game-theoretic model and from the previously studied setting of truth-biased plurality. We proceed by establishing that deciding on the existence of Nash equilibria in veto under truth-bias is an NP-hard problem. Finally, we characterize a tight (in a certain sense) subclass of instances for which the existence of a NE can be decided in polynomial time.", "Here we present limited case of exagons (hexa-hexa-exagons), exigation (pinch-exing) and their prolog implementation with Tcl/Tk interface.", "In SODs, planning\u2019s subject is to sway the world from one state to another in some optimal way. Thus, a plan is a sequence of actions that brings the world from the current state to a certain target state. The usual level of optimality of such plans is the number of steps (actions) prescribed by the plan to reach the desired state.", "Maintain effort toward saving the building or draw back and minimise the spread of fire? Concentrate on a multitude of smaller fires or allow controlled unification and deal with only one location? Will transportation routs be endangered? Are there still civilians evacuating from the area/building?", "In this paper we introduce a novel approach to continual planning and control, called Dynamics Based Control (DBC). The approach is similar in spirit to the Actor-Critic [6] approach to learning and estimation-based differential regulators of classical control theory [12]. However, DBC is not a learning algorithm, nor can it be subsumed within models of standard control theory. We provide a general framework for applying DBC to discrete Markovian environments, and discuss the key differences between it and a popular alternative for this type of environment\u2014Partially Observable Markov Decision Processes (POMDPs). We then show how a recently developed control scheme based on Extended Markov Tracking (EMT)[9, 10] can be seen as a suboptimal algorithm within the DBC framework, and discuss EMT\u2019s limitations relative to the general DBC approach.", "We present a novel planning architecture that is composed of strategic and tactical layers. The complementary roles of these two layers are discussed, along with their relationship to one another, and a mathematical model is presented. The planning architecture is motivated by the requirements of dynamic systems, and it provides a clearly-defined optimality measure for an agent operating in a stochastic environment. Although the architecture can be applied in any domain, it is thus particularly well-suited to highly dynamic environments. Preliminary experimental results are presented."]}, "collaboration_network": {"target": ["Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Svetlana Obraztsova", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Maria Polukarov", "Nick Jennings", "Nick Jennings", "Nick Jennings", "Nick Jennings", "Nick Jennings", "Nick Jennings", "Nick Jennings", "Nick Jennings", "Nick Jennings", "Claudia V. Goldman", "Claudia V. Goldman", "Claudia V. Goldman", "Claudia V. Goldman", "Haifeng Xu", "Haifeng Xu", "Haifeng Xu", "Haifeng Xu", "Edith Elkind", "Edith Elkind", "Edith Elkind", "Edith Elkind", "Edith Elkind", "Edith Elkind", "Omer Lev", "Omer Lev", "Omer Lev", "Omer Lev", "Omer Lev", "Omer Lev", "Lihi (Naamani) Dery", "Lihi (Naamani) Dery", "Lihi (Naamani) Dery", "Lihi (Naamani) Dery", "Evangelos Markakis", "Evangelos Markakis", "Evangelos Markakis", "Sarit Kraus", "Sarit Kraus", "Sarit Kraus", "Enrico Gerding", "Enrico Gerding", "Enrico Gerding", "Enrico Gerding", "Milind Tambe", "Milind Tambe", "Shaddin Dughmi", "Shaddin Dughmi", "Albert X. Jiang", "Albert X. Jiang", "Jiarui Gan", "Jiarui Gan", "Kobi Gal", "Victor Naroditskiy", "Khoi D. Hoang", "William Yeoh", "Meir Kalech", "Long Tran-Thanh", "Long Tran-Thanh", "Arunesh Sinha", "Alexandros A. Voudouris", "Lachlan Dufton", "Kate Larson", "Yoad Lewenberg", "Yevgeniy Vorobeychik"], "target_id": ["aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "aorQUi0AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "nL-zXO4AAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "FX5CsuYAAAAJ", "KiyKf6IAAAAJ", "KiyKf6IAAAAJ", "KiyKf6IAAAAJ", "KiyKf6IAAAAJ", "nLgg388AAAAJ", "nLgg388AAAAJ", "nLgg388AAAAJ", "nLgg388AAAAJ", "MsGRzsoAAAAJ", "MsGRzsoAAAAJ", "MsGRzsoAAAAJ", "MsGRzsoAAAAJ", "MsGRzsoAAAAJ", "MsGRzsoAAAAJ", "LTMdHpwAAAAJ", "LTMdHpwAAAAJ", "LTMdHpwAAAAJ", "LTMdHpwAAAAJ", "LTMdHpwAAAAJ", "LTMdHpwAAAAJ", "UM0rCaoAAAAJ", "UM0rCaoAAAAJ", "UM0rCaoAAAAJ", "UM0rCaoAAAAJ", "VDyjbagAAAAJ", "VDyjbagAAAAJ", "VDyjbagAAAAJ", "PfgeYGwAAAAJ", "PfgeYGwAAAAJ", "PfgeYGwAAAAJ", "TIuriUgAAAAJ", "TIuriUgAAAAJ", "TIuriUgAAAAJ", "TIuriUgAAAAJ", "YOVZiJkAAAAJ", "YOVZiJkAAAAJ", "jn-B_MoAAAAJ", "jn-B_MoAAAAJ", "kzglGGEAAAAJ", "kzglGGEAAAAJ", "25GYJiwAAAAJ", "25GYJiwAAAAJ", "lNHuLiQAAAAJ", "tuEpptIAAAAJ", "HxpHciQAAAAJ", "HdLFmroAAAAJ", "vo5iW3wAAAAJ", "YBQai3gAAAAJ", "YBQai3gAAAAJ", "Iuq3Qv8AAAAJ", "YL5djpEAAAAJ", "qXXO29QAAAAJ", "QK-c5esAAAAJ", "wPz9NscAAAAJ", "ptI-HHkAAAAJ"], "type": ["Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU"], "location": ["Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "King's College London", "King's College London", "King's College London", "King's College London", "King's College London", "King's College London", "King's College London", "King's College London", "King's College London", "King's College London", "Loughborough University", "Loughborough University", "Loughborough University", "Loughborough University", "Loughborough University", "Loughborough University", "Loughborough University", "Loughborough University", "Loughborough University", "General Motors", "General Motors", "General Motors", "General Motors", "University of Chicago", "University of Chicago", "University of Chicago", "University of Chicago", "University of Oxford", "University of Oxford", "University of Oxford", "University of Oxford", "University of Oxford", "University of Oxford", "BGU", "BGU", "BGU", "BGU", "BGU", "BGU", "Ariel University", "Ariel University", "Ariel University", "Ariel University", "Athens University of Economics and Business", "Athens University of Economics and Business", "Athens University of Economics and Business", "Bar-Ilan University", "Bar-Ilan University", "Bar-Ilan University", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "Harvard", "Harvard", "USC", "USC", "Trinity University", "Trinity University", "University of Oxford", "University of Oxford", "BGU", "Unknown", "Washington University", "Washington University", "BGU", "University of Warwick", "University of Warwick", "Rutgers University", "University of Essex", "University of Waterloo", "University of Waterloo", "Hebrew University of Jerusalem", "Washington University"], "year": [2017, 2017, 2019, 2018, 2016, 2019, 2020, "Unknown", 2015, 2021, 2014, 2015, 2020, 2022, "Unknown", 2021, 2015, "Unknown", 2015, 2015, 2021, 2019, 2017, 2016, 2017, 2015, 2021, 2015, 2016, 2015, 2015, "Unknown", 2015, 2016, 2017, 2015, 2009, 2016, 2019, 2015, 2010, "Unknown", 2013, 2015, 2008, 2010, 2009, 2016, 2014, 2012, 2002, 2003, 2015, 2019, 2015, 2015, 2015, "Unknown", 2008, 2021, 2017, 2016, 2015, 2017, 2015, 2016, 2015, "Unknown", 2017, 2016, 2019, 2021, 2015, 2015, 2015, 2014, 2012, 2011, 2009, 2013, 2008, 2008, 2015, 2015, 2015, 2015, 2015, 2015, 2021, 2019, 2012, 2013, 2020, 2020, 2019, "Unknown", 2019, 2015, 2021, 2010, 2010, 2013, 2020], "title": ["Haste makes waste: a case to favour voting bots", "Distant Truth: Bias Under Vote Distortion Costs.", "These Polar Twins: Opinion Dynamics of Intervals", "Voting application: where we will go for dinner?", "Between fairness and a mistrial: Consensus under a deadline", "Stategic Candidacy with Keen Candidates", "RMIX: Risk-sensitive multi-agent reinforcement learning", "Neither here nor there", "On the convergence of iterative voting: how restrictive should restricted dynamics be?", "RMIX: Learning risk-sensitive policies for cooperative reinforcement learning agents", "Faustian Dynamics in Sarkar's Social Cycle", "Convergence to equilibria in strategic candidacy", "Manipulating elections by selecting issues", "Off-Beat Multi-Agent Reinforcement Learning", "King\u2019s Research Portal", "Protecting elections by recounting ballots", "Lie on the fly: Manipulative voters under iterative voting centre", "Truth-Bias Complexity in the Veto Voting Rule", "Strategic candidacy games with lazy candidates", "Analysis of equilibria in iterative voting schemes", "Mis-spoke or mis-lead: Achieving robustness in multi-agent communicative reinforcement learning", "Lie on the fly: Strategic voting in an iterative preference elicitation process", "Real candidacy games: a new model for strategic candidacy", "Nonmyopic voting dynamics: An optimistic approach", "Doodle poll games", "Farsighted voting dynamics", "Reaching consensus under a deadline", "Beyond plurality: Truth-bias in binary scoring rules", "Trembling Hand Equilibria of Plurality Voting", "Strategic candidacy games with lazy candidates", "Convergence to equilibria in strategic candidacy", "King\u2019s Research Portal", "On the convergence of iterative voting: how restrictive should restricted dynamics be?", "Nonmyopic voting dynamics: An optimistic approach", "Doodle poll games", "Farsighted voting dynamics", "Generalised fictitious play for a continuum of anonymous players", "Trembling Hand Equilibria of Plurality Voting", "Stategic Candidacy with Keen Candidates", "Convergence to equilibria in strategic candidacy", "Cultivating desired behaviour: Policy teaching via environment-dynamics tweaks", "King\u2019s Research Portal", "Computing pure Bayesian-Nash equilibria in games with finite actions and continuous types", "On the convergence of iterative voting: how restrictive should restricted dynamics be?", "Approximating mixed Nash equilibria using smooth fictitious play in simultaneous auctions", "A hybrid controller based on the egocentric perceptual principle", "Generalised fictitious play for a continuum of anonymous players", "Trembling Hand Equilibria of Plurality Voting", "Strategic information disclosure to people with multiple alternatives", "Strategic advice provision in repeated human-agent interactions", "Non-approximability of Centralized Control", "The complexity of multiagent systems: The price of silence", "Security games with information leakage: Modeling and computation", "Imitative follower deception in stackelberg games", "Information disclosure as a means to security", "Exploring information asymmetry in two-stage security games", "Strategic candidacy games with lazy candidates", "King\u2019s Research Portal", "Approximating mixed Nash equilibria using smooth fictitious play in simultaneous auctions", "Protecting elections by recounting ballots", "Doodle poll games", "Trembling Hand Equilibria of Plurality Voting", "Analysis of equilibria in iterative voting schemes", "Distant Truth: Bias Under Vote Distortion Costs.", "Farsighted voting dynamics", "Nonmyopic voting dynamics: An optimistic approach", "Beyond plurality: Truth-bias in binary scoring rules", "Truth-Bias Complexity in the Veto Voting Rule", "Haste makes waste: a case to favour voting bots", "Between fairness and a mistrial: Consensus under a deadline", "Lie on the fly: Strategic voting in an iterative preference elicitation process", "Reaching consensus under a deadline", "Analysis of equilibria in iterative voting schemes", "Beyond plurality: Truth-bias in binary scoring rules", "On the convergence of iterative voting: how restrictive should restricted dynamics be?", "Strategic information disclosure to people with multiple alternatives", "Strategic advice provision in repeated human-agent interactions", "Giving advice to people in path selection problems", "Generalised fictitious play for a continuum of anonymous players", "Computing pure Bayesian-Nash equilibria in games with finite actions and continuous types", "Approximating mixed Nash equilibria using smooth fictitious play in simultaneous auctions (short paper)", "Approximating mixed Nash equilibria using smooth fictitious play in simultaneous auctions", "Security games with information leakage: Modeling and computation", "Exploring information asymmetry in two-stage security games", "Security games with information leakage: Modeling and computation", "Exploring information asymmetry in two-stage security games", "Security games with information leakage: Modeling and computation", "Information disclosure as a means to security", "Protecting elections by recounting ballots", "Imitative follower deception in stackelberg games", "Strategic advice provision in repeated human-agent interactions", "Computing pure Bayesian-Nash equilibria in games with finite actions and continuous types", "New algorithms for continuous distributed constraint optimization problems", "New algorithms for continuous distributed constraint optimization problems", "Lie on the fly: Strategic voting in an iterative preference elicitation process", "King\u2019s Research Portal", "Imitative follower deception in stackelberg games", "Security games with information leakage: Modeling and computation", "Protecting elections by recounting ballots", "Cultivating desired behaviour: Policy teaching via environment-dynamics tweaks", "Cultivating desired behaviour: Policy teaching via environment-dynamics tweaks", "Ishikawa play.", "Manipulating elections by selecting issues"], "link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:fPk4N6BV_jEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:M05iB0D1s5AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:70eg2SAEIzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:vV6vV6tmYwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:vV6vV6tmYwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:ufrVoPGSRksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:NaGl4SEjCO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:NaGl4SEjCO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&citation_for_view=JwJRnmAAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=JwJRnmAAAAAJ&cstart=20&pagesize=80&citation_for_view=JwJRnmAAAAAJ:blknAaTinKkC"]}, "published_by_year": {"Year": ["2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Publications": [2, 1, 1, 4, 4, 2, 5, 2, 2, 1, 2, 2, 2, 10, 3, 4, 1, 5, 5, 6, 4, 2, 11]}, "citations_by_year": {"Year": ["2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [0, 2, 2, 11, 8, 19, 25, 20, 20, 18, 12, 28, 34, 57, 61, 75, 69, 90, 86, 141, 182, 128, 26]}, "all_time_h_index": 18, "all_time_i10_index": 28, "all_time_i20_index": 16, "h_index_by_year": {"Year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [0, 1, 1, 3, 3, 4, 4, 5, 5, 6, 6, 6, 8, 8, 10, 12, 13, 13, 13, 15, 15, 18]}, "h_index_by_publication_year": {"Publication Year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 9, 3, 4, 0, 2, 3, 5, 1, 1]}, "avg_citations_by_publication_year": {"Publication Year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [4.0, 92.0, 14.0, 5.0, 4.5, 2.0, 2.4, 15.0, 7.0, 40.0, 27.0, 25.5, 27.0, 37.5, 9.333333333333334, 10.5, 0.0, 6.6, 18.6, 17.166666666666668, 0.5, 0.5]}, "h_index_by_years_from_publication_year": {"Publication Year": [2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 4, 6, 7, 8, 8, 9, 9, 9, 0, 1, 2, 3, 3, 3, 3, 3, 1, 2, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 3, 3, 3, 2, 5, 5, 0, 1, 1]}}