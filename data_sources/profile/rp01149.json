{"full_name": "Zhang Hanwang", "email": "hanwangzhang@ntu.edu.sg", "google_scholar": "https://scholar.google.com/citations?user=YG0DFyYAAAAJ&hl=en&oi=ao", "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp01149", "designation": "Associate Professor, School of Computer Science and Engineering", "image_path": "./profile_img/zhang_hanwang.jpg", "biography": "Hanwang Zhang is an Assistant Professor at Nanyang Technological University, Singapore. He was a research scientist at the Department of Computer Science, Columbia University, USA and a senior research fellow at the School of Computing, National University of Singapore, Singapore. He has received the B.Eng (Hons.) degree in computer science from Zhejiang University, Hangzhou, China, in 2009, and the Ph.D. degree in computer science from the National University of Singapore in 2014. His research interest includes computer vision, multimedia, and social media. Dr. Zhang is the recipient of the Best Demo runner-up award in ACM MM 2012, the Best Student Paper award in ACM MM 2013, and the Best Paper Honorable Mention in ACM SIGIR 2016. He is also the winner of Best Ph.D. Thesis Award of School of Computing, National University of Singapore, 2014.", "orcid": "https://orcid.org/0000-0001-7374-8739", "other_websites": ["https://personal.ntu.edu.sg/hanwangzhang/"], "bachelor_degree": null, "masters": null, "phd": null, "name": "Zhang Hanwang", "id": "rp01149", "publications": {"Publication Year": ["2017", "2017", "2016", "2017", "2017", "2019", "2017", "2017", "2020", "2018", "2019", "2020", "2017", "2018", "2020", "2021", "2016", "2020", "2018", "2020", "2020", "2015", "2020", "2019", "2019", "2020", "2018", "2013", "2019", "2017", "2016", "2021", "2021", "2018", "2020", "2019", "2021", "2017", "2014", "2020", "2019", "2018", "2016", "2021", "2017", "2014", "2021", "2019", "2019", "2019", "2016", "2018", "2018", "2020", "2018", "2016", "2015", "2022", "2022", "2021", "2019", "2014", "2021", "2012", "2019", "2020", "2018", "2012", "2016", "2021", "2021", "2020", "2017", "2021", "2021", "2017", "2014", "2015", "2015", "2014", "2021", "2020", "2014", "2021", "2019", "2014", "2021", "2021", "2019", "2018", "2021", "2017", "2016", "2020", "2022", "2019", "2018", "2012", "2021", "2017", "2022", "2022", "2021", "2019", "2020", "2020", "2019", "2022", "2017", "2016", "2015", "2016", "2016", "2021", "2021", "2020", "2016", "2022", "2022", "2017", "2022", "2022", "2021", "2017", "2012", "2023", "2022", "2022", "2017", "2023", "2022", "2021", "2016", "2015", "2023", "2023", "2023", "2019", "2018", "2017", "2023", "2023", "2023", "2022", "2018", "2016", "2015", "2009", "2023", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "2016", "2016", "2023", "2023", "2023", "2023", "2023", "2020", "2020", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "Title": ["Neural collaborative filtering", "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning", "Fast matrix factorization for online recommendation with implicit feedback", "Attentional factorization machines: Learning the weight of feature interactions via attention networks", "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention", "Auto-encoding scene graphs for image captioning", "Video Captioning With Attention-Based LSTM and Semantic Consistency", "Visual Translation Embedding Network for Visual Relation Detection", "Unbiased scene graph generation from biased training", "Attributed social network embedding", "Learning to compose dynamic tree structures for visual contexts", "Long-tailed classification by keeping the good and removing the bad momentum causal effect", "Video question answering via gradually refined attention over appearance and motion", "Zero-shot visual recognition using semantics-preserving adversarial embedding networks", "Causal intervention for weakly-supervised semantic segmentation", "Counterfactual vqa: A cause-effect look at language bias", "Discrete collaborative filtering", "Counterfactual Samples Synthesizing for Robust Visual Question Answering", "Self-supervised video hashing with hierarchical binary auto-encoder", "Visual commonsense r-cnn", "Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration", "Learning Image and User Features for Recommendation in Social Networks", "Feature pyramid transformer", "Explainable and explicit visual reasoning over scene graphs", "Learning to assemble neural module tree networks for visual grounding", "Interventional few-shot learning", "Grounding Referring Expressions in Images by Variational Context", "Attribute-augmented semantic hierarchy: towards bridging semantic gap and intention gap in image retrieval", "Counterfactual Critic Multi-Agent Training for Scene Graph Generation", "PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN", "Micro tells macro: predicting the popularity of micro-videos via a transductive model", "Distilling Causal Effect of Data in Class-Incremental Learning", "Counterfactual zero-shot and open-set visual recognition", "Low-shot learning via covariance-preserving adversarial augmentation networks", "Two causal principles for improving visual dialog", "Context-aware visual policy network for fine-grained image captioning", "Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue", "Video visual relation detection", "Multimedia summarization for social events in microblog stream", "More Grounded Image Captioning by Distilling Image-Text Matching Model", "Recursive visual attention in visual dialog", "Context-Aware Visual Policy Network for Sequence-Level Image Captioning", "Learning from collective intelligence: Feature learning using social images and tags", "Deconfounded image captioning: A causal retrospect", "Enhancing micro-video understanding by harnessing external sounds", "Start from scratch: Towards automatically identifying, modeling, and naming visual attributes", "Causal attention for vision-language tasks", "Learning to compose and reason with language tree structures for visual grounding", "Learning to collocate neural modules for image captioning", "Multi-level policy and reward-based deep reinforcement learning framework for image captioning", "Play and rewind: Optimizing binary representations of videos by self-supervised temporal hashing", "Shuffle-then-assemble: learning object-agnostic visual relationship features", "More is better: Precise and detailed image captioning using online positive recall and missing concepts mining", "Learning to Segment the Tail", "Learning to guide decoding for image captioning", "Deep Fusion of Multiple Semantic Cues for Complex Event Recognition", "Deep aging face verification with large gaps", "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base", "Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation", "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph", "Making history matter: History-advantage sequence training for visual dialog", "Robust (semi) nonnegative graph embedding", "Causal Attention for Unbiased Visual Recognition", "Detecting group activities with multi-camera context", "Fast discrete collaborative multi-modal hashing for large-scale multimedia retrieval", "Self-adaptive neural module transformer for visual question answering", "Discrete Factorization Machines for Fast Feature-based Recommendation", "Attribute feedback", "Online collaborative learning for open-vocabulary visual classifiers", "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding", "Transporting Causal Mechanisms for Unsupervised Domain Adaptation", "Iterative context-aware graph inference for visual dialog", "Improving event extraction via multimodal integration", "Self-supervised learning disentangled group representation as feature", "Introspective distillation for robust question answering", "Event classification in microblogs via social tracking", "One of a kind: User profiling by social curation", "Visual coding in a semantic hierarchy", "Enhancing video event recognition using automatically constructed semantic-visual knowledge base", "Perception-guided multimodal feature fusion for photo aesthetics assessment", "Cross-GCN: Enhancing Graph Convolutional Network with -Order Feature Interactions", "Auto-encoding and distilling scene graphs for image captioning", "Attribute-augmented semantic hierarchy: Towards a unified framework for content-based image retrieval", "The Blessings of Unlabeled Background in Untrimmed Videos", "Learning Using Privileged Information for Food Recognition", "Image tagging with social assistance", "Self-Regulation for Semantic Segmentation", "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?", "Deepchannel: Salience estimation by contrastive learning for extractive document summarization", "Multi-Level Policy and Reward Reinforcement Learning for Image Captioning.", "Empowering Language Understanding with Counterfactual Reasoning", "VideoWhisper: Toward Discriminative Unsupervised Video Feature Learning With Attention-Based Recurrent Neural Networks", "Discrete image hashing using large weakly annotated photo collections", "Occlusion-Aware GAN for Face De-Occlusion in the Wild", "Cross-domain empirical risk minimization for unbiased long-tailed classification", "Question-Aware Tube-Switch Network for Video Question Answering", "Recurrent attention network using spatial-temporal relations for action recognition", "Robust non-negative graph embedding: Towards noisy data, unreliable graphs, and noisy labels", "Auto-Parsing Network for Image Captioning and Visual Question Answering", "Multi-view feature selection and classification for Alzheimer\u2019s disease diagnosis", "Invariant Feature Learning for Generalized Long-Tailed Classification", "Deconfounded visual grounding", "Conda: Unsupervised domain adaptation for lidar segmentation via regularized domain concatenation", "Single-shot Semantic Image Inpainting with Densely Connected Generative Networks", "Stochastic dynamics for video infilling", "Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning", "Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions", "Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning", "I Know What You Want to Express: Sentence Element Inference by Incorporating External Knowledge Base", "Learning content\u2013social influential features for influence analysis", "Learning features from large-scale, noisy and social image-tag collection", "L2, p-norm and sample constraint based feature selection and classification for AD diagnosis", "Robust regression based face recognition with fast outlier removal", "Causal interventional training for image recognition", "Align R-CNN: A pairwise head network for visual relationship detection", "General Partial Label Learning via Dual Bipartite Graph Autoencoder", "Deep learning generic features for cross-media retrieval", "Equivariance and Invariance Inductive Bias for Learning from Insufficient Data", "PR\u2010NET: progressively\u2010refined neural network for image manipulation localization", "Object trajectory proposal", "Identifying hard noise in long-tailed sample distribution", "Discriminative style learning for cross-domain image captioning", "On Non-Random Missing Labels in Semi-Supervised Learning", "Deep semantic indexing using convolutional localization network with region-based visual attention for image database", "Visual query attributes suggestion", "Compositional prompt tuning with motion cues for open-vocabulary video relation detection", "Respecting transfer gap in knowledge distillation", "Class is invariant to context and vice versa: on learning invariance for out-of-distribution generalization", "andTatSeng Chua,\u201c", "Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection", "Nico challenge: Out-of-distribution generalization for image recognition challenges", "Counterfactual vqa: A causeeffect look at language bias. 2021 IEEE", "Mental visual browsing", "Multi-view Semi-supervised Learning for Web Image Annotation", "Fast Diffusion Model", "Debiased Fine-Tuning for Vision-language Models by Prompt Regularization", "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning", "Learning to embed sentences using attentive recursive trees", "Venue Prediction for Social Images by Exploiting Rich Temporal Patterns in LBSNs", "Matryoshka Peek: Toward Learning Fine-Grained, Robust, Discriminative Features for Product Search", "Counterfactual Active Learning for Out-of-Distribution Generalization", "DisCo: Disentangled Control for Referring Human Dance Generation in Real World", "Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery", "RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval", "Auto-encoding scene graphs for image captioning. 2019 IEEE", "Saliency meets spatial quantization: A practical framework for large scale product search", "Hashing with inductive supervised learning", "Web image interpretation: semi-supervised mining annotated words", "Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models", "Vl-nms: Breaking proposal bottlenecks in two-stage visual-language matching", "Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning", "Equivariant Similarity for Vision-Language Foundation Models", "Semantic Scene Completion with Cleaner Self", "Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition", "Evaluating and Mitigating Static Bias of Action Representations in the Background and the Foreground", "Attention-based Class Activation Diffusion for Weakly-Supervised Semantic Segmentation", "An Intention-Aware Interactive System for Mobile Video Browsing", "Mental Visual Indexing: Towards Fast Video Browsing", "An Overview of Challenges in Egocentric Text-Video Retrieval", "Compositional Prompting Video-language Models to Understand Procedure in Instructional Videos", "Learning Trajectory-Word Alignments for Video-Language Tasks", "Adaptively Clustering Neighbor Elements for Image Captioning", "Random Boxes Are Open-world Object Detectors", "Two causal principles for improving visual dialog. In 2020 IEEE", "Counterfactual samples synthesizing for robust visual question answering. In 2020 IEEE", "Supplementary Material: Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground", "Supplementary Material for Interventional Few-Shot Learning", "Appendix for \u201cRespecting Transfer Gap in Knowledge Distillation\u201d", "Supplementary Material for \u201cInvariant Feature Learning for Generalized Long-Tailed Classification\u201d", "Supplementary Material for \u201cLong-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect\u201d"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5uu-MYFt2EcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:o4Qvs5Y5TLQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:HkunuPqSaCsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:6e4D8M0GhXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:fHS53ZCY-AEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:K1AqHrCRkPsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:o0tJHVzi6PsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5McdzzY_mmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:V8JMcbNWlSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:vPKCt-r_nWsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:BzPC9jo9PWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:knQODA9bSJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:IwGqIHSOC8kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:jvrRlaHg2sAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:LGh1lRt-7sUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:zDhyt2jClVkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:W8gSVh4kTHQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:QC-2xSqExF4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:YIdQ7BAI8VoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:InLRhUNj-rwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:dYRx7efp7U0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:QAOzB4mb83kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:zmHQPunddckC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:SZzxPo9m4nkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:EVUe1p4rgj0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:n2kG1_zAmykC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:YXeC4bxG7-IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:N7YTE_TVRugC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:DoBzxrgsGIIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:9UF2BbDYXHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UFuRdyijzaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBxuCEhX224C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:WTkkuPxyGkUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8EvVLpklxGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBJCJstOu-sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:RwQ8IgSj6xkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3YQzwbzlpKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tswL-GKFg8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:onKP9CxGSkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vYYylRVofzEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vkDViGfkvYEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:2hrGa7H818QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:NRnkAyzcrGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:_cFkbNEifk0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:96M4k3P_OWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:MIUEjqm7qCUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UPMPWMAU16oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:7uOkfv6XYJ4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:D_YRMw8gybsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:N5XbD978G_MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3ssty3PwuTgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ExBYd_ZNEOYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:HKuYlFsi-qEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:-XtG2q47PdUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:5kvG9DmRKWYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:5bfplxN71z4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bNB6FdaCRgEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:Usae8uB1euoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:B71Od6CY_ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yqZeyBDMhiUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:EO1llL0aI9sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:VFGfXyYpp08C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:U_h31ocWZrQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ZM__uENUXnMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:AOwgc6nnr1EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:esGtpfCv0y8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ae0xyBWlIcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bYbwfsIO_fQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ECvNt0vk_34C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8moDcb_GFzgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R8TPKZP7usQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:aJ-3-MYELVsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:BSZS1fLCB98C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:KqW5X_olkfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UarirCmVI0EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ovGv7akYl-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tJ_1M1FLfZ0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:LZ55yeyZZb4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:JQPmwQThujIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:rVBkUn48wTwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:t7izwRedFcYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:MStqg_gSLBcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R1TrASrH5esC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:X41XOdD1uaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:Qs68GpkTGbMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:6B7w4NK6UsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ThYxFsVAhEkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:SgTKrLvt1DcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:eP5N1itS6b0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:oi8PhiKYDwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:hwlm9Y4obscC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:6MVT8mER-OMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:xMZGxf1v-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:fSKd39tHJ84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:cUMtEw7vMgQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:BDTQWqO089sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sg-YnEhjH50C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:hzvVvKpMvx4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:mzRTpvMFn3IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:csTP5AdqY_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:W91e3rS6dHEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:AmQcmOVUwm0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:121WXH0ir18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:H8haQKU-3ZsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5-bGDoUgDrYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:BHd7YmozNHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:b8wqy9DyMDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:pk6FPx6l9xIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:fhZv66dCuXAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-Rb0Dl3RFGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5AT-p8ixKR4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:qCaWouos7ogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:YiZc1oW-E3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:z0_F5_TITjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:DGpvO1n63MYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:D4n_APcuzvwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:jg3FfqxSHJQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:3s2jc9hNhkQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:xUD2GqFNeDMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:RgznTc0nqo4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:kFM4V80Y5EsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lDOOmgye57wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lYbyOjaXH8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:F-1HeU7gMOYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:r5WMYYrv30cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:HOg0WoviCygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-rwTIRUNLawC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:SPgmg5JLkoEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:mJbmKSuM8toC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:FmXltp8y-ZkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:bXvxhPDrUX0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:QxtoOqDH1aQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:yRRszJvrVdAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sw8WaQuuSIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:ipSP0SfFaZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:M3hCyctIOBoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:mRAYM1lGiDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lAj_JhtUatoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:_WP0DvM6eX8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:UwpDZDXJMDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:m8MyhXdlT-4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:F9HO9s0W2bwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:dhpJJ7xvgBgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:EGhj4itiAA0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:GWiaReNCd0YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:tsQ8sfJ2mf4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:ult01sCh7k0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:JVJ6OkSwpHsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:P--cqP0FnSgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:_IwLajd9sWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Y1W0x10ZrwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:0dtNEdnCFDAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:gPamCmV4epEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:8s22W2WWFy4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:9PmR9Y676FQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:YnriW4MgZhwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:dw5aoL0HVgwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-A4928QJj7oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:9LrdxYebArsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:QSG1pgF8pGAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:zBCjByih94YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:7f0ks1Axv_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Nb0HLiwjfsIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:J3KpcKIlIpsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:f5lEeLvKxmwC"], "Topic": ["Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Federated Learning", "Others", "Artificial Intelligenc", "Federated Learning", "Others", "Others", "Others", "Federated Learning", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Computer Vision", "Federated Learning", "Others", "Others", "Others", "Optimization Techniq", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Artificial Intelligenc", "Federated Learning", "Artificial Intelligenc", "Geospatial Technology", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Artificial Intelligenc", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Others", "Others", "Geospatial Technology", "Artificial Intelligenc", "Artificial Intelligenc", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others"], "# of Citations": [5541, 1857, 1066, 833, 810, 677, 597, 573, 514, 426, 384, 349, 333, 314, 305, 280, 281, 265, 236, 231, 219, 216, 209, 212, 190, 185, 181, 161, 160, 152, 150, 145, 142, 141, 129, 124, 124, 124, 124, 121, 114, 103, 103, 99, 100, 98, 96, 93, 86, 87, 83, 81, 76, 76, 75, 75, 67, 61, 64, 60, 61, 60, 56, 57, 54, 53, 53, 53, 49, 50, 50, 46, 44, 42, 39, 37, 36, 36, 36, 35, 32, 33, 33, 31, 32, 31, 29, 28, 28, 28, 27, 27, 27, 26, 25, 25, 25, 25, 22, 23, 20, 20, 19, 19, 17, 16, 17, 15, 16, 16, 16, 15, 14, 10, 12, 11, 11, 9, 9, 10, 9, 9, 9, 9, 9, 8, 5, 7, 7, 5, 5, 5, 5, 5, 2, 3, 3, 3, 3, 3, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can\u00a0\u2026", "Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, ie, the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism---a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (ie, attentive spatial locations at multiple layers) and what (ie, attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.", "This paper contributes improvements on both the effectiveness and efficiency of Matrix Factorization (MF) methods for implicit feedback. We highlight two critical issues of existing works. First, due to the large space of unobserved feedback, most existing works resort to assign a uniform weight to the missing data to reduce computational complexity. However, such a uniform assumption is invalid in real-world settings. Second, most methods are also designed in an offline setting and fail to keep up with the dynamic nature of online data. We address the above two issues in learning MF models from implicit feedback. We first propose to weight the missing data based on item popularity, which is more effective and flexible than the uniform-weight assumption. However, such a non-uniform weighting poses efficiency challenge in learning the model. To address this, we specifically design a new learning algorithm based\u00a0\u2026", "Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a  relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github.com/hexiangnan/attentional_factorization_machine", "Multimedia content is dominating today's Web information. The nature of multimedia user-item interactions is 1/0 binary implicit feedback (e.g., photo likes, video views, song downloads, etc.), which can be collected at a larger scale with a much lower cost than explicit feedback (e.g., product ratings). However, the majority of existing collaborative filtering (CF) systems are not well-designed for multimedia recommendation, since they ignore the implicitness in users' interactions with multimedia content. We argue that, in multimedia recommendation, there exists item- and component-level implicitness which blurs the underlying users' preferences. The item-level implicitness means that users' preferences on items (e.g. photos, videos, songs, etc.) are unknown, while the component-level implicitness means that inside each item users' preferences on different components (e.g. regions in an image, frames of a video, etc\u00a0\u2026", "We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language inductive bias into the encoder-decoder image captioning framework for more human-like captions. Intuitively, we humans use the inductive bias to compose collocations and contextual inference in discourse. For example, when we see the relation\" person on bike\", it is natural to replace\" on\" with\" ride\" and infer\" person riding bike on a road\" even the\" road\" is not evident. Therefore, exploiting such bias as a language prior is expected to help the conventional encoder-decoder models less likely to overfit to the dataset bias and focus on reasoning. Specifically, we use the scene graph---a directed graph (G) where an object node is connected by adjective nodes and relationship nodes---to represent the complex structural layout of both image (I) and sentence (S). In the textual domain, we use SGAE to learn a dictionary (D) that helps to reconstruct sentences in the S-> G-> D-> S pipeline, where D encodes the desired language prior; in the vision-language domain, we use the shared D to guide the encoder-decoder in the I-> G-> D-> S pipeline. Thanks to the scene graph representation and shared dictionary, the inductive bias is transferred across domains in principle. We validate the effectiveness of SGAE on the challenging MS-COCO image captioning benchmark, eg, our SGAE-based single-model achieves a new state-of-the-art 127.8 CIDEr-D on the Karpathy split, and a competitive 125.5 CIDEr-D (c40) on the official server even compared to other ensemble models. Code has been made available at: https://github. com/yangxuntu/SGAE.", "Recent progress in using long short-term memory (LSTM) for image captioning has motivated the exploration of their applications for video captioning. By taking a video as a sequence of features, an LSTM model is trained on video-sentence pairs and learns to associate a video to a sentence. However, most existing methods compress an entire video shot or frame into a static representation, without considering attention mechanism which allows for selecting salient features. Furthermore, existing approaches usually model the translating error, but ignore the correlations between sentence semantics and visual content. To tackle these issues, we propose a novel end-to-end framework named aLSTMs, an attention-based LSTM model with semantic consistency, to transfer videos to natural sentences. This framework integrates attention mechanism with LSTM to capture salient structures of video, and explores the\u00a0\u2026", "Visual relations, such as\" person ride bike\" and\" bike next to car\", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, ie, subject+ predicate= object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language", "Today's scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, eg, collapsing diverse\" human walk on/sit on/lay on beach\" into\" human on beach\". Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects. However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, eg, good context prior (eg,\" person read book\" rather than\" eat\") and bad long-tailed bias (eg,\" near\" dominating\" behind/in front of\"). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We first build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect (TDE) as the proposed final predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit on the SGG benchmark Visual Genome and several prevailing models, we observed significant improvements over the previous state-of-the-art methods.", "Embedding network data into a low-dimensional vector space has shown promising performance for many real-world applications, such as node classification and entity retrieval. However, most existing methods focused only on leveraging network structure. For social networks, besides the network structure, there also exists rich information about social actors, such as user profiles of friendship networks and textual content of citation networks. These rich attribute information of social actors reveal the homophily effect, exerting huge impacts on the formation of social networks. In this paper, we explore the rich evidence source of attributes in social networks to improve network embedding. We propose a generic Attributed Social Network Embedding framework (ASNE), which learns representations for social actors (i.e., nodes) by preserving both the structural proximity and attribute proximity. While the structural\u00a0\u2026", "We propose to compose dynamic tree structures that place the objects in an image into a visual context, helping visual reasoning tasks such as scene graph generation and visual Q&A. Our visual context tree model, dubbed VCTree, has two key advantages over existing structured object representations including chains and fully-connected graphs: 1) The efficient and expressive binary tree encodes the inherent parallel/hierarchical relationships among objects, eg,\" clothes\" and\" pants\" are usually co-occur and belong to\" person\"; 2) the dynamic structure varies from image to image and task to task, allowing more content-/task-specific message passing among objects. To construct a VCTree, we design a score function that calculates the task-dependent validity between each object pair, and the tree is the binary version of the maximum spanning tree from the score matrix. Then, visual contexts are encoded by bidirectional TreeLSTM and decoded by task-specific models. We develop a hybrid learning procedure which integrates end-task supervised learning and the tree structure reinforcement learning, where the former's evaluation result serves as a self-critic for the latter's structure exploration. Experimental results on two benchmarks, which require reasoning over contexts: Visual Genome for scene graph generation and VQA2. 0 for visual Q&A, show that VCTree outperforms state-of-the-art results while discovering interpretable visual context structures.", "As the class size grows, maintaining a balanced dataset across many classes is challenging because the data are long-tailed in nature; it is even impossible when the sample-of-interest co-exists with each other in one collectable unit, eg, multiple visual instances in one image. Therefore, long-tailed classification is the key to deep learning at scale. However, existing methods are mainly based on re-weighting/re-sampling heuristics that lack a fundamental theory. In this paper, we establish a causal inference framework, which not only unravels the whys of previous methods, but also derives a new principled solution. Specifically, our theory shows that the SGD momentum is essentially a confounder in long-tailed classification. On one hand, it has a harmful causal effect that misleads the tail prediction biased towards the head. On the other hand, its induced mediation also benefits the representation learning and head prediction. Our framework elegantly disentangles the paradoxical effects of the momentum, by pursuing the direct causal effect caused by an input sample. In particular, we use causal intervention in training, and counterfactual reasoning in inference, to remove the bad''while keep the good''. We achieve new state-of-the-arts on three long-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100, ImageNet-LT for image classification and LVIS for instance segmentation.", "Recently image question answering (ImageQA) has gained lots of attention in the research community. However, as its natural extension, video question answering (VideoQA) is less explored. Although both tasks look similar, VideoQA is more challenging mainly because of the complexity and diversity of videos. As such, simply extending the ImageQA methods to videos is insufficient and suboptimal. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this paper, we consider exploiting the appearance and motion information resided in the video with a novel attention mechanism. More specifically, we propose an end-to-end model which gradually refines its attention over the appearance and motion features of the video using the question as guidance. The question is processed word by word until the model generates the final\u00a0\u2026", "We propose a novel framework called Semantics-Preserving Adversarial Embedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test images and their classes are both unseen during training. SP-AEN aims to tackle the inherent problem\u2014semantic loss\u2014in the prevailing family of embedding-based ZSL, where some semantics would be discarded during training if they are non-discriminative for training classes, but could become critical for recognizing test classes. Specifically, SP-AEN prevents the semantic loss by introducing an independent visual-to-semantic space embedder which disentangles the semantic space into two subspaces for the two arguably conflicting objectives: classification and reconstruction. Through adversarial learning of the two subspaces, SP-AEN can transfer the semantics from the reconstructive subspace to the discriminative one, accomplishing the improved zero-shot recognition of unseen classes. Comparing with prior works, SP-AEN can not only improve classification but also generate photo-realistic images, demonstrating the effectiveness of semantic preservation. On four popular benchmarks: CUB, AWA, SUN and aPY, SP-AEN considerably outperforms other state-of-the-art methods by an absolute performance difference of 12.2%, 9.3%, 4.0%, and 3.6% in terms of harmonic mean values.", "We present a causal inference framework to improve Weakly-Supervised Semantic Segmentation (WSSS). Specifically, we aim to generate better pixel-level pseudo-masks by using only image-level labels--the most crucial step in WSSS. We attribute the cause of the ambiguous boundaries of pseudo-masks to the confounding context, eg, the correct image-level classification of\" horse\" and\" person\" may be not only due to the recognition of each instance, but also their co-occurrence context, making the model inspection (eg, CAM) hard to distinguish between the boundaries. Inspired by this, we propose a structural causal model to analyze the causalities among images, contexts, and class labels. Based on it, we develop a new method: Context Adjustment (CONTA), to remove the confounding bias in image-level classification and thus provide better pseudo-masks as ground-truth for the subsequent segmentation model. On PASCAL VOC 2012 and MS-COCO, we show that CONTA boosts various popular WSSS methods to new state-of-the-arts.", "Recent VQA models may tend to rely on language bias as a shortcut and thus fail to sufficiently learn the multi-modal knowledge from both vision and language. In this paper, we investigate how to capture and mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of questions on answers and reduce the language bias by subtracting the direct language effect from the total causal effect. Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA backbones and fusion strategies, 2) achieves competitive performance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any argumented data. The code is available at https://github. com/yuleiniu/cfvqa.", "We address the efficiency problem of Collaborative Filtering (CF) by hashing users and items as latent vectors in the form of binary codes, so that user-item affinity can be efficiently calculated in a Hamming space. However, existing hashing methods for CF employ binary code learning procedures that most suffer from the challenging discrete constraints. Hence, those methods generally adopt a two-stage learning scheme composed of relaxed optimization via discarding the discrete constraints, followed by binary quantization. We argue that such a scheme will result in a large quantization loss, which especially compromises the performance of large-scale CF that resorts to longer binary codes. In this paper, we propose a principled CF hashing framework called Discrete Collaborative Filtering (DCF), which directly tackles the challenging discrete optimization that should have been treated adequately in hashing\u00a0\u2026", "Despite Visual Question Answering (VQA) has realized impressive progress over the last few years, today's VQA models tend to capture superficial linguistic correlations in the train set and fail to generalize to the test set with different QA distributions. To reduce the language biases, several recent works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on VQA-CP. However, since the complexity of design, current methods are unable to equip the ensemble-based models with two indispensable characteristics of an ideal VQA model: 1) visual-explainable: the model should rely on the right visual regions when making decisions. 2) question-sensitive: the model should be sensitive to the linguistic variations in question. To this end, we propose a model-agnostic Counterfactual Samples Synthesizing (CSS) training scheme. The CSS generates numerous counterfactual training samples by masking critical objects in images or words in questions, and assigning different ground-truth answers. After training with the complementary samples (ie, the original and generated samples), the VQA models are forced to focus on all critical objects and words, which significantly improves both visual-explainable and question-sensitive abilities. In return, the performance of these models is further boosted. Extensive ablations have shown the effectiveness of CSS. Particularly, by building on top of the model LMH, we achieve a record-breaking performance of 58.95% on VQA-CP v2, with 6.5% gains.", "Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework dubbed self-supervised video hashing (SSVH), which is able to capture the temporal nature of videos in an end-to-end learning to hash fashion. We specifically address two central problems: 1) how to design an encoder-decoder architecture to generate binary codes for videos and 2) how to equip the binary codes with the ability of accurate video retrieval. We design a hierarchical binary auto-encoder to model the temporal dependencies in videos with multiple granularities, and embed the videos into binary codes with less computations than the stacked architecture. Then, we\u00a0\u2026", "We present a novel unsupervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for high-level tasks such as captioning and VQA. Given a set of detected object regions in an image (eg, using Faster R-CNN), like any other unsupervised feature learning methods (eg, word2vec), the proxy training objective of VC R-CNN is to predict the contextual objects of a region. However, they are fundamentally different: the prediction of VC R-CNN is by using causal intervention: P (Y| do (X)), while others are by using the conventional likelihood: P (Y| X). This is also the core reason why VC R-CNN can learn\" sense-making\" knowledge like chair can be sat--while not just\" common\" co-occurrences such as chair is likely to exist if table is observed. We extensively apply VC R-CNN features in prevailing models of three popular tasks: Image Captioning, VQA, and VCR, and observe consistent performance boosts across them, achieving many new state-of-the-arts.", "Filter pruning has been widely applied to neural network compression and acceleration. Existing methods usually utilize pre-defined pruning criteria, such as Lp-norm, to prune unimportant filters. There are two major limitations to these methods. First, existing methods fail to consider the variety of filter distribution across layers. To extract features of the coarse level to the fine level, the filters of different layers have various distributions. Therefore, it is not suitable to utilize the same pruning criteria to different functional layers. Second, prevailing layer-by-layer pruning methods process each layer independently and sequentially, failing to consider that all the layers in the network collaboratively make the final prediction. In this paper, we propose Learning Filter Pruning Criteria (LFPC) to solve the above problems. Specifically, we develop a differentiable pruning criteria sampler. This sampler is learnable and optimized by the validation loss of the pruned network obtained from the sampled criteria. In this way, we could adaptively select the appropriate pruning criteria for different functional layers. Besides, when evaluating the sampled criteria, LFPC comprehensively consider the contribution of all the layers at the same time. Experiments validate our approach on three image classification benchmarks. Notably, on ILSVRC-2012, our LFPC reduces more than 60% FLOPs on ResNet-50 with only 0.83% top-5 accuracy loss.", "Good representations of data do help in many machine learning tasks such as recommendation. It is often a great challenge for traditional recommender systems to learn representative features of both users and images in large social networks, in particular, social curation networks, which are characterized as the extremely sparse links between users and images, and the extremely diverse visual contents of images. To address the challenges, we propose a novel deep model which learns the unified feature representations for both users and images. This is done by transforming the heterogeneous user-image networks into homogeneous low-dimensional representations, which facilitate a recommender to trivially recommend images to users by feature similarity. We also develop a fast online algorithm that can be easily scaled up to large networks in an asynchronously parallel way. We conduct extensive experiments on a representative subset of Pinterest, containing 1,456,540 images and 1,000,000 users. Results of image recommendation experiments demonstrate that our feature learning approach significantly outperforms other state-of-the-art recommendation methods.", "Feature interactions across space and scales underpin modern visual recognition systems because they introduce beneficial visual contexts. Conventionally, spatial contexts are passively hidden in the CNN\u2019s increasing receptive fields or actively encoded by non-local convolution. Yet, the non-local spatial interactions are not across scales, and thus they fail to capture the non-local contexts of objects (or parts) residing in different scales. To this end, we propose a fully active feature interaction across both space and scales, called Feature Pyramid Transformer (FPT). It transforms any feature pyramid into another feature pyramid of the same size but with richer contexts, by using three specially designed transformers in self-level, top-down, and bottom-up interaction fashion. FPT serves as a generic visual backbone with fair computational overhead. We conduct extensive experiments in both instance-level (i.e\u00a0\u2026", "We aim to dismantle the prevalent black-box neural architectures used in complex visual reasoning tasks, into the proposed eXplainable and eXplicit Neural Modules (XNMs), which advance beyond existing neural module networks towards using scene graphs---objects as nodes and the pairwise relationships as edges---for explainable and explicit reasoning with structured knowledge. XNMs allow us to pay more attention to teach machines how to\" think\", regardless of what they\" look\". As we will show in the paper, by using scene graphs as an inductive bias, 1) we can design XNMs in a concise and flexible fashion, ie, XNMs merely consist of 4 meta-types, which significantly reduce the number of parameters by 10 to 100 times, and 2) we can explicitly trace the reasoning-flow in terms of graph attentions. XNMs are so generic that they support a wide range of scene graph implementations with various qualities. For example, when the graphs are detected perfectly, XNMs achieve 100% accuracy on both CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound for visual reasoning; when the graphs are noisily detected from real-world images, XNMs are still robust to achieve a competitive 67.5% accuracy on VQAv2. 0, surpassing the popular bag-of-objects attention models without graph structures.", "Visual grounding, a task to ground (ie, localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail.", "We uncover an ever-overlooked deficiency in the prevailing Few-Shot Learning (FSL) methods: the pre-trained knowledge is indeed a confounder that limits the performance. This finding is rooted from our causal assumption: a Structural Causal Model (SCM) for the causalities among the pre-trained knowledge, sample features, and labels. Thanks to it, we propose a novel FSL paradigm: Interventional Few-Shot Learning (IFSL). Specifically, we develop three effective IFSL algorithmic implementations based on the backdoor adjustment, which is essentially a causal intervention towards the SCM of many-shot learning: the upper-bound of FSL in a causal view. It is worth noting that the contribution of IFSL is orthogonal to existing fine-tuning and meta-learning based FSL methods, hence IFSL can improve all of them, achieving a new 1-/5-shot state-of-the-art on miniImageNet, tieredImageNet, and cross-domain CUB. Code is released at https://github. com/yue-zhongqi/ifsl.", "We focus on grounding (ie, localizing or linking) referring expressions in images, eg,``largest elephant standing behind baby elephant''. This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context---visual attributes (eg,``largest'',``baby'') and relationships (eg,``behind'') that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Our model exploits the reciprocal relation between the referent and context, ie, either of them influences the estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced. We also extend the model to the unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings. The code is available at url {https://github. com/yuleiniu/vc/", "This paper presents a novel Attribute-augmented Semantic Hierarchy (A2 SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in Content-based Image Retrieval (CBIR). A2 SH organizes the semantic concepts into multiple semantic levels and augments each concept with a set of related attributes, which describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. A hierarchical semantic similarity function is learnt to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedbacks on attributes and images. These feedbacks are then used to refine the search results based on A2 SH. We develop a content-based image retrieval system based on the proposed A2 SH. We conduct extensive\u00a0\u2026", "Scene graphs---objects as nodes and visual relationships as edges---describe the whereabouts and interactions of objects in an image for comprehensive scene understanding. To generate coherent scene graphs, almost all existing methods exploit the fruitful visual context by modeling message passing among objects. For example,\" person\" on\" bike\" can help to determine the relationship\" ride\", which in turn contributes to the confidence of the two objects. However, we argue that the visual context is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes should not be penalized equally. To this end, we propose a Counterfactual critic Multi-Agent Training (CMAT) approach. CMAT is a multi-agent policy gradient method that frames objects into cooperative agents, and then directly maximizes a graph-level metric as the reward. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the predictions of other agents. Extensive validations on the challenging Visual Genome benchmark show that CMAT achieves a state-of-the-art performance by significant gains under various settings and metrics.", "We aim to tackle a novel vision task called Weakly Supervised Visual Relation Detection (WSVRD) to detect\" subject-predicate-object\" relations in an image with object relation groundtruths available only at the image level. This is motivated by the fact that it is extremely expensive to label the combinatorial relations between objects at the instance level. Compared to the extensively studied problem, Weakly Supervised Object Detection (WSOD), WSVRD is more challenging as it needs to examine a large set of regions pairs, which is computationally prohibitive and more likely stuck in a local optimal solution such as those involving wrong spatial context. To this end, we present a Parallel, Pairwise Region-based, Fully Convolutional Network (PPR-FCN) for WSVRD. It uses a parallel FCN architecture that simultaneously performs pair selection and classification of single regions and region pairs for object and relation detection, while sharing almost all computation shared over the entire image. In particular, we propose a novel position-role-sensitive score map with pairwise RoI pooling to efficiently capture the crucial context associated with a pair of objects. We demonstrate the superiority of PPR-FCN over all baselines in solving the WSVRD challenge by using results of extensive experiments over two visual relation benchmarks.", "Micro-videos, a new form of user generated contents (UGCs), are gaining increasing enthusiasm. Popular micro-videos have enormous commercial potential in many ways, such as online marketing and brand tracking. In fact, the popularity prediction of traditional UGCs including tweets, web images, and long videos, has achieved good theoretical underpinnings and great practical success. However, little research has thus far been conducted to predict the popularity of the bite-sized videos. This task is non-trivial due to three reasons: 1) micro-videos are short in duration and of low quality; 2) they can be described by multiple heterogeneous channels, spanning from social, visual, acoustic to textual modalities; and 3) there are no available benchmark dataset and discriminant features that are suitable for this task. Towards this end, we present a transductive multi-modal learning model. The proposed model is\u00a0\u2026", "We propose a causal framework to explain the catastrophic forgetting in Class-Incremental Learning (CIL) and then derive a novel distillation method that is orthogonal to the existing anti-forgetting techniques, such as data replay and feature/label distillation. We first 1) place CIL into the framework, 2) answer why the forgetting happens: the causal effect of the old data is lost in new training, and then 3) explain how the existing techniques mitigate it: they bring the causal effect back. Based on the causal framework, we propose to distill the Colliding Effect between the old and the new data, which is fundamentally equivalent to the causal effect of data replay, but without any cost of replay storage. Thanks to the causal effect analysis, we can further capture the Incremental Momentum Effect of the data stream, removing which can help to retain the old effect overwhelmed by the new data effect, and thus alleviate the forgetting of the old class in testing. Extensive experiments on three CIL benchmarks: CIFAR-100, ImageNet-Sub&Full, show that the proposed causal effect distillation can improve various state-of-the-art CIL methods by a large margin (0.72%-9.06%)", "We present a novel counterfactual framework for both Zero-Shot Learning (ZSL) and Open-Set Recognition (OSR), whose common challenge is generalizing to the unseen-classes by only training on the seen-classes. Our idea stems from the observation that the generated samples for unseen-classes are often out of the true distribution, which causes severe recognition rate imbalance between the seen-class (high) and unseen-class (low). We show that the key reason is that the generation is not Counterfactual Faithful, and thus we propose a faithful one, whose generation is from the sample-specific counterfactual question: What would the sample look like, if we set its class attribute to a certain class, while keeping its sample attribute unchanged? Thanks to the faithfulness, we can apply the Consistency Rule to perform unseen/seen binary classification, by asking: Would its counterfactual still look like itself? If\" yes\", the sample is from a certain class, and\" no\" otherwise. Through extensive experiments on ZSL and OSR, we demonstrate that our framework effectively mitigates the seen/unseen imbalance and hence significantly improves the overall performance. Note that this framework is orthogonal to existing methods, thus, it can serve as a new baseline to evaluate how ZSL/OSR models generalize. Codes are available at https://github. com/yue-zhongqi/gcm-cf.", "Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation on novel classes can be inductively biased, we explicitly preserve covariance information as the``variability''of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.", "This paper unravels the design tricks adopted by us, the champion team MReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for improving Visual Dialog (VisDial). By\" improving\", we mean that they can promote almost every existing VisDial model to the state-of-the-art performance on the leader-board. Such a major improvement is only due to our careful inspection on the causality behind the model and data, finding that the community has overlooked two causalities in VisDial. Intuitively, Principle 1 suggests: we should remove the direct input of the dialog history to the answer model, otherwise a harmful shortcut bias will be introduced; Principle 2 says: there is an unobserved confounder for history, question, and answer, leading to spurious correlations from training data. In particular, to remove the confounder suggested in Principle 2, we propose several causal intervention algorithms, which make the training fundamentally different from the traditional likelihood estimation. Note that the two principles are model-agnostic, so they are applicable in any VisDial model.", "With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., \u201cman riding horse\u201d) and visual comparisons (e.g., \u201csmall(er) cat\u201d). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context\u00a0\u2026", "Recommendation is a prevalent and critical service in information systems. To provide personalized suggestions to users, industry players embrace machine learning, more specifically, building predictive models based on the click behavior data. This is known as the Click-Through Rate (CTR) prediction, which has become the gold standard for building personalized recommendation service. However, we argue that there is a significant gap between clicks and user satisfaction --- it is common that a user is \"cheated\" to click an item by the attractive title/cover of the item. This will severely hurt user's trust on the system if the user finds the actual content of the clicked item disappointing. What's even worse, optimizing CTR models on such flawed data will result in the Matthew Effect, making the seemingly attractive but actually low-quality items be more frequently recommended. In this paper, we formulate the\u00a0\u2026", "As a bridge to connect vision and language, visual relations between objects in the form of relation triplet $\u0142angle subject,predicate,object\\rangle$, such as \"person-touch-dog'' and \"cat-above-sofa'', provide a more comprehensive visual content understanding beyond objects. In this paper, we propose a novel vision task named Video Visual Relation Detection (VidVRD) to perform visual relation detection in videos instead of still images (ImgVRD). As compared to still images, videos provide a more natural set of features for detecting visual relations, such as the dynamic relations like \"A-follow-B'' and \"A-towards-B'', and temporally changing relations like \"A-chase-B'' followed by \"A-hold-B''. However, VidVRD is technically more challenging than ImgVRD due to the difficulties in accurate object tracking and diverse relation appearances in video domain. To this end, we propose a VidVRD method, which consists of\u00a0\u2026", "Microblogging services have revolutionized the way people exchange information. Confronted with the ever-increasing numbers of social events and the corresponding microblogs with multimedia contents, it is desirable to provide visualized summaries to help users to quickly grasp the essence of these social events for better understanding. While existing approaches mostly focus only on text-based summary, microblog summarization with multiple media types (e.g., text, image, and video) is scarcely explored. In this paper, we propose a multimedia social event summarization framework to automatically generate visualized summaries from the microblog stream of multiple media types. Specifically, the proposed framework comprises three stages, as follows. 1) A noise removal approach is first devised to eliminate potentially noisy images. An effective spectral filtering model is exploited to estimate the probability\u00a0\u2026", "Visual attention not only improves the performance of image captioners, but also serves as a visual interpretation to qualitatively measure the caption rationality and model transparency. Specifically, we expect that a captioner can fix its attentive gaze on the correct objects while generating the corresponding words. This ability is also known as grounded image captioning. However, the grounding accuracy of existing captioners is far from satisfactory. To improve the grounding accuracy while retaining the captioning quality, it is expensive to collect the word-region alignment as strong supervision. To this end, we propose a Part-of-Speech (POS) enhanced image-text matching model (SCAN): POS-SCAN, as the effective knowledge distillation for more grounded image captioning. The benefits are two-fold: 1) given a sentence and an image, POS-SCAN can ground the objects more accurately than SCAN; 2) POS-SCAN serves as a word-region alignment regularization for the captioner's visual attention module. By showing benchmark experimental results, we demonstrate that conventional image captioners equipped with POS-SCAN can significantly improve the grounding accuracy without strong supervision. Last but not the least, we explore the indispensable Self-Critical Sequence Training (SCST) in the context of grounded image captioning and show that the image-text matching score can serve as a reward for more grounded captioning.", "Visual dialog is a challenging vision-language task, which requires the agent to answer multi-round questions about an image. It typically needs to address two major problems:(1) How to answer visually-grounded questions, which is the core challenge in visual question answering (VQA);(2) How to infer the co-reference between questions and the dialog history. An example of visual co-reference is: pronouns (eg,\" they\") in the question (eg,\" Are they on or off?\") are linked with nouns (eg,\" lamps\") appearing in the dialog history (eg,\" How many lamps are there?\") and the object grounded in the image. In this work, to resolve the visual co-reference for visual dialog, we propose a novel attention mechanism called Recursive Visual Attention (RvA). Specifically, our dialog agent browses the dialog history until the agent has sufficient confidence in the visual co-reference resolution, and refines the visual attention recursively. The quantitative and qualitative experimental results on the large-scale VisDial v0. 9 and v1. 0 datasets demonstrate that the proposed RvA not only outperforms the state-of-the-art methods, but also achieves reasonable recursion and interpretable attention maps without additional annotations. The code is available at https://github. com/yuleiniu/rva.", "Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the \"exposure bias'' during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (eg, visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (\\eg, \"man riding horse'') and comparisons (eg. \"smaller cat\"). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides\u00a0\u2026", "Feature representation for visual content is the key to the progress of many fundamental applications such as annotation and cross-modal retrieval. Although recent advances in deep feature learning offer a promising route towards these tasks, they are limited in application domains where high-quality and large-scale training data are expensive to obtain. In this article, we propose a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, in particular, largely social images and tags. Differing from existing feature learning approaches that rely on high-quality image-label supervision, our weak supervision is acquired by mining the visual-semantic embeddings from noisy, sparse, and diverse social image collections. The resultant image-word embedding space can be used to (1) fine-tune deep visual models for\u00a0\u2026", "Dataset bias in vision-language tasks is becoming one of the main problems which hinders the progress of our community. Existing solutions lack a principled analysis about why modern image captioners easily collapse into dataset bias. In this paper, we present a novel perspective: Deconfounded Image Captioning (DIC), to find out the answer of this question, then retrospect modern neural image captioners, and finally propose a DIC framework: DICv1.0 to alleviate the negative effects brought by dataset bias. DIC is based on causal inference, whose two principles: the backdoor and front-door adjustments, help us review previous studies and design new effective models. In particular, we showcase that DICv1.0 can strengthen two prevailing captioning models and can achieve a single-model 131.1 CIDEr-D and 128.4 c40 CIDEr-D on Karpathy split and online split of the challenging MS COCO dataset\u00a0\u2026", "Different from traditional long videos, micro-videos are much shorter and usually recorded at a specific place with mobile devices. To better understand the semantics of a micro-video and facilitate downstream applications, it is crucial to estimate the venue where the micro-video is recorded, for example, in a concert or on a beach. However, according to our statistics over two million micro-videos, only $1.22%$ of them were labeled with location information. For the remaining large number of micro-videos without location information, we have to rely on their content to estimate their venue categories. This is a highly challenging task, as micro-videos are naturally multi-modal (with textual, visual and, acoustic content), and more importantly, the quality of each modality varies greatly for different micro-videos. In this work, we focus on enhancing the acoustic modality for the venue category estimation task. This is\u00a0\u2026", "Higher-level semantics such as visual attributes are crucial for fundamental multimedia applications. We present a novel attribute discovery approach that can automatically identify, model and name attributes from an arbitrary set of image and text pairs that can be easily gathered on the Web. Different from conventional attribute discovery methods, our approach does not rely on any pre-defined vocabularies and human labeling. Therefore, we are able to build a large visual knowledge base without any human efforts. The discovery is based on a novel deep architecture, named Independent Component Multimodal Autoencoder (ICMAE), that can continually learn shared higher-level representations across the visual and textual modalities. With the help of the resultant representations encoding strong visual and semantic evidences, we propose to (a) identify attributes and their corresponding high-quality training\u00a0\u2026", "We present a novel attention mechanism: Causal Attention (CATT), to remove the ever-elusive confounding effect in existing attention-based vision-language models. This effect causes harmful bias that misleads the attention module to focus on the spurious correlations in training data, damaging the model generalization. As the confounder is unobserved in general, we use the front-door adjustment to realize the causal intervention, which does not require any knowledge on the confounder. Specifically, CATT is implemented as a combination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention (CS-ATT), where the latter forcibly brings other samples into every IS-ATT, mimicking the causal intervention. CATT abides by the QKV convention and hence can replace any attention module such as top-down attention and self-attention in Transformers. CATT improves various popular attention-based vision-language models by considerable margins. In particular, we show that CATT has great potential in large-scale pre-training, eg, it can promote the lighter LXMERT [??], which uses fewer data and less computational power, comparable to the heavier UNITER [??]. Code is published in https://github. com/yangxuntu/lxmertcatt.", "Grounding natural language in images, such as localizing \u201cthe black dog on the left of the tree\u201d, is one of the core problems in artificial intelligence, as it needs to comprehend the fine-grained language compositions. However, existing solutions merely rely on the association between the holistic language features and visual features, while neglect the nature of composite reasoning implied in the language. In this paper, we propose a natural language grounding model that can automatically compose a binary tree structure for parsing the language and then perform visual reasoning along the tree in a bottom-up fashion. We call our model  RvG-Tree : Recursive Grounding Tree, which is inspired by the intuition that any language expression can be recursively decomposed into two constituent parts, and the grounding confidence score can be recursively accumulated by calculating their grounding scores returned by\u00a0\u2026", "We do not speak word by word from scratch; our brain quickly structures a pattern like sth do sth at someplace and then fill in the detailed description. To render existing encoder-decoder image captioners such human-like reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the\" inner pattern\" connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q&A, where the language (ie, question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design---one for function words and three for visual content words (eg, noun, adjective, and verb), 2) soft module fusion and multi-step module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (eg, adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, eg, by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline.", "Image captioning is one of the most challenging tasks in AI because it requires an understanding of both complex visuals and natural language. Because image captioning is essentially a sequential prediction task, recent advances in image captioning have used reinforcement learning (RL) to better explore the dynamics of word-by-word generation. However, the existing RL-based image captioning methods rely primarily on a single policy network and reward function-an approach that is not well matched to the multi-level (word and sentence) and multi-modal (vision and language) nature of the task. To solve this problem, we propose a novel multi-level policy and reward RL framework for image captioning that can be easily integrated with RNN-based captioning models, language metrics, or visual-semantic functions for optimization. Specifically, the proposed framework includes two modules: 1) a multi-level\u00a0\u2026", "We focus on hashing videos into short binary codes for efficient Content-based Video Retrieval (CBVR), which is a fundamental technique that supports access to the ever-growing abundance of videos on the Web. Existing video hash functions are built on three isolated stages: frame pooling, relaxed learning, and binarization, which have not adequately explored the temporal order of video frames in a joint binary optimization model, resulting in severe information loss. In this paper, we propose a novel unsupervised video hashing framework called Self-Supervised Temporal Hashing (SSTH) that is able to capture the temporal nature of videos in an end-to-end learning-to-hash fashion. Specifically, the hash function of SSTH is an encoder RNN equipped with the proposed Binary LSTM (BLSTM) that generates binary codes for videos. The hash function is learned in a self-supervised fashion, where a decoder RNN\u00a0\u2026", "Due to fact that it is prohibitively expensive to completely annotate visual relationships, ie, the (obj1, rel, obj2) triplets, relationship models are inevitably biased to object classes of limited pairwise patterns, leading to poor generalization to rare or unseen object combinations. Therefore, we are interested in learning object-agnostic visual features for more generalizable relationship models. By``agnostic'', we mean that the feature is less likely biased to the classes of paired objects. To alleviate the bias, we propose a novel Shuffle-Then-Assemble pre-training strategy. First, we discard all the triplet relationship annotations in an image, leaving two unpaired object domains without obj1-obj2 alignment. Then, our feature learning is to recover possible obj1-obj2 pairs. In particular, we design a cycle of residual transformations between the two domains, where the identity mappings encourage the RoI features to capture shared but not object-specific visual patterns. Extensive experiments on two visual relationship benchmarks show that by using our pre-trained features, naive relationship models can be consistently improved and even outperform other state-of-the-art relationship models.", "Recently, a great progress in automatic image captioning has been achieved by using semantic concepts detected from the image. However, we argue that existing concepts-to-caption framework, in which the concept detector is trained using the image-caption pairs to minimize the vocabulary discrepancy, suffers from the deficiency of insufficient concepts. The reasons are two-fold: 1) the extreme imbalance between the number of occurrence positive and negative samples of the concept and 2) the incomplete labeling in training captions caused by the biased annotation and usage of synonyms. In this paper, we propose a method, termed  online positive recall and missing concepts mining , to overcome those problems. Our method adaptively re-weights the loss of different samples according to their predictions for online positive recall and uses a two-stage optimization strategy for missing concepts mining. In this\u00a0\u2026", "Real-world visual recognition requires handling the extreme sample imbalance in large-scale long-tailed data. We propose a\" divide&conquer\" strategy for the challenging LVIS task: divide the whole data into balanced parts and then apply incremental learning to conquer each one. This derives a novel learning paradigm: class-incremental few-shot learning, which is especially effective for the challenge evolving over time: 1) the class imbalance among the old class knowledge review and 2) the few-shot data in new-class learning. We call our approach Learning to Segment the Tail (LST). In particular, we design an instance-level balanced replay scheme, which is a memory-efficient approximation to balance the instance-level samples from the old-class images. We also propose to use a meta-module for new-class learning, where the module parameters are shared across incremental phases, gaining the learning-to-learn knowledge incrementally, from the data-rich head to the data-poor tail. We empirically show that: at the expense of a little sacrifice of head-class forgetting, we can gain a significant 8.3% AP improvement for the tail classes with less than 10 instances, achieving an overall 2.0% AP boost for the whole 1,230 classes.", "Recently, much advance has been made in image captioning, and an encoder-decoder framework has achieved outstanding performance for this task. In this paper, we propose an extension of the encoder-decoder framework by adding a component called guiding network. The guiding network models the attribute properties of input images, and its output is leveraged to compose the input of the decoder at each time step. The guiding network can be plugged into the current encoder-decoder framework and trained in an end-to-end manner. Hence, the guiding vector can be adaptively learned according to the signal from the decoder, making itself to embed information from both image and language. Additionally, discriminative supervision can be employed to further improve the quality of guidance. The advantages of our proposed approach are verified by experiments carried out on the MS COCO dataset.", "We present a deep learning strategy to fuse multiple semantic cues for complex event recognition. In particular, we tackle the recognition task by answering how to jointly analyze human actions (who is doing what), objects (what), and scenes (where). First, each type of semantic features (e.g., human action trajectories) is fed into a corresponding multi-layer feature abstraction pathway, followed by a fusion layer connecting all the different pathways. Second, the correlations of how the semantic cues interacting with each other are learned in an unsupervised cross-modality autoencoder fashion. Finally, by fine-tuning a large-margin objective deployed on this deep architecture, we are able to answer the question on how the semantic cues of who, what, and where compose a complex event. As compared with the traditional feature fusion methods (e.g., various early or late strategies), our method jointly learns the\u00a0\u2026", "Along with the long-time evolution of popular social networks, e.g. Facebook, social media analysis research inevitably arrived at the era of considering face/user recognition with large age gaps. However, related research with adequate subjects and large age gaps is surprisingly rare. In this work, we first collect a so-called cross-age face (CAFE) dataset, ranging from child, to young, to adult, to old groups. Then, we propose a novel framework, called deep aging face verification (DAFV), for this challenging task. DAFV includes two modules: aging pattern synthesis and aging face verification. The aging pattern synthesis module synthesizes the faces of all age groups for the input face of an arbitrary age, and the core structure is a deep aging-aware denoising auto-encoder ( a 2 -DAE) with multiple outputs. The aging face verification module then takes the synthesized aging patterns of a face pair as the input, and\u00a0\u2026", "Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including ~120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro serves for both KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines.", "Extracting class activation maps (CAM) is arguably the most standard step of generating pseudo masks for weakly-supervised semantic segmentation (WSSS). Yet, we find that the crux of the unsatisfactory pseudo masks is the binary cross-entropy loss (BCE) widely used in CAM. Specifically, due to the sum-over-class pooling nature of BCE, each pixel in CAM may be responsive to multiple classes co-occurring in the same receptive field. As a result, given a class, its hot CAM pixels may wrongly invade the area belonging to other classes, or the non-hot ones may be actually a part of the class. To this end, we introduce an embarrassingly simple yet surprisingly effective method: Reactivating the converged CAM with BCE by using softmax cross-entropy loss (SCE), dubbed ReCAM. Given an image, we use CAM to extract the feature pixels of every single class, and use them with the class label to learn another fully-connected layer (after the backbone) with SCE. Once converged, we extract ReCAM in the same way as in CAM. Thanks to the contrastive nature of SCE, the pixel response is disentangled into different classes and hence less mask ambiguity is expected. The evaluation on both PASCAL VOC and MS COCO shows that ReCAM can not only generates high-quality masks, but also supports plug-and-play in any CAM variant with little overhead.", "Multi-hop Question Answering (QA) is a challenging task because it requires precise reasoning with entity relations at every step towards the answer. The relations can be represented in terms of labels in knowledge graph (e.g., \\textit{spouse}) or text in text corpus (e.g., \\textit{they have been married for 26 years}). Existing models usually infer the answer by predicting the sequential relation path or aggregating the hidden graph features. The former is hard to optimize, and the latter lacks interpretability. In this paper, we propose TransferNet, an effective and transparent model for multi-hop QA, which supports both label and text relations in a unified framework. TransferNet jumps across entities at multiple steps. At each step, it attends to different parts of the question, computes activated scores for relations, and then transfer the previous entity scores along activated relations in a differentiable way. We carry out extensive experiments on three datasets and demonstrate that TransferNet surpasses the state-of-the-art models by a large margin. In particular, on MetaQA, it achieves 100\\% accuracy in 2-hop and 3-hop questions. By qualitative analysis, we show that TransferNet has transparent and interpretable intermediate results.", "We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (ie, encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage--a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0. 9&v1. 0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.", "Nonnegative matrix factorization (NMF) has received considerable attention in image processing, computer vision, and patter recognition. An important variant of NMF is nonnegative graph embedding (NGE), which encodes the statistical or geometric information of data in the process of matrix factorization. The NGE offers a general framework for unsupervised/supervised settings. However, NGE-like algorithms often suffer from noisy data, unreliable graphs, and noisy labels, which are commonly encountered in real-world applications. To address these issues, in this paper, we first propose a robust nonnegative graph embedding (RNGE) framework, where the joint sparsity in both graph embedding and data reconstruction endues robustness to undesirable noises. Next, we present a robust seminonnegative graph embedding (RsNGE) framework, which only constrains the coefficient matrix to be nonnegative while\u00a0\u2026", "Attention module does not always help deep models learn causal features that are robust in any confounding context, eg, a foreground object feature is invariant to different backgrounds. This is because the confounders trick the attention to capture spurious correlations that benefit the prediction when the training and testing data are IID (identical & independent distribution); while harm the prediction when the data are OOD (out-of-distribution). The sole fundamental solution to learn causal attention is by causal intervention, which requires additional annotations of the confounders, eg, a\" dog\" model is learned within\" grass+ dog\" and\" road+ dog\" respectively, so the\" grass\" and\" road\" contexts will no longer confound the\" dog\" recognition. However, such annotation is not only prohibitively expensive, but also inherently problematic, as the confounders are elusive in nature. In this paper, we propose a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. In particular, multiple CaaMs can be stacked and integrated in conventional attention CNN and self-attention Vision Transformer. In OOD settings, deep models with CaaM outperform those without it significantly; even in IID settings, the attention localization is also improved by CaaM, showing a great potential in applications that require robust visual saliency. Codes are available at https://github. com/Wangt-CN/CaaM.", "Human group activities detection in multi-camera CCTV surveillance videos is a pressing demand on smart surveillance. Previous works on this topic are mainly based on camera topology inference that is hard to apply to real-world unconstrained surveillance videos. In this paper, we propose a new approach for multi-camera group activities detection. Our approach simultaneously exploits intra-camera and inter-camera contexts without topology inference. Specifically, a discriminative graphical model with hidden variables is developed. The intra-camera and inter-camera contexts are characterized by the structure of hidden variables. By automatically optimizing the structure, the contexts are effectively explored. Furthermore, we propose a new spatiotemporal feature, named vigilant area (VA), to characterize the quantity and appearance of the motion in an area. This feature is effective for group activity\u00a0\u2026", "Many achievements have been made on learning to hash for uni-modal and cross-modal retrieval. However, it is still an unsolved problem that how to directly and efficiently learn discriminative discrete hash codes for the multimedia retrieval, where both query and database samples are represented with heterogeneous multi-modal features. With this motivation, we propose a Fast Discrete Collaborative Multi-modal Hashing (FDCMH) method in this paper. We first propose an efficient collaborative multi-modal mapping that first transforms heterogeneous multi-modal features into the unified factors to exploit the complementarity of multi-modal features and preserve the semantic correlations in multiple modalities with linear computation and space complexity. Such shared factors also bridge the heterogeneous modality gap and remove the inter-modality redundancy. Further, we develop an asymmetric hashing\u00a0\u2026", "Vision and language understanding is one of the most fundamental and difficult tasks in Multimedia Intelligence. Simultaneously Visual Question Answering (VQA) is even more challenging since it requires complex  reasoning  steps to the correct answer. To achieve this, Neural Module Network (NMN) and its variants rely on parsing the natural language question into a module layout (i.e., a problem-solving program). In particular, this process follows a feedforward encoder-decoder pipeline: the encoder embeds the question into a static vector and the decoder generates the layout. However, we argue that such conventional encoder-decoder neglects the dynamic nature of question comprehension (i.e., we should attend to different words from step to step) and per-module intermediate results (i.e., we should discard module performing badly) in the reasoning steps. In this paper, we present a novel NMN, called Self\u00a0\u2026", "User and item features of side information are crucial for accurate recommendation. However, the large number of feature dimensions, e.g., usually larger than 10^7, results in expensive storage and computational cost. This prohibits fast recommendation especially on mobile applications where the computational resource is very limited. In this paper, we develop a generic feature-based recommendation model, called Discrete Factorization Machine (DFM), for fast and accurate recommendation. DFM binarizes the real-valued model parameters (e.g., float32) of every feature embedding into binary codes (e.g., boolean), and thus supports efficient storage and fast user-item score computation. To avoid the severe quantization loss of the binarization, we propose a convergent updating rule that resolves the challenging discrete optimization of DFM. Through extensive experiments on two real-world datasets, we show that 1) DFM consistently outperforms state-of-the-art binarized recommendation models, and 2) DFM shows very competitive performance compared to its real-valued version (FM), demonstrating the minimized quantization loss. This work is accepted by IJCAI 2018.", "This work presents a new interactive Content Based Image Retrieval (CBIR) scheme, termed Attribute Feedback (AF). Unlike traditional relevance feedback purely founded on low-level visual features, the Attribute Feedback system shapes users' information needs more precisely and quickly by collecting feedbacks on intermediate level semantic attributes. At each interactive iteration, AF first determines the most informative binary attributes for feedbacks, preferring the attributes that frequently (rarely) appear in current search results but are unlikely (likely) to be users' interest. The binary attribute feedbacks are then augmented by a new type of attributes, \"affinity attributes\", each of which is off-line learnt to describe the distance between user's envisioned image(s) and a retrieved image with respect to the corresponding affinity attribute. Based on the feedbacks on binary and affinity attributes, the images in corpus\u00a0\u2026", "We focus on learning open-vocabulary visual classifiers, which scale up to a large portion of natural language vocabulary (eg, over tens of thousands of classes). In particular, the training data are large-scale weakly labeled Web images since it is difficult to acquire sufficient well-labeled data at this category scale. In this paper, we propose a novel online learning paradigm towards this challenging task. Different from traditional N-way independent classifiers that generally fail to handle the extremely sparse and inter-related labels, our classifiers learn from continuous label embeddings discovered by collaboratively decomposing the sparse image-label matrix. Leveraging on the structure of the proposed collaborative learning formulation, we develop an efficient online algorithm that can jointly learn the label embeddings and visual classifiers. The algorithm can learn over 30,000 classes of 1,000 training images within 1 second on a standard GPU. Extensively experimental results on four benchmarks demonstrate the effectiveness of our method.", "The prevailing framework for solving referring expression grounding is based on a two-stage process: 1) detecting proposals with an object detector and 2) grounding the referent to one of the proposals. Existing two-stage solutions mostly focus on the grounding step, which aims to align the expressions with the proposals. In this paper, we argue that these methods overlook an obvious mismatch between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (ie, expression-agnostic), hoping that the proposals contain all right instances in the expression (ie, expression-aware). Due to this mismatch, current two-stage methods suffer from a severe performance drop between detected and ground-truth proposals. To this end, we propose Ref-NMS, which is the first method to yield expression-aware proposals at the first stage. Ref-NMS regards all nouns in the expression as critical objects, and introduces a lightweight module to predict a score for aligning each box with a critical object. These scores can guide the NMS operation to filter out the boxes irrelevant to the expression, increasing the recall of critical objects, resulting in a significantly improved grounding performance. Since Ref-NMS is agnostic to the grounding step, it can be easily integrated into any state-of-the-art two-stage method. Extensive ablation studies on several backbones, benchmarks, and tasks consistently demonstrate the superiority of Ref-NMS. Codes are available at: https://github. com/ChopinSharp/ref-nms.", "Existing Unsupervised Domain Adaptation (UDA) literature adopts the covariate shift and conditional shift assumptions, which essentially encourage models to learn common features across domains. However, due to the lack of supervision in the target domain, they suffer from the semantic loss: the feature will inevitably lose non-discriminative semantics in source domain, which is however discriminative in target domain. We use a causal view---transportability theory---to identify that such loss is in fact a confounding effect, which can only be removed by causal intervention. However, the theoretical solution provided by transportability is far from practical for UDA, because it requires the stratification and representation of the unobserved confounder that is the cause of the domain gap. To this end, we propose a practical solution: Transporting Causal Mechanisms (TCM), to identify the confounder stratum and representations by using the domain-invariant disentangled causal mechanisms, which are discovered in an unsupervised fashion. Our TCM is both theoretically and empirically grounded. Extensive experiments show that TCM achieves state-of-the-art performance on three challenging UDA benchmarks: ImageCLEF-DA, Office-Home, and VisDA-2017. Codes are available at https://github. com/yue-zhongqi/tcm.", "Visual dialog is a challenging task that requires the comprehension of the semantic dependencies among implicit visual and textual contexts. This task can refer to the relation inference in a graphical model with sparse contexts and unknown graph structure (relation descriptor), and how to model the underlying context-aware relation inference is critical. To this end, we propose a novel Context-Aware Graph (CAG) neural network. Each node in the graph corresponds to a joint semantic feature, including both object-based (visual) and history-related (textual) context representations. The graph structure (relations in dialog) is iteratively updated using an adaptive top-K message passing mechanism. Specifically, in every message passing step, each node selects the most K relevant nodes, and only receives messages from them. Then, after the update, we impose graph attention on all the nodes to get the final graph embedding and infer the answer. In CAG, each node has dynamic relations in the graph (different related K neighbor nodes), and only the most relevant nodes are attributive to the context-aware relational graph inference. Experimental results on VisDial v0. 9 and v1. 0 datasets show that CAG outperforms comparative methods. Visualization results further validate the interpretability of our method.", "In this paper, we focus on improving Event Extraction (EE) by incorporating visual knowledge with words and phrases from text documents. We first discover visual patterns from large-scale text-image pairs in a weakly-supervised manner and then propose a multimodal event extraction algorithm where the event extractor is jointly trained with textual features and visual patterns. Extensive experimental results on benchmark data sets demonstrate that the proposed multimodal EE method can achieve significantly better performance on event extraction: absolute 7.1% F-score gain on event trigger labeling and 8.5% F-score gain on event argument labeling.", "A good visual representation is an inference map from observations (images) to features (vectors) that faithfully reflects the hidden modularized generative factors (semantics). In this paper, we formulate the notion of\" good\" representation from a group-theoretic view using Higgins' definition of disentangled representation, and show that existing Self-Supervised Learning (SSL) only disentangles simple augmentation features such as rotation and colorization, thus unable to modularize the remaining semantics. To break the limitation, we propose an iterative SSL algorithm: Iterative Partition-based Invariant Risk Minimization (IP-IRM), which successfully grounds the abstract semantics and the group acting on them into concrete contrastive learning. At each iteration, IP-IRM first partitions the training samples into two subsets that correspond to an entangled group element. Then, it minimizes a subset-invariant contrastive loss, where the invariance guarantees to disentangle the group element. We prove that IP-IRM converges to a fully disentangled representation and show its effectiveness on various benchmarks. Codes are available at https://github. com/Wangt-CN/IP-IRM.", "Question answering (QA) models are well-known to exploit data bias, eg, the language prior in visual QA and the position bias in reading comprehension. Recent debiasing methods achieve good out-of-distribution (OOD) generalizability with a considerable sacrifice of the in-distribution (ID) performance. Therefore, they are only applicable in domains where the test distribution is known in advance. In this paper, we present a novel debiasing method called Introspective Distillation (IntroD) to make the best of both worlds for QA. Our key technical contribution is to blend the inductive bias of OOD and ID by introspecting whether a training sample fits in the factual ID world or the counterfactual OOD one. Experiments on visual QA datasets VQA v2, VQA-CP, and reading comprehension dataset SQuAD demonstrate that our proposed IntroD maintains the competitive OOD performance compared to other debiasing methods, while sacrificing little or even achieving better ID performance compared to the non-debiasing ones.", "Social media websites have become important information sharing platforms. The rapid development of social media platforms has led to increasingly large-scale social media data, which has shown remarkable societal and marketing values. There are needs to extract important events in live social media streams. However, microblogs event classification is challenging due to two facts, i.e., the short/conversational nature and the incompatible meanings between the text and the corresponding image in social posts, and the rapidly evolving contents. In this article, we propose to conduct event classification via deep learning and social tracking. First, we introduce a Multi-modal Multi-instance Deep Network (M2DN) for microblogs classification, which is able to handle the weakly labeled microblogs data oriented from the incompatible meanings inside microblogs. Besides predicting each microblogs as predefined\u00a0\u2026", "Social Curation Service (SCS) is a new type of emerging social media platform, where users can select, organize and keep track of multimedia contents they like. In this paper, we take advantage of this great opportunity and target at the very starting point in social media: user profiling, which supports fundamental applications such as personalized search and recommendation. As compared to other profiling methods in conventional Social Network Services (SNS), our work benefits from the two distinguishable characteristics of SCS: a) organized multimedia user-generated contents, and b) content-centric social network. Based on these two characteristics, we are able to deploy the state-of-the-art multimedia analysis techniques to establish content-based user profiles by extracting user preferences and their social relations. First, we automatically construct a content-based user preference ontology and learn the\u00a0\u2026", "In recent years, tremendous research endeavours have been dedicated to seeking effective visual representations for facilitating various multimedia applications, such as visual annotation and retrieval. Nonetheless, existing approaches can hardly achieve satisfactory performance due to the scarcity of fully exploring semantic properties of visual codes. In this paper, we present a novel visual coding approach, termed as hierarchical semantic visual coding (HSVC), to effectively encode visual objects (e.g., image and video) in a semantic hierarchy. Specifically, we first construct a semantic-enriched dictionary hierarchy, which is comprised of dictionaries corresponding to all concepts in a semantic hierarchy as well as their hierarchical semantic correlation. Moreover, we devise an on-line semantic coding model, which simultaneously 1) exploits the rich hierarchical semantic prior knowledge in the learned dictionary\u00a0\u2026", "The task of recognizing events from video has attracted a lot of attention in recent years. However, due to the complex nature of user-defined events, the use of purely audio- visual content analysis without domain knowledge has been found to be grossly inadequate. In this paper, we propose to construct a semantic-visual knowledge base to encode the rich event-centric concepts and their relationships from the well- established lexical databases, including FrameNet, as well as the concept-specific visual knowledge from ImageNet. Based on this semantic-visual knowledge bases, we design an effective system for video event recognition. Specifically, in order to narrow the semantic gap between the high-level complex events and low-level visual representations, we utilize the event-centric semantic concepts encoded in the knowledge base as the intermediate-level event representation, which offers both human\u00a0\u2026", "Photo aesthetic quality evaluation is a challenging task in multimedia and computer vision fields. Conventional approaches suffer from the following three drawbacks: 1) the deemphasized role of semantic content that is many times more important than low-level visual features in photo aesthetics; 2) the difficulty to optimally fuse low-level and high-level visual cues in photo aesthetics evaluation; and 3) the absence of a sequential viewing path in the existing models, as humans perceive visually salient regions sequentially when viewing a photo.  To solve these problems, we propose a new aesthetic descriptor that mimics humans sequentially perceiving visually/semantically salient regions in a photo. In particular, a weakly supervised learning paradigm is developed to project the local aesthetic descriptors (graphlets in this work) into a low-dimensional semantic space. Thereafter, each graphlet can be described by\u00a0\u2026", "Graph Convolutional Network (GCN) is an emerging technique that performs learning and reasoning on graph data. It operates feature learning on the graph structure, through aggregating the features of the neighbor nodes to obtain the embedding of each target node. Owing to the strong representation power, recent research shows that GCN achieves state-of-the-art performance on several tasks such as recommendation and linked document classification. Despite its effectiveness, we argue that existing designs of GCN forgo modeling cross features, making GCN less effective for tasks or data where cross features are important. Although neural network can approximate any continuous function, including the multiplication operator for modeling feature crosses, it can be rather inefficient to do so (i.e., wasting many parameters at the risk of overfitting) if there is no explicit design. To this end, we design a new\u00a0\u2026", "We propose scene graph auto-encoder (SGAE) that incorporates the language inductive bias into the encoder-decoder image captioning framework for more human-like captions. Intuitively, we humans use the inductive bias to compose collocations and contextual inferences in discourse. For example, when we see the relation \u201ca person on a bike\u201d, it is natural to replace \u201con\u201d with \u201cride\u201d and infer \u201ca person riding a bike on a road\u201d even when the \u201croad\u201d is not evident. Therefore, exploiting such bias as a language prior is expected to help the conventional encoder-decoder models reason as we humans and generate more descriptive captions. Specifically, we use the scene graph\u2014a directed graph (  ) where an object node is connected by adjective nodes and relationship nodes\u2014to represent the complex structural layout of both image (  ) and sentence (  ). In the language domain, we use SGAE to learn a dictionary set (  ) that\u00a0\u2026", "This article presents a novel attribute-augmented semantic hierarchy (A2SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in content-based image retrieval (CBIR). A2SH organizes semantic concepts into multiple semantic levels and augments each concept with a set of related attributes. The attributes are used to describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. An hierarchical semantic similarity function is learned to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedback on attributes and images. This feedback is then used to refine the search results based on A2SH. We use A2SH as a basis to develop a unified content-based image retrieval system. We conduct extensive\u00a0\u2026", "Weakly-supervised Temporal Action Localization (WTAL) aims to detect the action segments with only video-level action labels in training. The key challenge is how to distinguish the action of interest segments from the background, which is unlabelled even on the video-level. While previous works treat the background as\" curses\", we consider it as\" blessings\". Specifically, we first use causal analysis to point out that the common localization errors are due to the unobserved confounder that resides ubiquitously in visual recognition. Then, we propose a Temporal Smoothing PCA-based (TS-PCA) deconfounder, which exploits the unlabelled background to model an observed substitute for the unobserved confounder, to remove the confounding effect. Note that the proposed deconfounder is model-agnostic and non-intrusive, and hence can be applied in any WTAL method without model re-designs. Through extensive experiments on four state-of-the-art WTAL methods, we show that the deconfounder can improve all of them on the public datasets: THUMOS-14 and ActivityNet-1.3.", "Food recognition for user-uploaded images is crucial in visual diet tracking, an emerging application linking multimedia and healthcare domains. However, it is challenging due to the various visual appearances of food images. This is caused by different conditions when taking the photos, such as angles, distances, light conditions, food containers, and background scenes. To alleviate such a semantic gap, this paper presents a cross-modal alignment and transfer network (ATNet), which is motivated by the paradigm of learning using privileged information (LUPI). It additionally utilizes the ingredients in food images as an \"intelligent teacher\" in the training stage to facilitate cross-modal information passing. Specifically, ATNet first uses a pair of synchronized autoencoders to build the base image and ingredient channels for information flow. Subsequently, the information passing is enabled through a two-stage cross\u00a0\u2026", "Image tagging, also known as image annotation and image conception detection, has been extensively studied in the literature. However, most existing approaches can hardly achieve satisfactory performance owing to the deficiency and unreliability of the manually-labeled training data. In this paper, we propose a new image tagging scheme, termed social assisted media tagging (SAMT), which leverages the abundant user-generated images and the associated tags as the \"social assistance\" to learn the classifiers. We focus on addressing the following major challenges: (a) the noisy tags associated to the web images; and (b) the desirable robustness of the tagging model. We present a joint image tagging framework which simultaneously refines the erroneous tags of the web images as well as learns the reliable image classifiers. In particular, we devise a novel tag refinement module for identifying and\u00a0\u2026", "In this paper, we seek reasons for the two major failure cases in Semantic Segmentation (SS): 1) missing small objects or minor object parts, and 2) mislabeling minor parts of large objects as wrong classes. We have an interesting finding that Failure-1 is due to the underuse of detailed features and Failure-2 is due to the underuse of visual contexts. To help the model learn a better trade-off, we introduce several Self-Regulation (SR) losses for training SS neural networks. By\" self\", we mean that the losses are from the model per se without using any additional data or supervision. By applying the SR losses, the deep layer features are regulated by the shallow ones to preserve more details; meanwhile, shallow layer classification logits are regulated by the deep ones to capture more semantics. We conduct extensive experiments on both weakly and fully supervised SS tasks, and the results show that our approach consistently surpasses the baselines. We also validate that SR losses are easy to implement in various state-of-the-art SS models, eg, SPGNet and OCRNet, incurring little computational overhead during training and none for testing", "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, eg, word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.", "We propose DeepChannel, a robust, data-efficient, and interpretable neural model for extractive document summarization. Given any document-summary pair, we estimate a salience score, which is modeled using an attention-based deep neural network, to represent the salience degree of the summary for yielding the document. We devise a contrastive training strategy to learn the salience estimation network, and then use the learned salience score as a guide and iteratively extract the most salient sentences from the document as our generated summary. In experiments, our model not only achieves state-of-the-art ROUGE scores on CNN/Daily Mail dataset, but also shows strong robustness in the out-of-domain test on DUC2007 test set. Moreover, our model reaches a ROUGE-1 F-1 score of 39.41 on CNN/Daily Mail test set with merely 1/100 training set, demonstrating a tremendous data efficiency.", "Image captioning is one of the most challenging hallmark of AI, due to its complexity in visual and natural language understanding. As it is essentially a sequential prediction task, recent advances in image captioning use Reinforcement Learning (RL) to better explore the dynamics of word-by-word generation. However, existing RL-based image captioning methods mainly rely on a single policy network and reward function that does not well fit the multi-level (word and sentence) and multi-modal (vision and language) nature of the task. To this end, we propose a novel multi-level policy and reward RL framework for image captioning. It contains two modules: 1) Multi-Level Policy Network that can adaptively fuse the word-level policy and the sentence-level policy for the word generation; and 2) Multi-Level Reward Function that collaboratively leverages both vision-language reward and language-language reward to guide the policy. Further, we propose a guidance term to bridge the policy and the reward for RL optimization. Extensive experiments and analysis on MSCOCO and Flickr30k show that the proposed framework can achieve competing performances with respect to different evaluation metrics.", "Present language understanding methods have demonstrated extraordinary ability of recognizing patterns in texts via machine learning. However, existing methods indiscriminately use the recognized patterns in the testing phase that is inherently different from us humans who have counterfactual thinking, e.g., to scrutinize for the hard testing samples. Inspired by this, we propose a Counterfactual Reasoning Model, which mimics the counterfactual thinking by learning from few counterfactual samples. In particular, we devise a generation module to generate representative counterfactual samples for each factual sample, and a retrospective module to retrospect the model prediction by comparing the counterfactual and factual samples. Extensive experiments on sentiment analysis (SA) and natural language inference (NLI) validate the effectiveness of our method.", "We present VideoWhisper, a novel approach for unsupervised video representation learning. Based on the observation that the frame sequence encodes the temporal dynamics of a video (e.g., object movement and event evolution), we treat the frame sequential order as a self-supervision to learn video representations. Unlike other unsupervised video feature learning methods based on frame-level feature reconstruction that is sensitive to visual variance, VideoWhisper is driven by a novel video \u201csequence-to-whisper\u201d learning strategy. Specifically, for each video sequence, we use a prelearned visual dictionary to generate a sequence of high-level semantics, dubbed \u201cwhisper,\u201d which can be considered as the language describing the video dynamics. In this way, we model VideoWhisper as an end-to-end sequence-to-sequence learning model using attention-based recurrent neural networks. This model is\u00a0\u2026", "We address the problem of image hashing by learning binary codes from large and weakly supervised photo collections. Due to the explosive growth of user generated media on the Web, this problem is becoming critical for large-scale visual applications like image retrieval. While most existing hashing methods fail to address this challenge well, our method shows promising improvement due to the following two key advantages. First, we formulate a novel hashing objective that can effectively mine implicit weak supervision by collaborative filtering. Second, we propose a discrete hashing algorithm, offered with efficient optimization, to overcome the inferior optimizations in obtaining binary codes from real-valued solutions. In this way, our method can be considered as a weakly-supervised discrete hashing framework which jointly learns image semantics and their corresponding binary codes. Through training on one million weakly annotated images, our experimental results demonstrate that image retrieval using the proposed hashing method outperforms the other state-of-the-art ones on image and video benchmarks.", "Occluded faces-as a common scene in real life-have a significant negative impact on most face recognition systems. Existing methods try to remove the occlusions by a single-stage generative adversarial network (GAN), which is unaware of the occlusion and thus has difficulties in generalizing to a large variety of occlusion types, e.g., different objects at various positions. To this end, we propose the two-stage Occlusion-Aware GAN (OA-GAN), where the first GAN is for disentangling the occlusions, which will be served as the additional input of the second GAN for synthesizing the final de-occluded faces. In this way, our two-stage model can handle diverse occlusions in the wild and is naturally more explainable because of its awareness of the occluded objects. Extensive experiments on both synthetic and real-world datasets validate the superiority of the two-stage OAGAN design. Furthermore, by applying the\u00a0\u2026", "We address the overlooked unbiasedness in existing long-tailed classification methods: we find that their overall improvement is mostly attributed to the biased preference of\" tail\" over\" head\", as the test distribution is assumed to be balanced; however, when the test is as imbalanced as the long-tailed training data---let the test respect Zipf's law of nature---the\" tail\" bias is no longer beneficial overall because it hurts the\" head\" majorities. In this paper, we propose Cross-Domain Empirical Risk Minimization (xERM) for training an unbiased test-agnostic model to achieve strong performances on both test distributions, which empirically demonstrates that xERM fundamentally improves the classification by learning better feature representation rather than the\" head vs. tail\" game. Based on causality, we further theoretically explain why xERM achieves unbiasedness: the bias caused by the domain selection is removed by adjusting the empirical risks on the imbalanced domain and the balanced but unseen domain.", "Video Question & Answering (VideoQA), a task to answer questions in videos, involves rich spatio-temporal content (e.g., appearance and motion) and requires multi-hop reasoning process. However, existing methods usually deal with appearance and motion separately and fail to synchronize the attentions on appearance and motion features, neglecting two key properties of video QA: (1) appearance and motion features are usually concomitant and complementary to each other at time slice level. Some questions rely on joint representations of both kinds of features at some point in the video; (2) appearance and motion have different importance in multi-step reasoning. In this paper, we propose a novel Question- Aware Tube-Switch Network (TSN) for video question answering which contains (1) a Mix module to synchronously combine the appearance and motion representation at time slice level, achieving fine\u00a0\u2026", "Action recognition in videos, which contains many complex and semantic contents, is still a challenging task in computer vision research. In this paper, we propose a novel attention mechanism that leverages the gate system of Long Short Term Memory (LSTM) to compute the attention weights for action recognition. The proposed attention mechanism is embedded in a recurrent attention network that can explore the spatial-temporal relations between different local regions to concentrate important ones. For more accurate attention, we derive a new attention unit from the standard LSTM unit so as how important the local region is only depends on its input gate. Because of exploring spatial-temporal relations and using attention unit, our model can attend more accurately and thus achieve a better action recognition performance. We evaluate our proposed model on three datasets: UCF101, HMDB51 and Hollywood2\u00a0\u2026", "Non-negative data factorization has been widely used recently. However, existing techniques, such as Non-negative Graph Embedding (NGE), often suffer from noisy data, unreliable graphs, and noisy labels, which are commonly encountered in real-world applications. To address these issues, in this paper, we propose a Robust Non-negative Graph Embedding (RNGE) framework. The joint sparsity in both graph embedding and reconstruction endues the robustness of RNGE. We develop an elegant multiplicative updating solution that can solve RNGE efficiently and prove the convergence rigourously. RNGE is robust to unreliable graphs, as well as both sample and label noises in training data. Moreover, RNGE provides a general formulation such that all the algorithms unified with the graph embedding framework can be easily extended to obtain their robust non-negative solutions. We conduct extensive\u00a0\u2026", "We propose an Auto-Parsing Network (APN) to discover and exploit the input data's hidden tree structures for improving the effectiveness of the Transformer-based vision-language systems. Specifically, we impose a Probabilistic Graphical Model (PGM) parameterized by the attention operations on each self-attention layer to incorporate sparse assumption. We use this PGM to softly segment an input sequence into a few clusters where each cluster can be treated as the parent of the inside entities. By stacking these PGM constrained self-attention layers, the clusters in a lower layer compose into a new sequence, and the PGM in a higher layer will further segment this sequence. Iteratively, a sparse tree can be implicitly parsed, and this tree's hierarchical knowledge is incorporated into the transformed embeddings, which can be used for solving the target vision-language tasks. Specifically, we showcase that our APN can strengthen Transformer based networks in two major vision-language tasks: Captioning and Visual Question Answering. Also, a PGM probability-based parsing algorithm is developed by which we can discover what the hidden structure of input is during the inference.", "In our present society, Alzheimer\u2019s disease (AD) is the most common dementia form in elderly people and has been a big social health problem worldwide. In this paper, we propose a novel multi-view classification method based on l 2,p  -norm regularization for Alzheimer\u2019s Disease (AD) diagnosis. Unlike the previous l 2,1 -norm regularized methods using concatenated multi-view features, we further consider the intra-structure and inter-structure relations between features of different views and use a more flexible l 2,p  -norm regularization in our objective function. We also proposed a more suitable loss function to measure the loss between labels and predicted values for classification task. It experimentally demonstrated that this method enhances the performance of disease status classification, comparing to the state-of-the-art methods.", "Existing long-tailed classification (LT) methods only focus on tackling the class-wise imbalance that head classes have more samples than tail classes, but overlook the attribute-wise imbalance. In fact, even if the class is balanced, samples within each class may still be long-tailed due to the varying attributes. Note that the latter is fundamentally more ubiquitous and challenging than the former because attributes are not just implicit for most datasets, but also combinatorially complex, thus prohibitively expensive to be balanced. Therefore, we introduce a novel research problem: Generalized Long-Tailed classification (GLT), to jointly consider both kinds of imbalances. By \u201cgeneralized\u201d, we mean that a GLT method should naturally solve the traditional LT, but not vice versa. Not surprisingly, we find that most class-wise LT methods degenerate in our proposed two benchmarks: ImageNet-GLT and MSCOCO-GLT. We\u00a0\u2026", "We focus on the confounding bias between language and location in the visual grounding pipeline, where we find that the bias is the major visual reasoning bottleneck. For example, the grounding process is usually a trivial languagelocation association without visual reasoning, eg, grounding any language query containing sheep to the nearly central regions, due to that most queries about sheep have ground-truth locations at the image center. First, we frame the visual grounding pipeline into a causal graph, which shows the causalities among image, query, target location and underlying confounder. Through the causal graph, we know how to break the grounding bottleneck: deconfounded visual grounding. Second, to tackle the challenge that the confounder is unobserved in general, we propose a confounder-agnostic approach called: Referring Expression Deconfounder (RED), to remove the confounding bias. Third, we implement RED as a simple language attention, which can be applied in any grounding method. On popular benchmarks, RED improves various state-of-the-art grounding methods by a significant margin. Code is available at: https://github. com/JianqiangH/Deconfounded_VG.", "Transferring knowledge learned from the labeled source domain to the raw target domain for unsupervised domain adaptation (UDA) is essential to the scalable deployment of autonomous driving systems. State-of-the-art methods in UDA often employ a key idea: utilizing joint supervision signals from both source and target domains for self-training. In this work, we improve and extend this aspect. We present ConDA, a concatenation-based domain adaptation framework for LiDAR segmentation that: 1) constructs an intermediate domain consisting of fine-grained interchange signals from both source and target domains without destabilizing the semantic coherency of objects and background around the ego-vehicle; and 2) utilizes the intermediate domain for self-training. To improve the network training on the source domain and self-training on the intermediate domain, we propose an anti-aliasing regularizer and\u00a0\u2026", "Semantic image inpainting - a task to speculate and fill in large missing areas of a natural image, has shown exciting progress with the introduction of generative adversarial networks (GANs). But due to lack of sufficient understanding of semantic and spatial context, existing methods easily generate blurred boundary and distorted structure, which are inconsistent with the surrounding area. In this paper, we propose a new end-to-end framework named Single-shot Densely Connected Generative Network (SSDCGN), which generates visually realistic and semantically distinct pixels for the missing content by a battery of symmetric encoder-decoder groups. To maximize semantic extraction and realize precise spatial context localization, we involve a deeper densely skip connection in our network. Extensive experiments on Paris StreetView and ImageNet datasets show the superiority of our method.", "In this paper, we introduce a stochastic dynamics video infilling (SDVI) framework to generate frames between long intervals in a video. Our task differs from video interpolation which aims to produce transitional frames for a short interval between every two frames and increase the temporal resolution. Our task, namely video infilling, however, aims to infill long intervals with plausible frame sequences. Our framework models the infilling as a constrained stochastic generation process and sequentially samples dynamics from the inferred distribution. SDVI consists of two parts:(1) a bi-directional constraint propagation module to guarantee the spatial-temporal coherence among frames,(2) a stochastic sampling process to generate dynamics from the inferred distributions. Experimental results show that SDVI can generate clear frame sequences with varying contents. Moreover, motions in the generated sequence are realistic and able to transfer smoothly from the given start frame to the terminal frame.", "When we humans tell a long paragraph about an image, we usually first implicitly compose a mental \"script'' and then comply with it to generate the paragraph. Inspired by this, we render the modern encoder-decoder based image paragraph captioning model such ability by proposing Hierarchical Scene Graph Encoder-Decoder (HSGED) for generating coherent and distinctive paragraphs. In particular, we use the image scene graph as the \"script\" to incorporate rich semantic knowledge and, more importantly, the hierarchical constraints into the model. Specifically, we design a sentence scene graph RNN (SSG-RNN) to generate sub-graph level topics, which constrain the word scene graph RNN (WSG-RNN) to generate the corresponding sentences. We propose irredundant attention in SSG-RNN to improve the possibility of abstracting topics from rarely described sub-graphs and inheriting attention in WSG-RNN\u00a0\u2026", "We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., \u201clargest elephant standing behind baby elephant\u201d. This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context - visual attributes (e.g., \u201clargest\u201d, \u201cbaby\u201d) and relationships (e.g., \u201cbehind\u201d) that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Specifically, our framework exploits the reciprocal relation between the referent and\u00a0\u2026", "Neural discrete reasoning (NDR) has shown remarkable progress in combining deep models with discrete reasoning. However, we find that existing NDR solution suffers from large performance drop on hypothetical questions, eg \u201cwhat the annualized rate of return would be if the revenue in 2020 was doubled\u201d. The key to hypothetical question answering (HQA) is counterfactual thinking, which is a natural ability of human reasoning but difficult for deep models. In this work, we devise a Learning to Imagine (L2I) module, which can be seamlessly incorporated into NDR models to perform the imagination of unseen counterfactual. In particular, we formulate counterfactual thinking into two steps: 1) identifying the fact to intervene, and 2) deriving the counterfactual from the fact and assumption, which are designed as neural networks. Based on TAT-QA, we construct a very challenging HQA dataset with 8,283 hypothetical questions. We apply the proposed L2I to TAGOP, the state-of-the-art solution on TAT-QA, validating the rationality and effectiveness of our approach.", "Sentence auto-completion is an important feature that saves users many keystrokes in typing the entire sentence by providing suggestions as they type. Despite its value, the existing sentence auto-completion methods, such as query completion models, can hardly be applied to solving the object completion problem in sentences with the form of (subject, verb, object), due to the complex natural language description and the data deficiency problem. Towards this goal, we treat an SVO sentence as a three-element triple (subject, sentence pattern, object), and cast the sentence object completion problem as an element inference problem. These elements in all triples are encoded into a unified low-dimensional embedding space by our proposed TRANSFER model, which leverages the external knowledge base to strengthen the representation learning performance. With such representations, we can provide reliable\u00a0\u2026", "We address how to measure the information propagation probability between users given certain contents. In sharp contrast to existing works that oversimplify the propagation model as predefined distributions, our approach fundamentally attempts to answer why users are influenced (e.g., by content or relations) and whether the corresponding influential features (e.g., hidden factors) can be inferred from the propagation in the entire network. In particular, we propose a novel method to deeply learn the unified feature representations for both user pair and content, where the homogeneous feature similarity can be used to estimate the propagation probability between users with given content. The features are dubbed content\u2013social influential feature since we consider not only the content of the propagation information but also how it propagates over the social network. We design a fast asynchronous parallel\u00a0\u2026", "Feature representation for multimedia content is the key to the progress of many fundamental multimedia tasks. Although recent advances in deep feature learning offer a promising route towards these tasks, they are limited in application to domains where high-quality and large-scale training data are hard to obtain. In this paper, we propose a novel deep feature learning paradigm based on large, noisy and social image-tag collections, which can be acquired from the inexhaustible social multimedia content on the Web. Instead of learning features from high-quality image-label supervision, we propose to learn from the image-word semantic relations, in a way of seeking a unified image-word embedding space, where the pairwise feature similarities preserve the semantic relations in the original image-word pairs. We offer an easy-to-use implementation for the proposed paradigm, which is fast and compatible for\u00a0\u2026", "Recent studies have witnessed the effectiveness of L 2, 1-norm based methods on AD/MCI diagnosis. Nonetheless, most of them suffer from the following three main problems:(1) L 2, 1-norm based loss function does not take into account different distances between target labels and prediction values;(2) L 2, 1-norm based feature selection does not possess sufficient flexibility to adapt to different types of data sources and select more informative features;(3) intrinsic correlation between the processes of feature selection and classification (or regression) are inevitably ignored. In this paper, we propose a novel method which incorporates additional flexibility and adaptability by employing the more generalized L 2, p-norm based prediction loss function and L 2, q-norm based feature selection, as well as utilizes a joint model to perform feature selection and classification simultaneously. Besides, we introduce a\u00a0\u2026", "In this paper, we propose a new robust face recognition method through pixel selection. The method is based on the subspace assumption that a face can be represented by a linear combination in terms of the samples from the same subject. In order to obtain a reliable representation, only a subset of pixels with respect to smallest residuals are taken into the estimation. Outlying pixels which deviate from the linear model of the majority are removed using a robust estimation technique \u2014 least trimmed squares regression (LTS). By this method, the representation residual with each class is computed from only the clean data, which gives a more discriminant classification rule. The proposed algorithm provides a novel way to tackle the crucial occlusion problem in face recognition. Evaluation of the proposed algorithm is conducted on several public databases for the cases of both artificial and nature occlusions\u00a0\u2026", "Deep learning models often fit undesired dataset bias in training. In this paper, we formulate the bias using  causal inference , which helps us uncover the ever-elusive causalities among the key factors in training, and thus pursue the desired causal effect without the bias. We start from revisiting the process of building a visual recognition system, and then propose a structural causal model (SCM) for the key variables involved in dataset collection and recognition model: object, common sense, bias, context, and label prediction. Based on the SCM, one can observe that there are \u201cgood\u201d and \u201cbad\u201d biases. Intuitively, in the image where a car is driving on a high way in a desert, the \u201cgood\u201d bias denoting the common-sense context is the highway, and the \u201cbad\u201d bias accounting for the noisy context factor is the desert. We tackle this problem with a novel causal interventional training ( CIT ) approach, where we control the\u00a0\u2026", "Scene graphs connect individual objects with visual relationships. They serve as a comprehensive scene representation for downstream multimodal tasks. However, by exploring recent progress in Scene Graph Generation (SGG), we find that the performance of recent works is highly limited by the pairwise relationship modeling by naive feature concatenation. Such pairwise features lack sufficient object interaction due to the mis-aligned object parts, resulting in non-discriminative pairwise features for visual relationship prediction. For example, naive concatenated pairwise feature usually make the model fail to discriminate between  riding  and  feeding  for object pair  person  and  horse . To this end, we design a meta-architecture\u2014 learning-to-align \u2014 for dynamic object feature concatenation. We call our model:  Align R-CNN . Specifically, we introduce a novel attention-based multiple region alignment module\u00a0\u2026", "We formulate a practical yet challenging problem: General Partial Label Learning (GPLL). Compared to the traditional Partial Label Learning (PLL) problem, GPLL relaxes the supervision assumption from instance-level\u2014a label set partially labels an instance\u2014to group-level: 1) a label set partially labels a group of instances, where the within-group instance-label link annotations are missing, and 2) cross-group links are allowed\u2014instances in a group may be partially linked to the label set from another group. Such ambiguous group-level supervision is more practical in real-world scenarios as additional annotation on the instance-level is no longer required, e. g., face-naming in videos where the group consists of faces in a frame, labeled by a name set in the corresponding caption. In this paper, we propose a novel graph convolutional network (GCN) called Dual Bipartite Graph Autoencoder (DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the cross-group correlations to represent the instance groups as dual bipartite graphs: within-group and cross-group, which reciprocally complements each other to resolve the linking ambiguities. Second, we design a GCN autoencoder to encode and decode them, where the decodings are considered as the refined results. It is worth noting that DB-GAE is self-supervised and transductive, as it only uses the group-level supervision without a separate offline training stage. Extensive experiments on two real-world datasets demonstrate that DB-GAE significantly outperforms the best baseline over absolute 0.159 F1-score and 24.8% accuracy. We further offer analysis on various levels\u00a0\u2026", "Cross-media retrieval is an imperative approach to handle the explosive growth of multimodal data on the web. However, how to effectively uncover the correlations between multimodal data has been a barrier to successful retrieval of cross-media data. The traditional approaches learn the connection between multiple modalities by direct utilization of hand-crafted low-level heterogeneous features and the learned correlation are merely constructed in terms of high-level feature representation. To well exploit the intrinsic structures of multimodal data, it is essential to build up an interpretable correlation between multimodal data. In this paper, we propose a deep model to learn the high-level feature representation shared by multiple modalities for cross-media retrieval. We learn the discriminative high-level feature representation in a data-driven manner before faithfully encoding the multimodal correlations\u00a0\u2026", "We are interested in learning robust models from insufficient data, without the need for any externally pre-trained checkpoints. First, compared to sufficient data, we show why insufficient data renders the model more easily biased to the limited training environments that are usually different from testing. For example, if all the training swan samples are \u201cwhite\u201d, the model may wrongly use the \u201cwhite\u201d environment to represent the intrinsic class swan. Then, we justify that equivariance inductive bias can retain the class feature while invariance inductive bias can remove the environmental feature, leaving the class feature that generalizes to any environmental changes in testing. To impose them on learning, for equivariance, we demonstrate that any off-the-shelf contrastive-based self-supervised feature learning method can be deployed; for invariance, we propose a class-wise invariant risk minimization (IRM) that\u00a0\u2026", "Current deep learning\u2010based image manipulation localization methods achieve impressive performance when rich spatial features and information are fully utilized. However, most of them suffer from the irrelevance of semantic awareness when identifying various manipulation categories. This leads to false alarms on recognizing forged regions. In this paper, we propose a Progressively\u2010Refined Neural Network (PR\u2010Net), to localize the tampered regions progressively under a coarse\u2010to\u2010fine workflow. Specifically, PR\u2010Net is composed of a Feature Extractor (FE) that captures feature intrinsic correlations and a Mask Generation Module (MGM) with three refining generators. The FE takes a CNN to extract the image features and introduces an attention mechanism Convolution Block Attention Module (CBAM) to suppress the image content and guide the extractor in exploring the inconsistencies between the\u00a0\u2026", "We propose a novel method for video object proposal to generate sequences of bounding boxes for each object candidate in videos, namely object trajectory proposals. Unlike the image-based methods that produce object proposals independently in each video frame, our method generates temporally consistent proposals in the form of object trajectories, which is crucial for subsequent analysis of object appearance and motion characteristics. Given a video, we extract motion seeds through estimating the outliers of global motion, and generate moving object trajectory proposals from the seeds. By ignoring the motion outliers, we consistently sample bounding boxes with pruning to form static object trajectory proposals. Finally, we rank both the moving and static object trajectory proposals under a unified scoring mechanism. The experimental results show that our method can effectively generate object trajectory\u00a0\u2026", "Conventional de-noising methods rely on the assumption that all samples are independent and identically distributed, so the resultant classifier, though disturbed by noise, can still easily identify the noises as the outliers of training distribution. However, the assumption is unrealistic in large-scale data that is inevitably long-tailed. Such imbalanced training data makes a classifier less discriminative for the tail classes, whose previously \u201ceasy\u201d noises are now turned into \u201chard\u201d ones\u2014they are almost as outliers as the clean tail samples. We introduce this new challenge as Noisy Long-Tailed Classification (NLT). Not surprisingly, we find that most de-noising methods fail to identify the hard noises, resulting in significant performance drop on the three proposed NLT benchmarks: ImageNet-NLT, Animal10-NLT, and Food101-NLT. To this end, we design an iterative noisy learning framework called Hard-to-Easy (H2E). Our\u00a0\u2026", "The cross-domain image captioning, which is trained on a source domain and generalized to other domains, usually faces the large domain shift problem. Although prior work has attempted to leverage both paired source and unpaired target data to minimize this shift, the performance is still unsatisfactory. One main reason lies in the large discrepancy in language expression between two domains, where diverse language styles are adopted to describe an image from different views, resulting in different semantic descriptions for an image. To tackle this problem, this paper proposes a Style-based Cross-domain Image Captioner (SCIC) which incorporates the discriminative style information into the encoder-decoder framework, and interprets an image as a special sentence according to external style instructions. Technically, we design a novel \u201cInstruction-based LSTM\u201d, which adds the instruct gate to collect a style\u00a0\u2026", "Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naive Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution. Different from existing SSL solutions that overlook the role of \"class\" in causing the non-randomness, e.g., users are more likely to label popular classes, we explicitly incorporate \"class\" into SSL. Our method is three-fold: 1) We propose Class-Aware Propensity (CAP) that exploits the unlabeled data to train an improved classifier using the biased labeled data. 2) To encourage rare class training, whose model is low-recall but high-precision that discards too many pseudo-labeled data, we propose Class-Aware Imputation (CAI) that dynamically decreases (or increases) the pseudo-label assignment threshold for rare (or frequent) classes. 3) Overall, we integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for training an unbiased SSL model. Under various MNAR settings and ablations, our method not only significantly outperforms existing baselines but also surpasses other label bias removal SSL methods. Please check our code at: https://github.com/JoyHuYY1412/CADR-FixMatch.", "In this paper, we introduce a novel deep semantic indexing method, a.k.a. captioning, for image database. Our method can automatically generate a natural language caption describing an image as a semantic reference to index the image. Specifically, we use a convolutional localization network to generate a pool of region proposals from an image, and then leverage the visual attention mechanism to sequentially generate the meaningful language words. Compared with previous methods, our approach can efficiently generate compact captions, which can guarantee higher level of semantic indexing for image database. We evaluate our approach on two widely-used benchmark datasets: Flickr30K, and MS COCO. Experimental results across various evaluation metrics show the superiority of our approach as compared with other visual attention based approaches.", "Query suggestion is an effective solution to help users deliver their search intent. While many query suggestion approaches have been proposed for test-based image retrieval with query-by-keywords, query suggestion for content-based image retrieval (CBIR) with query-by-example (QBE) has been seldom studied. QBE usually suffers from the \"intention gap\" problem, especially when the user fails to get an appropriate query image to express his search intention precisely. In this paper, we propose a novel query suggestion scheme named Visual Query Attributes Suggestion (VQAS) for image search with QBE. Given a query image, informative attributes are suggested to the user as complements to the query. These attributes reflect the visual properties and key components of the query. By selecting some suggested attributes, the user can provide more precise search intent which is not captured by the query\u00a0\u2026", "Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary predictions trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatio-temporal motion patterns of the subject-object compositions. Without bells and whistles, our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompts. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD.", "Knowledge distillation (KD) is essentially a process of transferring a teacher model's behavior, eg, network response, to a student model. The network response serves as additional supervision to formulate the machine domain, which uses the data collected from the human domain as a transfer set. Traditional KD methods hold an underlying assumption that the data collected in both human domain and machine domain are both independent and identically distributed (IID). We point out that this naive assumption is unrealistic and there is indeed a transfer gap between the two domains. Although the gap offers the student model external knowledge from the machine domain, the imbalanced teacher knowledge would make us incorrectly estimate how much to transfer from teacher to student per sample on the non-IID transfer set. To tackle this challenge, we propose Inverse Probability Weighting Distillation (IPWD) that estimates the propensity of a training sample belonging to the machine domain, and assigns its inverse amount to compensate for under-represented samples. Experiments on CIFAR-100 and ImageNet demonstrate the effectiveness of\\ours~ for both two-stage distillation and one-stage self-distillation.", "Out-Of-Distribution generalization (OOD) is all about learning invariance against environmental changes. If the context (In this paper, the word \u201ccontext\u201d denotes any class-agnostic attributes such as color, texture and background. The formal definition can be found in Appendix, A.2.) in every class is evenly distributed, OOD would be trivial because the context can be easily removed due to an underlying principle: class is invariant to context. However, collecting such a balanced dataset is impractical. Learning on imbalanced data makes the model bias to context and thus hurts OOD. Therefore, the key to OOD is context balance. We argue that the widely adopted assumption in prior work\u2014the context bias can be directly annotated or estimated from biased class prediction\u2014renders the context incomplete or even incorrect. In contrast, we point out the ever-overlooked other side of the above principle: context is also\u00a0\u2026", null, "Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the binary anomaly label is only given on the video level, but the output requires snippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing in WSVAD. However, MIL is notoriously known to suffer from many false alarms because the snippet-level detector is easily biased towards the abnormal snippets with simple context, confused by the normality with the same bias, and missing the anomaly with a different pattern. To this end, we propose a new MIL framework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve WSVAD. At each MIL training iteration, we use the current detector to divide the samples into two groups with different context biases: the most confident abnormal/normal snippets and the rest ambiguous ones. Then, by seeking the invariant features across the two sample groups, we can remove the variant context biases. Extensive experiments on benchmarks UCF-Crime and TAD demonstrate the effectiveness of our UMIL. Our code is provided at https://github. com/ktr-hubrt/UMIL.", "NICO challenge of out-of-distribution (OOD) generalization for image recognition features two tracks: common context generalization and hybrid context generalization, based on a newly proposed OOD dataset called NICO. Strong distribution shifts between the training and test data are constructed for both tracks. In contrast to the current OOD generalization benchmarks where models are tested on a single domain, NICO challenge tests models on multiple domains for a thorough and comprehensive evaluation. To prevent the leakage of target context knowledge and encourage novel and creative solutions instead of leveraging additional training data, we prohibit the model initialization with pretrained parameters, which is not noticed in the previous benchmarks for OOD generalization. To ensure the random initialization of models, we verify and retrain models from all top-10 teams and test them on the private\u00a0\u2026", null, "We present a surprisingly easy-to-use video browser for helping users to pinpoint a specific video shot in mind, within a long video. At each interactive iteration, the only user effort required is to click 1 shot, which most visually relates to the user\u2019s mental target, out of 8 displayed shots. Then, the system updates the browsing model and display another 8 shots for the next iteration. The proposed system is underpinned by a theoretically-sound Bayesian framework that maintains the probabilities of all the video shots segmented from the long video. This framework guarantees that we can find the target shot out of around 1-h video within 3\u20135 iterations. We believe that our system will perform well in the Video Broswer Showdown game of MMM 2016.", "With the explosive increasing of web image data, image annotation has become a critical research issue for image semantic index and search. In this work, we propose a novel model, termed as multi-view semi-supervised learning (MVSSL), for robust image annotation task. Specifically, we exploit both labeled images and unlabeled images to uncover the intrinsic data structural information. Meanwhile, to comprehensively describe an individual datum, we take advantage of the correlated and complemental information derived from multiple facets of image data (i.e., multiple views or features). We devise a robust pair-wise constraint on outcomes of different views to achieve annotation consistency. Furthermore, we integrate a robust classifier learning component via l2,1 loss, which can provide effective noise identification power during the learning process. Finally, we devise an efficient iterative algorithm to solve\u00a0\u2026", "Despite their success in real data synthesis, diffusion models (DMs) often suffer from slow and costly training and sampling issues, limiting their broader applications. To mitigate this, we propose a Fast Diffusion Model (FDM) which improves the diffusion process of DMs from a stochastic optimization perspective to speed up both training and sampling. Specifically, we first find that the diffusion process of DMs accords with the stochastic optimization process of stochastic gradient descent (SGD) on a stochastic time-variant problem. Note that momentum SGD uses both the current gradient and an extra momentum, achieving more stable and faster convergence. We are inspired to introduce momentum into the diffusion process to accelerate both training and sampling. However, this comes with the challenge of deriving the noise perturbation kernel from the momentum-based diffusion process. To this end, we frame the momentum-based process as a Damped Oscillation system whose critically damped state -- the kernel solution -- avoids oscillation and thus has a faster convergence speed of the diffusion process. Empirical results show that our FDM can be applied to several popular DM frameworks, e.g. VP, VE, and EDM, and reduces their training cost by about 50% with comparable image synthesis performance on CIFAR-10, FFHQ, and AFHQv2 datasets. Moreover, FDM decreases their sampling steps by about  to achieve similar performance under the same deterministic samplers. The code is available at https://github.com/sail-sg/FDM.", "We present a new paradigm for fine-tuning large-scale visionlanguage pre-trained models on downstream task, dubbed Prompt Regularization (ProReg). Different from traditional fine-tuning which easily overfits to the downstream task data, ProReg uses the prediction by prompting the pretrained model to regularize the fine-tuning. The motivation is: by prompting the large model \"a photo of a [CLASS]\", the fil-lin answer is only dependent on the pretraining encyclopedic knowledge while independent of the task data distribution, which is usually biased. Specifically, given a training sample prediction during fine-tuning, we first calculate its KullbackLeibler loss of the prompt prediction and Cross-Entropy loss of the ground-truth label, and then combine them with a proposed sample-wise adaptive trade-off weight, which automatically adjusts the transfer between the pretrained and downstream domains. On various out-of-distribution benchmarks, we show the consistently strong performance of ProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning, and other state-of-the-art methods.", "Humans tend to decompose a sentence into different parts like sth do sth at someplace and then fill each part with certain content. Inspired by this, we follow the principle of modular design to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the widely used neural module networks in VQA, where the language (i.e., question) is fully observable, the task of collocating visual-linguistic modules is more challenging. This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: (1) distinguishable module design\u2014four modules in the encoder including one linguistic module for function words and three visual modules for different content words (i.e., noun, adjective, and verb) and\u00a0\u2026", "Sentence embedding is an effective feature representation for most deep learning-based NLP tasks. One prevailing line of methods is using recursive latent tree-structured networks to embed sentences with task-specific structures. However, existing models have no explicit mechanism to emphasize taskinformative words in the tree structure. To this end, we propose an Attentive Recursive Tree model (AR-Tree), where the words are dynamically located according to their importance in the task. Specifically, we construct the latent tree for a sentence in a proposed important-first strategy, and place more attentive words nearer to the root; thus, AR-Tree can inherently emphasize important words during the bottomup composition of the sentence embedding. We propose an end-to-end reinforced training strategy for AR-Tree, which is demonstrated to consistently outperform, or be at least comparable to, the state-of-the-art sentence embedding methods on three sentence understanding tasks.", "Location (or equivalently, \u201cvenue\u201d) is a crucial facet of user generated images in social media (aka. social images) to describe the events of people\u2019s daily lives. While many existing works focus on predicting the venue category based on image content, we tackle the grand challenge of predicting the specific venue of a social image. Simply using the visual content of a social image is insufficient for this purpose due its high diversity. In this work, we leverage users\u2019 check-in histories in location-based social networks (LBSNs), which contain rich temporal movement patterns, to complement the limitations of using visual signals alone. In particular, we explore the transition patterns on successive check-ins and periodical patterns on venue categories from users\u2019 check-in behaviors in Foursquare. For example, users tend to check-in\u00a0to cinemas nearby after having meals at a restaurant (transition patterns), and\u00a0\u2026", "In sharp contrast to the traditional category/subcategory level image retrieval, product image search aims to find the images containing the exact same product. This is a challenging problem because in addition to being robust under different imaging conditions such as varying viewpoints and illumination changes, the features should also be able to distinguish the specific product among many similar products. Consequently, it is important to utilize a large dataset, containing many product classes, to learn a strongly discriminative representation. Building such a dataset requires laborious manual annotation. Toward learning fine-grained, robust, discriminative features for product image search, we present a novel paradigm that can construct the required dataset without any human annotation. Unlike other fine-grained recognition works that rely on high-quality annotated datasets and are very narrowly focused on a\u00a0\u2026", "We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance.", "Generative AI has made significant strides in computer vision, particularly in image/video synthesis conditioned on text descriptions. Despite the advancements, it remains challenging especially in the generation of human-centric content such as dance synthesis. Existing dance synthesis methods struggle with the gap between synthesized content and real-world dance scenarios. In this paper, we define a new problem setting: Referring Human Dance Generation, which focuses on real-world dance scenarios with three important properties: (i) Faithfulness: the synthesis should retain the appearance of both human subject foreground and background from the reference image, and precisely follow the target pose; (ii) Generalizability: the model should generalize to unseen human subjects, backgrounds, and poses; (iii) Compositionality: it should allow for composition of seen/unseen subjects, backgrounds, and poses from different sources. To address these challenges, we introduce a novel approach, DISCO, which includes a novel model architecture with disentangled control to improve the faithfulness and compositionality of dance synthesis, and an effective human attribute pre-training for better generalizability to unseen humans. Extensive qualitative and quantitative results demonstrate that DISCO can generate high-quality human dance images and videos with diverse appearances and flexible motions. Code, demo, video and visualization are available at: https://disco-dance.github.io/.", "Novel Class Discovery (NCD) aims to discover unknown classes without any annotation, by exploiting the transferable knowledge already learned from a base set of known classes. Existing works hold an impractical assumption that the novel class distribution prior is uniform, yet neglect the imbalanced nature of real-world data. In this paper, we relax this assumption by proposing a new challenging task: distribution-agnostic NCD, which allows data drawn from arbitrary unknown class distributions and thus renders existing methods useless or even harmful. We tackle this challenge by proposing a new method, dubbed\" Bootstrapping Your Own Prior (BYOP)\", which iteratively estimates the class prior based on the model prediction itself. At each iteration, we devise a dynamic temperature technique that better estimates the class prior by encouraging sharper predictions for less-confident samples. Thus, BYOP obtains more accurate pseudo-labels for the novel samples, which are beneficial for the next training iteration. Extensive experiments show that existing methods suffer from imbalanced class distributions, while BYOP outperforms them by clear margins, demonstrating its effectiveness across various distribution scenarios.", "Seas of videos are uploaded daily with the popularity of social channels; thus, retrieving the most related video contents with user textual queries plays a more crucial role. Most methods consider only one joint embedding space between global visual and textual features without considering the local structures of each modality. Some other approaches consider multiple embedding spaces consisting of global and local features separately, ignoring rich inter-modality correlations. We propose a novel mixture-of-expert transformer RoME that disentangles the text and the video into three levels; the roles of spatial contexts, temporal contexts, and object contexts. We utilize a transformer-based attention mechanism to fully exploit visual and text embeddings at both global and local levels with mixture-of-experts for considering inter-modalities and structures' correlations. The results indicate that our method outperforms the state-of-the-art methods on the YouCook2 and MSR-VTT datasets, given the same visual backbone without pre-training. Finally, we conducted extensive ablation studies to elucidate our design choices.", null, "Product image search aims to retrieve similar product images based on a query image. While deep learning based features work well in retrieving images of the same category (e.g. \u201csearching for T-shirts from all the clothing images\u201d), they perform poorly when retrieving variants of images within the same category (e.g. \u201csearching for uniform of Chelsea football club from all T-shirts image\u201d), since it requires fine grained matching on image details. In this paper, we present a spatial quantization approach that utilizes spatial pyramid pooling (SPP) and vector of locally aggregated descriptors (VLAD) to extract more discriminative features for style-aware product search. By using the proposed spatial quantization, spatial information is encoded into the image feature to improve the fine grained product image search. Finally, the experiments on a large scale real world dataset provided by Alibaba large-scale image search\u00a0\u2026", "Recent years have witnessed the effectiveness and efficiency of learning-based hashing methods which generate short binary codes preserving the Euclidean similarity in the original space of high dimension. However, because of their complexities and out-of-sample problems, most of methods are not appropriate for embedding of large-scale datasets. In this paper, we have proposed a new supervised hashing method to generate class-specific hash codes, which uses an inductive process based on the Inductive Manifold Hashing (IMH) model and leverage supervised information into hash codes generation to address these difficulties and boost the hashing quality. It is experimentally shown that this method gets excellent performance of image classification and retrieval on large-scale multimedia dataset just with very short binary codes.", "An image is worth of thousand words. Automatic Web image annotation is a practical and effective way for both Web image retrieval and image understanding. However, current annotation techniques are very difficult to get natural language interpretation for images such as ldquopandas eat bamboordquo. In this paper, we proposed an approach to interpret image semantics through semi-supervised mining annotated words. The idea in this approach mainly consists of three parts: at first, the visibility of annotated words of target image is calculated by semi-supervised learning approach from the landmark words in WordNet; then the annotated words are used as queries to retrieve matched Web pages; at last, the meaningful sentences in the matched Web pages are ranked as the interpretation of target image by semi-supervised learning approach. Experiments conducted on real-world Web images demonstrate the\u00a0\u2026", "Text-to-video (T2V) synthesis has gained increasing attention in the community, in which the recently emerged diffusion models (DMs) have promisingly shown stronger performance than the past approaches. While existing state-of-the-art DMs are competent to achieve high-resolution video generation, they may largely suffer from key limitations (e.g., action occurrence disorders, crude video motions) with respect to the intricate temporal dynamics modeling, one of the crux of video synthesis. In this work, we investigate strengthening the awareness of video dynamics for DMs, for high-quality T2V generation. Inspired by human intuition, we design an innovative dynamic scene manager (dubbed as Dysen) module, which includes (step-1) extracting from input text the key actions with proper time-order arrangement, (step-2) transforming the action schedules into the dynamic scene graph (DSG) representations, and (step-3) enriching the scenes in the DSG with sufficient and reasonable details. Taking advantage of the existing powerful LLMs (e.g., ChatGPT) via in-context learning, Dysen realizes (nearly) human-level temporal dynamics understanding. Finally, the resulting video DSG with rich action scene details is encoded as fine-grained spatio-temporal features, integrated into the backbone T2V DM for video generating. Experiments on popular T2V datasets suggest that our framework consistently outperforms prior arts with significant margins, especially in the scenario with complex actions. Project page at https://haofei.vip/Dysen-VDM", "The prevailing framework for matching multimodal inputs is based on a two-stage process: (1) detecting proposals with an object detector and (2) matching text queries with proposals. Existing two-stage solutions mostly focus on the matching step. In this article, we argue that these methods overlook an obvious mismatch between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (i.e., query-agnostic), hoping that the proposals contain all instances mentioned in the text query (i.e., query-aware). Due to this mismatch, chances are that proposals relevant to the text query are suppressed during the filtering process, which in turn bounds the matching performance. To this end, we propose VL-NMS, which is the first method to yield query-aware proposals at the first stage. VL-NMS regards all mentioned instances as critical objects and introduces a lightweight\u00a0\u2026", "In recent years, there has been a surge of interest and rapid development in large-scale pre-training due to the explosive growth of both data and model parameters. Large-scale training has achieved impressive performance milestones across a wide range of practical problems, including natural language processing, computer vision, recommendation systems, robotics, and other basic research areas like bioinformatics. Different from early nonneural models and small models that rely heavily on hand-crafted features, statistical methods, and accurate human annotations, neural models can automatically learn low-level distributed representations and high-level latent semantic information from data. However, the huge numbers of parameters in deep neural models may lead to over-fitting and poor generalization, which has prompted massive efforts to exploit how to pre-train large-scale models on large-scale data\u00a0\u2026", "This study explores the concept of equivariance in vision-language foundation models (VLMs), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. Unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. This allows VLMs to generalize better to nuanced and unseen multimodal compositions. However, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. For example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? To this end, we propose EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we present a new challenging benchmark EqBen. Compared to the existing evaluation sets, EqBen is the first to focus on \"visual-minimal change\". Extensive experiments show the lack of equivariance in current VLMs and validate the effectiveness of EqSim. Code is available at \\url{https://github.com/Wangt-CN/EqBen}.", "Semantic Scene Completion (SSC) transforms an image of single-view depth and/or RGB 2D pixels into 3D voxels, each of whose semantic labels are predicted. SSC is a well-known ill-posed problem as the prediction model has to\" imagine\" what is behind the visible surface, which is usually represented by Truncated Signed Distance Function (TSDF). Due to the sensory imperfection of the depth camera, most existing methods based on the noisy TSDF estimated from depth values suffer from 1) incomplete volumetric predictions and 2) confused semantic labels. To this end, we use the ground-truth 3D voxels to generate a perfect visible surface, called TSDF-CAD, and then train a\" cleaner\" SSC model. As the model is noise-free, it is expected to focus more on the\" imagination\" of unseen voxels. Then, we propose to distill the intermediate\" cleaner\" knowledge into another model with noisy TSDF input. In particular, we use the 3D occupancy feature and the semantic relations of the\" cleaner self\" to supervise the counterparts of the\" noisy self\" to respectively address the above two incorrect predictions. Experimental results validate that the proposed method improves the noisy counterparts with 3.1% IoU and 2.2% mIoU for measuring scene completion and SSC, and also achieves new state-of-the-art accuracy on the popular NYU dataset. The code is available at https://github. com/fereenwong/CleanerS.", "We tackle the data scarcity challenge in few-shot point cloud recognition of 3D objects by using a joint prediction from a conventional 3D model and a well-pretrained 2D model. Surprisingly, such an ensemble, though seems trivial, has hardly been shown effective in recent 2D-3D models. We find out the crux is the less effective training for the\" joint hard samples\", which have high confidence prediction on different wrong labels, implying that the 2D and 3D models do not collaborate well. To this end, our proposed invariant training strategy, called INVJOINT, does not only emphasize the training more on the hard samples, but also seeks the invariance between the conflicting 2D and 3D ambiguous predictions. INVJOINT can learn more collaborative 2D and 3D representations for better ensemble. Extensive experiments on 3D shape classification with widely-adopted ModelNet10/40, ScanObjectNN and Toys4K, and shape retrieval with ShapeNet-Core validate the superiority of our INVJOINT.", "Deep neural networks for video action recognition easily learn to utilize shortcut static features, such as background and objects instead of motion features. This results in poor generalization to atypical videos such as soccer playing on concrete surfaces (instead of soccer fields). However, due to the rarity of out-of-distribution (OOD) data, quantitative evaluation of static bias remains a difficult task. In this paper, we synthesize new sets of benchmarks to evaluate static bias of action representations, including SCUB for static cues in the background, and SCUF for static cues in the foreground. Further, we propose a simple yet effective video data augmentation technique, StillMix, that automatically identifies bias-inducing video frames; unlike similar augmentation techniques, StillMix does not need to enumerate or precisely segment biased content. With extensive experiments, we quantitatively compare and analyze existing action recognition models on the created benchmarks to reveal their characteristics. We validate the effectiveness of StillMix and show that it improves TSM (Lin, Gan, and Han 2021) and Video Swin Transformer (Liu et al. 2021) by more than 10% of accuracy on SCUB for OOD action recognition.", "Extracting class activation maps (CAM) is a key step for weakly-supervised semantic segmentation (WSSS). The CAM of convolution neural networks fails to capture long-range feature dependency on the image and result in the coverage on only foreground object parts, i.e., a lot of false negatives. An intuitive solution is ``coupling'' the CAM with the long-range attention matrix of visual transformers (ViT) We find that the direct ``coupling'', e.g., pixel-wise multiplication of attention and activation, achieves a more global coverage (on the foreground), but unfortunately goes with a great increase of false positives, i.e., background pixels are mistakenly included. This paper aims to tackle this issue. It proposes a new method to couple CAM and Attention matrix in a probabilistic Diffusion way, and dub it AD-CAM. Intuitively, it integrates ViT attention and CAM activation in a conservative and convincing way. Conservative is achieved by refining the attention between a pair of pixels based on their respective attentions to common neighbors, where the intuition is two pixels having very different neighborhoods are rarely dependent, i.e., their attention should be reduced. Convincing is achieved by diffusing a pixel's activation to its neighbors (on the CAM) in proportion to the corresponding attentions (on the AM). In experiments, our results on two challenging WSSS benchmarks PASCAL VOC and MS~COCO show that AD-CAM as pseudo labels can yield stronger WSSS models than the state-of-the-art variants of CAM.", "Video browsing describes an interactive process where the user wants to find a target shot in a long video. Therefore, it is crucial for a video browsing system to be fast and accurate with minimum user effort. Previously, video browsing systems suffered from either lots of user efforts or heavy computation cost, which makes them less effective. We propose an RNN-based \\textit{intention-aware} system to overcome these limitations. At each interactive round, the user only needs to select one of the displayed shots that is most visually similar to her mental target and then the user's choice will further tailor the search to the target. The search model update merely requires vector inner products which means the system is highly responsive.", "Video browsing describes an interactive process where users want to find a target shot in a long video. Therefore, it is crucial for a video browsing system to be fast and accurate with minimum user effort. In sharp contrast to traditional Relevance Feedback (RF), we propose a novel paradigm for fast video browsing dubbed Mental Visual Indexing (MVI). At each interactive round, the user only needs to select one of the displayed shots that is most visually similar to her mental target and then the user's choice will further tailor the search to the target. The search model update given a user feedback only requires vector inner products, which makes MVI highly responsive. MVI is underpinned by a sequence model in terms of Recurrent Neural Network (RNN), which is trained by automatically generated shot sequences from a rigorous Bayesian framework, which simulates user feedback process. Experimental results on\u00a0\u2026", "Text-video retrieval contains various challenges, including biases coming from diverse sources. We highlight some of them supported by illustrations to open a discussion. Besides, we address one of the biases, frame length bias, with a simple method which brings a very incremental but promising increase. We conclude with future directions.", "Instructional videos are very useful for completing complex daily tasks, which naturally contain abundant clip-narration pairs. Existing works for procedure understanding are keen on pretraining various video-language models with these pairs and then fine-tuning downstream classifiers and localizers in predetermined category space. These video-language models are proficient at representing short-term actions, basic objects, and their combinations, but they are still far from understanding long-term procedures. In addition, the predetermined procedure category faces the problem of combination disaster and is inherently inapt to unseen procedures. Therefore, we propose a novel compositional prompt learning (CPL) framework to understand long-term procedures by prompting short-term video-language models and reformulating several classical procedure understanding tasks into general video-text matching\u00a0\u2026", "Aligning objects with words plays a critical role in Image-Language BERT (IL-BERT) and Video-Language BERT (VDL-BERT). Different from the image case where an object covers some spatial patches, an object in a video usually appears as an object trajectory, i.e., it spans over a few spatial but longer temporal patches and thus contains abundant spatiotemporal contexts. However, modern VDL-BERTs neglect this trajectory characteristic that they usually follow IL-BERTs to deploy the patch-to-word (P2W) attention while such attention may over-exploit trivial spatial contexts and neglect significant temporal contexts. To amend this, we propose a novel TW-BERT to learn Trajectory-Word alignment for solving video-language tasks. Such alignment is learned by a newly designed trajectory-to-word (T2W) attention. Besides T2W attention, we also follow previous VDL-BERTs to set a word-to-patch (W2P) attention in the cross-modal encoder. Since T2W and W2P attentions have diverse structures, our cross-modal encoder is asymmetric. To further help this asymmetric cross-modal encoder build robust vision-language associations, we propose a fine-grained ``align-before-fuse'' strategy to pull close the embedding spaces calculated by the video and text encoders. By the proposed strategy and T2W attention, our TW-BERT achieves SOTA performances on text-to-video retrieval tasks, and comparable performances on video question answering tasks with some VDL-BERTs trained on much more data. The code will be available in the supplementary material.", "We design a novel global-local Transformer named \\textbf{Ada-ClustFormer} (\\textbf{ACF}) to generate captions. We use this name since each layer of ACF can adaptively cluster input elements to carry self-attention (Self-ATT) for learning local context. Compared with other global-local Transformers which carry Self-ATT in fixed-size windows, ACF can capture varying graininess, \\eg, an object may cover different numbers of grids or a phrase may contain diverse numbers of words. To build ACF, we insert a probabilistic matrix C into the Self-ATT layer. For an input sequence {{s}_1,...,{s}_N , C_{i,j} softly determines whether the sub-sequence {s_i,...,s_j} should be clustered for carrying Self-ATT. For implementation, {C}_{i,j} is calculated from the contexts of {{s}_i,...,{s}_j}, thus ACF can exploit the input itself to decide which local contexts should be learned. By using ACF to build the vision encoder and language decoder, the captioning model can automatically discover the hidden structures in both vision and language, which encourages the model to learn a unified structural space for transferring more structural commonalities. The experiment results demonstrate the effectiveness of ACF that we achieve CIDEr of 137.8, which outperforms most SOTA captioning models and achieve comparable scores compared with some BERT-based models. The code will be available in the supplementary material.", "We show that classifiers trained with random region proposals achieve state-of-the-art Open-world Object Detection (OWOD): they can not only maintain the accuracy of the known objects (w/training labels), but also considerably improve the recall of unknown ones (w/o training labels). Specifically, we propose RandBox, a Fast R-CNN based architecture trained on random proposals at each training iteration, surpassing existing Faster R-CNN and Transformer based OWOD. Its effectiveness stems from the following two benefits introduced by randomness. First, as the randomization is independent of the distribution of the limited known objects, the random proposals become the instrumental variable that prevents the training from being confounded by the known objects. Second, the unbiased training encourages more proposal explorations by using our proposed matching score that does not penalize the random proposals whose prediction scores do not match the known objects. On two benchmarks: Pascal-VOC/MS-COCO and LVIS, RandBox significantly outperforms the previous state-of-the-art in all metrics. We also detail the ablations on randomization and loss designs. Codes and other details are in Appendix.", null, null, "No 25.25 72.83 44.65 81.94 36.58 85.03 Mixup 27.64 74.48 47.16 81.85 36.62 82.06 VideoMix 29.37 72.50 42.59 72.21 32.68 76.07 SDN 27.14 71.14 48.47 83.83 34.87 81.88 BE 26.67 72.99 46.62 81.73 35.99 85.30 ActorCutMix 29.02 74.02 56.88 79.60 36.97 81.07 FAME 29.50 73.83 28.21 71.70 39.61 81.56 StillMix 30.77 85.51 57.30 88.80 47.38 92.46 is difficult to separate the pixels of foreground motion and foreground static cues for debiasing foreground static bias. As a result, in this paper, we propose StillMix to debias without the need to explicitly extract foreground static cues within a frame. In addition, due to this difficulty, it is hard to create test videos by simply replacing the foreground static cues and preserving the foreground motion. Thus, we alternatively create videos with conflicting foreground cues (Figure 1 of the main paper) and SCUFO videos (Sec. 4 of the main paper) to evaluate foreground static bias.", "In this section, we will show that in our causal graph for many-shot learning, the sampling ID I is essentially an instrumental variable for X\u2192 Y that achieves P (Y| I)\u2248 P (Y| do (X)). Before introducing instrumental variable, we first formally define d-separation [7], which gives a criterion to study the dependencies between nodes (data variables) in any structural causal model. d-separation. A set of nodes Z blocks a path p if and only if 1) p contains a chain A\u2192 B\u2192 C or a fork A\u2190 B\u2192 C and the middle node B is in Z; 2) p contains a collider A\u2192 B\u2190 C such that the middle node B and its descendants are not in Z. If conditioning on Z blocks every path between X and Y, we say X and Y are d-separated conditional on Z, ie, X and Y are independent given Z (X\u22a5\u22a5 Y| Z).Instrumental Variable. For a structual causal model G, a variable Z is an instrumental variable (IV) to X\u2192 Y by satisfying the graphical criteria [9]: 1)(Z\u22a5\u22a5 Y) GX; 2)(Z\u22a5\u22a5 X) G, where GX is the manipulated graph where all incoming arrows to node X are deleted. For the SCM of many-shot learning in Figure 4 (a), it is easy to see that I satisfies both criteria and therefore it is an IV for X\u2192 Y. However, in the few-shot SCM in Figure 4 (b), the paths I\u2190 X\u2190 D\u2192 C\u2192 Y and I\u2190 X\u2192 C\u2192 Y are not blocked in GX, which means the first criterion is not met (I\u22a5\u22a5 Y) GX and I is not an instrumental variable in the few-shot learning case.", "Implementation. We follow the training details of Tian et al.[18] for CIFAR-100 and Zhou et al.[23] for ImageNet. Specifically, for CIFAR-100, we set the mini-batch size as 64 and an initial learning rate as 0.05. We train the model for 240 epochs. The learning rate is decayed by 10 every 30 epochs after 150 epochs. We initialize the learning rate as 0.01 for MobileNetV2, ShuffleNetV1 and ShuffleNetV2, and as 0.05 for other models. The experiments are conducted using one NVIDIA TITAN RTX GPU. For ImageNet, we train the model for 100 epochs. We set the mini-batch size as 256, an initial learning rate as 0.1, and decay it by 10 every 30 epochs. The experiments are conducted using four Tesla V100 GPUs.Architectures. We follow Tian et al.[18] for the choice of network architectures. Specifically,\u201cWRN-dw\u201d denotes Wide Residual Network (WRN)[21] with depth d and width factor w.\u201cresnet-d\u201d represents cifar-style ResNet [3] with 3 groups of basic blocks, each with 16, 32, and 64 channels respectively. For example, resnet8x4 and resnet32x4 represent a 4 times wider network, ie, with 64, 128, and 256 channels respectively.\u201cResNet-d\u201d represents the ImageNet-style ResNet with Bottleneck blocks and more channels. MobileNetV2 [16] has a width multiplier of 0.5.\u201cvgg\u201d denotes VGGNet [17] that is adapted from its original ImageNet counterpart.\u201cShuffleNetV1\u201d[22],\u201cShuffleNetV2\u201d[10] are adapted with input size as 32x32.", "This supplementary material includes: 1) implementation details of the proposed IFL and the baseline methods; 2) a detailed analysis of our problem formulation; 3) more evidences of the re-sampling strategy in the proposed environment construction; 4) more details of the dataset construction for the proposed ImageNet-GLT and MSCOCOGLT benchmarks; 5) more experimental results of the proposed Invariant Feature Learning framework.", "This supplementary material includes: 1) additional explanations of Assumption 1; 2) revisiting previous methods in long-tailed classification; 3) the Background-Exempted Inference for object detection and instance segmentation; 4) the difference between re-balancing NDE and the proposed TDE; 5) additional ablation studies."]}, "collaboration_network": {"target": ["Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Tat-Seng Chua", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Xiangnan He", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Kaihua Tang", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Liqiang Nie (\u8042\u793c\u5f3a), IAPR Fellow", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Qianru Sun \u5b59\u5029\u8339", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Xian-Sheng Hua (\u534e\u5148\u80dc)(IEEE Fellow)", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Long Chen\uff08\u9648\u9686\uff09", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Yulei Niu", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Meng Wang, IEEE Fellow, IAPR Fellow", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Shih-Fu Chang", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "Jianfei Cai", "XU YANG", "XU YANG", "XU YANG", "XU YANG", "XU YANG", "XU YANG", "XU YANG", "XU YANG", "XU YANG", "XU YANG", "Jiaxin Shi", "Jiaxin Shi", "Jiaxin Shi", "Jiaxin Shi", "Jiaxin Shi", "Jiaxin Shi", "Jingyuan Chen", "Jingyuan Chen", "Jingyuan Chen", "Jingyuan Chen", "Jingyuan Chen", "Jingyuan Chen", "Tan Wang\uff08\u738b\u8c2d\uff09", "Tan Wang\uff08\u738b\u8c2d\uff09", "Tan Wang\uff08\u738b\u8c2d\uff09", "Tan Wang\uff08\u738b\u8c2d\uff09", "Tan Wang\uff08\u738b\u8c2d\uff09", "Tan Wang\uff08\u738b\u8c2d\uff09", "Tan Wang\uff08\u738b\u8c2d\uff09", "Tan Wang\uff08\u738b\u8c2d\uff09", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Shuicheng Yan, Fellow of AAAI, ACM, SAEng, IEEE, IAPR", "Daqing Liu (\u5218\u5927\u5e86)", "Daqing Liu (\u5218\u5927\u5e86)", "Daqing Liu (\u5218\u5927\u5e86)", "Daqing Liu (\u5218\u5927\u5e86)", "Daqing Liu (\u5218\u5927\u5e86)", "Yue Zhongqi", "Yue Zhongqi", "Yue Zhongqi", "Yue Zhongqi", "Yue Zhongqi", "Yue Zhongqi", "Yue Zhongqi", "Juanzi Li", "Juanzi Li", "Juanzi Li", "Juanzi Li", "Juanzi Li", "Jinhui Tang (\u5510\u91d1\u8f89)", "Jinhui Tang (\u5510\u91d1\u8f89)", "Jinhui Tang (\u5510\u91d1\u8f89)", "Jinhui Tang (\u5510\u91d1\u8f89)", "Fuli Feng", "Fuli Feng", "Fuli Feng", "Fuli Feng", "Fuli Feng", "Fuli Feng", "Kyaw Zaw Lin", "Kyaw Zaw Lin", "Kyaw Zaw Lin", "Kyaw Zaw Lin", "Feng Wu \u5434\u67ab", "Feng Wu \u5434\u67ab", "Feng Wu \u5434\u67ab", "Lei Hou (\u4faf\u78ca)", "Lei Hou (\u4faf\u78ca)", "Lei Hou (\u4faf\u78ca)", "Lei Hou (\u4faf\u78ca)", "Lianli Gao", "Lianli Gao", "Yue Gao", "Yue Gao", "Yue Gao", "Yue Gao", "Zhiwu Lu", "Zhiwu Lu", "Zhiwu Lu", "Miao Chun Yan", "Miao Chun Yan", "Miao Chun Yan", "Miao Chun Yan", "Ji-Rong Wen", "Ji-Rong Wen", "Ji-Rong Wen", "Xindi Shang", "Xindi Shang", "Xindi Shang", "Xindi Shang", "Xindi Shang", "Xindi Shang", "Xindi Shang", "Xindi Shang", "Xinting Hu", "Xinting Hu", "Xinting Hu", "Wenhan Luo", "Baoyuan Wu", "Hao Ye", "Xing Xu \uff08\u5f90\u884c\uff09", "Wenjie WANG", "Wenjie WANG", "YANLI JI", "YANLI JI", "YANLI JI", "Na Zhao", "Na Zhao", "Na Zhao", "Alireza Zareian", "Alireza Zareian", "Zhou Zhao", "Dejing Xu", "Tongwei Ren", "Tongwei Ren", "Liangming Pan", "Weizhi Nie", "Weizhi Nie", "Jingkuan Song\uff08\u5b8b\u4e95\u5bbd\uff09", "Joo Hwee Lim", "Joo Hwee Lim", "Joo Hwee Lim", "Joo Hwee Lim", "Joo Hwee Lim", "Linchao Zhu (\u6731\u9716\u6f6e)", "PING LIU", "Yi Yang", "Jingjing Li", "Jingjing Li", "Chongyang Gao", "Chongyang Gao", "Chongyang Gao", "Mike Z. SHOU", "Hang Gao", "Jingfan Guo", "Zhenzhen Hu", "Yuanen Zhou(\u5468\u613f\u6069)", "Mingxing Zhang", "Mingxing Zhang", "Mingxing Zhang", "Mingxing Zhang", "Lin Ma", "Wenhao Jiang", "Xinpeng Chen", "Zhiyuan Liu", "Zhiyuan Liu", "Luoqi Liu", "Zhiyong Cheng", "Lei Zhu", "Rui Liu", "Zhenguang Liu", "Hongzhi Li", "Joseph G. Ellis", "Spencer Whitehead", "Xuelong Li \uff08\u674e\u5b66\u9f99\uff09", "Dacheng Tao", "Xun Yang", "Chen Liang", "Xinshuai Dong", "Luu Anh Tuan", "Ning Xu", "Liu Weichen", "Qiangeng(Charlie) Xu", "Ulrich Neumann", "Mitra Tajrobehkar", "Bo Wu", "Brian Chen", "Xiang Wang", "Xiang Wang", "Weiyue Wang", "Meng Lei (\u5b5f\u96f7)", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang", "Jianqiang Huang"], "target_id": ["Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "Z9DWCBEAAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "X45Go24AAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "WuO1sSkAAAAJ", "yywVMhUAAAAJ", "yywVMhUAAAAJ", "yywVMhUAAAAJ", "yywVMhUAAAAJ", "yywVMhUAAAAJ", "yywVMhUAAAAJ", "yywVMhUAAAAJ", "yywVMhUAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "fNfrGMIAAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "6G-l4o0AAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "-gtmMpIAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "WXd3dDwAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "rHagaaIAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "OMVTRscAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "N6czCoUAAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "SqdxMH0AAAAJ", "8XcQHUEAAAAJ", "8XcQHUEAAAAJ", "8XcQHUEAAAAJ", "8XcQHUEAAAAJ", "8XcQHUEAAAAJ", "8XcQHUEAAAAJ", "o_G2qa0AAAAJ", "o_G2qa0AAAAJ", "o_G2qa0AAAAJ", "o_G2qa0AAAAJ", "o_G2qa0AAAAJ", "o_G2qa0AAAAJ", "wFduC9EAAAAJ", "wFduC9EAAAAJ", "wFduC9EAAAAJ", "wFduC9EAAAAJ", "wFduC9EAAAAJ", "wFduC9EAAAAJ", "wFduC9EAAAAJ", "wFduC9EAAAAJ", "DNuiPHwAAAAJ", "DNuiPHwAAAAJ", "DNuiPHwAAAAJ", "DNuiPHwAAAAJ", "DNuiPHwAAAAJ", "DNuiPHwAAAAJ", "DNuiPHwAAAAJ", "DNuiPHwAAAAJ", "TbBfOVEAAAAJ", "TbBfOVEAAAAJ", "TbBfOVEAAAAJ", "TbBfOVEAAAAJ", "TbBfOVEAAAAJ", "7Iyz9ZYAAAAJ", "7Iyz9ZYAAAAJ", "7Iyz9ZYAAAAJ", "7Iyz9ZYAAAAJ", "7Iyz9ZYAAAAJ", "7Iyz9ZYAAAAJ", "7Iyz9ZYAAAAJ", "SgNB-ioAAAAJ", "SgNB-ioAAAAJ", "SgNB-ioAAAAJ", "SgNB-ioAAAAJ", "SgNB-ioAAAAJ", "ByBLlEwAAAAJ", "ByBLlEwAAAAJ", "ByBLlEwAAAAJ", "ByBLlEwAAAAJ", "QePM4u8AAAAJ", "QePM4u8AAAAJ", "QePM4u8AAAAJ", "QePM4u8AAAAJ", "QePM4u8AAAAJ", "QePM4u8AAAAJ", "jhiwopwAAAAJ", "jhiwopwAAAAJ", "jhiwopwAAAAJ", "jhiwopwAAAAJ", "5bInRDEAAAAJ", "5bInRDEAAAAJ", "5bInRDEAAAAJ", "YnIq4hsAAAAJ", "YnIq4hsAAAAJ", "YnIq4hsAAAAJ", "YnIq4hsAAAAJ", "zsm2dpYAAAAJ", "zsm2dpYAAAAJ", "UTDfWocAAAAJ", "UTDfWocAAAAJ", "UTDfWocAAAAJ", "UTDfWocAAAAJ", "OUXS8doAAAAJ", "OUXS8doAAAAJ", "OUXS8doAAAAJ", "fmXGRJgAAAAJ", "fmXGRJgAAAAJ", "fmXGRJgAAAAJ", "fmXGRJgAAAAJ", "tbxCHJgAAAAJ", "tbxCHJgAAAAJ", "tbxCHJgAAAAJ", "iVilW3kAAAAJ", "iVilW3kAAAAJ", "iVilW3kAAAAJ", "iVilW3kAAAAJ", "iVilW3kAAAAJ", "iVilW3kAAAAJ", "iVilW3kAAAAJ", "iVilW3kAAAAJ", "o6h6sVMAAAAJ", "o6h6sVMAAAAJ", "o6h6sVMAAAAJ", "g20Q12MAAAAJ", "JNTG1KoAAAAJ", "rqknyvYAAAAJ", "aTWbAiEAAAAJ", "Ma5DtmoAAAAJ", "Ma5DtmoAAAAJ", "aGbEdhEAAAAJ", "aGbEdhEAAAAJ", "aGbEdhEAAAAJ", "KOL2dMwAAAAJ", "KOL2dMwAAAAJ", "KOL2dMwAAAAJ", "Ioe0SGsAAAAJ", "Ioe0SGsAAAAJ", "IIoFY90AAAAJ", "Rfc4s_wAAAAJ", "WtnXVHoAAAAJ", "WtnXVHoAAAAJ", "JcjjOTUAAAAJ", "aNwEZxkAAAAJ", "aNwEZxkAAAAJ", "F5Zy9V4AAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "BjEDX4EAAAAJ", "9ZukE28AAAAJ", "KRz4JecAAAAJ", "RMSuNFwAAAAJ", "9x5saoEAAAAJ", "9x5saoEAAAAJ", "HEAgatAAAAAJ", "HEAgatAAAAAJ", "HEAgatAAAAAJ", "h1-3lSoAAAAJ", "cwXx4EQAAAAJ", "-PsRtB_TuTQC", "N6Llq94AAAAJ", "MWUBrhYAAAAJ", "lnUuHQQAAAAJ", "lnUuHQQAAAAJ", "lnUuHQQAAAAJ", "lnUuHQQAAAAJ", "DAn1pA4AAAAJ", "rAlT64IAAAAJ", "6pDqDPIAAAAJ", "dT0v5u0AAAAJ", "dT0v5u0AAAAJ", "nw4XTwMAAAAJ", "0ffIKdIAAAAJ", "gw4ISc4AAAAJ", "ADP-1goAAAAJ", "OP2ySB8AAAAJ", "xuQrV1AAAAAJ", "VnvFnacAAAAJ", "ZdfkFuAAAAAJ", "ahUibskAAAAJ", "RwlJNLcAAAAJ", "ro8lzsUAAAAJ", "2MgsdcoAAAAJ", "A7JyL1sAAAAJ", "d6ixOGYAAAAJ", "okruzyMAAAAJ", "UozjxW8AAAAJ", "MKCb-mcAAAAJ", "MHet2VoAAAAJ", "e9z0ZAkAAAAJ", "6ozI_ZMAAAAJ", "7zfiaA8AAAAJ", "HdhaQB0AAAAJ", "HdhaQB0AAAAJ", "-L4RC_4AAAAJ", "KuukGGkAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ", "UqAybqgAAAAJ"], "type": ["Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "NTU", "NTU", "NTU", "NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "NTU", "Outside NTU", "NTU", "Unknown", "Outside NTU", "Outside SCSE", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "location": ["National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Harbin Institute of Technology", "Harbin Institute of Technology", "Harbin Institute of Technology", "Harbin Institute of Technology", "Harbin Institute of Technology", "Harbin Institute of Technology", "Harbin Institute of Technology", "Harbin Institute of Technology", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Singapore Management University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Hong Kong University of Science and Technology", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Hefei University of Technology", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Columbia University", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Monash University Malaysia", "Southeast University", "Southeast University", "Southeast University", "Southeast University", "Southeast University", "Southeast University", "Southeast University", "Southeast University", "Southeast University", "Southeast University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Zhejiang University", "Zhejiang University", "Zhejiang University", "Zhejiang University", "Zhejiang University", "Zhejiang University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Unknown", "Unknown", "Unknown", "Unknown", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "University of Science and Technology of China", "University of Science and Technology of China", "University of Science and Technology of China", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "UESTC", "UESTC", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Tsinghua University", "Renmin University of China", "Renmin University of China", "Renmin University of China", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Renmin University of China", "Renmin University of China", "Renmin University of China", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Imperial College London", "Unknown", "Zhejiang University", "UESTC", "National University of Singapore", "National University of Singapore", "UESTC", "UESTC", "UESTC", "Singapore University of Technology and Design", "Singapore University of Technology and Design", "Singapore University of Technology and Design", "Columbia University", "Columbia University", "Zhejiang University", "Zhejiang University", "Nanjing University", "Nanjing University", "University of California at Santa Barbara", "Tianjin University", "Tianjin University", "Columbia University", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "A*STAR", "Zhejiang University", "Unknown", "Zhejiang University", "UESTC", "UESTC", "Northwestern University", "Northwestern University", "Northwestern University", "Columbia University", "UC Berkeley", "University of Minnesota", "Hefei University of Technology", "Hefei University of Technology", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Tsinghua University", "Tsinghua University", "Unknown", "Unknown", "Unknown", "Chinese University of Hong Kong", "Zhejiang University", "Microsoft", "Unknown", "Unknown", "Unknown", "University of Sydney", "University of Science and Technology of China", "Tsinghua University", "Carnegie Mellon University", "Nanyang Technological University", "Tianjin University", "Nanyang Technological University", "Unknown", "USC", "Nanyang Technological University", "Unknown", "Columbia University", "University of Science and Technology of China", "University of Science and Technology of China", "USC", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "year": [2023, 2012, 2015, 2014, 2018, 2012, 2016, 2014, 2014, 2021, 2016, 2014, 2018, 2016, 2017, 2016, 2016, 2022, 2014, 2017, 2016, 2013, 2021, 2016, 2017, 2015, 2017, 2012, 2016, 2017, 2012, 2021, 2017, 2015, 2014, 2016, 2018, 2017, 2017, 2019, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2021, 2017, 2017, 2019, 2018, 2017, 2016, 2021, 2022, 2017, 2017, 2018, 2023, 2018, 2016, 2022, 2019, 2019, 2020, 2021, 2021, 2022, 2022, 2021, 2020, 2020, 2017, 2017, 2016, 2018, 2017, 2017, 2017, 2018, 2022, 2020, 2021, 2022, 2020, 2020, 2022, 2023, 2023, 2020, 2021, 2022, 2021, 2021, 2021, 2023, 2021, 2023, 2022, 2020, "Unknown", 2022, 2022, 2021, 2023, 2020, 2020, 2023, 2020, 2021, 2021, 2021, 2021, 2022, 2022, 2021, 2021, 2018, 2019, 2022, 2020, 2019, 2023, 2023, 2017, 2020, 2022, 2022, 2019, 2020, 2022, 2019, 2021, 2021, 2023, 2021, 2019, 2018, 2012, 2016, 2020, 2012, 2015, 2016, 2016, 2016, 2020, 2020, 2019, 2021, 2018, 2018, 2018, 2019, 2019, 2017, 2017, 2020, 2017, 2023, 2021, 2019, 2021, 2018, 2019, 2020, 2021, 2020, 2021, 2023, 2019, 2021, 2023, 2018, 2019, 2020, 2021, 2020, 2019, 2020, 2022, 2021, 2019, 2019, 2016, 2021, 2020, 2017, 2018, 2020, 2023, 2023, 2020, 2021, 2022, 2022, 2021, 2021, 2012, 2021, 2012, 2015, 2017, 2014, 2014, 2013, 2019, 2019, 2018, 2019, 2020, 2020, "Unknown", 2023, 2021, 2023, 2021, 2021, 2019, 2022, 2021, 2019, 2019, 2020, 2021, 2023, 2020, 2021, 2021, 2018, 2021, 2022, 2023, 2017, 2016, 2017, 2017, 2019, 2019, 2018, 2021, 2022, 2019, 2019, 2018, 2017, 2014, 2017, 2014, 2013, 2019, 2021, 2019, 2019, 2021, 2020, 2021, 2019, 2023, 2021, 2016, 2016, 2016, 2016, 2017, 2015, 2017, 2016, 2021, 2020, 2021, 2019, 2019, 2017, 2017, 2021, 2023, 2017, 2018, 2018, 2016, 2017, 2016, 2018, 2020, 2017, 2017, 2017, 2017, 2022, 2019, 2018, 2018, 2023, 2021, 2023, 2022, 2022, 2020, 2020, 2020, 2015, 2019, 2020, 2021, 2023, 2018, 2018, 2017, 2020, 2020, 2017, 2015, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2015, 2019, 2019, 2018, 2022, 2017, 2017, 2017, 2015, 2019, 2019, 2019, 2021, 2021, 2018, 2020, 2020, 2020, 2021, 2020, 2020, 2016, 2017, 2020, 2019, 2020, 2020, 2021, 2020, 2020, 2022, "Unknown", 2022, 2020, 2021], "title": ["Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models", "Attribute feedback", "Enhancing video event recognition using automatically constructed semantic-visual knowledge base", "Start from scratch: Towards automatically identifying, modeling, and naming visual attributes", "More is better: Precise and detailed image captioning using online positive recall and missing concepts mining", "Detecting group activities with multi-camera context", "Online collaborative learning for open-vocabulary visual classifiers", "Multimedia summarization for social events in microblog stream", "Image tagging with social assistance", "Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue", "Play and rewind: Optimizing binary representations of videos by self-supervised temporal hashing", "Robust (semi) nonnegative graph embedding", "Attributed social network embedding", "An Intention-Aware Interactive System for Mobile Video Browsing", "VideoWhisper: Toward Discriminative Unsupervised Video Feature Learning With Attention-Based Recurrent Neural Networks", "Deep learning generic features for cross-media retrieval", "Fast matrix factorization for online recommendation with implicit feedback", "Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning", "One of a kind: User profiling by social curation", "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning", "Learning content\u2013social influential features for influence analysis", "Attribute-augmented semantic hierarchy: towards bridging semantic gap and intention gap in image retrieval", "Cross-GCN: Enhancing Graph Convolutional Network with -Order Feature Interactions", "Discrete collaborative filtering", "I Know What You Want to Express: Sentence Element Inference by Incorporating External Knowledge Base", "Learning Image and User Features for Recommendation in Social Networks", "Attentional factorization machines: Learning the weight of feature interactions via attention networks", "Visual query attributes suggestion", "Micro tells macro: predicting the popularity of micro-videos via a transductive model", "Matryoshka Peek: Toward Learning Fine-Grained, Robust, Discriminative Features for Product Search", "Robust non-negative graph embedding: Towards noisy data, unreliable graphs, and noisy labels", "Empowering Language Understanding with Counterfactual Reasoning", "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention", "Learning features from large-scale, noisy and social image-tag collection", "Attribute-augmented semantic hierarchy: Towards a unified framework for content-based image retrieval", "Discrete image hashing using large weakly annotated photo collections", "Venue Prediction for Social Images by Exploiting Rich Temporal Patterns in LBSNs", "Neural collaborative filtering", "Visual Translation Embedding Network for Visual Relation Detection", "Learning Using Privileged Information for Food Recognition", "Saliency meets spatial quantization: A practical framework for large scale product search", "Learning from collective intelligence: Feature learning using social images and tags", "Mental visual browsing", "Mental Visual Indexing: Towards Fast Video Browsing", "Deep Fusion of Multiple Semantic Cues for Complex Event Recognition", "Video visual relation detection", "Object trajectory proposal", "Cross-GCN: Enhancing Graph Convolutional Network with -Order Feature Interactions", "Attentional factorization machines: Learning the weight of feature interactions via attention networks", "Enhancing micro-video understanding by harnessing external sounds", "Learning to compose and reason with language tree structures for visual grounding", "Discrete Factorization Machines for Fast Feature-based Recommendation", "Video question answering via gradually refined attention over appearance and motion", "Fast matrix factorization for online recommendation with implicit feedback", "Empowering Language Understanding with Counterfactual Reasoning", "Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning", "Neural collaborative filtering", "andTatSeng Chua,\u201c", "Venue Prediction for Social Images by Exploiting Rich Temporal Patterns in LBSNs", "Counterfactual Active Learning for Out-of-Distribution Generalization", "Attributed social network embedding", "Discrete collaborative filtering", "Class is invariant to context and vice versa: on learning invariance for out-of-distribution generalization", "Auto-encoding scene graphs for image captioning", "Learning to compose dynamic tree structures for visual contexts", "Unbiased scene graph generation from biased training", "Distilling Causal Effect of Data in Class-Incremental Learning", "Align R-CNN: A pairwise head network for visual relationship detection", "Identifying hard noise in long-tailed sample distribution", "Invariant Feature Learning for Generalized Long-Tailed Classification", "Counterfactual vqa: A cause-effect look at language bias", "Learning to Segment the Tail", "Long-tailed classification by keeping the good and removing the bad momentum causal effect", "Enhancing micro-video understanding by harnessing external sounds", "I Know What You Want to Express: Sentence Element Inference by Incorporating External Knowledge Base", "Micro tells macro: predicting the popularity of micro-videos via a transductive model", "Discrete Factorization Machines for Fast Feature-based Recommendation", "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention", "Neural collaborative filtering", "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning", "Venue Prediction for Social Images by Exploiting Rich Temporal Patterns in LBSNs", "Class is invariant to context and vice versa: on learning invariance for out-of-distribution generalization", "Feature pyramid transformer", "Causal interventional training for image recognition", "Deconfounded visual grounding", "Interventional few-shot learning", "Causal intervention for weakly-supervised semantic segmentation", "Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation", "Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection", "Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition", "Visual commonsense r-cnn", "Self-Regulation for Semantic Segmentation", "Attention-based Class Activation Diffusion for Weakly-Supervised Semantic Segmentation", "Counterfactual zero-shot and open-set visual recognition", "Causal Attention for Unbiased Visual Recognition", "Self-supervised learning disentangled group representation as feature", "Semantic Scene Completion with Cleaner Self", "Transporting Causal Mechanisms for Unsupervised Domain Adaptation", "Compositional prompt tuning with motion cues for open-vocabulary video relation detection", "Equivariance and Invariance Inductive Bias for Learning from Insufficient Data", "Interventional few-shot learning", "Supplementary Material for Interventional Few-Shot Learning", "Cross-domain empirical risk minimization for unbiased long-tailed classification", "Class is invariant to context and vice versa: on learning invariance for out-of-distribution generalization", "Distilling Causal Effect of Data in Class-Incremental Learning", "Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition", "Causal intervention for weakly-supervised semantic segmentation", "Self-adaptive neural module transformer for visual question answering", "Random Boxes Are Open-world Object Detectors", "Feature pyramid transformer", "Transporting Causal Mechanisms for Unsupervised Domain Adaptation", "On Non-Random Missing Labels in Semi-Supervised Learning", "Self-Regulation for Semantic Segmentation", "Counterfactual zero-shot and open-set visual recognition", "Identifying hard noise in long-tailed sample distribution", "Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation", "Counterfactual vqa: A cause-effect look at language bias", "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding", "Zero-shot visual recognition using semantics-preserving adversarial embedding networks", "Learning Using Privileged Information for Food Recognition", "Respecting transfer gap in knowledge distillation", "Counterfactual Samples Synthesizing for Robust Visual Question Answering", "Counterfactual Critic Multi-Agent Training for Scene Graph Generation", "Compositional prompt tuning with motion cues for open-vocabulary video relation detection", "Vl-nms: Breaking proposal bottlenecks in two-stage visual-language matching", "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning", "Two causal principles for improving visual dialog", "Cross-domain empirical risk minimization for unbiased long-tailed classification", "Nico challenge: Out-of-distribution generalization for image recognition challenges", "Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions", "Unbiased scene graph generation from biased training", "Respecting transfer gap in knowledge distillation", "Recursive visual attention in visual dialog", "Introspective distillation for robust question answering", "On Non-Random Missing Labels in Semi-Supervised Learning", "Debiased Fine-Tuning for Vision-language Models by Prompt Regularization", "Counterfactual vqa: A cause-effect look at language bias", "Single-shot Semantic Image Inpainting with Densely Connected Generative Networks", "Self-supervised video hashing with hierarchical binary auto-encoder", "Detecting group activities with multi-camera context", "Learning from collective intelligence: Feature learning using social images and tags", "Feature pyramid transformer", "Robust non-negative graph embedding: Towards noisy data, unreliable graphs, and noisy labels", "Deep aging face verification with large gaps", "Play and rewind: Optimizing binary representations of videos by self-supervised temporal hashing", "Deep Fusion of Multiple Semantic Cues for Complex Event Recognition", "Learning content\u2013social influential features for influence analysis", "Iterative context-aware graph inference for visual dialog", "More Grounded Image Captioning by Distilling Image-Text Matching Model", "Question-Aware Tube-Switch Network for Video Question Answering", "Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding", "Low-shot learning via covariance-preserving adversarial augmentation networks", "Zero-shot visual recognition using semantics-preserving adversarial embedding networks", "Grounding Referring Expressions in Images by Variational Context", "Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions", "Counterfactual Critic Multi-Agent Training for Scene Graph Generation", "Visual Translation Embedding Network for Visual Relation Detection", "PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN", "General Partial Label Learning via Dual Bipartite Graph Autoencoder", "Improving event extraction via multimodal integration", "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning", "Auto-Parsing Network for Image Captioning and Visual Question Answering", "Auto-encoding scene graphs for image captioning", "Deconfounded image captioning: A causal retrospect", "Shuffle-then-assemble: learning object-agnostic visual relationship features", "Learning to collocate neural modules for image captioning", "Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning", "Causal attention for vision-language tasks", "Auto-encoding and distilling scene graphs for image captioning", "Auto-Parsing Network for Image Captioning and Visual Question Answering", "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning", "Auto-encoding scene graphs for image captioning", "Deconfounded image captioning: A causal retrospect", "Learning Trajectory-Word Alignments for Video-Language Tasks", "Shuffle-then-assemble: learning object-agnostic visual relationship features", "Learning to collocate neural modules for image captioning", "Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning", "Causal attention for vision-language tasks", "Auto-encoding and distilling scene graphs for image captioning", "Deepchannel: Salience estimation by contrastive learning for extractive document summarization", "Unbiased scene graph generation from biased training", "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base", "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph", "Explainable and explicit visual reasoning over scene graphs", "Learning to embed sentences using attentive recursive trees", "Micro tells macro: predicting the popularity of micro-videos via a transductive model", "The Blessings of Unlabeled Background in Untrimmed Videos", "Self-adaptive neural module transformer for visual question answering", "Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention", "Venue Prediction for Social Images by Exploiting Rich Temporal Patterns in LBSNs", "Learning to Segment the Tail", "Equivariant Similarity for Vision-Language Foundation Models", "DisCo: Disentangled Control for Referring Human Dance Generation in Real World", "Visual commonsense r-cnn", "Counterfactual zero-shot and open-set visual recognition", "Equivariance and Invariance Inductive Bias for Learning from Insufficient Data", "Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation", "Causal Attention for Unbiased Visual Recognition", "Self-supervised learning disentangled group representation as feature", "Attribute feedback", "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?", "Robust non-negative graph embedding: Towards noisy data, unreliable graphs, and noisy labels", "Deep aging face verification with large gaps", "Event classification in microblogs via social tracking", "Robust (semi) nonnegative graph embedding", "Attribute-augmented semantic hierarchy: Towards a unified framework for content-based image retrieval", "Attribute-augmented semantic hierarchy: towards bridging semantic gap and intention gap in image retrieval", "Learning to compose and reason with language tree structures for visual grounding", "Learning to assemble neural module tree networks for visual grounding", "Context-Aware Visual Policy Network for Sequence-Level Image Captioning", "Context-aware visual policy network for fine-grained image captioning", "More Grounded Image Captioning by Distilling Image-Text Matching Model", "Interventional few-shot learning", "Supplementary Material for Interventional Few-Shot Learning", "Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection", "Transporting Causal Mechanisms for Unsupervised Domain Adaptation", "Random Boxes Are Open-world Object Detectors", "Counterfactual zero-shot and open-set visual recognition", "Self-supervised learning disentangled group representation as feature", "Deepchannel: Salience estimation by contrastive learning for extractive document summarization", "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base", "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph", "Explainable and explicit visual reasoning over scene graphs", "Learning to embed sentences using attentive recursive trees", "Causal intervention for weakly-supervised semantic segmentation", "Self-Regulation for Semantic Segmentation", "Semantic Scene Completion with Cleaner Self", "Feature pyramid transformer", "Cross-GCN: Enhancing Graph Convolutional Network with -Order Feature Interactions", "Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue", "Discrete Factorization Machines for Fast Feature-based Recommendation", "Empowering Language Understanding with Counterfactual Reasoning", "Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning", "Counterfactual Active Learning for Out-of-Distribution Generalization", "Matryoshka Peek: Toward Learning Fine-Grained, Robust, Discriminative Features for Product Search", "Saliency meets spatial quantization: A practical framework for large scale product search", "Visual Translation Embedding Network for Visual Relation Detection", "PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN", "Context-aware visual policy network for fine-grained image captioning", "Learning to assemble neural module tree networks for visual grounding", "Context-Aware Visual Policy Network for Sequence-Level Image Captioning", "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph", "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base", "Deepchannel: Salience estimation by contrastive learning for extractive document summarization", "Learning to embed sentences using attentive recursive trees", "Self-supervised video hashing with hierarchical binary auto-encoder", "Video Captioning With Attention-Based LSTM and Semantic Consistency", "Image tagging with social assistance", "Event classification in microblogs via social tracking", "Attribute-augmented semantic hierarchy: Towards a unified framework for content-based image retrieval", "Attribute-augmented semantic hierarchy: towards bridging semantic gap and intention gap in image retrieval", "Recursive visual attention in visual dialog", "Counterfactual vqa: A cause-effect look at language bias", "Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions", "Learning Using Privileged Information for Food Recognition", "Distilling Causal Effect of Data in Class-Incremental Learning", "Learning to Segment the Tail", "On Non-Random Missing Labels in Semi-Supervised Learning", "Recursive visual attention in visual dialog", "Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning", "Counterfactual vqa: A cause-effect look at language bias", "Learning from collective intelligence: Feature learning using social images and tags", "Online collaborative learning for open-vocabulary visual classifiers", "Mental visual browsing", "Deep learning generic features for cross-media retrieval", "Video visual relation detection", "Learning features from large-scale, noisy and social image-tag collection", "Object trajectory proposal", "Discrete image hashing using large weakly annotated photo collections", "Distilling Causal Effect of Data in Class-Incremental Learning", "Learning to Segment the Tail", "On Non-Random Missing Labels in Semi-Supervised Learning", "Learning to compose dynamic tree structures for visual contexts", "Learning to compose dynamic tree structures for visual contexts", "Attentional factorization machines: Learning the weight of feature interactions via attention networks", "Video Captioning With Attention-Based LSTM and Semantic Consistency", "Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue", "Counterfactual Active Learning for Out-of-Distribution Generalization", "Deep semantic indexing using convolutional localization network with region-based visual attention for image database", "Recurrent attention network using spatial-temporal relations for action recognition", "More is better: Precise and detailed image captioning using online positive recall and missing concepts mining", "Learning content\u2013social influential features for influence analysis", "VideoWhisper: Toward Discriminative Unsupervised Video Feature Learning With Attention-Based Recurrent Neural Networks", "Discrete image hashing using large weakly annotated photo collections", "Low-shot learning via covariance-preserving adversarial augmentation networks", "General Partial Label Learning via Dual Bipartite Graph Autoencoder", "Video question answering via gradually refined attention over appearance and motion", "Video question answering via gradually refined attention over appearance and motion", "Object trajectory proposal", "Video visual relation detection", "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base", "Multi-level policy and reward-based deep reinforcement learning framework for image captioning", "Multi-Level Policy and Reward Reinforcement Learning for Image Captioning.", "Self-supervised video hashing with hierarchical binary auto-encoder", "Invariant Training 2D-3D Joint Hard Samples for Few-Shot Point Cloud Recognition", "Align R-CNN: A pairwise head network for visual relationship detection", "An Overview of Challenges in Egocentric Text-Video Retrieval", "Identifying hard noise in long-tailed sample distribution", "RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval", "Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration", "Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration", "Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration", "Enhancing video event recognition using automatically constructed semantic-visual knowledge base", "Fast discrete collaborative multi-modal hashing for large-scale multimedia retrieval", "Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning", "Auto-Parsing Network for Image Captioning and Visual Question Answering", "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning", "Low-shot learning via covariance-preserving adversarial augmentation networks", "Low-shot learning via covariance-preserving adversarial augmentation networks", "Video visual relation detection", "More Grounded Image Captioning by Distilling Image-Text Matching Model", "More Grounded Image Captioning by Distilling Image-Text Matching Model", "Deep semantic indexing using convolutional localization network with region-based visual attention for image database", "Visual coding in a semantic hierarchy", "Recurrent attention network using spatial-temporal relations for action recognition", "More is better: Precise and detailed image captioning using online positive recall and missing concepts mining", "Learning to guide decoding for image captioning", "Learning to guide decoding for image captioning", "Learning to guide decoding for image captioning", "Deepchannel: Salience estimation by contrastive learning for extractive document summarization", "Learning to embed sentences using attentive recursive trees", "Deep aging face verification with large gaps", "Fast discrete collaborative multi-modal hashing for large-scale multimedia retrieval", "Fast discrete collaborative multi-modal hashing for large-scale multimedia retrieval", "Discrete Factorization Machines for Fast Feature-based Recommendation", "Invariant Feature Learning for Generalized Long-Tailed Classification", "Improving event extraction via multimodal integration", "Improving event extraction via multimodal integration", "Improving event extraction via multimodal integration", "Visual coding in a semantic hierarchy", "Learning Using Privileged Information for Food Recognition", "Learning Using Privileged Information for Food Recognition", "Deepchannel: Salience estimation by contrastive learning for extractive document summarization", "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?", "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?", "Multi-Level Policy and Reward Reinforcement Learning for Image Captioning.", "Occlusion-Aware GAN for Face De-Occlusion in the Wild", "Stochastic dynamics for video infilling", "Stochastic dynamics for video infilling", "Align R-CNN: A pairwise head network for visual relationship detection", "General Partial Label Learning via Dual Bipartite Graph Autoencoder", "General Partial Label Learning via Dual Bipartite Graph Autoencoder", "Micro tells macro: predicting the popularity of micro-videos via a transductive model", "Enhancing micro-video understanding by harnessing external sounds", "Stochastic dynamics for video infilling", "Learning Using Privileged Information for Food Recognition", "Two causal principles for improving visual dialog", "Unbiased scene graph generation from biased training", "The Blessings of Unlabeled Background in Untrimmed Videos", "Self-adaptive neural module transformer for visual question answering", "Visual commonsense r-cnn", "Attention-based Class Activation Diffusion for Weakly-Supervised Semantic Segmentation", "Supplementary Material for \u201cLong-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect\u201d", "Deconfounded visual grounding", "Long-tailed classification by keeping the good and removing the bad momentum causal effect", "Self-supervised learning disentangled group representation as feature"], "link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:EGhj4itiAA0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ovGv7akYl-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ExBYd_ZNEOYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:esGtpfCv0y8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:onKP9CxGSkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3YQzwbzlpKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:N5XbD978G_MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:EO1llL0aI9sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:vPKCt-r_nWsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:0dtNEdnCFDAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ThYxFsVAhEkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:qCaWouos7ogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:HkunuPqSaCsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:121WXH0ir18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:KqW5X_olkfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:o4Qvs5Y5TLQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5-bGDoUgDrYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:N7YTE_TVRugC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tJ_1M1FLfZ0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:W8gSVh4kTHQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:H8haQKU-3ZsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:QAOzB4mb83kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:6e4D8M0GhXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:RgznTc0nqo4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UFuRdyijzaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:ipSP0SfFaZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:xMZGxf1v-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:6B7w4NK6UsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:fHS53ZCY-AEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:BHd7YmozNHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:JQPmwQThujIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:SgTKrLvt1DcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sw8WaQuuSIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5uu-MYFt2EcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5McdzzY_mmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:t7izwRedFcYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:m8MyhXdlT-4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:NRnkAyzcrGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:SPgmg5JLkoEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:gPamCmV4epEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:5kvG9DmRKWYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tswL-GKFg8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:DGpvO1n63MYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tJ_1M1FLfZ0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:6e4D8M0GhXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:96M4k3P_OWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UPMPWMAU16oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:AOwgc6nnr1EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:IwGqIHSOC8kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:HkunuPqSaCsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:6B7w4NK6UsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:121WXH0ir18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5uu-MYFt2EcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:F-1HeU7gMOYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sw8WaQuuSIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:M3hCyctIOBoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:vPKCt-r_nWsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:W8gSVh4kTHQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lYbyOjaXH8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:K1AqHrCRkPsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:BzPC9jo9PWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:V8JMcbNWlSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBxuCEhX224C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-Rb0Dl3RFGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:D4n_APcuzvwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:BDTQWqO089sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:zDhyt2jClVkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:HKuYlFsi-qEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:knQODA9bSJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:96M4k3P_OWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:H8haQKU-3ZsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UFuRdyijzaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:AOwgc6nnr1EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:fHS53ZCY-AEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5uu-MYFt2EcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:o4Qvs5Y5TLQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sw8WaQuuSIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lYbyOjaXH8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:zmHQPunddckC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:fhZv66dCuXAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sg-YnEhjH50C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:n2kG1_zAmykC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:LGh1lRt-7sUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:Usae8uB1euoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:r5WMYYrv30cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:P--cqP0FnSgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:InLRhUNj-rwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:MStqg_gSLBcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Y1W0x10ZrwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:WTkkuPxyGkUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:VFGfXyYpp08C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R8TPKZP7usQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:JVJ6OkSwpHsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bYbwfsIO_fQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:kFM4V80Y5EsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:YiZc1oW-E3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:n2kG1_zAmykC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:7f0ks1Axv_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:oi8PhiKYDwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lYbyOjaXH8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBxuCEhX224C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:P--cqP0FnSgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:LGh1lRt-7sUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ZM__uENUXnMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-A4928QJj7oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:zmHQPunddckC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bYbwfsIO_fQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:3s2jc9hNhkQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:MStqg_gSLBcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:WTkkuPxyGkUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:D4n_APcuzvwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:Usae8uB1euoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:zDhyt2jClVkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ae0xyBWlIcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:jvrRlaHg2sAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:t7izwRedFcYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lDOOmgye57wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:QC-2xSqExF4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:DoBzxrgsGIIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:kFM4V80Y5EsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:GWiaReNCd0YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:o4Qvs5Y5TLQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBJCJstOu-sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:oi8PhiKYDwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:HOg0WoviCygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:AmQcmOVUwm0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:V8JMcbNWlSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:lDOOmgye57wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vkDViGfkvYEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:aJ-3-MYELVsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:3s2jc9hNhkQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:bXvxhPDrUX0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:zDhyt2jClVkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:mzRTpvMFn3IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:YIdQ7BAI8VoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:NRnkAyzcrGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:zmHQPunddckC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:xMZGxf1v-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:5bfplxN71z4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:N5XbD978G_MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:5kvG9DmRKWYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5-bGDoUgDrYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ECvNt0vk_34C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vYYylRVofzEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:hwlm9Y4obscC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ae0xyBWlIcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8EvVLpklxGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:jvrRlaHg2sAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:YXeC4bxG7-IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:AmQcmOVUwm0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:DoBzxrgsGIIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5McdzzY_mmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:9UF2BbDYXHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5AT-p8ixKR4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8moDcb_GFzgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:QxtoOqDH1aQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:fSKd39tHJ84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:K1AqHrCRkPsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:_cFkbNEifk0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3ssty3PwuTgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:7uOkfv6XYJ4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:W91e3rS6dHEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:MIUEjqm7qCUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:LZ55yeyZZb4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:fSKd39tHJ84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:QxtoOqDH1aQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:K1AqHrCRkPsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:_cFkbNEifk0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:YnriW4MgZhwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3ssty3PwuTgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:7uOkfv6XYJ4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:W91e3rS6dHEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:MIUEjqm7qCUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:LZ55yeyZZb4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:X41XOdD1uaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:V8JMcbNWlSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bNB6FdaCRgEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:B71Od6CY_ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:SZzxPo9m4nkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:yRRszJvrVdAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UFuRdyijzaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:rVBkUn48wTwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ZM__uENUXnMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:fHS53ZCY-AEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sw8WaQuuSIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:HKuYlFsi-qEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:ult01sCh7k0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:mRAYM1lGiDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:InLRhUNj-rwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:WTkkuPxyGkUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:YiZc1oW-E3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:Usae8uB1euoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:VFGfXyYpp08C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R8TPKZP7usQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R1TrASrH5esC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:xMZGxf1v-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:5bfplxN71z4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:BSZS1fLCB98C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:EO1llL0aI9sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:JQPmwQThujIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:N7YTE_TVRugC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UPMPWMAU16oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:EVUe1p4rgj0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:2hrGa7H818QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:RwQ8IgSj6xkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vYYylRVofzEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:n2kG1_zAmykC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:7f0ks1Axv_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:r5WMYYrv30cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bYbwfsIO_fQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-A4928QJj7oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:WTkkuPxyGkUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R8TPKZP7usQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:X41XOdD1uaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bNB6FdaCRgEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:B71Od6CY_ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:SZzxPo9m4nkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:yRRszJvrVdAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:LGh1lRt-7sUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:MStqg_gSLBcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:JVJ6OkSwpHsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:zmHQPunddckC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tJ_1M1FLfZ0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3YQzwbzlpKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:AOwgc6nnr1EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:6B7w4NK6UsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:121WXH0ir18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:M3hCyctIOBoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:ipSP0SfFaZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:m8MyhXdlT-4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:5McdzzY_mmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:9UF2BbDYXHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:RwQ8IgSj6xkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:EVUe1p4rgj0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:2hrGa7H818QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:B71Od6CY_ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bNB6FdaCRgEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:X41XOdD1uaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:yRRszJvrVdAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:YIdQ7BAI8VoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:o0tJHVzi6PsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:BSZS1fLCB98C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:JQPmwQThujIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:N7YTE_TVRugC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vkDViGfkvYEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:zDhyt2jClVkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:AmQcmOVUwm0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:t7izwRedFcYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBxuCEhX224C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:HKuYlFsi-qEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:3s2jc9hNhkQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vkDViGfkvYEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:tsQ8sfJ2mf4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:zDhyt2jClVkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:NRnkAyzcrGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:esGtpfCv0y8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:SPgmg5JLkoEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:qCaWouos7ogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tswL-GKFg8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:BHd7YmozNHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:DGpvO1n63MYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:SgTKrLvt1DcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBxuCEhX224C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:HKuYlFsi-qEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:3s2jc9hNhkQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:BzPC9jo9PWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:BzPC9jo9PWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:6e4D8M0GhXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:o0tJHVzi6PsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:3YQzwbzlpKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:M3hCyctIOBoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:xUD2GqFNeDMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:6MVT8mER-OMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ExBYd_ZNEOYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5-bGDoUgDrYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ThYxFsVAhEkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:SgTKrLvt1DcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8EvVLpklxGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5AT-p8ixKR4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:IwGqIHSOC8kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:IwGqIHSOC8kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:DGpvO1n63MYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tswL-GKFg8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:bNB6FdaCRgEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:D_YRMw8gybsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:Qs68GpkTGbMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:YIdQ7BAI8VoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:P--cqP0FnSgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-Rb0Dl3RFGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:8s22W2WWFy4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:D4n_APcuzvwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:_WP0DvM6eX8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:dYRx7efp7U0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:dYRx7efp7U0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:dYRx7efp7U0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ovGv7akYl-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:U_h31ocWZrQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:W91e3rS6dHEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:fSKd39tHJ84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:QxtoOqDH1aQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8EvVLpklxGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8EvVLpklxGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:tswL-GKFg8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vYYylRVofzEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:vYYylRVofzEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:xUD2GqFNeDMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UarirCmVI0EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:6MVT8mER-OMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ExBYd_ZNEOYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:-XtG2q47PdUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:-XtG2q47PdUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:-XtG2q47PdUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:X41XOdD1uaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:yRRszJvrVdAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:5bfplxN71z4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:U_h31ocWZrQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:U_h31ocWZrQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:AOwgc6nnr1EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:BDTQWqO089sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8moDcb_GFzgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8moDcb_GFzgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:8moDcb_GFzgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UarirCmVI0EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:t7izwRedFcYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:t7izwRedFcYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:X41XOdD1uaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R1TrASrH5esC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R1TrASrH5esC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:Qs68GpkTGbMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:eP5N1itS6b0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:csTP5AdqY_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:csTP5AdqY_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:-Rb0Dl3RFGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5AT-p8ixKR4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:5AT-p8ixKR4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:UFuRdyijzaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:96M4k3P_OWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:csTP5AdqY_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:t7izwRedFcYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:yBJCJstOu-sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:V8JMcbNWlSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:rVBkUn48wTwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:ZM__uENUXnMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:InLRhUNj-rwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Y1W0x10ZrwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:f5lEeLvKxmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=100&pagesize=100&citation_for_view=YG0DFyYAAAAJ:Sg-YnEhjH50C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&citation_for_view=YG0DFyYAAAAJ:knQODA9bSJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=YG0DFyYAAAAJ&cstart=20&pagesize=80&citation_for_view=YG0DFyYAAAAJ:R8TPKZP7usQC"]}, "published_by_year": {"Year": ["2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Publications": [1, 0, 0, 4, 1, 7, 7, 16, 19, 14, 18, 20, 23, 16, 19, 5]}, "citations_by_year": {"Year": ["2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [1, 0, 0, 1, 9, 28, 69, 192, 511, 1214, 2408, 3236, 4435, 5563, 5004, 42]}, "all_time_h_index": 60, "all_time_i10_index": 118, "all_time_i20_index": 102, "h_index_by_year": {"Year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 1, 1, 1, 2, 3, 6, 11, 17, 25, 29, 37, 48, 55, 60]}, "h_index_by_publication_year": {"Publication Year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 0, 0, 4, 1, 7, 5, 11, 15, 12, 17, 16, 19, 9, 3]}, "avg_citations_by_publication_year": {"Publication Year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [2.0, 0.0, 0.0, 36.0, 161.0, 59.57142857142857, 54.0, 118.625, 584.0, 123.92857142857143, 131.33333333333334, 139.9, 60.43478260869565, 16.0625, 1.631578947368421]}, "h_index_by_years_from_publication_year": {"Publication Year": [2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 6, 7, 7, 7, 7, 7, 7, 7, 1, 5, 5, 5, 5, 5, 5, 5, 5, 4, 7, 9, 9, 9, 10, 11, 11, 5, 9, 12, 13, 14, 15, 15, 6, 10, 12, 12, 12, 12, 6, 11, 13, 15, 17, 6, 12, 15, 16, 7, 14, 19, 6, 9, 3]}}