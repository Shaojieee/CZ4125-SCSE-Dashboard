{"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:u-coK7KVo8oC": {"external_link": "http://openaccess.thecvf.com/content_CVPR_2020/html/Wei_Combating_Noisy_Labels_by_Agreement_A_Joint_Training_Method_with_CVPR_2020_paper.html", "authors": ["Hongxin Wei", "Lei Feng", "Xiangyu Chen", "Bo An"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "pages": "13726-13735", "description": "Deep Learning with noisy labels is a practically challenging problem in weakly-supervised learning. The state-of-the-art approaches\" Decoupling\" and\" Co-teaching+\" claim that the\" disagreement\" strategy is crucial for alleviating the problem of learning with noisy labels. In this paper, we start from a different perspective and propose a robust learning paradigm called JoCoR, which aims to reduce the diversity of two networks during training. Specifically, we first use two networks to make predictions on the same mini-batch data and calculate a joint loss with Co-Regularization for each training example. Then we select small-loss examples to update the parameters of both two networks simultaneously. Trained by the joint loss, these two networks would be more and more similar due to the effect of Co-Regularization. Extensive experimental results on corrupted data from benchmark datasets including MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is superior to many state-of-the-art approaches for learning with noisy labels.", "total_citations": {"2019": 1, "2020": 16, "2021": 88, "2022": 140, "2023": 155}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:HDshCWvjkbEC": {"external_link": "https://aamas.csc.liv.ac.uk/Proceedings/aamas2012/papers/1A_1.pdf", "authors": ["Eric Shieh", "Bo An", "Rong Yang", "Milind Tambe", "Craig Baldwin", "Joseph DiRenzo", "Ben Maule", "Garrett Meyer"], "publication_date": "2012/6/4", "book": "Proceedings of the 11th international conference on autonomous agents and multiagent systems-volume 1", "pages": "13-20", "description": "While three deployed applications of game theory for security have recently been reported at AAMAS [12], we as a community remain in the early stages of these deployments; there is a continuing need to understand the core principles for innovative security applications of game theory. Towards that end, this paper presents PROTECT, a game-theoretic system deployed by the United States Coast Guard (USCG) in the port of Boston for scheduling their patrols. USCG has termed the deployment of PROTECT in Boston a success, and efforts are underway to test it in the port of New York, with the potential for nationwide deployment. PROTECT is premised on an attacker-defender Stackelberg game model and offers five key innovations. First, this system is a departure from the assumption of perfect adversary rationality noted in previous work, relying instead on a quantal response (QR) model of the adversary\u2019s behavior\u2014to the best of our knowledge, this is the first real-world deployment of the QR model. Second, to improve PROTECT\u2019s efficiency, we generate a compact representation of the defender\u2019s strategy space, exploiting equivalence and dominance. Third, we show how to practically model a real maritime patrolling problem as a Stackelberg game. Fourth, our experimental results illustrate that PROTECT\u2019s QR model more robustly handles real-world uncertainties than a perfect rationality model. Finally, in evaluating PROTECT, this paper for the first time provides real-world data:(i) comparison of human-generated vs PROTECT security schedules, and (ii) results from an Adversarial Perspective Team\u2019s (human mock attackers) analysis.", "total_citations": {"2012": 17, "2013": 28, "2014": 27, "2015": 51, "2016": 36, "2017": 37, "2018": 23, "2019": 35, "2020": 30, "2021": 22, "2022": 14, "2023": 21}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:d1gkVwhDpl0C": {"external_link": "https://aamas.csc.liv.ac.uk/Proceedings/aamas2010/pdf/01%20Full%20Papers/20_04_FP_0757.pdf", "authors": ["Bo An", "Victor Lesser", "David Irwin", "Michael Zink"], "publication_date": "2010/5/10", "book": "Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume 1", "pages": "981-988", "description": "We consider the problem of allocating networked resources in dynamic environment, such as cloud computing platforms, where providers strategically price resources to maximize their utility. Resource allocation in these environments, where both providers and consumers are selfish agents, presents numerous challenges since the number of consumers and their resource demand is highly dynamic. While numerous auction-based approaches have been proposed in the literature, this paper explores an alternative approach where providers and consumers automatically negotiate resource leasing contracts. Since resource demand and supply can be dynamic and uncertain, we propose a distributed negotiation mechanism where agents negotiate over both a contract price and a decommitment penalty, which allows agents to decommit from contracts at a cost. We compare our approach experimentally, using representative scenarios and workloads, to both combinatorial auctions and the fixed-price model used by Amazon\u2019s Elastic Compute Cloud, and show that the negotiation model achieves a higher social welfare.", "total_citations": {"2010": 3, "2011": 15, "2012": 21, "2013": 35, "2014": 39, "2015": 26, "2016": 27, "2017": 24, "2018": 18, "2019": 18, "2020": 13, "2021": 14, "2022": 6, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:SdhP9T11ey4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8064175/", "authors": ["Yanhai Xiong", "Jiarui Gan", "Bo An", "Chunyan Miao", "Ana LC Bazzan"], "publication_date": "2017/10/10", "journal": "IEEE Transactions on Intelligent Transportation Systems", "volume": "19", "issue": "8", "pages": "2493-2504", "publisher": "IEEE", "description": "To reduce the air pollution and improve the energy efficiency, many countries and cities (e.g., Singapore) are on the way of introducing electric vehicles (EVs) to replace the vehicles serving in current traffic system. Effective placement of charging stations is essential for the rapid development of EVs, because it is necessary for providing convenience for EVs and ensuring the efficiency of the traffic network. However, existing works mostly concentrate on the mileage anxiety from EV users but ignore their strategic and competitive charging behaviors. To capture the competitive and strategic charging behaviors of the EV users, we consider that an EV user\u2019s charging cost, which is dependent on other EV users\u2019 choices, consists of the travel cost to access the charging station and the queuing cost in charging stations. First, we formulate the Charging Station Placement Problem (CSPP) as a bilevel optimization problem\u00a0\u2026", "total_citations": {"2016": 7, "2017": 11, "2018": 20, "2019": 43, "2020": 48, "2021": 49, "2022": 56, "2023": 27}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:TIZ-Mc8IlK0C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10500", "authors": ["Shanshan Feng", "Gao Cong", "Bo An", "Yeow Meng Chee"], "publication_date": "2017/2/10", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "31", "issue": "1", "description": "With the increasing popularity of location-aware social media applications, Point-of-Interest (POI) recommendation has recently been extensively studied. However, most of the existing studies explore from the users' perspective, namely recommending POIs for users. In contrast, we consider a new research problem of predicting users who will visit a given POI in a given future period. The challenge of the problem lies in the difficulty to effectively learn POI sequential transition and user preference, and integrate them for prediction. In this work, we propose a new latent representation model POI2Vec that is able to incorporate the geographical influence, which has been shown to be very important in modeling user mobility behavior. Note that existing representation models fail to incorporate the geographical influence. We further propose a method to jointly model the user preference and POI sequential transition influence for predicting potential visitors for a given POI. We conduct experiments on 2 real-world datasets to demonstrate the superiority of our proposed approach over the state-of-the-art algorithms for both next POI prediction and future user prediction.", "total_citations": {"2017": 3, "2018": 21, "2019": 41, "2020": 50, "2021": 50, "2022": 38, "2023": 24}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:YohjEiUPhakC": {"external_link": "https://ink.library.smu.edu.sg/sis_research/4792/", "authors": ["Arunesh Sinha", "Fei Fang", "Bo An", "Christopher Kiekintveld", "Milind Tambe"], "publication_date": "2018", "pages": "5494", "publisher": "IJCAI", "description": "The Stackelberg Security Game (SSG) model has been immensely influential in security research since it was introduced roughly a decade ago. Furthermore, deployed SSG-based applications are one of most successful examples of game theory applications in the real world. We present a broad survey of recent technical advances in SSG and related literature, and then look to the future by highlighting the new potential applications and open research problems in SSG.", "total_citations": {"2018": 7, "2019": 19, "2020": 30, "2021": 38, "2022": 47, "2023": 44}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:UHK10RUVsp4C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/19070", "authors": ["Fei Fang", "Thanh Nguyen", "Rob Pickles", "Wai Lam", "Gopalasamy Clements", "Bo An", "Amandeep Singh", "Milind Tambe", "Andrew Lemieux"], "publication_date": "2016/2/18", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "30", "issue": "2", "pages": "3966-3973", "description": "Poaching is a serious threat to the conservation of key species and whole ecosystems. While conducting foot patrols is the most commonly used approach in many countries to prevent poaching, such patrols often do not make the best use of limited patrolling resources. To remedy this situation, prior work introduced a novel emerging application called PAWS (Protection Assistant for Wildlife Security); PAWS was proposed as a game-theoretic (\u201csecurity games\u201d) decision aid to optimize the use of patrolling resources.", "total_citations": {"2016": 10, "2017": 25, "2018": 15, "2019": 27, "2020": 19, "2021": 21, "2022": 23, "2023": 15}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:BJbdYPG6LGMC": {"external_link": "https://www.ijcai.org/proceedings/2020/0305.pdf", "authors": ["Lei Feng", "Senlin Shu", "Zhuoyi Lin", "Fengmao Lv", "Li Li", "Bo An"], "publication_date": "2021/1/7", "book": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence", "pages": "2206-2212", "description": "Trained with the standard cross entropy loss, deep neural networks can achieve great performance on correctly labeled data. However, if the training data is corrupted with label noise, deep models tend to overfit the noisy labels, thereby achieving poor generation performance. To remedy this issue, several loss functions have been proposed and demonstrated to be robust to label noise. Although most of the robust loss functions stem from Categorical Cross Entropy (CCE) loss, they fail to embody the intrinsic relationships between CCE and other loss functions. In this paper, we propose a general framework dubbed Taylor cross entropy loss to train deep models in the presence of label noise. Specifically, our framework enables to weight the extent of fitting the training labels by controlling the order of Taylor Series for CCE, hence it can be robust to label noise. In addition, our framework clearly reveals the intrinsic relationships between CCE and other loss functions, such as Mean Absolute Error (MAE) and Mean Squared Error (MSE). Moreover, we present a detailed theoretical analysis to certify the robustness of this framework. Extensive experimental results on benchmark datasets demonstrate that our proposed approach significantly outperforms the state-of-the-art counterparts.", "total_citations": {"2020": 3, "2021": 20, "2022": 56, "2023": 37}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:WJVC3Jt7v1AC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/4233", "authors": ["Lei Feng", "Bo An"], "publication_date": "2019/7/17", "journal": "Proceedings of the AAAI conference on artificial intelligence", "volume": "33", "issue": "01", "pages": "3542-3549", "description": "Partial label learning deals with the problem where each training instance is assigned a set of candidate labels, only one of which is correct. This paper provides the first attempt to leverage the idea of self-training for dealing with partially labeled examples. Specifically, we propose a unified formulation with proper constraints to train the desired model and perform pseudo-labeling jointly. For pseudo-labeling, unlike traditional self-training that manually differentiates the ground-truth label with enough high confidence, we introduce the maximum infinity norm regularization on the modeling outputs to automatically achieve this consideratum, which results in a convex-concave optimization problem. We show that optimizing this convex-concave problem is equivalent to solving a set of quadratic programming (QP) problems. By proposing an upper-bound surrogate objective function, we turn to solving only one QP problem for improving the optimization efficiency. Extensive experiments on synthesized and real-world datasets demonstrate that the proposed approach significantly outperforms the state-of-the-art partial label learning approaches.", "total_citations": {"2019": 4, "2020": 22, "2021": 29, "2022": 25, "2023": 25}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:LgRImbQfgY4C": {"external_link": "https://proceedings.neurips.cc/paper/2020/hash/7bd28f15a49d5e5848d6ec70e584e625-Abstract.html", "authors": ["Lei Feng", "Jiaqi Lv", "Bo Han", "Miao Xu", "Gang Niu", "Xin Geng", "Bo An", "Masashi Sugiyama"], "publication_date": "2020", "journal": "Advances in neural information processing systems", "volume": "33", "pages": "10948-10960", "description": "Partial-label learning (PLL) is a multi-class classification problem, where each training example is associated with a set of candidate labels. Even though many practical PLL methods have been proposed in the last two decades, there lacks a theoretical understanding of the consistency of those methods-none of the PLL methods hitherto possesses a generation process of candidate label sets, and then it is still unclear why such a method works on a specific dataset and when it may fail given a different dataset. In this paper, we propose the first generation model of candidate label sets, and develop two PLL methods that are guaranteed to be provably consistent, ie, one is risk-consistent and the other is classifier-consistent. Our methods are advantageous, since they are compatible with any deep network or stochastic optimizer. Furthermore, thanks to the generation model, we would be able to answer the two questions above by testing if the generation model matches given candidate label sets. Experiments on benchmark and real-world datasets validate the effectiveness of the proposed generation model and two PLL methods.", "total_citations": {"2020": 1, "2021": 16, "2022": 34, "2023": 42}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:IjCSPb-OGe4C": {"external_link": "https://link.springer.com/article/10.1007/s10458-010-9137-2", "authors": ["Bo An", "Victor Lesser", "Kwang Mong Sim"], "publication_date": "2011/7", "journal": "Autonomous Agents and Multi-Agent Systems", "volume": "23", "pages": "114-153", "publisher": "Springer US", "description": "In electronic commerce markets where selfish agents behave individually, agents often have to acquire multiple resources in order to accomplish a high level task with each resource acquisition requiring negotiations with multiple resource providers. Thus, it is crucial to efficiently coordinate these interrelated negotiations. This paper presents the design and implementation of agents that concurrently negotiate with other entities for acquiring multiple resources. Negotiation agents in this paper are designed to adjust (1) the number of tentative agreements for each resource and (2) the amount of concession they are willing to make in response to changing market conditions and negotiation situations. In our approach, agents utilize a time-dependent negotiation strategy in which the reserve price of each resource is dynamically determined by (1) the likelihood that negotiation will not be successfully completed\u00a0\u2026", "total_citations": {"2010": 1, "2011": 5, "2012": 7, "2013": 9, "2014": 8, "2015": 10, "2016": 13, "2017": 9, "2018": 8, "2019": 7, "2020": 3, "2021": 3, "2022": 4, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:_FxGoFyzp5QC": {"external_link": "https://dl.acm.org/doi/pdf/10.1145/1978721.1978729", "authors": ["Bo An", "James Pita", "Eric Shieh", "Milind Tambe", "Chris Kiekintveld", "Janusz Marecki"], "publication_date": "2011/3/1", "journal": "ACM SIGecom Exchanges", "volume": "10", "issue": "1", "pages": "31-34", "publisher": "ACM", "description": "We provide an overview of two recent applications of security games. We describe new features and challenges introduced in the new applications.", "total_citations": {"2011": 1, "2012": 17, "2013": 6, "2014": 8, "2015": 6, "2016": 4, "2017": 8, "2018": 10, "2019": 10, "2020": 8, "2021": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:q-HalDI95KYC": {"external_link": "https://proceedings.mlr.press/v162/wei22d.html", "authors": ["Hongxin Wei", "Renchunzi Xie", "Hao Cheng", "Lei Feng", "Bo An", "Yixuan Li"], "publication_date": "2022/6/28", "conference": "International Conference on Machine Learning", "pages": "23631-23644", "publisher": "PMLR", "description": "Detecting out-of-distribution inputs is critical for the safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in-and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm)\u2014a simple fix to the cross-entropy loss\u2014by enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of output\u2019s norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in-and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30% on common benchmarks.", "total_citations": {"2021": 1, "2022": 13, "2023": 68}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:bz8QjSJIRt4C": {"external_link": "https://personal.ntu.edu.sg/boan/papers/IJCAI18_Label.pdf", "authors": ["Lei Feng", "Bo An"], "publication_date": "2018/7/13", "conference": "IJCAI", "pages": "2107-2113", "description": "In partial label learning, each training example is assigned a set of candidate labels, only one of which is the ground-truth label. Existing partial label learning frameworks either assume each candidate label of equal confidence or consider the ground-truth label as a latent variable hidden in the indiscriminate candidate label set, while the different labeling confidence levels of the candidate labels are regrettably ignored. In this paper, we formalize the different labeling confidence levels as the latent label distributions, and propose a novel unified framework to estimate the latent label distributions while training the model simultaneously. Specifically, we present a biconvex formulation with constrained local consistency and adopt an alternating method to solve this optimization problem. The process of alternating optimization exactly facilitates the mutual adaption of the model training and the constrained label propagation. Extensive experimental results on controlled UCI datasets as well as real-world datasets clearly show the effectiveness of the proposed approach.", "total_citations": {"2018": 1, "2019": 10, "2020": 15, "2021": 18, "2022": 21, "2023": 18}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:epqYDVWIO7EC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7562443/", "authors": ["Wanyuan Wang", "Jiuchuan Jiang", "Bo An", "Yichuan Jiang", "Bing Chen"], "publication_date": "2016/9/7", "journal": "IEEE transactions on cybernetics", "volume": "47", "issue": "12", "pages": "4208-4222", "publisher": "IEEE", "description": "Crowdsourcing has become a popular service computing paradigm for requesters to integrate the ubiquitous human-intelligence services for tasks that are difficult for computers but trivial for humans. This paper focuses on crowdsourcing complex tasks by team formation in social networks (SNs) where a requester connects to a large number of workers. A good indicator of efficient team collaboration is the social connection among workers. Most previous social team formation approaches, however, either assume that the requester can maintain information of all workers and can directly communicate with them to build teams, or assume that the workers are cooperative and be willing to join the specific team built by the requester, both of which are impractical in many real situations. To this end, this paper first models each worker as a selfish entity, where the requester prefers to hire inexpensive workers that require\u00a0\u2026", "total_citations": {"2017": 3, "2018": 10, "2019": 17, "2020": 11, "2021": 9, "2022": 16, "2023": 13}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:IWHjjKOFINEC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/7864", "authors": ["Bo An", "Milind Tambe", "Fernando Ordonez", "Eric Shieh", "Christopher Kiekintveld"], "publication_date": "2011/8/4", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "25", "issue": "1", "pages": "587-593", "description": "Given the real-world deployments of attacker-defender Stackelberg security games, robustness to deviations from expected attacker behaviors has now emerged as a critically important issue. This paper provides four key contributions in this context. First, it identifies a fundamentally problematic aspect of current algorithms for security games. It shows that there are many situations where these algorithms face multiple equilibria, and they arbitrarily select one that may hand the defender a significant disadvantage, particularly if the attacker deviates from its equilibrium strategies due to unknown constraints. Second, for important subclasses of security games, it identifies situations where we will face such multiple equilibria. Third, to address these problematic situations, it presents two equilibrium refinement algorithms that can optimize the defender's utility if the attacker deviates from equilibrium strategies. Finally, it experimentally illustrates that the refinement approach achieved significant robustness in consideration of attackers' deviation due to unknown constraints.", "total_citations": {"2012": 9, "2013": 9, "2014": 8, "2015": 7, "2016": 6, "2017": 11, "2018": 7, "2019": 4, "2020": 6, "2021": 3, "2022": 5, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:RYcK_YlVTxYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6511612/", "authors": ["Han Yu", "Zhiqi Shen", "Chunyan Miao", "Bo An"], "publication_date": "2012/12/4", "source": "2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology", "volume": "2", "pages": "486-493", "publisher": "IEEE", "description": "Crowd sourcing (CS) systems offer a new way for businesses and individuals to leverage on the power of mass collaboration to accomplish complex tasks in a divide-and-conquer manner. In existing CS systems, no facility has been provided for analyzing the trustworthiness of workers and providing decision support for allocating tasks to workers, which leads to high dependency of the quality of work on the behavior of workers in CS systems as shown in this paper. To address this problem, trust management mechanisms are urgently needed. Traditional trust management techniques are focused on identifying the most trustworthy service providers (SPs) as accurately as possible. Little thoughts were given to the question of how to utilize these SPs due to two common assumptions: 1) an SP can serve an unlimited number of requests in one time unit, and 2) a service consumer (SC) only needs to select one SP for\u00a0\u2026", "total_citations": {"2011": 1, "2012": 1, "2013": 4, "2014": 5, "2015": 16, "2016": 11, "2017": 9, "2018": 4, "2019": 9, "2020": 6, "2021": 4, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:eq2jaN3J8jMC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/8236", "authors": ["Bo An", "David Kempe", "Christopher Kiekintveld", "Eric Shieh", "Satinder Singh", "Milind Tambe", "Yevgeniy Vorobeychik"], "publication_date": "2012", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "26", "issue": "1", "pages": "1241-1248", "description": "Randomized first-mover strategies of Stackelberg games are used in several deployed applications to allocate limited resources for the protection of critical infrastructure. Stackelberg games model the fact that a strategic attacker can surveil and exploit the defender's strategy, and randomization guards against the worst effects by making the defender less predictable. In accordance with the standard game-theoretic model of Stackelberg games, past work has typically assumed that the attacker has perfect knowledge of the defender's randomized strategy and will react correspondingly. In light of the fact that surveillance is costly, risky, and delays an attack, this assumption is clearly simplistic: attackers will usually act on partial knowledge of the defender's strategies. The attacker's imperfect estimate could present opportunities and possibly also threats to a strategic defender.", "total_citations": {"2012": 1, "2013": 9, "2014": 14, "2015": 13, "2016": 11, "2017": 6, "2018": 6, "2019": 7, "2020": 2, "2021": 2, "2022": 2, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:GFxP56DSvIMC": {"external_link": "http://proceedings.mlr.press/v119/feng20a.html?ref=https://githubhelp.com", "authors": ["Lei Feng", "Takuo Kaneko", "Bo Han", "Gang Niu", "Bo An", "Masashi Sugiyama"], "publication_date": "2020/11/21", "conference": "International Conference on Machine Learning", "pages": "3072-3081", "publisher": "PMLR", "description": "A complementary label (CL) simply indicates an incorrect class of an example, but learning with CLs results in multi-class classifiers that can predict the correct class. Unfortunately, the problem setting only allows a single CL for each example, which notably limits its potential since our labelers may easily identify multiple CLs (MCLs) to one example. In this paper, we propose a novel problem setting to allow MCLs for each example and two ways for learning with MCLs. In the first way, we design two wrappers that decompose MCLs into many single CLs, so that we could use any method for learning with CLs. However, the supervision information that MCLs hold is conceptually diluted after decomposition. Thus, in the second way, we derive an unbiased risk estimator; minimizing it processes each set of MCLs as a whole and possesses an estimation error bound. We further improve the second way into minimizing properly chosen upper bounds. Experiments show that the former way works well for learning with MCLs but the latter is even better.", "total_citations": {"2020": 10, "2021": 18, "2022": 21, "2023": 26}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&citation_for_view=PEEpuNwAAAAJ:YFjsv_pBGBYC": {"external_link": "https://pubsonline.informs.org/doi/abs/10.1287/inte.2013.0700", "authors": ["Bo An", "Fernando Ord\u00f3\u00f1ez", "Milind Tambe", "Eric Shieh", "Rong Yang", "Craig Baldwin", "Joseph DiRenzo III", "Kathryn Moretti", "Ben Maule", "Garrett Meyer"], "publication_date": "2013/10", "journal": "Interfaces", "volume": "43", "issue": "5", "pages": "400-420", "publisher": "INFORMS", "description": "In this paper, we describe the model, theory developed, and deployment of PROTECT, a game-theoretic system that the United States Coast Guard (USCG) uses to schedule patrols in the Port of Boston. The USCG evaluated PROTECT\u2019s deployment in the Port of Boston as a success and is currently evaluating the system in the Port of New York, with the potential for nationwide deployment. PROTECT is premised on an attacker-defender Stackelberg game model; however, its development and implementation required both theoretical contributions and detailed evaluations. We describe the work required in the deployment, which we group into five key innovations. First, we propose a compact representation of the defender\u2019s strategy space by exploiting equivalence and dominance, to make PROTECT efficient enough to solve real-world sized problems. Second, this system does not assume that adversaries are\u00a0\u2026", "total_citations": {"2012": 1, "2013": 0, "2014": 5, "2015": 5, "2016": 4, "2017": 13, "2018": 5, "2019": 3, "2020": 11, "2021": 8, "2022": 12, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:kz9GbA2Ns4gC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/IJCAI17_ADML.pdf", "authors": ["Mengchen Zhao", "Bo An", "Wei Gao", "Teng Zhang"], "publication_date": "2017/8/19", "conference": "IJCAI", "pages": "3945-3951", "description": "Label contamination attack (LCA) is an important type of data poisoning attack where an attacker manipulates the labels of training data to make the learned model beneficial to him. Existing work on LCA assumes that the attacker has full knowledge of the victim learning model, whereas the victim model is usually a black-box to the attacker. In this paper, we develop a Projected Gradient Ascent (PGA) algorithm to compute LCAs on a family of empirical risk minimizations and show that an attack on one victim model can also be effective on other victim models. This makes it possible that the attacker designs an attack against a substitute model and transfers it to a black-box victim model. Based on the observation of the transferability, we develop a defense algorithm to identify the data points that are most likely to be attacked. Empirical studies show that PGA significantly outperforms existing baselines and linear learning models are better substitute models than nonlinear ones.", "total_citations": {"2017": 2, "2018": 7, "2019": 6, "2020": 14, "2021": 11, "2022": 16, "2023": 16}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:AvfA0Oy_GE0C": {"external_link": "https://ojs.aaai.org/index.php/aimagazine/article/view/2710/2611", "authors": ["Fei Fang", "Thanh H Nguyen", "Rob Pickles", "Wai Y Lam", "Gopalasamy R Clements", "Bo An", "Amandeep Singh", "Brian C Schwedock", "Milin Tambe", "Andrew Lemieux"], "publication_date": "2017/3/31", "journal": "AI Magazine", "volume": "38", "issue": "1", "pages": "23-36", "description": "Poaching is considered a major driver for the population drop of key species such as tigers, elephants, and rhinos, which can be detrimental to whole ecosystems. While conducting foot patrols is the most commonly used approach in many countries to prevent poaching, such patrols often do not make the best use of the limited patrolling resources.", "total_citations": {"2017": 1, "2018": 14, "2019": 10, "2020": 16, "2021": 11, "2022": 11, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:b0M2c_1WBrUC": {"external_link": "https://ojs.aaai.org/index.php/ICAPS/article/view/13614", "authors": ["Yevgeniy Vorobeychik", "Bo An", "Milind Tambe", "Satinder Singh"], "publication_date": "2014/5/11", "journal": "Proceedings of the International Conference on Automated Planning and Scheduling", "volume": "24", "pages": "314-322", "description": "Stackelberg games form the core of a number of tools deployed for computing optimal patrolling strategies in adversarial domains, such as the US Federal Air Marshall Service and the US Coast Guard. In traditional Stackelberg security game models the attacker knows only the probability that each target is covered by the defender, but is oblivious to the detailed timing of the coverage schedule. In many real-world situations, however, the attacker can observe the current location of the defender and can exploit this knowledge to reason about the defender\u2019s future moves. We show that this general modeling framework can be captured using adversarial patrolling games (APGs) in which the defender sequentially moves between targets, with moves constrained by a graph, while the attacker can observe the defender\u2019s current location and his (stochastic) policy concerning future moves. We offer a very general model of infinite-horizon discounted adversarial patrolling games. Our first contribution is to show that defender policies that condition only on the previous defense move (ie, Markov stationary policies) can be arbitrarily suboptimal for general APGs. We then offer a mixed-integer non-linear programming (MINLP) formulation for computing optimal randomized policies for the defender that can condition on history of bounded, but arbitrary, length, as well as a mixed-integer linear programming (MILP) formulation to approximate these, with provable quality guarantees. Additionally, we present a non-linear programming (NLP) formulation for solving zero-sum APGs. We show experimentally that MILP significantly outperforms the MINLP formulation\u00a0\u2026", "total_citations": {"2014": 4, "2015": 10, "2016": 8, "2017": 9, "2018": 7, "2019": 9, "2020": 8, "2021": 4, "2022": 3, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:rO6llkc54NcC": {"external_link": "https://www.ifaamas.org/Proceedings/aamas2013/docs/p1315.pdf", "authors": ["Han Yu", "Zhiqi Shen", "Chunyan Miao", "Bo An"], "publication_date": "2013/5/6", "book": "Proceedings of the 2013 international conference on autonomous agents and multi-agent systems", "pages": "1315-1316", "description": "A crowdsourcing system is a useful platform for utilizing the intelligence and skills of the mass. Nevertheless, like any open system that involves the exchange of things of value, selfish and malicious behaviors exist in crowdsourcing systems and need to be mitigated. Trust management has been proven to be a viable solution in many systems. However, a major difference between crowdsourcing systems and existing trust models designed for multi-agent systems is that human trustees have limited task processing capacity per unit time compared to an intelligent agent program. This paper recognizes a problem in current trust-aware decision-making methods for task assignment in crowdsourcing platforms. On the one hand, trust-based methods over-assign tasks to trusted workers, while on the other hand, workload-based solutions do not give sufficient guarantees on the quality of work. The proposed solution, the social welfare optimizing reputation-aware decision-making (SWORD) approach, strikes a balance between the two and is shown through extensive simulations to significantly improve social welfare of crowdsourcing platforms compared to related work.", "total_citations": {"2012": 1, "2013": 2, "2014": 3, "2015": 13, "2016": 21, "2017": 6, "2018": 6, "2019": 6, "2020": 2, "2021": 3, "2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:ns9cj8rnVeAC": {"external_link": "https://aamas.csc.liv.ac.uk/Proceedings/aamas2013/docs/p223.pdf", "authors": ["Bo An", "Matthew Brown", "Yevgeniy Vorobeychik", "Milind Tambe"], "publication_date": "2013/5/6", "book": "Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems", "pages": "223-230", "description": "Stackelberg games have been used in several deployed applications to allocate limited resources for critical infrastructure protection. These resource allocation strategies are randomized to prevent a strategic attacker from using surveillance to learn and exploit patterns in the allocation. Past work has typically assumed that the attacker has perfect knowledge of the defender\u2019s randomized strategy or can learn the defender\u2019s strategy after conducting a fixed period of surveillance. In consideration of surveillance cost, these assumptions are clearly simplistic since attackers may act with partial knowledge of the defender\u2019s strategies and may dynamically decide whether to attack or conduct more surveillance. In this paper, we propose a natural model of limited surveillance in which the attacker dynamically determine a place to stop surveillance in consideration of his updated belief based on observed actions and surveillance cost. We show an upper bound on the maximum number of observations the attacker can make and show that the attacker\u2019s optimal stopping problem can be formulated as a finite state space MDP. We give mathematical programs to compute optimal attacker and defender strategies. We compare our approaches with the best known previous solutions and experimental results show that the defender can achieve significant improvement in expected utility by taking the attacker\u2019s optimal stopping decision into account, validating the motivation of our work.", "total_citations": {"2012": 1, "2013": 1, "2014": 11, "2015": 12, "2016": 8, "2017": 13, "2018": 4, "2019": 5, "2020": 5, "2021": 2, "2022": 4, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:Ehil0879vHcC": {"external_link": "https://proceedings.mlr.press/v119/wang20i.html", "authors": ["Rundong Wang", "Xu He", "Runsheng Yu", "Wei Qiu", "Bo An", "Zinovi Rabinovich"], "publication_date": "2020/11/21", "conference": "International Conference on Machine Learning", "pages": "9908-9918", "publisher": "PMLR", "description": "We consider the problem of the limited-bandwidth communication for multi-agent reinforcement learning, where agents cooperate with the assistance of a communication protocol and a scheduler. The protocol and scheduler jointly determine which agent is communicating what message and to whom. Under the limited bandwidth constraint, a communication protocol is required to generate informative messages. Meanwhile, an unnecessary communication connection should not be established because it occupies limited resources in vain. In this paper, we develop an Informative Multi-Agent Communication (IMAC) method to learn efficient communication protocols as well as scheduling. First, from the perspective of communication theory, we prove that the limited bandwidth constraint requires low-entropy messages throughout the transmission. Then inspired by the information bottleneck principle, we learn a valuable and compact communication protocol and a weight-based scheduler. To demonstrate the efficiency of our method, we conduct extensive experiments in various cooperative and competitive multi-agent tasks with different numbers of agents and different bandwidths. We show that IMAC converges faster and leads to efficient communication among agents under the limited bandwidth as compared to many baseline methods.", "total_citations": {"2020": 5, "2021": 13, "2022": 27, "2023": 21}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:2KloaMYe4IUC": {"external_link": "https://www.ifaamas.org/Proceedings/aamas2016/pdfs/p468.pdf", "authors": ["Jinhua Song", "Yang Gao", "Hao Wang", "Bo An"], "publication_date": "2016/5/9", "book": "Proceedings of the 2016 international conference on autonomous agents & multiagent systems", "pages": "468-476", "description": "Markov decision processes (MDPs) have been studied for many decades. Recent research in using transfer learning methods to solve MDPs has shown that knowledge learned from one MDP may be used to solve a similar MDP better. In this paper, we propose two metrics for measuring the distance between finite MDPs. Our metrics are based on the Hausdorff metric which measures the distance between two subsets of a metric space and the Kantorovich metric for measuring the distance between probabilistic distributions. Our metrics can be used to compute the distance between reinforcement learning tasks that are modeled as MDPs. The second contribution of this paper is that we apply the metrics to direct transfer learning by finding the similar source tasks. Our third contribution is that we propose two knowledge transfer methods which transfer value functions of the selected source tasks to the target task. Extensive experimental results show that our metrics are effective in finding similar tasks and significantly improve the performance of transfer learning with the transfer methods.", "total_citations": {"2016": 1, "2017": 0, "2018": 6, "2019": 9, "2020": 12, "2021": 13, "2022": 17, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:LhH-TYMQEocC": {"external_link": "https://www.ijcai.org/proceedings/2019/0318.pdf", "authors": ["Lei Feng", "Bo An"], "publication_date": "2019/8/10", "conference": "IJCAI", "pages": "2294-2300", "description": "Partial label learning is a weakly supervised learning framework, in which each instance is provided with multiple candidate labels while only one of them is correct. Most of the existing approaches focus on leveraging the instance relationships to disambiguate the given noisy label space, while it is still unclear whether we can exploit potentially useful information in label space to alleviate the label ambiguities. This paper gives a positive answer to this question for the first time. Specifically, if two instances do not share any common candidate labels, they cannot have the same groundtruth label. By exploiting such dissimilarity relationships from label space, we propose a novel approach that aims to maximize the latent semantic differences of the two instances whose groundtruth labels are definitely different, while training the desired model simultaneously, thereby continually enlarging the gap of label confidences between two instances of different classes. Extensive experiments on artificial and real-world partial label datasets show that our approach significantly outperforms state-of-the-art counterparts.", "total_citations": {"2019": 3, "2020": 13, "2021": 22, "2022": 15, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:B3FOqHPlNUQC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6847689/", "authors": ["Yujing Hu", "Yang Gao", "Bo An"], "publication_date": "2014/7/2", "journal": "IEEE transactions on cybernetics", "volume": "45", "issue": "4", "pages": "647-662", "publisher": "IEEE", "description": "One important approach of multiagent reinforcement learning (MARL) is equilibrium-based MARL, which is a combination of reinforcement learning and game theory. Most existing algorithms involve computationally expensive calculation of mixed strategy equilibria and require agents to replicate the other agents' value functions for equilibrium computing in each state. This is unrealistic since agents may not be willing to share such information due to privacy or safety concerns. This paper aims to develop novel and efficient MARL algorithms without the need for agents to share value functions. First, we adopt pure strategy equilibrium solution concepts instead of mixed strategy equilibria given that a mixed strategy equilibrium is often computationally expensive. In this paper, three types of pure strategy profiles are utilized as equilibrium solution concepts: pure strategy Nash equilibrium, equilibrium-dominating\u00a0\u2026", "total_citations": {"2015": 6, "2016": 6, "2017": 7, "2018": 3, "2019": 11, "2020": 11, "2021": 8, "2022": 3, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:zA6iFVUQeVQC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/trustijcai13.pdf", "authors": ["Han Yu", "Chunyan Miao", "Bo An", "Cyril Leung", "Victor R Lesser"], "publication_date": "2013/6/29", "conference": "Twenty-Third International Joint Conference on Artificial Intelligence", "description": "Trust is an important mechanism enabling agents to self-police open and dynamic multi-agent systems (ODMASs). Trusters evaluate the reputation of trustees based on their past observed performance, and use this information to guide their future interaction decisions. Existing trust models tend to concentrate trusters\u2019 interactions on a small number of highly reputable trustees to minimize risk exposure. When a trustee\u2019s servicing capacity is limited, such an approach may cause long delays for trusters and subsequently damage the reputation of trustees. To mitigate this problem, we propose a reputation management approach for trustee agents based on distributed constraint optimization. It helps a trustee to make situation-aware decisions on which incoming requests to serve and prevent the resulting reputation score from being affected by factors out of the trustee\u2019s control. The approach is evaluated through theoretical analysis and within a simulated, highly dynamic multi-agent environment. The results show that it can achieve close to optimally efficient utilization of the trustee agents\u2019 collective capacity in an ODMAS, promotes fair treatment of trustee agents based on their behavior, and significantly outperforms related work in enhancing social welfare.", "total_citations": {"2012": 1, "2013": 1, "2014": 2, "2015": 10, "2016": 13, "2017": 10, "2018": 7, "2019": 3, "2020": 2, "2021": 3, "2022": 2, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:Aul-kAQHnToC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/4234", "authors": ["Lei Feng", "Bo An", "Shuo He"], "publication_date": "2019/7/17", "journal": "Proceedings of the AAAI conference on artificial intelligence", "volume": "33", "issue": "01", "pages": "3550-3557", "description": "It is well-known that exploiting label correlations is crucially important to multi-label learning. Most of the existing approaches take label correlations as prior knowledge, which may not correctly characterize the real relationships among labels. Besides, label correlations are normally used to regularize the hypothesis space, while the final predictions are not explicitly correlated. In this paper, we suggest that for each individual label, the final prediction involves the collaboration between its own prediction and the predictions of other labels. Based on this assumption, we first propose a novel method to learn the label correlations via sparse reconstruction in the label space. Then, by seamlessly integrating the learned label correlations into model training, we propose a novel multi-label learning approach that aims to explicitly account for the correlated predictions of labels while training the desired model simultaneously. Extensive experimental results show that our approach outperforms the state-of-the-art counterparts.", "total_citations": {"2019": 5, "2020": 12, "2021": 16, "2022": 16, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:u5HHmVD_uO8C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4407274/", "authors": ["Michael Krainin", "Bo An", "Victor Lesser"], "publication_date": "2007/11/2", "conference": "2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT'07)", "pages": "138-145", "publisher": "IEEE", "description": "Through automated negotiation we aim to improve task allocation in a distributed sensor network. In particular, we look at a type of adaptive weather-sensing radar that permits the radar to focus its scanning on certain regions of the atmosphere. Current control systems can only computationally handle the decision making for a small number of radars because of the complexity of the process. One solution is to partition the radars into smaller, independent sets. Redundant scanning of tasks and loss of cooperative scanning capabilities can occur as a result. With negotiation we can reduce these occurrences, helping to ensure that the correct radars scan tasks based on the overall social welfare. We develop a distributed negotiation model where on each cycle the overall system utility improves or remains constant. Experimental results show that as compared to the centralized task allocation mechanism, the proposed\u00a0\u2026", "total_citations": {"2007": 1, "2008": 5, "2009": 2, "2010": 13, "2011": 10, "2012": 4, "2013": 4, "2014": 2, "2015": 3, "2016": 2, "2017": 0, "2018": 0, "2019": 2, "2020": 0, "2021": 1, "2022": 3, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:u-x6o8ySG0sC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4014561/", "authors": ["Bo An", "Kwang Mong Sim", "Liang Gui Tang", "Shuang Qing Li", "Dai Jie Cheng"], "publication_date": "2006/11/20", "journal": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)", "volume": "36", "issue": "6", "pages": "1261-1272", "publisher": "IEEE", "description": "While there are several existing mechanisms and systems addressing the crucial and difficult issues of automated one-to-many negotiation, this paper develops a flexible one-to-many negotiation mechanism for software agents. Unlike the existing general one-to-many negotiation mechanism, in which an agent should wait until it has received proposals from all its trading partners before generating counterproposals, in the flexible one-to-many negotiation mechanism, an agent can make a proposal in a flexible way during negotiation, i.e., negotiation is conducted in continuous time. To decide when to make a proposal, two strategies based on fixed waiting time and a fixed waiting ratio are proposed. Results from a series of experiments suggest that, guided by the two strategies for deciding when to make a proposal, the flexible negotiation mechanism achieved more favorable trading outcomes as compared with\u00a0\u2026", "total_citations": {"2007": 2, "2008": 6, "2009": 3, "2010": 5, "2011": 3, "2012": 3, "2013": 7, "2014": 6, "2015": 4, "2016": 7, "2017": 4, "2018": 2, "2019": 1, "2020": 2, "2021": 2, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:r0BpntZqJG4C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/8436", "authors": ["Eric Shieh", "Bo An", "Rong Yang", "Milind Tambe", "Craig Baldwin", "Joseph DiRenzo", "Ben Maule", "Garrett Meyer"], "publication_date": "2012", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "26", "issue": "1", "pages": "2173-2179", "description": "Building upon previous security applications of computational game theory, this paper presents PROTECT, a game-theoretic system deployed by the United States Coast Guard (USCG) in the port of Boston for scheduling their patrols. USCG has termed the deployment of PROTECT in Boston a success, and efforts are underway to test it in the port of New York, with the potential for nationwide deployment. PROTECT is premised on an attacker-defender Stackelberg game model and offers five key innovations. First, this system is a departure from the assumption of perfect adversary rationality noted in previous work, relying instead on a quantal response (QR) model of the adversary's behavior-to the best of our knowledge, this is the first real-world deployment of the QR model. Second, to improve PROTECT's efficiency, we generate a compact representation of the defender's strategy space, exploiting equivalence and dominance. Third, we show how to practically model a real maritime patrolling problem as a Stackelberg game. Fourth, our experimental results illustrate that PROTECT's QR model more robustly handles real-world uncertainties than a perfect rationality model. Finally, in evaluating PROTECT, this paper provides real-world data:(i) comparison of human-generated vs PROTECT security schedules, and (ii) results from an Adversarial Perspective Team's (human mock attackers) analysis.", "total_citations": {"2012": 1, "2013": 2, "2014": 3, "2015": 7, "2016": 4, "2017": 12, "2018": 6, "2019": 9, "2020": 5, "2021": 6, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:p2g8aNsByqUC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/8804", "authors": ["Thanh Nguyen", "Amulya Yadav", "Bo An", "Milind Tambe", "Craig Boutilier"], "publication_date": "2014/6/21", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "28", "issue": "1", "description": "Stackelberg security games (SSGs) have been deployed in a number of real-world domains. One key challenge in these applications is the assessment of attacker payoffs, which may not be perfectly known. Previous work has studied SSGs with uncertain payoffs modeled by interval uncertainty and provided maximin-based robust solutions. In contrast, in this work we propose the use of the less conservative minimax regret decision criterion for such payoff-uncertain SSGs and present the first algorithms for computing minimax regret for SSGs. We also address the challenge of preference elicitation, using minimax regret to develop the first elicitation strategies for SSGs. Experimental results validate the effectiveness of our approaches.", "total_citations": {"2015": 12, "2016": 5, "2017": 6, "2018": 10, "2019": 10, "2020": 4, "2021": 3, "2022": 4, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:O3NaXMp0MMsC": {"external_link": "https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2401", "authors": ["Bo An", "Eric Shieh", "Milind Tambe", "Rong Yang", "Craig Baldwin", "Joseph DiRenzo", "Ben Maule", "Garrett Meyer"], "publication_date": "2012/12/21", "journal": "Ai Magazine", "volume": "33", "issue": "4", "pages": "96-96", "description": "While three deployed applications of game theory for security have recently been reported, we as a community of agents and AI researchers remain in the early stages of these deployments; there is a continuing need to understand the core principles for innovative security applications of game theory. Towards that end, this paper presents PROTECT, a game-theoretic system deployed by the United States Coast Guard (USCG) in the port of Boston for scheduling their patrols. USCG has termed the deployment of PROTECT in Boston a success, and efforts are underway to test it in the port of New York, with the potential for nationwide deployment.", "total_citations": {"2012": 2, "2013": 2, "2014": 2, "2015": 4, "2016": 7, "2017": 7, "2018": 7, "2019": 7, "2020": 5, "2021": 1, "2022": 6, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:W5xh706n7nkC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10030", "authors": ["Mengchen Zhao", "Bo An", "Christopher Kiekintveld"], "publication_date": "2016/2/21", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "30", "issue": "1", "description": "Highly targeted spear phishing attacks are increasingly common, and have been implicated in many major security breeches. Email filtering systems are the first line of defense against such attacks. These filters are typically configured with uniform thresholds for deciding whether or not to allow a message to be delivered to a user. However, users have very significant differences in both their susceptibility to phishing attacks as well as their access to critical information and credentials that can cause damage. Recent work has considered setting personalized thresholds for individual users based on a Stackelberg game model. We consider two important extensions of the previous model. First, in our model user values can be substitutable, modeling cases where multiple users provide access to the same information or credential. Second, we consider attackers who make sequential attack plans based on the outcome of previous attacks. Our analysis starts from scenarios where there is only one credential and then extends to more general scenarios with multiple credentials. For single-credential scenarios, we demonstrate that the optimal defense strategy can be found by solving a binary combinatorial optimization problem called PEDS. For multiple-credential scenarios, we formulate it as a bilevel optimization problem for finding the optimal defense strategy and then reduce it to a single level optimization problem called PEMS using complementary slackness conditions. Experimental results show that both PEDS and PEMS lead to significant higher defender utilities than two existing benchmarks in different parameter settings. Also, both PEDS and\u00a0\u2026", "total_citations": {"2016": 5, "2017": 10, "2018": 9, "2019": 9, "2020": 6, "2021": 7, "2022": 1, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:t6usbXjVLHcC": {"external_link": "https://scholar.google.com/scholar?cluster=15770887310108949548&hl=en&oi=scholarr", "authors": ["Bo An", "Milind Tambe", "Arunesh Sinha"], "publication_date": "2017/11/2", "journal": "Improving Homeland Security Decisions", "pages": "485", "publisher": "Cambridge University Press", "description": "Security is a critical concern around the world that arises in protecting our ports, airports, transportation or other critical national infrastructure, curtailing the illegal flow of drugs, weapons, and money, and suppressing urban crime, as well as in protecting wildlife, fish, and forests from poachers and smugglers; and it arises in problems ranging from physical to cyber-physical systems. In all of these challenges, we have limited security resources that prevent full security coverage at all times. Instead, limited security resources must be deployed intelligently, taking into account differences in priorities of targets requiring security coverage, the responses of adversaries to the security posture, and potential uncertainty over the types, capabilities, knowledge, and priorities of adversaries faced. Game theory is well suited to adversarial reasoning for security resource allocation and scheduling problems. Casting the problem as a Bayesian Stackelberg game in consideration of uncertainties, we have developed new algorithms for efficiently solving such games to provide randomized patrolling or inspection strategies. These algorithms have led to some initial successes in this arena, leading to advances over previous approaches in security resource scheduling and allocation, eg, by addressing key weaknesses of predictability of human schedulers. These algorithms are now deployed in multiple applications: ARMOR has been deployed at the Los Angeles International Airport (LAX) since 2007 to randomize checkpoints on the roadways entering the airport and canine patrol routes within the airport terminals (Pita et al., 2008); IRIS is a game-theoretic\u00a0\u2026", "total_citations": {"2016": 2, "2017": 6, "2018": 3, "2019": 6, "2020": 8, "2021": 7, "2022": 12, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:VOx2b1Wkg3QC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/9323", "authors": ["Jiarui Gan", "Bo An", "Yevgeniy Vorobeychik"], "publication_date": "2015/2/16", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "29", "issue": "1", "description": "Stackelberg security games have been widely deployed in recent years to schedule security resources. An assumption in most existing security game models is that one security resource assigned to a target only protects that target. However, in many important real-world security scenarios, when a resource is assigned to a target, it exhibits protection externalities: that is, it also protects other \u201cneighbouring\u201d targets. We investigate such Security Games with Protection Externalities (SPEs). First, we demonstrate that computing a strong Stackelberg equilibrium for an SPE is NP-hard, in contrast with traditional Stackelberg security games which can be solved in polynomial time. On the positive side, we propose a novel column generation based approach\u2014CLASPE\u2014to solve SPEs. CLASPE features the following novelties: 1) a novel mixed-integer linear programming formulation for the slave problem; 2) an extended greedy approach with a constant-factor approximation ratio to speed up the slave problem; and 3) a linear-scale linear programming that efficiently calculates the upper bounds of target-defined subproblems for pruning. Our experimental evaluation demonstrates that CLASPE enable us to scale to realistic-sized SPE problem instances.", "total_citations": {"2015": 6, "2016": 7, "2017": 7, "2018": 10, "2019": 6, "2020": 8, "2021": 1, "2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:WbkHhVStYXYC": {"external_link": "https://dl.acm.org/doi/abs/10.5555/2615731.2615791", "authors": ["Han Yu", "Chunyan Miao", "Bo An", "Zhiqi Shen", "Cyril Leung"], "publication_date": "2014/5/5", "book": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems", "pages": "357-364", "description": "Compared to automated entities, human trustees have two distinct characteristics: 1) they are resource constrained (with limited time and effort to serve requests), and 2) their utility is not linearly related to income. Existing research in reputation-aware task delegation did not consider these two issues together. This limits their effectiveness in human-agent collectives such as crowdsourcing systems. In this paper, we propose a distributed reputation-aware task allocation approach - RATA-NL - to address these issues simultaneously. It is designed to help an individual human trustee determine the optimal number of task requests to accept at each time step based on his situation to maximize his long term well-being. The resulting task allocation maximizes social welfare through efficient utilization of the collective capacity of the trustees, and provides provable performance guarantees. RATA-NL has been compared\u00a0\u2026", "total_citations": {"2015": 13, "2016": 18, "2017": 4, "2018": 3, "2019": 1, "2020": 1, "2021": 2, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:4fKUyHm3Qg0C": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0167923614001808", "authors": ["Han Yu", "Zhiqi Shen", "Chunyan Miao", "Bo An", "Cyril Leung"], "publication_date": "2014/10/1", "journal": "Decision Support Systems", "volume": "66", "pages": "102-113", "publisher": "North-Holland", "description": "In open online communities such as e-commerce, participants need to rely on services provided by others in order to thrive. Accurately estimating the trustworthiness of a potential interaction partner is vital to a participant's well-being. It is generally recognized in the research community that third-party testimony sharing is an effective way for participants to gain knowledge about the trustworthiness of potential interaction partners without having to incur the risk of actually interacting with them. However, the presence of biased testimonies adversely affects a participant's long term well-being. Existing trust computational models often require complicated manual tuning of key parameters to combat biased testimonies. Such an approach heavily involves subjective judgments and adapts poorly to changes in an environment. In this study, we propose the Actor\u2013Critic Trust (ACT) model, which is an adaptive trust evidence\u00a0\u2026", "total_citations": {"2015": 4, "2016": 11, "2017": 15, "2018": 4, "2019": 5, "2020": 4, "2021": 2, "2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:u9iWguZQMMsC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/8794", "authors": ["Yue Yin", "Bo An", "Manish Jain"], "publication_date": "2014/6/21", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "28", "issue": "1", "description": "High profile large scale public events are attractive targets for terrorist attacks. The recent Boston Marathon bombings on April 15, 2013 have further emphasized the importance of protecting public events. The security challenge is exacerbated by the dynamic nature of such events: eg, the impact of an attack at different locations changes over time as the Boston marathon participants and spectators move along the race track. In addition, the defender can relocate security resources among potential attack targets at any time and the attacker may act at any time during the event. This paper focuses on developing efficient patrolling algorithms for such dynamic domains with continuous strategy spaces for both the defender and the attacker. We aim at computing optimal pure defender strategies, since an attacker does not have an opportunity to learn and respond to mixed strategies due to the relative infrequency of such events. We propose SCOUT-A, which makes assumptions on relocation cost, exploits payoff representation and computes optimal solutions efficiently. We also propose SCOUT-C to compute the exact optimal defender strategy for general cases despite the continuous strategy spaces. SCOUT-C computes the optimal defender strategy by constructing an equivalent game with discrete defender strategy space, then solving the constructed game. Experimental results show that both SCOUT-A and SCOUT-C significantly outperform other existing strategies.", "total_citations": {"2015": 4, "2016": 7, "2017": 9, "2018": 6, "2019": 11, "2020": 7, "2021": 2, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:D03iK_w7-QYC": {"external_link": "https://www.ijcai.org/Proceedings/13/Papers/414.pdf", "authors": ["Jiarui Gan", "Bo An", "Haizhong Wang", "Xiaoming Sun", "Zhongzhi Shi"], "publication_date": "2013/6/30", "conference": "Twenty-Third International Joint Conference on Artificial Intelligence", "description": "In Beijing, most taxi drivers intentionally avoid working during peak hours despite of the huge customer demand within these peak periods. This dilemma is mainly due to the fact that taxi drivers\u2019 congestion costs are not reflected in the current taxi fare structure. To resolve this problem, we propose a new pricing scheme to provide taxi drivers with extra incentives to work during peak hours. This differs from previous studies of taxi market by considering market variance over multiple periods, taxi drivers\u2019 profit-driven decisions, and their scheduling constraints regarding the interdependence among different periods. The major challenge of this research is the computational intensiveness to identify optimal strategy due to the exponentially large size of a taxi driver\u2019s strategy space and the scheduling constraints. We develop an atom schedule method to overcome these issues. It reduces the magnitude of the problem while satisfying the constraints to filter out infeasible pure strategies. Simulation results based on real data show the effectiveness of the proposed methods, which opens up a new door to improving the efficiency of taxi market in megacities (eg, Beijing).", "total_citations": {"2014": 2, "2015": 3, "2016": 2, "2017": 8, "2018": 8, "2019": 7, "2020": 3, "2021": 6, "2022": 5, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:SpbeaW3--B0C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/11838", "authors": ["Mengchen Zhao", "Bo An", "Yaodong Yu", "Sulin Liu", "Sinno Pan"], "publication_date": "2018/4/26", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "32", "issue": "1", "description": "Multi-task learning (MTL) is a machine learning paradigm that improves the performance of each task by exploiting useful information contained in multiple related tasks. However, the relatedness of tasks can be exploited by attackers to launch data poisoning attacks, which has been demonstrated a big threat to single-task learning. In this paper, we provide the first study on the vulnerability of MTL. Specifically, we focus on multi-task relationship learning (MTRL) models, a popular subclass of MTL models where task relationships are quantized and are learned directly from training data. We formulate the problem of computing optimal poisoning attacks on MTRL as a bilevel program that is adaptive to arbitrary choice of target tasks and attacking tasks. We propose an efficient algorithm called PATOM for computing optimal attack strategies. PATOM leverages the optimality conditions of the subproblem of MTRL to compute the implicit gradients of the upper level objective function. Experimental results on real-world datasets show that MTRL models are very sensitive to poisoning attacks and the attacker can significantly degrade the performance of target tasks, by either directly poisoning the target tasks or indirectly poisoning the related tasks exploiting the task relatedness. We also found that the tasks being attacked are always strongly correlated, which provides a clue for defending against such attacks.", "total_citations": {"2018": 5, "2019": 6, "2020": 9, "2021": 9, "2022": 10, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:Fu2w8maKXqMC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10028", "authors": ["Zhen Wang", "Yue Yin", "Bo An"], "publication_date": "2016/2/21", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "30", "issue": "1", "description": "In recent years, terrorist organizations (eg, ISIS or al-Qaeda) are increasingly directing terrorists to launch coordinated attacks in their home countries. One example is the Paris shootings on January 7, 2015. By monitoring potential terrorists, security agencies are able to detect and stop terrorist plots at their planning stage. Although security agencies may have knowledge about potential terrorists (eg, who they are, how they interact), they usually have limited resources and cannot monitor all terrorists. Moreover, a terrorist planner may strategically choose to arouse terrorists considering the security agency's monitoring strategy. This paper makes five key contributions toward the challenging problem of computing optimal monitoring strategies: 1) A new Stackelberg game model for terrorist plot detection; 2) A modified double oracle framework for computing the optimal strategy effectively; 3) Complexity results for both defender and attacker oracle problems; 4) Novel mixed-integer linear programming (MILP) formulations for best response problems of both players; and 5) Effective approximation algorithms for generating suboptimal responses for both players. Experimental evaluation shows that our approach can obtain a robust enough solution outperforming widely-used centrality based heuristics significantly and scale up to realistic-sized problems.", "total_citations": {"2016": 6, "2017": 7, "2018": 10, "2019": 6, "2020": 8, "2021": 4, "2022": 4, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:LPZeul_q3PIC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6888505/", "authors": ["Yujing Hu", "Yang Gao", "Bo An"], "publication_date": "2014/8/29", "journal": "IEEE transactions on cybernetics", "volume": "45", "issue": "7", "pages": "1289-1302", "publisher": "IEEE", "description": "An important approach in multiagent reinforcement learning (MARL) is equilibrium-based MARL, which adopts equilibrium solution concepts in game theory and requires agents to play equilibrium strategies at each state. However, most existing equilibrium-based MARL algorithms cannot scale due to a large number of computationally expensive equilibrium computations (e.g., computing Nash equilibria is PPAD-hard) during learning. For the first time, this paper finds that during the learning process of equilibrium-based MARL, the one-shot games corresponding to each state's successive visits often have the same or similar equilibria (for some states more than 90% of games corresponding to successive visits have similar equilibria). Inspired by this observation, this paper proposes to use equilibrium transfer to accelerate equilibrium-based MARL. The key idea of equilibrium transfer is to reuse previously\u00a0\u2026", "total_citations": {"2015": 1, "2016": 5, "2017": 4, "2018": 10, "2019": 9, "2020": 8, "2021": 3, "2022": 1, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:NXb4pA-qfm4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8423113/", "authors": ["Wanyuan Wang", "Zhanpeng He", "Peng Shi", "Weiwei Wu", "Yichuan Jiang", "Bo An", "Zhifeng Hao", "Bing Chen"], "publication_date": "2018/7/30", "journal": "IEEE Transactions on Mobile Computing", "volume": "18", "issue": "6", "pages": "1419-1432", "publisher": "IEEE", "description": "With the increasing complexity of tasks that are crowdsourced, requesters need to form teams of professional workers that can satisfy complex task skill requirements. Team crowdsourcing in social networks (SNs) provides a promising solution for complex task crowdsourcing, where the requester hires a team of professional workers that are also socially connected can work together collaboratively. Previous social team formation approaches have mainly focused on the algorithmic aspect for social welfare maximization; however, within the traditional objective of maximizing social welfare alone, selfish workers can manipulate the crowdsourcing market by behaving untruthfully. This dishonest behavior discourages other workers from participating and is unprofitable for the requester. To address this strategic social team crowdsourcing problem, truthful mechanisms are developed to guarantee that a worker's utility is\u00a0\u2026", "total_citations": {"2019": 4, "2020": 10, "2021": 11, "2022": 9, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:Tyk-4Ss8FVUC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/ijcai07.pdf", "authors": ["Bo An", "Chunyan Miao", "Zhiqi Shen"], "publication_date": "2007/1/6", "journal": "IJCAI", "volume": "7", "pages": "1193-1198", "description": "Although there are some research efforts toward resource allocation in multi-agent systems (MAS), most of these work assume that each agent has complete information about other agents. This research investigates interactions among selfish, rational, and autonomous agents in resource allocation, each with incomplete information about other entities, and each seeking to maximize its expected utility. This paper presents a proportional resource allocation mechanism and gives a game theoretical analysis of the optimal strategies and the analysis shows the existence of equilibrium in the incomplete information setting. By augmenting the resource allocation mechanism with a deal optimization mechanism, trading agents can be programmed to optimize resource allocation results by updating beliefs and resubmitting bids. Experimental results showed that by having a deal optimization stage, the resource allocation mechanism produced generally optimistic outcomes (close to market equilibrium).", "total_citations": {"2008": 1, "2009": 1, "2010": 2, "2011": 3, "2012": 4, "2013": 3, "2014": 2, "2015": 7, "2016": 12, "2017": 5, "2018": 1, "2019": 0, "2020": 1, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:lgwcVrK6X84C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/17287", "authors": ["Runsheng Yu", "Yu Gong", "Xu He", "Yu Zhu", "Qingwen Liu", "Wenwu Ou", "Bo An"], "publication_date": "2021/5/18", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "35", "issue": "12", "pages": "10772-10780", "description": "A common challenge in personalized user preference prediction is the cold-start problem. Due to the lack of user-item interactions, directly learning from the new users' log data causes serious over-fitting problem. Recently, many existing studies regard the cold-start personalized preference prediction as a few-shot learning problem, where each user is the task and recommended items are the classes, and the gradient-based meta learning method (MAML) is leveraged to address this challenge. However, in real-world application, the users are not uniformly distributed (ie, different users may have different browsing history, recommended items, and user profiles. We define the major users as the users in the groups with large numbers of users sharing similar user information, and other users are the minor users), existing MAML approaches tend to fit the major users and ignore the minor users. To address the task-overfitting problem, we propose a novel personalized adaptive meta learning approach to consider both the major and the minor users with three key contributions: 1) We are the first to present a personalized adaptive learning rate meta-learning approach to improve the performance of MAML by focusing on both the major and minor users. 2) To provide better personalized learning rates for each user, we introduce a similarity-based method to find similar users as a reference and a tree-based method to store users' features for fast search. 3) To reduce the memory usage, we design a memory agnostic regularizer to further reduce the space complexity to constant while maintain the performance. Experiments on MovieLens, BookCrossing\u00a0\u2026", "total_citations": {"2021": 4, "2022": 19, "2023": 20}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:F9fV5C73w3QC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8818337/", "authors": ["Jiuchuan Jiang", "Bo An", "Yichuan Jiang", "Chenyan Zhang", "Zhan Bu", "Jie Cao"], "publication_date": "2019/8/28", "journal": "IEEE Transactions on Systems, Man, and Cybernetics: Systems", "volume": "51", "issue": "7", "pages": "4417-4432", "publisher": "IEEE", "description": "Previous crowdsourcing studies often adopted the individual-oriented approach that outsources a task to an individual worker or team formation-based approach that outsources a task to an artificially formed team of workers. Nowadays, workers are often naturally organized into groups through social networks. To address such common issue of grouped workers in real crowdsourcing systems, this article explores a novel crowdsourcing paradigm in which the task allocation targets are naturally existing worker groups but not individual workers or artificially formed teams as before. Because a natural group might not possess all required skills and needs to coordinate with other groups in the social network contexts for performing a complex task, a concept of contextual crowdsourcing value is presented to measure a group's capacity to complete a task by coordinating with its contextual groups, which determines the\u00a0\u2026", "total_citations": {"2019": 1, "2020": 11, "2021": 5, "2022": 17, "2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:-f6ydRqryjwC": {"external_link": "https://cdn.aaai.org/ocs/4255/4255-19481-1-PB.pdf", "authors": ["Yevgeniy Vorobeychik", "Bo An", "Milind Tambe"], "publication_date": "2012/3/23", "conference": "2012 AAAI Spring Symposium Series", "description": "Defender-Attacker Stackelberg games are the foundations of tools deployed for computing optimal patrolling strategies in adversarial domains such as the United states Federal Air Marshals Service and the United States Coast Guard, among others. In Stackelberg game models of these systems the attacker knows only the probability that each target is covered by the defender, but is oblivious to the detailed timing of the coverage schedule. In many real-world situations, however, the attacker can observe the current location of the defender and can exploit this knowledge to reason about the defender\u2019s future moves. We study Stackelberg security games in which the defender sequentially moves between targets, with moves constrained by an exogenously specified graph, while the attacker can observe the defender\u2019s current location and his (stochastic) policy concerning future moves. We offer five contributions:(1) We model this adversarial patrolling game (APG) as a stochastic game with special structure and present several alternative formulations that leverage the general nonlinear programming (NLP) approach for computing equilibria in zero-sum stochastic games. We show that our formulations yield significantly better solutions than previous approaches.(2) We extend the NLP formulation for APG allow for attacks that may take multiple time steps to unfold.(3) We provide an approximate MILP formulation that uses discrete defender move probabilities.(4) We experimentally demonstrate the efficacy of an NLP-based approach, and systematically study the impact of network topology on the results.(5) We extend our model to allow the\u00a0\u2026", "total_citations": {"2012": 2, "2013": 0, "2014": 3, "2015": 3, "2016": 5, "2017": 6, "2018": 5, "2019": 4, "2020": 6, "2021": 3, "2022": 0, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:LO7wyVUgiFcC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0950705118303460", "authors": ["Jinhua Song", "Hao Wang", "Yang Gao", "Bo An"], "publication_date": "2018/11/1", "journal": "Knowledge-Based Systems", "volume": "159", "pages": "244-258", "publisher": "Elsevier", "description": "Collecting labels for data is important for many practical applications (e.g., data mining). However, this process can be expensive and time-consuming since it needs extensive efforts of domain experts. To decrease the cost, many recent works combine crowdsourcing, which outsources labeling tasks (usually in the form of questions) to a large group of non-expert workers, and active learning, which actively selects the best instances to be labeled, to acquire labeled datasets. However, for difficult tasks where workers are uncertain about their answers, asking for discrete labels might lead to poor performance due to the low-quality labels. In this paper, we design questions to get continuous worker responses which are more informative and contain workers\u2019 labels as well as their confidence. As crowd workers may make mistakes, multiple workers are hired to answer each question. Then, we propose a new\u00a0\u2026", "total_citations": {"2019": 7, "2020": 12, "2021": 10, "2022": 7, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:WC23djZS0W4C": {"external_link": "https://proceedings.neurips.cc/paper/2021/hash/428fca9bc1921c25c5121f9da7815cde-Abstract.html", "authors": ["Hongxin Wei", "Lue Tao", "Renchunzi Xie", "Bo An"], "publication_date": "2021/12/6", "journal": "Advances in Neural Information Processing Systems", "volume": "34", "pages": "7978-7992", "description": "Learning with noisy labels is a practically challenging problem in weakly supervised learning. In the existing literature, open-set noises are always considered to be poisonous for generalization, similar to closed-set noises. In this paper, we empirically show that open-set noisy labels can be non-toxic and even benefit the robustness against inherent noisy labels. Inspired by the observations, we propose a simple yet effective regularization by introducing Open-set samples with Dynamic Noisy Labels (ODNL) into training. With ODNL, the extra capacity of the neural network can be largely consumed in a way that does not interfere with learning patterns from clean data. Through the lens of SGD noise, we show that the noises induced by our method are random-direction, conflict-free and biased, which may help the model converge to a flat minimum with superior stability and enforce the model to produce conservative predictions on Out-of-Distribution instances. Extensive experimental results on benchmark datasets with various types of noisy labels demonstrate that the proposed method not only enhances the performance of many existing robust algorithms but also achieves significant improvement on Out-of-Distribution detection tasks even in the label noise setting.", "total_citations": {"2021": 2, "2022": 13, "2023": 26}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:b1wdh0AR-JQC": {"external_link": "https://www.ijcai.org/proceedings/2017/0516.pdf", "authors": ["Qingyu Guo", "Boyuan An", "Branislav Bosansk\u00fd", "Christopher Kiekintveld"], "publication_date": "2017/8/19", "conference": "IJCAI", "pages": "3691-3699", "description": "The Strong Stackelberg Equilibrium (SSE) has drawn extensive attention recently in several security domains. However, the SSE concept neglects the advantage of defender\u2019s strategic revelation of her private information, and overestimates the observation ability of the adversaries. In this paper, we overcome these restrictions and analyze the tradeoff between strategic secrecy and commitment in security games. We propose a Disguised-resource Security Game (DSG) where the defender strategically disguises some of her resources. We compare strategic information revelation with public commitment and formally show that they have different advantages depending the payoff structure. To compute the Perfect Bayesian Equilibrium (PBE), several novel approaches are provided, including a novel algorithm based on support set enumeration, and an approximation algorithm for \u03f5-PBE. Extensive experimental evaluation shows that both strategic secrecy and Stackelberg commitment are critical measures in security domain, and our approaches can efficiently solve PBEs for realistic-sized problems.", "total_citations": {"2017": 1, "2018": 4, "2019": 7, "2020": 8, "2021": 8, "2022": 8, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:evX43VCCuoAC": {"external_link": "https://www.academia.edu/download/77683980/p749.pdf", "authors": ["Yanhai Xiong", "Jiarui Gan", "Bo An", "Chunyan Miao", "Yeng Chai Soh"], "publication_date": "2016/5/9", "conference": "AAMAS", "pages": "749-757", "description": "The rapid development of Electric Vehicles (EVs) seen in recent years has been drawing increasing attentions from the public, markets, decision-makers, and academia. Notwithstanding the progress, issues still remain. Because of the widely complained disadvantages of limited battery capacity and long charging time, charging convenience has become a top concern that greatly hinders the adoption of EVs. Specialized EV charging station, which provides more than 10 times faster charging speed than domestic charging, is therefore a critical element for successful EV promotion. While most existing researches focus on optimizing spatial placement of charging stations, they are inflexible and inefficient against rapidly changing urban structure and traffic pattern. Therefore, this paper approaches the management of EV charging stations from the pricing perspective as a more flexible and adaptive complement to established charging station placement. In this paper, we build a realistic pricing model in consideration of residential travel pattern and EV drivers\u2019 self-interested charging behavior, traffic congestion, and operating expense of charging stations. We formulate the pricing problem as a mixed integer non-convex optimization problem, and propose a scalable algorithm to solve it. Experiments on both mock and real data are also conducted, which show scalability of our algorithm as well as our solution\u2019s significant improvement over existing approaches.", "total_citations": {"2017": 8, "2018": 8, "2019": 5, "2020": 5, "2021": 6, "2022": 6, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:dhFuZR0502QC": {"external_link": "https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2420", "authors": ["Manish Jain", "Bo An", "Milind Tambe"], "publication_date": "2012/9/20", "source": "AI Magazine", "volume": "33", "issue": "3", "pages": "14-14", "description": "A key feature of the AAMAS conference is its emphasis on ties to real-world applications. The focus of this article is to provide a broad overview of application-focused papers published at the AAMAS 2010 and 2011 conferences. More specifically, recent applications at AAMAS could be broadly categorized as belonging to research areas of security, sustainability and safety. We outline the domains of applications, key research thrusts underlying each such application area, and emerging trends.", "total_citations": {"2012": 3, "2013": 2, "2014": 5, "2015": 5, "2016": 7, "2017": 8, "2018": 6, "2019": 1, "2020": 0, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:hsZV8lGYWTMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8354911/", "authors": ["Xiaobo Ma", "Yihui He", "Xiapu Luo", "Jianfeng Li", "Mengchen Zhao", "Bo An", "Xiaohong Guan"], "publication_date": "2018/5/4", "journal": "IEEE Intelligent Systems", "volume": "33", "issue": "4", "pages": "49-61", "publisher": "IEEE", "description": "Security surveillance is important in smart cities. Deploying numerous cameras is a common approach. Given the importance of vehicles in a metropolis, using vehicle traffic patterns to strategically place cameras could potentially facilitate security surveillance. This article constitutes the first effort toward building the link between vehicle traffic and camera placement for better security surveillance.", "total_citations": {"2018": 5, "2019": 12, "2020": 9, "2021": 2, "2022": 8, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:cWzG1nlazyYC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3308558.3313533", "authors": ["Qingyu Guo", "Zhao Li", "Bo An", "Pengrui Hui", "Jiaming Huang", "Long Zhang", "Mengchen Zhao"], "publication_date": "2019/5/13", "book": "The world wide web conference", "pages": "616-626", "description": "Fraud transactions are one of the major threats faced by online e-commerce platforms. Recently, deep learning based classifiers have been deployed to detect fraud transactions. Inspired by findings on adversarial examples, this paper is the first to analyze the vulnerability of deep fraud detector to slight perturbations on input transactions, which is very challenging since the sparsity and discretization of transaction data result in a non-convex discrete optimization. Inspired by the iterative Fast Gradient Sign Method (FGSM) for the L8 attack, we first propose the Iterative Fast Coordinate Method (IFCM) for discrete L1 and L2 attacks which is efficient to generate large amounts of instances with satisfactory effectiveness. We then provide two novel attack algorithms to solve the discrete optimization. The first one is the Augmented Iterative Search (AIS) algorithm, which repeatedly searches for effective \u201csimple\u201d perturbation\u00a0\u2026", "total_citations": {"2018": 2, "2019": 1, "2020": 9, "2021": 10, "2022": 8, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:BzfGm06jWhQC": {"external_link": "https://proceedings.neurips.cc/paper/2021/hash/c2626d850c80ea07e7511bbae4c76f4b-Abstract.html", "authors": ["Wei Qiu", "Xinrun Wang", "Runsheng Yu", "Rundong Wang", "Xu He", "Bo An", "Svetlana Obraztsova", "Zinovi Rabinovich"], "publication_date": "2021/12/6", "journal": "Advances in Neural Information Processing Systems", "volume": "34", "pages": "23049-23062", "description": "Current value-based multi-agent reinforcement learning methods optimize individual Q values to guide individuals' behaviours via centralized training with decentralized execution (CTDE). However, such expected, ie, risk-neutral, Q value is not sufficient even with CTDE due to the randomness of rewards and the uncertainty in environments, which causes the failure of these methods to train coordinating agents in complex environments. To address these issues, we propose RMIX, a novel cooperative MARL method with the Conditional Value at Risk (CVaR) measure over the learned distributions of individuals' Q values. Specifically, we first learn the return distributions of individuals to analytically calculate CVaR for decentralized execution. Then, to handle the temporal nature of the stochastic outcomes during executions, we propose a dynamic risk level predictor for risk level tuning. Finally, we optimize the CVaR policies with CVaR values used to estimate the target in TD error during centralized training and the CVaR values are used as auxiliary local rewards to update the local distribution via Quantile Regression loss. Empirically, we show that our method outperforms many state-of-the-art methods on various multi-agent risk-sensitive navigation scenarios and challenging StarCraft II cooperative tasks, demonstrating enhanced coordination and revealing improved sample efficiency.", "total_citations": {"2021": 2, "2022": 14, "2023": 20}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:-_dYPAW6P2MC": {"external_link": "https://people.cs.umass.edu/~yzick/papers/flow.interdiction.ijcai16.pdf", "authors": ["Qingyu Guo", "Bo An", "Yair Zick", "Chunyan Miao"], "publication_date": "2016/7/9", "conference": "IJCAI", "pages": "2507-2513", "description": "Large scale smuggling of illegal goods is a longstanding problem, with $1.4 b and thousands of agents assigned to protect the borders from such activity in the US-Mexico border alone. Illegal smuggling activities are usually blocked via inspection stations or ad-hoc checkpoints/roadblocks. Security resources are insufficient to man all stations at all times; furthermore, smugglers regularly conduct surveillance activities. This paper makes several contributions toward the challenging task of optimally interdicting an illegal network flow: i) A new Stackelberg game model for network flow interdiction; ii) A novel Column and Constraint Generation approach for computing the optimal defender strategy; iii) Complexity analysis of the column generation subproblem; iv) Compact convex nonlinear programs for solving the subproblems; v) Novel greedy and heuristic approaches for subproblems with good approximation guarantee. Experimental evaluation shows that our approach can obtain a robust enough solution outperforming the existing methods and heuristic baselines significantly and scale up to realistic-sized problems.", "total_citations": {"2017": 7, "2018": 3, "2019": 6, "2020": 5, "2021": 4, "2022": 6, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:vDijr-p_gm4C": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0306457316301881", "authors": ["Jiuchuan Jiang", "Peng Shi", "Bo An", "Jianyong Yu", "Chongjun Wang"], "publication_date": "2017/1/1", "journal": "Information Processing & Management", "volume": "53", "issue": "1", "pages": "1-20", "publisher": "Pergamon", "description": "Scientists often collaborate with each other and may produce social influences through their collaboration on scientific activities. While the subject of ranking scientists has received significant attention in previous studies, these studies often examine the social influences of individual scientists and are based on the assumption that all collaborations between scientists are of the same type. However, these two limitations do not match real scientific collaborations. Currently, scientists are often associated with groups in which the scientists always study related research topics or have close collaboration relationships. Moreover, current scientists often collaborate through multiple relationship types, and different types of relationships may have different effects on the social influences of the scientists. To solve these two problems, this paper presents a model that measures the social influences of scientist groups based on\u00a0\u2026", "total_citations": {"2017": 2, "2018": 3, "2019": 6, "2020": 7, "2021": 2, "2022": 8, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:p__nRnzSRKYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7835207/", "authors": ["Haipeng Chen", "Bo An", "Dusit Niyato", "Yeng Chai Soh", "Chuanyan Miao"], "publication_date": "2017/1/26", "journal": "IEEE Journal on Selected Areas in Communications", "volume": "35", "issue": "3", "pages": "557-570", "publisher": "IEEE", "description": "In cloud computing, a private (secondary) cloud can: 1) outsource workload to public (primary) clouds via vertical federation or 2) share resources with other secondary clouds through horizontal federation to enhance its service quality. While there have been attempts to establish a joint vertical and horizontal cloud federation (VHCF), little is known regarding the economic aspects (e.g., what stable cooperation pattern will form, will it improve efficiency) of such a complex cloud network, where secondary clouds are self-interested. To fill the gap, we analyze the interrelated workload factoring and federation formation among secondary clouds, while providing scalable algorithms to assist them to optimally select partners and outsource workload. We use a game theoretic approach to model the federation formation of clouds as a coalition game with externalities. We adopt a pessimistic core to characterize the\u00a0\u2026", "total_citations": {"2017": 3, "2018": 8, "2019": 2, "2020": 3, "2021": 4, "2022": 7, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:EUQCXRtRnyEC": {"external_link": "https://teamcore.seas.harvard.edu/sites/projects.iq.harvard.edu/files/teamcore/files/2014_1_teamcore_aaaiss14.pdf", "authors": ["Milind Tambe", "Albert Xin Jiang", "Bo An", "Manish Jain"], "publication_date": "2014/3", "source": "AAAI spring symposium on applied computational game theory", "description": "The goal of this paper is to (re) introduce a real-world challenge problem for researchers in multiagent systems and beyond, where our collective efforts may have a significant impact on activities in the real-world. The challenge is in applying game theory for security: our goal is to not only introduce the research challenges for algorithmic and behavioral game theory in service of this problem, but also to provide initial exemplars of successes of deployed systems, and to challenges introduced by these deployments of computational game theory in the field. We also wish to provide an overview of key open research challenges and pointers to getting started in this research.", "total_citations": {"2014": 1, "2015": 8, "2016": 3, "2017": 1, "2018": 6, "2019": 4, "2020": 4, "2021": 3, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:kh2fBNsKQNwC": {"external_link": "https://batmanzzmc.github.io/assets/pdf/IJCAI18.pdf", "authors": ["Mengchen Zhao", "Zhao Li", "Bo An", "Haifeng Lu", "Yifan Yang", "Chen Chu"], "publication_date": "2018/7/13", "conference": "IJCAI", "pages": "3940-3946", "description": "Conducting fraud transactions has become popular among e-commerce sellers to make their products favorable to the platform and buyers, which decreases the utilization efficiency of buyer impressions and jeopardizes the business environment. Fraud detection techniques are necessary but not enough for the platform since it is impossible to recognize all the fraud transactions. In this paper, we focus on improving the platform\u2019s impression allocation mechanism to maximize its profit and reduce the sellers\u2019 fraudulent behaviors simultaneously. First, we learn a seller behavior model to predict the sellers\u2019 fraudulent behaviors from the real-world data provided by one of the largest ecommerce company in the world. Then, we formulate the platform\u2019s impression allocation problem as a continuous Markov Decision Process (MDP) with unbounded action space. In order to make the action executable in practice and facilitate learning, we propose a novel deep reinforcement learning algorithm DDPG-ANP that introduces an action norm penalty to the reward function. Experimental results show that our algorithm significantly outperforms existing baselines in terms of scalability and solution quality.", "total_citations": {"2018": 2, "2019": 4, "2020": 7, "2021": 8, "2022": 5, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:tzM49s52ZIMC": {"external_link": "https://www.vorobeychik.com/2016/csg.pdf", "authors": ["Qingyu Guo", "Bo An", "Yevgeniy Vorobeychik", "Long Tran-Thanh", "Jiarui Gan", "Chunyan Miao"], "publication_date": "2016/5/9", "book": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems", "pages": "159-167", "description": "Game theoretic models of security, and associated computational methods, have emerged as critical components of security posture across a broad array of domains, including airport security and coast guard. These approaches consider terrorists as motivated but independent entities. There is, however, increasing evidence that attackers, be it terrorists or cyber attackers, communicate extensively and form coalitions that can dramatically increase their ability to achieve malicious goals. To date, such cooperative decision making among attackers has been ignored in the security games literature. To address the issue of cooperation among attackers, we introduce a novel coalitional security game (CSG) model. A CSG consists of a set of attackers connected by a (communication or trust) network who can form coalitions as connected subgraphs of this network so as to attack a collection of targets. A defender in a CSG can delete a set of edges, incurring a cost for deleting each edge, with the goal of optimally limiting the attackers\u2019 ability to form effective coalitions (in terms of successfully attacking high value targets). We first show that a CSG is, in general, hard to approximate. Nevertheless, we develop a novel branch and price algorithm, leveraging a combination of column generation, relaxation, greedy approximation, and stabilization methods to enable scalable high-quality approximations of CSG solutions on realistic problem instances.", "total_citations": {"2016": 6, "2017": 4, "2018": 8, "2019": 5, "2020": 3, "2021": 2, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:dTyEYWd-f8wC": {"external_link": "https://www.academia.edu/download/77683859/Yin15_addressTime.pdf", "authors": ["Yue Yin", "Haifeng Xu", "Jiarui Gan", "Bo An", "Albert Xin Jiang"], "publication_date": "2015/6/22", "conference": "IJCAI", "pages": "681-688", "description": "Security agencies in the real world often need to protect targets with time-dependent values, eg, tourist sites where the number of travelers changes over time. Since the values of different targets often change asynchronously, the defender can relocate security resources among targets dynamically to make the best use of limited resources. We propose a game-theoretic scheme to develop dynamic, randomized security strategies in consideration of adversary\u2019s surveillance capability. This differs from previous studies on security games by considering varying target values and continuous strategy spaces of the security agency and the adversary. The main challenge lies in the computational intensiveness due to the continuous, hence infinite strategy spaces. We propose an optimal algorithm and an arbitrarily near-optimal algorithm to compute security strategies under different conditions. Experimental results show that both algorithms significantly outperform existing approaches.", "total_citations": {"2015": 1, "2016": 5, "2017": 8, "2018": 7, "2019": 3, "2020": 2, "2021": 2, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:BqipwSGYUEgC": {"external_link": "https://link.springer.com/article/10.1007/s10458-012-9209-6", "authors": ["Matthew Brown", "Bo An", "Christopher Kiekintveld", "Fernando Ord\u00f3\u00f1ez", "Milind Tambe"], "publication_date": "2014/1", "journal": "Autonomous agents and multi-agent systems", "volume": "28", "pages": "31-71", "publisher": "Springer US", "description": "The burgeoning area of security games has focused on real-world domains where security agencies protect critical infrastructure from a diverse set of adaptive adversaries. In such domains, decision makers have multiple competing objectives they must consider which may take different forms that are not readily comparable including safety, cost, and public perception. Thus, it can be difficult to know how to weigh the different objectives when deciding on a security strategy. To address the challenges of these domains, we propose a fundamentally different solution concept, multi-objective security games (MOSGs). Instead of a single optimal solution, MOSGs have a set of Pareto optimal (non-dominated) solutions referred to as the Pareto frontier, which can be generated by solving a sequence of constrained single-objective optimization problems (CSOPs). The Pareto frontier allows the decision maker to\u00a0\u2026", "total_citations": {"2014": 2, "2015": 3, "2016": 3, "2017": 6, "2018": 3, "2019": 2, "2020": 1, "2021": 5, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:sJsF-0ZLhtgC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3383313.3412233", "authors": ["Xu He", "Bo An", "Yanghua Li", "Haikai Chen", "Rundong Wang", "Xinrun Wang", "Runsheng Yu", "Xin Li", "Zhirong Wang"], "publication_date": "2020/9/22", "book": "Proceedings of the 14th ACM Conference on Recommender Systems", "pages": "210-219", "description": "With the rise of online e-commerce platforms, more and more customers prefer to shop online. To sell more products, online platforms introduce various modules to recommend items with different properties such as huge discounts. A web page often consists of different independent modules. The ranking policies of these modules are decided by different teams and optimized individually without cooperation, which might result in competition between modules. Thus, the global policy of the whole page could be sub-optimal. In this paper, we propose a novel multi-agent cooperative reinforcement learning approach with the restriction that different modules cannot communicate. Our contributions are three-fold. Firstly, inspired by a solution concept in game theory named correlated equilibrium, we design a signal network to promote cooperation of all modules by generating signals (vectors) for different modules\u00a0\u2026", "total_citations": {"2021": 6, "2022": 12, "2023": 11}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:TQgYirikUcIC": {"external_link": "https://teamcore.seas.harvard.edu/files/teamcore/files/2012_17_teamcore_aamas2012multifinal2.pdf", "authors": ["Matthew Brown", "Bo An", "Christopher Kiekintveld", "Fernando Ord\u00f3\u00f1ez", "Milind Tambe"], "publication_date": "2012/6/4", "conference": "AAMAS", "pages": "863-870", "description": "The burgeoning area of security games has focused on real-world domains where security agencies protect critical infrastructure from a diverse set of adaptive adversaries. There are security domains where the payoffs for preventing the different types of adversaries may take different forms (seized money, reduced crime, saved lives, etc) which are not readily comparable. Thus, it can be difficult to know how to weigh the different payoffs when deciding on a security strategy. To address the challenges of these domains, we propose a fundamentally different solution concept, multi-objective security games (MOSG), which combines security games and multiobjective optimization. Instead of a single optimal solution, MOSGs have a set of Pareto optimal (non-dominated) solutions referred to as the Pareto frontier. The Pareto frontier can be generated by solving a sequence of constrained single-objective optimization problems (CSOP), where one objective is selected to be maximized while lower bounds are specified for the other objectives. Our contributions include:(i) an algorithm, Iterative \u03f5-Constraints, for generating the sequence of CSOPs;(ii) an exact approach for solving an MILP formulation of a CSOP (which also applies to multi-objective optimization in more general Stackelberg games);(iii) heuristics that achieve speedup by exploiting the structure of security games to further constrain a CSOP;(iv) an approximate approach for solving an algorithmic formulation of a CSOP, increasing the scalability of our approach with quality guarantees. Additional contributions of this paper include proofs on the level of approximation and detailed\u00a0\u2026", "total_citations": {"2012": 2, "2013": 4, "2014": 3, "2015": 3, "2016": 2, "2017": 1, "2018": 2, "2019": 3, "2020": 1, "2021": 4, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:YsMSGLbcyi4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5285142/", "authors": ["Bo An", "Nicola Gatti", "Victor Lesser"], "publication_date": "2009/9/15", "conference": "2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology", "volume": "2", "pages": "423-426", "publisher": "IEEE", "description": "Automating negotiations in markets where multiple buyers and sellers operate is a scientific challenge of extraordinary importance. One-to-one negotiations are classically studied as bilateral bargaining problems, while one-to-many and many-to-many negotiations are studied as auctioning problems. This paper aims at bridging together these two approaches, analyzing agents\u2019 strategic behavior in one-to-many and many-to-many negotiations when agents follow the alternating-offers bargaining protocol [5]. First, we propose a novel mechanism that captures the peculiarities of these settings. Then, we preliminarily explore how uncertainty over reserve prices and deadlines can affect equilibrium strategies. Surprisingly, the computation of the equilibrium for realistic ranges of the parameters in one-to-many settings is reduced to the computation of the equilibrium either in one-to-one settings with uncertainty or in one\u00a0\u2026", "total_citations": {"2010": 2, "2011": 2, "2012": 3, "2013": 2, "2014": 3, "2015": 1, "2016": 4, "2017": 3, "2018": 2, "2019": 0, "2020": 1, "2021": 2, "2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:2osOgNQ5qMEC": {"external_link": "http://conferences.sigcomm.org/sigcomm/2009/posters/sigcomm-pd-2009-final53.pdf", "authors": ["Bo An", "Athanasios V Vasilakos", "Victor Lesser"], "publication_date": "2009/8/17", "journal": "ACM SIGCOMM", "volume": "9", "pages": "17-21", "description": "Enterprises currently employ Cloud services to improve the scalability of their services and resource providers strategically price resources to maximize their utilities. While Nash equilibrium is the dominant concept for studying such kind of interaction, evolutionary game theory seems more appropriate for modeling agents\u2019 strategic interactions as it relaxes many strong assumptions. This paper applies evolutionary dynamics to generate resource providers\u2019 evolutionary stable strategies. We present a sequential monte carlo approach for simulation of multi-population evolutionary dynamics in which each agent\u2019s strategy space is continuous. We use resampling and Gaussian smoothing to prevent degeneration of particle samples. Simulation results show that the proposed approach always converges to evolutionary stable strategies. Our approach is general in that it can be used to generate agents\u2019 evolutionary stable strategies for other resource allocation games.", "total_citations": {"2010": 3, "2011": 9, "2012": 3, "2013": 2, "2014": 6, "2015": 2, "2016": 1, "2017": 0, "2018": 1, "2019": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:ALROH1vI_8AC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/16142", "authors": ["Rundong Wang", "Hongxin Wei", "Bo An", "Zhouyan Feng", "Jun Yao"], "publication_date": "2021/5/18", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "35", "issue": "1", "pages": "626-633", "description": "Portfolio management via reinforcement learning is at the forefront of fintech research, which explores how to optimally reallocate a fund into different financial assets over the long term by trial-and-error. Existing methods are impractical since they usually assume each reallocation can be finished immediately and thus ignoring the price slippage as part of the trading cost. To address these issues, we propose a hierarchical reinforced stock trading system for portfolio management (HRPM). Concretely, we decompose the trading process into a hierarchy of portfolio management over trade execution and train the corresponding policies. The high-level policy gives portfolio weights at a lower frequency to maximize the long-term profit and invokes the low-level policy to sell or buy the corresponding shares within a short time window at a higher frequency to minimize the trading cost. We train two levels of policies via a pre-training scheme and an iterative training scheme for data efficiency. Extensive experimental results in the US market and the China market demonstrate that HRPM achieves significant improvement against many state-of-the-art approaches.", "total_citations": {"2021": 2, "2022": 10, "2023": 16}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:0izLItjtcgwC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3226028", "authors": ["Jiuchuan Jiang", "Bo An", "Yichuan Jiang", "Donghui Lin", "Zhan Bu", "Jie Cao", "Zhifeng Hao"], "publication_date": "2018/7/31", "journal": "ACM Transactions on Autonomous and Adaptive Systems (TAAS)", "volume": "13", "issue": "2", "pages": "1-32", "publisher": "ACM", "description": "Crowdsourcing has recently been significantly explored. Although related surveys have been conducted regarding this subject, each has mainly consisted of a review of a single aspect of crowdsourcing systems or on the application of crowdsourcing in a specific application domain. A crowdsourcing system is a comprehensive set of multiple entities, including various elements and processes. Multiagent computing has already been widely envisioned as a powerful paradigm for modeling autonomous multi-entity systems with adaptation to dynamic environments. Therefore, this article presents a novel multiagent perspective and approach to understanding crowdsourcing systems, which can be used to correlate the research on crowdsourcing and multiagent systems and inspire possible interdisciplinary research between the two areas. This article mainly discusses the following two aspects: (1) The multiagent\u00a0\u2026", "total_citations": {"2018": 1, "2019": 4, "2020": 8, "2021": 3, "2022": 6, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:i2xiXl-TujoC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/11337", "authors": ["Haipeng Chen", "Bo An", "Guni Sharon", "Josiah Hanna", "Peter Stone", "Chunyan Miao", "Yeng Soh"], "publication_date": "2018/4/25", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "32", "issue": "1", "description": "To alleviate traffic congestion in urban areas, electronic toll collection (ETC) systems are deployed all over the world. Despite the merits, tolls are usually pre-determined and fixed from day to day, which fail to consider traffic dynamics and thus have limited regulation effect when traffic conditions are abnormal. In this paper, we propose a novel dynamic ETC (DyETC) scheme which adjusts tolls to traffic conditions in realtime. The DyETC problem is formulated as a Markov decision process (MDP), the solution of which is very challenging due to its 1) multi-dimensional state space, 2) multi-dimensional, continuous and bounded action space, and 3) time-dependent state and action values. Due to the complexity of the formulated MDP, existing methods cannot be applied to our problem. Therefore, we develop a novel algorithm, PG-beta, which makes three improvements to traditional policy gradient method by proposing 1) time-dependent value and policy functions, 2) Beta distribution policy function and 3) state abstraction. Experimental results show that, compared with existing ETC schemes, DyETC increases traffic volume by around 8%, and reduces travel time by around 14: 6% during rush hour. Considering the total traffic volume in a traffic network, this contributes to a substantial increase to social welfare.", "total_citations": {"2018": 4, "2019": 5, "2020": 4, "2021": 5, "2022": 6, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:q3CdL3IzO_QC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8226780/", "authors": ["Jiuchuan Jiang", "Bo An", "Yichuan Jiang", "Donghui Lin"], "publication_date": "2017/12/19", "journal": "IEEE Transactions on Systems, Man, and Cybernetics: Systems", "volume": "50", "issue": "2", "pages": "617-632", "publisher": "IEEE", "description": "There are two problems in the traditional crowdsourcing systems for handling complex tasks. First, decomposing complex tasks into a set of micro-subtasks requires the decomposition capability of the requesters; thus, some requesters may abandon using crowdsourcing to accomplish a large number of complex tasks since they cannot bear such heavy burden by themselves. Second, tasks are often assigned redundantly to multiple workers to achieve reliable results, but reliability may not be ensured when there are many malicious workers in the crowd. Currently, it is observed that the workers are often connected through social networks, a feature that can significantly facilitate task allocation and task execution in crowdsourcing. Therefore, this paper investigates crowdsourcing in social networks and presents a novel context-aware reliable crowdsourcing approach. In our presented approach, the two problems in\u00a0\u2026", "total_citations": {"2018": 2, "2019": 5, "2020": 4, "2021": 8, "2022": 2, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:_Ybze24A_UAC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10285", "authors": ["Shangdong Yang", "Yang Gao", "Bo An", "Hao Wang", "Xingguo Chen"], "publication_date": "2016/3/2", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "30", "issue": "1", "description": "There are two classes of average reward reinforcement learning (RL) algorithms: model-based ones that explicitly maintain MDP models and model-free ones that do not learn such models. Though model-free algorithms are known to be more efficient, they often cannot converge to optimal policies due to the perturbation of parameters. In this paper, a novel model-free algorithm is proposed, which makes use of constant shifting values (CSVs) estimated from prior knowledge. To encourage exploration during the learning process, the algorithm constantly subtracts the CSV from the rewards. A terminating condition is proposed to handle the unboundedness of Q-values caused by such substraction. The convergence of the proposed algorithm is proved under very mild assumptions. Furthermore, linear function approximation is investigated to generalize our method to handle large-scale tasks. Extensive experiments on representative MDPs and the popular game Tetris show that the proposed algorithms significantly outperform the state-of-the-art ones.", "total_citations": {"2017": 2, "2018": 4, "2019": 5, "2020": 8, "2021": 5, "2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:W7OEmFMy1HYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4285615/", "authors": ["Bo An", "Zhiqi Shen", "Chunyan Miao", "Daijie Cheng"], "publication_date": "2007/8/8", "journal": "IEEE Transactions on Industrial Informatics", "volume": "3", "issue": "3", "pages": "234-245", "publisher": "IEEE", "description": "Coalition formation methods allow autonomous agents to join together in order to act as a coherent group in which they increase their individual gains by collaborating with each other. Although there are some research efforts toward coalition formation in multiagent systems (MAS), such as game theory-based approaches, these methods cannot be easily applied in real-world scenarios. Based on a novel social reasoning theory, namely, transitive dependence theory, this work proposes two dynamic coalition formation algorithms for coalition formation: 1) without and-action dependence and 2) with and-action dependence, respectively. While most related work addresses the problem of searching for the optimal coalition structure (CS), the proposed algorithms aim to find out the optimal coalitions for specific goals. Theoretical analysis and experimental results suggest that 1) the algorithm for coalition formation\u00a0\u2026", "total_citations": {"2009": 2, "2010": 4, "2011": 2, "2012": 1, "2013": 4, "2014": 2, "2015": 2, "2016": 0, "2017": 2, "2018": 2, "2019": 2, "2020": 1, "2021": 3, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:EkHepimYqZsC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/2939672.2939726", "authors": ["Bo An", "Haipeng Chen", "Noseong Park", "VS Subrahmanian"], "publication_date": "2016/8/13", "book": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "pages": "421-430", "description": "Though there are numerous traditional models to predict market share and demand along airline routes, the prediction of existing models is not precise enough and, to the best of our knowledge, there is no use of data-mining based forecasting techniques to improve airline profitability. We propose the MAP (Maximizing Airline Profits) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared with past methods to forecast market share and demand along airline routes, we introduce a novel Ensemble Forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes, and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson Correlation Coefficients (over 0.95 vs. 0.82 for market share, 0.98 vs. 0.77\u00a0\u2026", "total_citations": {"2017": 6, "2018": 2, "2019": 1, "2020": 7, "2021": 4, "2022": 4, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:olpn-zPbct0C": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAMAS15learning.pdf", "authors": ["Yujing Hu", "Yang Gao", "Bo An"], "publication_date": "2015/5/7", "conference": "AAMAS", "pages": "753-761", "description": "In many multi-agent systems, the interactions between agents are sparse and exploiting interaction sparseness in multiagent reinforcement learning (MARL) can improve the learning performance. Also, agents may have already learnt some single-agent knowledge (eg, local value function) before the multi-agent learning process. In this work, we investigate how such knowledge can be utilized to learn better policies in multi-agent systems with sparse interactions. We adopt game theory-based MARL as the basic learning approach since it can coordinate agents better. We contribute three knowledge transfer mechanisms. The first one is value function transfer, which directly transfers agents\u2019 local value functions to the learning algorithm. The second one is selective value function transfer, which only transfers the value functions in states where the environmental dynamics change slightly. The last mechanism is model transfer-based game abstraction, which further improves the former two mechanisms by abstracting the one-shot game in each state and reducing equilibrium computation. Experimental results in benchmarks show that with the three knowledge transfer mechanisms, all of the tested game theory-based MARL algorithms are drastically improved and also achieve better asymptotic performance than the state-of-the-art algorithm CQ-learning.", "total_citations": {"2016": 5, "2017": 2, "2018": 4, "2019": 4, "2020": 4, "2021": 2, "2022": 0, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:CHSYGLWDkRkC": {"external_link": "http://vorobeychik.com/2014/deception.pdf", "authors": ["Yue Yin", "Bo An", "Yevgeniy Vorobeychik", "Jun Zhuang"], "publication_date": "2013", "journal": "Proc. of AAAI", "volume": "1", "description": "Attacker-defender Stackelberg games have been used in several deployed applications of game theory for infrastructure security. Security resources of the defender are game-theoretically allocated to prevent a strategic attacker from using surveillance to learn and exploit patterns in the allocation. Existing work on security games assumes that the defender honestly displays her real security resources. We introduce a new model in which the defender may use deceptive resources (eg, a mock camera in the part for deterring potential adversaries, or a hidden camera on the road for detecting overspeed) to mislead the attacker. We provide algorithms for computing the defender\u2019s optimal strategy in consideration of deceptions. We also present experimental results evaluating the effectiveness of using deceptive strategies.", "total_citations": {"2015": 1, "2016": 0, "2017": 1, "2018": 1, "2019": 2, "2020": 4, "2021": 6, "2022": 6, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:eQOLeE2rZwMC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-642-24696-8_9", "authors": ["Bo An", "Victor Lesser"], "publication_date": "2011/11/4", "book": "New Trends in Agent-Based Complex Automated Negotiations", "pages": "145-149", "publisher": "Springer Berlin Heidelberg", "description": "This article analyzes important issues regarding the design of a successful negotiation agent for ANAC and presents the design of Yushu, one of the top-scoring agents in the first Automated Negotiating Agents Competition (ANAC). Yushu uses simple heuristics to implement a conservative concession strategy based on a dynamically computed measure of competitiveness and number of negotiations rounds left before the deadline.", "total_citations": {"2012": 1, "2013": 2, "2014": 4, "2015": 1, "2016": 7, "2017": 1, "2018": 2, "2019": 2, "2020": 3, "2021": 0, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:mlAyqtXpCwEC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0305054818303113", "authors": ["Xiaoxuan Hu", "Waiming Zhu", "Bo An", "Peng Jin", "Wei Xia"], "publication_date": "2019/4/1", "journal": "Computers & Operations Research", "volume": "104", "pages": "74-89", "publisher": "Pergamon", "description": "Earth observation satellite (EOS) constellation imaging and downloading integrated scheduling (EOSCIDIS) is a challenging optimization problem with several engineering applications. Given a large number of imaging requests and a constellation consisting of a group of EOSs, a workable schedule must be developed for each EOS to allow for imaging and downloading in the most efficient manner given a certain scheduling horizon. No studies have proposed exact algorithms for the EOSCIDIS problem, although it has considerable potential for applications. In this paper, we developed a branch and price (B&P) algorithm for the EOSCIDIS problem. First, we decomposed the problem by extracting the sub-structure of the problem, and the decomposition resulted in a master problem and multiple pricing problems. Solving the linear relaxed master problem with column generation (CG) method provides a very tight\u00a0\u2026", "total_citations": {"2020": 2, "2021": 9, "2022": 8, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:PoWvk5oyLR8C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/11336", "authors": ["Wanyuan Wang", "Bo An", "Yichuan Jiang"], "publication_date": "2018/4/25", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "32", "issue": "1", "description": "Peer grading, allowing students/peers to evaluate others' assignments, offers a promising solution for scaling evaluation and learning to large-scale educational systems. A key challenge in peer grading is motivating peers to grade diligently. While existing spot-checking (SC) mechanisms can prevent peer collusion where peers coordinate to report the uninformative grade, they unrealistically assume that peers have the same grading reliability and cost. This paper studies the general Optimal Spot-Checking (OptSC) problem of determining the probability each assignment needs to be checked to maximize assignments' evaluation accuracy aggregated from peers, and takes into consideration 1) peers' heterogeneous characteristics, and 2) peers' strategic grading behaviors to maximize their own utility. We prove that the bilevel OptSC is NP-hard to solve. By exploiting peers' grading behaviors, we first formulate a single level relaxation to approximate OptSC. By further exploiting structural properties of the relaxed problem, we propose an efficient algorithm to that relaxation, which also gives a good approximation of the original OptSC. Extensive experiments on both synthetic and real datasets show significant advantages of the proposed algorithm over existing approaches.", "total_citations": {"2019": 1, "2020": 3, "2021": 6, "2022": 11, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:TFP_iSt0sucC": {"external_link": "https://link.springer.com/chapter/10.1007/978-1-4614-5416-8_2", "authors": ["Manish Jain", "Bo An", "Milind Tambe"], "publication_date": "2013", "conference": "Moving Target Defense II: Application of Game Theory and Adversarial Modeling", "pages": "15-39", "publisher": "Springer New York", "description": "The goal of this chapter is to introduce a challenging real-world problem for researchers in multiagent systems and beyond, where our collective efforts may have a significant impact on activities in the real-world. The challenge is in applying game theory for security: our goal is not only to introduce the problem, but also to provide exemplars of initial successes of deployed systems in this problem arena. Furthermore, we present key ideas and algorithms for solving and understanding the characteristics large-scale real-world security games, and then present some key open research challenges in this area.", "total_citations": {"2014": 2, "2015": 1, "2016": 3, "2017": 3, "2018": 4, "2019": 1, "2020": 0, "2021": 4, "2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:yqoGN6RLRZoC": {"external_link": "https://www.ijcai.org/Proceedings/2019/0635.pdf", "authors": ["Wei Qiu", "Haipeng Chen", "Bo An"], "publication_date": "2019/8/10", "conference": "IJCAI", "pages": "4568-4574", "description": "Over the past decades, Electronic Toll Collection (ETC) systems have been proved the capability of alleviating traffic congestion in urban areas. Dynamic Electronic Toll Collection (DETC) was recently proposed to further improve the efficiency of ETC, where tolls are dynamically set based on traffic dynamics. However, computing the optimal DETC scheme is computationally difficult and existing approaches are limited to small scale or partial road networks, which significantly restricts the adoption of DETC. To this end, we propose a novel multi-agent reinforcement learning (RL) approach for DETC. We make several key contributions: i) an enhancement over the state-of-the-art RL-based method with a deep neural network representation of the policy and value functions and a temporal difference learning framework to accelerate the update of target values, ii) a novel edgebased graph convolutional neural network (eGCN) to extract the spatio-temporal correlations of the road network state features, iii) a novel cooperative multi-agent reinforcement learning (MARL) which divides the whole road network into partitions according to their geographic and economic characteristics and trains a tolling agent for each partition. Experimental results show that our approach can scale up to realistic-sized problems with robust performance and significantly outperform the stateof-the-art method.", "total_citations": {"2020": 3, "2021": 6, "2022": 7, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:_5tno0g5mFcC": {"external_link": "https://aaai.org/ojs/index.php/AAAI/article/view/3921/3799", "authors": ["Youzhi Zhang", "Qingyu Guo", "Bo An", "Long Tran-Thanh", "Nicholas R Jennings"], "publication_date": "2019/7/17", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "33", "issue": "01", "pages": "1262-1269", "description": "Most violent crimes happen in urban and suburban cities. With emerging tracking techniques, law enforcement officers can have real-time location information of the escaping criminals and dynamically adjust the security resource allocation to interdict them. Unfortunately, existing work on urban network security games largely ignores such information. This paper addresses this omission. First, we show that ignoring the real-time information can cause an arbitrarily large loss of efficiency. To mitigate this loss, we propose a novel NEtwork purSuiT game (NEST) model that captures the interaction between an escaping adversary and a defender with multiple resources and real-time information available. Second, solving NEST is proven to be NP-hard. Third, after transforming the non-convex program of solving NEST to a linear program, we propose our incremental strategy generation algorithm, including:(i) novel pruning techniques in our best response oracle; and (ii) novel techniques for mapping strategies between subgames and adding multiple best response strategies at one iteration to solve extremely large problems. Finally, extensive experiments show the effectiveness of our approach, which scales up to realistic problem sizes with hundreds of nodes on networks including the real network of Manhattan.", "total_citations": {"2019": 3, "2020": 6, "2021": 2, "2022": 5, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:Ri6SYOTghG4C": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0020025518303013", "authors": ["Xiaobo Ma", "Jianfeng Li", "Yajuan Tang", "Bo An", "Xiaohong Guan"], "publication_date": "2019/4/1", "journal": "Information Sciences", "volume": "479", "pages": "486-502", "publisher": "Elsevier", "description": "As an emerging threat, link flooding attacks (LFAs) target and congest core links that constitute Internet routing infrastructure, hence posing a growing threat to networks worldwide. Mitigating and defeating LFAs is particularly challenging for two reasons. First, arising from the end-to-end communication from bots to public servers (e.g., Web servers), the attack traffic flows could be indistinguishable from legitimate ones, and even unobservable to the victim network surrounded by the target links. Therefore, typical flow-filtering countermeasures deployed at the network perimeter become invalid when handling LFAs. Second, the target link and the victim network belong to an autonomous system (AS) different from the source ASs where the attack traffic flows originate. These source ASs, however, have no idea the target link is under attack, whereas they are in charge of routing decisions and thus capable of mitigating\u00a0\u2026", "total_citations": {"2018": 2, "2019": 3, "2020": 8, "2021": 4, "2022": 4, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:MLfJN-KU85MC": {"external_link": "https://eprints.soton.ac.uk/411980/", "authors": ["Youzhi Zhang", "Bo An", "Long Tran-Thanh", "Zhen Wang", "Jiarui Gan", "Nicholas R Jennings"], "publication_date": "2017", "description": "Preventing crimes or terrorist attacks in urban areas is challenging. Law enforcement officers need to respond quickly to catch the attacker on his escape route, which is subject to time-dependent traffic conditions on transportation networks. The attacker can strategically choose his escape path and driving speed to avoid being captured. Existing work on security resource allocation has not considered such scenarios with time-dependent strategies for both players. Therefore, in this paper, we study the problem of efficiently scheduling security resources for interdicting the escaping attacker. We propose: 1) a new defender-attacker security game model for escape interdiction on transportation networks; and 2) an efficient double oracle algorithm to compute the optimal defender strategy, which combines mixed-integer linear programming formulations for best response problems and effective approximation algorithms for improving the scalability of the algorithms. Experimental evaluation shows that our approach significantly outperforms baselines in solution quality and scales up to realistic-sized transportation networks with hundreds of intersections.", "total_citations": {"2017": 1, "2018": 3, "2019": 6, "2020": 2, "2021": 4, "2022": 2, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:DJbcl8HfkQkC": {"external_link": "https://arxiv.org/abs/1903.00900", "authors": ["Jiang Rong", "Tao Qin", "Bo An"], "publication_date": "2019/3/3", "journal": "arXiv preprint arXiv:1903.00900", "description": "The game of bridge consists of two stages: bidding and playing. While playing is proved to be relatively easy for computer programs, bidding is very challenging. During the bidding stage, each player knowing only his/her own cards needs to exchange information with his/her partner and interfere with opponents at the same time. Existing methods for solving perfect-information games cannot be directly applied to bidding. Most bridge programs are based on human-designed rules, which, however, cannot cover all situations and are usually ambiguous and even conflicting with each other. In this paper, we, for the first time, propose a competitive bidding system based on deep learning techniques, which exhibits two novelties. First, we design a compact representation to encode the private and public information available to a player for bidding. Second, based on the analysis of the impact of other players' unknown cards on one's final rewards, we design two neural networks to deal with imperfect information, the first one inferring the cards of the partner and the second one taking the outputs of the first one as part of its input to select a bid. Experimental results show that our bidding system outperforms the top rule-based program.", "total_citations": {"2019": 7, "2020": 6, "2021": 3, "2022": 5, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:Z5m8FVwuT1cC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10614", "authors": ["Jiarui Gan", "Bo An", "Yevgeniy Vorobeychik", "Brian Gauch"], "publication_date": "2017/2/10", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "31", "issue": "1", "description": "Most existing models of Stackelberg security games ignore the underlying topology of the space in which targets and defence resources are located. As a result, allocation of resources is restricted to a discrete collection of exogenously defined targets. However, in many practical security settings, defense resources can be located on a continuous plane. Better defense solutions could therefore be potentially achieved by placing resources in a space outside of actual targets (eg, between targets). To address this limitation, we propose a model called Security Game on a Plane (SGP) in which targets are distributed on a 2-dimensional plane, and security resources, to be allocated on the same plane, protect targets within a certain effective distance. We investigate the algorithmic aspects of SGP. We find that computing a strong Stackelberg equilibrium of an SGP is NP-hard even for zero-sum games, and these are inapproximable in general. On the positive side, we find an exact solution technique for general SGPs based on an existing approach, and develop a PTAS (polynomial-time approximation scheme) for zero-sum SGP to more fundamentally overcome the computational obstacle. Our experiments demonstrate the value of considering SGP and effectiveness of our algorithms.", "total_citations": {"2017": 2, "2018": 3, "2019": 7, "2020": 5, "2021": 2, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:3fE2CSJIrl8C": {"external_link": "https://link.springer.com/article/10.1007/s10458-012-9198-5", "authors": ["Bo An", "Nicola Gatti", "Victor Lesser"], "publication_date": "2013/5", "journal": "Autonomous agents and multi-agent systems", "volume": "26", "pages": "420-455", "publisher": "Springer US", "description": "The problem of finding agents\u2019 rational strategies in bargaining with incomplete information is well known to be challenging. The literature provides a collection of results for very narrow uncertainty settings, but no generally applicable algorithm. This lack has led researchers to develop heuristic approaches in an attempt to find outcomes that, even if not being of equilibrium, are mutually satisfactory. In the present paper, we focus on the principal bargaining protocol (i.e., the alternating-offers protocol) where there is uncertainty regarding one agent\u2019s reserve price. We provide an algorithm based on the combination of game theoretic analysis and search techniques which finds pure strategy sequential equilibria when they exist. Our approach is sound, complete and, in principle, can be applied to other uncertainty settings, e.g., uncertain discount factors, and uncertain weights of negotiation issues in multi\u00a0\u2026", "total_citations": {"2012": 1, "2013": 1, "2014": 4, "2015": 2, "2016": 5, "2017": 1, "2018": 2, "2019": 2, "2020": 1, "2021": 1, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:ufrVoPGSRksC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5342528/", "authors": ["Bo An", "Victor Lesser"], "publication_date": "2009/12/1", "journal": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)", "volume": "40", "issue": "3", "pages": "575-586", "publisher": "IEEE", "description": "We consider a multiagent resource allocation problem where individual users intend to route traffic by requesting the help of entities across a network, and a cost is incurred at each network node that depends on the amount of traffic to be routed. We propose to study contract-based network resource allocation. In our model, users and nodes in the network make contracts before nodes route traffic for the users. The problem is an interesting self-interested negotiation problem because it requires the complete assembly of a set of distinct resources, and there are multiple combinations of distinct resources that could satisfy the goal of negotiation. First, we characterize the network allocation problem and show that finding optimal allocations is -complete and is inapproximable. We take both Nash equilibrium and pairwise Nash equilibrium as the solution concepts to characterize the equilibrium allocations. We find\u00a0\u2026", "total_citations": {"2010": 1, "2011": 2, "2012": 1, "2013": 2, "2014": 1, "2015": 3, "2016": 4, "2017": 1, "2018": 2, "2019": 1, "2020": 1, "2021": 2, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:PYBJJbyH-FwC": {"external_link": "https://arxiv.org/abs/2108.03803", "authors": ["Wanqi Xue", "Wei Qiu", "Bo An", "Zinovi Rabinovich", "Svetlana Obraztsova", "Chai Kiat Yeo"], "publication_date": "2021/8/9", "journal": "arXiv preprint arXiv:2108.03803", "description": "Recent studies in multi-agent communicative reinforcement learning (MACRL) have demonstrated that multi-agent coordination can be greatly improved by allowing communication between agents. Meanwhile, adversarial machine learning (ML) has shown that ML models are vulnerable to attacks. Despite the increasing concern about the robustness of ML algorithms, how to achieve robust communication in multi-agent reinforcement learning has been largely neglected. In this paper, we systematically explore the problem of adversarial communication in MACRL. Our main contributions are threefold. First, we propose an effective method to perform attacks in MACRL, by learning a model to generate optimal malicious messages. Second, we develop a defence method based on message reconstruction, to maintain multi-agent coordination under message attacks. Third, we formulate the adversarial communication problem as a two-player zero-sum game and propose a game-theoretical method R-MACRL to improve the worst-case defending performance. Empirical results demonstrate that many state-of-the-art MACRL methods are vulnerable to message attacks, and our method can significantly improve their robustness.", "total_citations": {"2022": 7, "2023": 13}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:nZcligLrVowC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8970929/", "authors": ["Haipeng Chen", "Yan Jiao", "Zhiwei Qin", "Xiaocheng Tang", "Hao Li", "Bo An", "Hongtu Zhu", "Jieping Ye"], "publication_date": "2019/11/8", "conference": "2019 IEEE International Conference on Data Mining (ICDM)", "pages": "61-70", "publisher": "IEEE", "description": "For both the traditional street-hailing taxi industry and the recently emerged on-line ride-hailing, it has been a major challenge to improve the ride-hailing marketplace efficiency due to spatio-temporal imbalance between the supply and demand, among other factors. Despite the numerous approaches to improve marketplace efficiency using pricing and dispatch strategies, they usually optimize pricing or dispatch separately. In this paper, we show that these two processes are in fact intrinsically interrelated. Motivated by this observation, we make an attempt to simultaneously optimize pricing and dispatch strategies. However, such a joint optimization is extremely challenging due to the inherent huge scale and lack of a uniform model of the problem. To handle the high complexity brought by the new problem, we propose InBEDE (Integrating contextual Bandit with tEmporal DiffErence learning), a learning framework\u00a0\u2026", "total_citations": {"2020": 1, "2021": 5, "2022": 5, "2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:hMsQuOkrut0C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10565", "authors": ["Xiaohong Li", "Shuxin Li", "Jianye Hao", "Zhiyong Feng", "Bo An"], "publication_date": "2017/2/10", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "31", "issue": "1", "description": "The Man-In-The-Middle (MITM) attack is one of the most common attacks employed in the network hacking. MITM attackers can successfully invoke attacks such as denial of service (DoS) and port stealing, and lead to surprisingly harmful consequences for users in terms of both financial loss and security issues. The conventional defense approaches mainly consider how to detect and eliminate those attacks or how to prevent those attacks from being launched in the first place. This paper proposes a game-theoretic defense strategy from a different perspective, which aims at minimizing the loss that the whole system sustains given that the MITM attacks are inevitable. We model the interaction between the attacker and the defender as a Stackelberg security game and adopt the Strong Stackelberg Equilibrium (SSE) as the defender's strategy. Since the defender's strategy space is infinite in our model, we employ a novel method to reduce the searching space of computing the optimal defense strategy. Finally, we empirically evaluate our optimal defense strategy by comparing it with non-strategic defense strategies. The results indicate that our game-theoretic defense strategy significantly outperforms other non-strategic defense strategies in terms of decreasing the total losses against MITM attacks.", "total_citations": {"2017": 2, "2018": 0, "2019": 4, "2020": 9, "2021": 1, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:uJ-U7cs_P_0C": {"external_link": "https://vorobeychik.com/2016/voting_security_game.pdf", "authors": ["Yue Yin", "Yevgeniy Vorobeychik", "Bo An", "Noam Hazon"], "publication_date": "2016/7/9", "conference": "IJCAI", "pages": "538-545", "description": "Election control encompasses attempts from an external agent to alter the structure of an election in order to change its outcome. This problem is both a fundamental theoretical problem in social choice, and a major practical concern for democratic institutions. Consequently, this issue has received considerable attention, particularly as it pertains to different voting rules. In contrast, the problem of how election control can be prevented or deterred has been largely ignored. We introduce the problem of optimal protection against election control, where manipulation is allowed at the granularity of groups of voters (eg, voting locations), through a denialof-service attack, and the defender allocates limited protection resources to prevent control. We show that for plurality voting, election control through group deletion to prevent a candidate from winning is in P, while it is NP-Hard to prevent such control. We then present a double-oracle framework for computing an optimal prevention strategy, developing exact mixed-integer linear programming formulations for both the defender and attacker oracles (both of these subproblems we show to be NP-Hard), as well as heuristic oracles. Experiments conducted on both synthetic and real data demonstrate that the proposed computational framework can scale to realistic problem instances.", "total_citations": {"2017": 4, "2018": 5, "2019": 2, "2020": 6, "2021": 2, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:_B80troHkn4C": {"external_link": "https://link.springer.com/article/10.1007/s10458-015-9296-2", "authors": ["Yuan Liu", "Jie Zhang", "Bo An", "Sandip Sen"], "publication_date": "2016/7", "journal": "Autonomous Agents and Multi-Agent Systems", "volume": "30", "pages": "581-600", "publisher": "Springer US", "description": "In game theoretical analysis of incentive mechanisms, all players are assumed to be rational. Since it is likely that mechanism participants in the real world may not be fully rational, such mechanisms may not work as effectively as in the idealized settings for which they were designed. Therefore, it is important to evaluate the robustness of incentive mechanisms against various types of agents with bounded rational behaviors. Such evaluations would provide us with the information needed to choose mechanisms with desired properties in real environments. In this article, we first propose a general robustness measure, inspired by research in evolutionary game theory, as the maximal percentage of invaders taking non-equilibrium strategies such that the agents sustain the desired equilibrium strategy. We then propose a simulation framework based on evolutionary dynamics to empirically evaluate the\u00a0\u2026", "total_citations": {"2016": 3, "2017": 2, "2018": 2, "2019": 3, "2020": 2, "2021": 1, "2022": 4, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:HE397vMXCloC": {"external_link": "https://www.ifaamas.org/Proceedings/aamas2015/aamas/p523.pdf", "authors": ["Jiarui Gan", "Bo An", "Chunyan Miao"], "publication_date": "2015/5/4", "conference": "AAMAS", "pages": "523-531", "description": "Taxi service is an indispensable part of public transport in modern cities. However, due to its decentralized operation mode, taxi services in many cities are inefficient. Besides, the decentralized nature also poses significant challenges to analyzing and regulating taxi services. State of the art computational methods for optimizing taxi market efficiency suffer from two important limitations: 1) they cannot be scaled up efficiently; and 2) they cannot address complex real-world market situations where additional scheduling constraints need to be handled. In this paper, we propose two novel algorithms\u2014FLORA and FLORA-A\u2014to address the inadequacies. Using convex polytope representation techniques, FLORA provides a fully compact representation for taxi drivers\u2019 strategy space and scales up more efficiently than existing algorithms. FLORA-A avoids enumerating the entire exponentially large pure strategy space by gradually expanding the strategy space. It is the first known method capable of handling arbitrary scheduling constraints for optimizing taxi system efficiency. Experimental results show orders of magnitude improvement in speed FLORA provides, and the necessity of using FLORA-A as suggested by changes in the taxi drivers\u2019 operation strategy under different market conditions.", "total_citations": {"2015": 3, "2016": 2, "2017": 6, "2018": 5, "2019": 3, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:KlAtU1dfN6UC": {"external_link": "https://www.academia.edu/download/42808585/Agent-mediated_multi-step_optimization_f20160218-8194-1rpbur.pdf", "authors": ["Bo An", "Victor R Lesser", "David Westbrook", "Michael Zink"], "publication_date": "2011/5/2", "conference": "AAMAS", "pages": "609-616", "description": "Distributed collaborative adaptive sensing (DCAS) of the atmosphere is a new paradigm for detecting and predicting hazardous weather using a large dense network of short-range, low-powered radars to sense the lowest few kilometers of the earths atmosphere. In DCAS, radars are controlled by a collection of Meteorological Command and Control (MC&C) agents that instruct where to scan based on emerging weather conditions. Within this context, this work concentrates on designing efficient approaches for allocating sensing resources to cope with restricted real-time requirements and limited computational resources. We have developed a new approach based on explicit goals that can span multiple system heartbeats. This allows us to reason ahead about sensor allocations based on expected requirements of goals as they project forward in time. Each goal explicitly specifies end-users\u2019 preferences as well as a prediction of how a phenomena will move. We use a genetic algorithm to generate scanning strategies of each single MC&C and a distributed negotiation model to coordinate multiple MC&Cs\u2019 scanning strategies over multiple heartbeats. Simulation results show that as compared to simpler variants of our approach, the proposed distributed model achieved the highest social welfare. Our approach also has exhibited similarly very good performance in an operational radar testbed that is deployed in Oklahoma to observe severe weather events.", "total_citations": {"2012": 5, "2013": 4, "2014": 2, "2015": 2, "2016": 4, "2017": 1, "2018": 0, "2019": 0, "2020": 0, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=20&pagesize=80&citation_for_view=PEEpuNwAAAAJ:8k81kl-MbHgC": {"external_link": "https://cdn.aaai.org/ocs/2375/2375-10811-1-PB.pdf", "authors": ["Bo An", "Manish Jain", "Milind Tambe", "Christopher Kiekintveld"], "publication_date": "2011/3/20", "conference": "2011 AAAI Spring Symposium Series", "description": "Stackelberg games have been widely used to model patrolling or monitoring problems in security. In a Stackelberg security game, the defender commits to a strategy and the adversary makes its decision with knowledge of the leader\u2019s commitment. Algorithms for computing the defender\u2019s optimal strategy are used in deployed decision-support tools in use by the Los Angeles International Airport (LAX), the Federal Air Marshals Service, and the Transportation Security Administration (TSA). Those algorithms take into account various resource usage constraints defined by human users. However, those constraints may lead to poor (even infeasible) solutions due to users\u2019 insufficient information and bounded rationality. A mixed-initiative approach, in which human users and software assistants (agents) collaborate to make security decisions, is needed. Efficient human-agent interaction process leads to models with higher overall solution quality. This paper preliminarily analyzes the needs and challenges for such a mixed-initiative approach.", "total_citations": {"2011": 1, "2012": 4, "2013": 5, "2014": 3, "2015": 3, "2016": 1, "2017": 1, "2018": 0, "2019": 1, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:WF5omc3nYNoC": {"external_link": "https://content.iospress.com/articles/multiagent-and-grid-systems/mgs00094", "authors": ["Bo An", "Kwang Mong Sim", "Chun Yan Miao", "Zhi Qi Shen"], "publication_date": "2008/1/1", "journal": "Multiagent and Grid Systems", "volume": "4", "issue": "1", "pages": "5-23", "publisher": "IOS Press", "description": "In dynamic and complex negotiation environments, a negotiation agent can participate or quit negotiation at any time and can potentially reach an agreement with more than one trading partner as the result of the existence of dynamic outside options. Thus, it\u2019s important for a negotiation agent to make a decision on when to complete negotiation given its trading partners\u2019 current proposals and market dynamics. Rather than explicitly modeling all the trading partners, this paper presents a novel decision making strategy based on a tractable Markov chain model of negotiation process. An agent can use this model to determine whether to accept the best proposal of its trading partners or let negotiation proceed forward during each round of negotiation. Experimental results suggest that the proposed strategy achieved more favorable negotiation outcomes as compared with the general strategy.", "total_citations": {"2010": 1, "2011": 3, "2012": 2, "2013": 4, "2014": 6, "2015": 1, "2016": 1, "2017": 0, "2018": 0, "2019": 0, "2020": 1, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:4X0JR2_MtJMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9198894/", "authors": ["Lei Feng", "Jun Huang", "Senlin Shu", "Bo An"], "publication_date": "2020/9/16", "journal": "IEEE transactions on cybernetics", "volume": "52", "issue": "5", "pages": "3710-3721", "publisher": "IEEE", "description": "This article tackles the problem of multilabel learning with missing labels. For this problem, it is widely accepted that label correlations can be used to recover the ground-truth label matrix. Most of the existing approaches impose the low-rank assumption on the observed label matrix to exploit label correlations by decomposing it into two matrices, which describe the latent factors of instances and labels, respectively. The quality of these latent factors highly influences the recovery of ground-truth labels and the construction of the multilabel classification model. In this article, we propose recovering the ground-truth label matrix by regularized matrix factorization. Specifically, the latent factors of instances are regularized by the local topological structure derived from the feature space, which can be further used to induce an effective multilabel model. Moreover, the latent factors of labels and the label correlations are\u00a0\u2026", "total_citations": {"2021": 1, "2022": 8, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:PaBasH6fAo0C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9892077/", "authors": ["Zhe Wu", "Kai Li", "Hang Xu", "Yifan Zang", "Bo An", "Junliang Xing"], "publication_date": "2022/7/18", "conference": "2022 International Joint Conference on Neural Networks (IJCNN)", "pages": "1-8", "publisher": "IEEE", "description": "Opponent modeling is essential to exploit sub-optimal opponents in strategic interactions. Most previous works focus on building explicit models to predict the opponents' styles or strategies, which require a large amount of data to train the model and lack adaptability to unknown opponents. In this work, we propose a novel Learning to Exploit (L2E) framework for implicit opponent modeling. L2E acquires the ability to exploit opponents through a few interactions with different opponents during training of a neural network and can quickly adapt to new opponents with unknown styles during testing. To automatically produce challenging and diverse opponents for training, we further present a novel opponent strategy generation algorithm. We evaluate L2E on two poker games and one grid soccer game, which are the commonly used benchmarks for opponent modeling. Comprehensive experimental results indicate\u00a0\u2026", "total_citations": {"2021": 4, "2022": 11, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Bg7qf7VwUHIC": {"external_link": "https://www.ijcai.org/proceedings/2020/0433.pdf", "authors": ["Rundong Wang", "Runsheng Yu", "Bo An", "Zinovi Rabinovich"], "publication_date": "2021/1/7", "book": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence", "pages": "3131-3138", "description": "Hierarchical reinforcement learning (HRL) is a promising approach to solve tasks with long time horizons and sparse rewards. It is often implemented as a high-level policy assigning subgoals to a low-level policy. However, it suffers the high-level non-stationarity problem since the lowlevel policy is constantly changing. The nonstationarity also leads to the data efficiency problem: policies need more data at non-stationary states to stabilize training. To address these issues, we propose a novel HRL method: Interactive Influence-based Hierarchical Reinforcement Learning (I2HRL). First, inspired by agent modeling, we enable the interaction between the lowlevel and high-level policies, ie, the low-level policy sends its policy representation to the highlevel policy. The high-level policy makes decisions conditioned on the received low-level policy representation as well as the state of the environment. Second, we stabilize the training of the high-level policy via an information-theoretic regularization with minimal dependence on the changing low-level policy. Third, we propose the influence-based exploration to more frequently visit the non-stationary states where more transition data is needed. We experimentally validate the effectiveness of the proposed solution in several tasks in MuJoCo domains by demonstrating that our approach can significantly boost the learning performance and accelerate learning compared with stateof-the-art HRL methods.", "total_citations": {"2021": 3, "2022": 8, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:xtoqd-5pKcoC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/11581", "authors": ["Yanhai Xiong", "Haipeng Chen", "Mengchen Zhao", "Bo An"], "publication_date": "2018/4/26", "journal": "Proceedings of the AAAI conference on artificial intelligence", "volume": "32", "issue": "1", "description": "It has been an open challenge for self-interested agents to make optimal sequential decisions in complex multiagent systems, where agents might achieve higher utility via collaboration. The Microsoft Malmo Collaborative AI Challenge (MCAC), which is designed to encourage research relating to various problems in Collaborative AI, takes the form of a Minecraft mini-game where players might work together to catch a pig or deviate from cooperation, for pursuing high scores to win the challenge. Various characteristics, such as complex interactions among agents, uncertainties, sequential decision making and limited learning trials all make it extremely challenging to find effective strategies. We present HogRider---the champion agent of MCAC in 2017 out of 81 teams from 26 countries. One key innovation of HogRider is a generalized agent type hypothesis framework to identify the behavior model of the other agents, which is demonstrated to be robust to observation uncertainty. On top of that, a second key innovation is a novel Q-learning approach to learn effective policies against each type of the collaborating agents. Various ideas are proposed to adapt traditional Q-learning to handle complexities in the challenge, including state-action abstraction to reduce problem scale, a warm start approach using human reasoning for addressing limited learning trials, and an active greedy strategy to balance exploitation-exploration. Challenge results show that HogRider outperforms all the other teams by a significant edge, in terms of both optimality and stability.", "total_citations": {"2018": 3, "2019": 3, "2020": 4, "2021": 3, "2022": 1, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:QIV2ME_5wuYC": {"external_link": "https://search.proquest.com/openview/41217873de4d19233e2875aeac94df45/1?pq-origsite=gscholar&cbl=18750", "authors": ["Bo An"], "publication_date": "2010", "institution": "University of Massachusetts Amherst", "description": "The problem of constructing and analyzing systems of intelligent, autonomous agents is becoming more and more important. These agents may include people, physical robots, virtual humans, software programs acting on behalf of human beings, or sensors. In a large class of multi-agent scenarios, agents may have different capabilities, preferences, objectives, and constraints. Therefore, efficient allocation of resources among multiple agents is often difficult to achieve. Automated negotiation (bargaining) is the most widely used approach for multi-agent resource allocation and it has received increasing attention in the recent years. However, information uncertainty, existence of multiple contracting partners and competitors, agents\u2019 incentive to maximize individual utilities, and market dynamics make it difficult to calculate agents\u2019 rational equilibrium negotiation strategies and develop successful negotiation agents\u00a0\u2026", "total_citations": {"2012": 3, "2013": 5, "2014": 4, "2015": 0, "2016": 2, "2017": 2, "2018": 0, "2019": 0, "2020": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:zYLM7Y9cAGgC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-540-76282-9_1", "authors": ["Bo An", "Kwang Mong Sim", "Liang Gui Tang", "Chun Yan Miao", "Zhi Qi Shen", "Dai Jie Cheng"], "publication_date": "2008", "journal": "Rational, Robust, and Secure Negotiations in Multi-Agent Systems", "pages": "3-23", "publisher": "Springer Berlin Heidelberg", "description": "This work focuses on automated negotiation in dynamic and complex negotiation environments which often have the following three characteristics (1) an agent can reach an agreement with more than one trading partner, i.e., there are outside options in negotiation. Outside options can increase agents' bargaining power and can in.uence agents' reserve proposals or negotiation strategies; (2) agents can dynamically enter or leave the market; and (3) agents have incomplete information about the other entities. The remainder of this chapter is organized as follows. Section 2 presents the negotiation model and the MCDM strategy algorithm. In Sect. 3, we will explain the details of the Markov chain model. In Sect. 4, we examine the performance of the MCDM strategy through experimentation. Section 5 summarizes related work. In the.nal section, some conclusions are presented and ideas for future work are\u00a0\u2026", "total_citations": {"2008": 1, "2009": 0, "2010": 2, "2011": 1, "2012": 2, "2013": 2, "2014": 1, "2015": 1, "2016": 3, "2017": 1, "2018": 0, "2019": 2, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:8xutWZnSdmoC": {"external_link": "https://aaai.org/ojs/index.php/AAAI/article/view/5610", "authors": ["Youzhi Zhang", "Bo An"], "publication_date": "2020/4/3", "journal": "Proceedings of the AAAI conference on artificial intelligence", "volume": "34", "issue": "02", "pages": "2318-2325", "description": "The study of finding the equilibrium for multiplayer games is challenging. This paper focuses on computing Team-Maxmin Equilibria (TMEs) in zero-sum multiplayer Extensive-Form Games (EFGs), which describes the optimal strategies for a team of players who share the same goal but they take actions independently against an adversary. TMEs can capture many realistic scenarios, including: 1) a team of players play against a target player in poker games; and 2) defense resources schedule and patrol independently in security games. However, the study of efficiently finding TMEs within any given accuracy in EFGs is almost completely unexplored. To fill this gap, we first study the inefficiency caused by computing the equilibrium where team players correlate their strategies and then transforming it into the mixed strategy profile of the team and show that this inefficiency can be arbitrarily large. Second, to efficiently solve the non-convex program for finding TMEs directly, we develop the Associated Recursive Asynchronous Multiparametric Disaggregation Technique (ARAMDT) to approximate multilinear terms in the program with two novel techniques: 1) an asynchronous precision method to reduce the number of constraints and variables for approximation by using different precision levels to approximate these terms; and 2) an associated constraint method to reduce the feasible solution space of the mixed-integer linear program resulting from ARAMDT by exploiting the relation between these terms. Third, we develop a novel iterative algorithm to efficiently compute TMEs within any given accuracy based on ARAMDT. Our algorithm is orders of\u00a0\u2026", "total_citations": {"2020": 3, "2021": 3, "2022": 5, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:a3BOlSfXSfwC": {"external_link": "https://aaai.org/ojs/index.php/AAAI/article/view/3886", "authors": ["Jan Karwowski", "Jacek Ma\u0144dziuk", "Adam \u017bychowski", "Filip Grajek", "Bo An"], "publication_date": "2019/7/17", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "33", "issue": "01", "pages": "970-977", "description": "This paper introduces a new type of Security Games (SG) played on a plane with targets moving along predefined straight line trajectories and its respective Mixed Integer Linear Programming (MILP) formulation. Three approaches for solving the game are proposed and experimentally evaluated: application of an MILP solver to finding exact solutions for small-size games, MILP-based extension of recently published zero-sum SG approach to the case of generalsum games for finding approximate solutions of medium-size games, and the use of Memetic Algorithm (MA) for mediumsize and large-size game instances, which are beyond MILP\u2019s scalability. Utilization of MA is, to the best of our knowledge, a new idea in the field of SG. The novelty of proposed solution lies specifically in efficient chromosome-based game encoding and dedicated local improvement heuristics. In vast majority of test cases with known equilibrium profiles, the method leads to optimal solutions with high stability and approximately linear time scalability. Another advantage is an iteration-based construction of the system, which makes the approach essentially an anytime method. This property is of paramount importance in case of restrictive time limits, which could hinder the possibility of calculating an exact solution. On a general note, we believe that MA-based methods may offer a viable alternative to MILP solvers for complex games that require application of approximate solving methods.", "total_citations": {"2019": 5, "2020": 1, "2021": 3, "2022": 4, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:XiSMed-E-HIC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0004370217300048", "authors": ["Galit Haim", "Bo An", "Sarit Kraus"], "publication_date": "2017/5/1", "journal": "Artificial Intelligence", "volume": "246", "pages": "34-52", "publisher": "Elsevier", "description": "This paper proposes a novel agent-design for a three-player game involving human players and computer agents. The game is analogous to settings in which participants repeatedly negotiate over contracts, such as cell-phones and credit card plans. The game comprises three players, two service providers who compete to sign contracts with a single customer player. The service providers compete to make repeated contract offers to the customer consisting of resource exchanges in the game. Customers can join and leave contracts at will. We computed sub-game perfect equilibrium strategies for all players that were based on making contracts involving commitments between the customer player and one of the service provider players. We conducted extensive empirical studies (spanning over 500 participants) comparing the performance of computer agents using different types of equilibrium strategies with that\u00a0\u2026", "total_citations": {"2016": 2, "2017": 2, "2018": 5, "2019": 2, "2020": 3, "2021": 1, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:EYYDruWGBe4C": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3041217", "authors": ["Bo An", "Haipeng Chen", "Noseong Park", "VS Subrahmanian"], "publication_date": "2017/3/22", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "volume": "8", "issue": "4", "pages": "1-28", "publisher": "ACM", "description": "Although numerous traditional models predict market share and demand along airline routes, the prediction of existing models is not precise enough, and to the best of our knowledge, there is no use of data mining--based forecasting techniques for improving airline profitability. We propose the maximizing airline profits (MAP) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared to past methods used to forecast market share and demand along airline routes, we introduce a novel ensemble forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson correlation coefficients (greater than 0.95 vs. 0.82 for market share, 0.98 vs. 0\u00a0\u2026", "total_citations": {"2018": 2, "2019": 1, "2020": 3, "2021": 5, "2022": 2, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:qjMakFHDy7sC": {"external_link": "https://ifaamas.org/Proceedings/aamas08/proceedings/pdf/paper/AAMAS08_0763.pdf", "authors": ["Bo An", "Victor Lesser", "Kwang Mong Sim"], "publication_date": "2008", "journal": "Proceedings of the 7th international conference on autonomous agents and multiagent systems (AAMAS 2008), Estoril, Portugal", "pages": "1553-1556", "description": "This paper presents the design and implementation of negotiation agents that negotiate with other entities for acquiring multiple resources. In our approach, agents utilize a time-dependent negotiation strategy in which the reserve price of each negotiation issue is dynamically determined by 1) the likelihood that negotiation will not be successfully completed (conflict probability), 2) the expected agreement price of the issue, and 3) the expected number of final agreements. Results from a series of experiments indicate that on average, our negotiation strategy achieved higher average utility than traditional negotiation strategies.", "total_citations": {"2009": 2, "2010": 3, "2011": 2, "2012": 2, "2013": 0, "2014": 1, "2015": 1, "2016": 2, "2017": 2, "2018": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:6yz0xqPARnAC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/16728", "authors": ["Youzhi Zhang", "Bo An", "Jakub \u010cern\u00fd"], "publication_date": "2021/5/18", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "35", "issue": "6", "pages": "5813-5821", "description": "Computational game theory has many applications in the modern world in both adversarial situations and the optimization of social good. While there exist many algorithms for computing solutions in two-player interactions, finding optimal strategies in multiplayer interactions efficiently remains an open challenge. This paper focuses on computing the multiplayer Team-Maxmin Equilibrium with Coordination device (TMECor) in zero-sum extensive-form games. TMECor models scenarios when a team of players coordinates ex ante against an adversary. Such situations can be found in card games (eg, in Bridge and Poker), when a team works together to beat a target player but communication is prohibited; and also in real world, eg, in forest-protection operations, when coordinated groups have limited contact during interdicting illegal loggers. The existing algorithms struggle to find a TMECor efficiently because of their high computational costs. To compute a TMECor in larger games, we make the following key contributions:(1) we propose a hybrid-form strategy representation for the team, which preserves the set of equilibria;(2) we introduce a column-generation algorithm with a guaranteed finite-time convergence in the infinite strategy space based on a novel best-response oracle;(3) we develop an associated-representation technique for the exact representation of the multilinear terms in the best-response oracle; and (4) we experimentally show that our algorithm is several orders of magnitude faster than prior state-of-the-art algorithms in large games.", "total_citations": {"2021": 2, "2022": 10, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:IUKN3-7HHlwC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8611152/", "authors": ["Xiaobo Ma", "Bo An", "Mengchen Zhao", "Xiapu Luo", "Lei Xue", "Zhenhua Li", "Tony TN Miu", "Xiaohong Guan"], "publication_date": "2019/1/13", "journal": "IEEE Transactions on Dependable and Secure Computing", "volume": "17", "issue": "4", "pages": "795-812", "publisher": "IEEE", "description": "With the advancement of large-scale coordinated attacks, the adversary is shifting away from traditional distributed denial of service (DDoS) attacks against servers to sophisticated DDoS attacks against Internet infrastructures. Link flooding attacks (LFAs) are such powerful attacks against Internet links. Employing network measurement techniques, the defender could detect the link under attack. However, given the large number of Internet links, the defender can only monitor a subset of the links simultaneously, whereas any link might be attacked. Therefore, it remains challenging to practically deploy detection methods. This paper addresses this challenge from a game-theoretic perspective, and proposes a randomized approach (like security patrolling) to optimize LFA detection strategies. Specifically, we formulate the LFA detection problem as a Stackelberg security game, and design randomized detection\u00a0\u2026", "total_citations": {"2019": 1, "2020": 2, "2021": 6, "2022": 5, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:UeHWp8X0CEIC": {"external_link": "https://ifaamas.org/Proceedings/aamas08/proceedings/pdf/paper/AAMAS08_0770.pdf", "authors": ["Bo An", "Fred Douglis", "Fan Ye"], "publication_date": "2008/5/12", "book": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 2", "pages": "551-558", "description": "In cooperating systems such as grids [4] and collaborative streaming analysis [2], autonomous sites can establish \u201cagreements\u201d to arrange access to remote resources for a period of time [1]. The determination of which resources to reserve to accomplish a task need not be known a priori, because there exist multiple plans for accomplishing the same task and they may require access to different resources [3]. While these plans can be functionally equivalent, they may have different performance/cost tradeoffs and may use a variety of resources, both local and belonging to other sites. The negotiation schedule, ie, the order in which remote resources are negotiated, determines how quickly one plan can be selected and deployed; it also decides the utility for running the plan. This paper studies the problem of optimizing negotiation schedules in cooperative systems with multiple plans. We first provide a votingbased heuristic that reduces the complexity O (n!) of the exhaustive search to O (mnq). We also present a weight-based heuristic that further reduces the complexity to O (mn). Experimental results show that, on average, 1) the voting-based approach achieved 6% higher utility than the weight-based approach but the voting-based approach has a much higher computation cost than the weightbased approach, 2) the two proposed approaches achieved almost 50% higher utility than a randomized approach; and 3) the average utility produced by the two proposed approaches are within almost 90% of that of the optimal results with reasonable plan sizes.", "total_citations": {"2009": 2, "2010": 2, "2011": 4, "2012": 2, "2013": 1, "2014": 1, "2015": 1, "2016": 2, "2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:CdxZDUztZiMC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3582560", "authors": ["Shuo Sun", "Rundong Wang", "Bo An"], "publication_date": "2023/3/24", "journal": "ACM Transactions on Intelligent Systems and Technology", "volume": "14", "issue": "3", "pages": "1-29", "publisher": "ACM", "description": "Quantitative trading (QT), which refers to the usage of mathematical models and data-driven techniques in analyzing the financial market, has been a popular topic in both academia and financial industry since 1970s. In the last decade, reinforcement learning (RL) has garnered significant interest in many domains such as robotics and video games, owing to its outstanding ability on solving complex sequential decision making problems. RL\u2019s impact is pervasive, recently demonstrating its ability to conquer many challenging QT tasks. It is a flourishing research direction to explore RL techniques\u2019 potential on QT tasks. This paper aims at providing a comprehensive survey of research efforts on RL-based methods for QT tasks. More concretely, we devise a taxonomy of RL-based QT models, along with a comprehensive summary of the state of the art. Finally, we discuss current challenges and propose future research\u00a0\u2026", "total_citations": {"2022": 6, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:4xDN1ZYqzskC": {"external_link": "https://link.springer.com/article/10.1007/s10458-020-09484-5", "authors": ["Yanhai Xiong", "Bo An", "Sarit Kraus"], "publication_date": "2021/4", "journal": "Autonomous Agents and Multi-Agent Systems", "volume": "35", "pages": "1-19", "publisher": "Springer US", "description": "Optimal placement of charging stations for electric vehicles (EVs) is critical for providing convenient charging service to EV owners and promoting public acceptance of EVs. There has been a lot of work on EV charging station placement, yet EV drivers\u2019 charging strategy, which plays an important role in deciding charging stations\u2019 performance, is missing. EV drivers make choice among charging stations according to various factors, including the distance, the charging fare and queuing condition in different stations etc. In turn, some factors, like queuing condition, is greatly influenced by EV drivers\u2019 choices. As more EVs visit the same station, longer queuing duration should be expected. This work first proposes a behavior model to capture the decision making of EV drivers in choosing charging stations, based on which an optimal charging station placement model is presented to minimize the social cost\u00a0\u2026", "total_citations": {"2021": 1, "2022": 8, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:sNmaIFBj_lkC": {"external_link": "https://aaai.org/ojs/index.php/AAAI/article/download/4031/3909", "authors": ["Qingyu Guo", "Jiarui Gan", "Fei Fang", "Long Tran-Thanh", "Milind Tambe", "Bo An"], "publication_date": "2019/7/17", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "33", "issue": "01", "pages": "2020-2028", "description": "Strong Stackelberg equilibrium (SSE) is the standard solution concept of Stackelberg security games. As opposed to the weak Stackelberg equilibrium (WSE), the SSE assumes that the follower breaks ties in favor of the leader and this is widely acknowledged and justified by the assertion that the defender can often induce the attacker to choose a preferred action by making an infinitesimal adjustment to her strategy. Unfortunately, in security games with resource assignment constraints, the assertion might not be valid; it is possible that the defender cannot induce the desired outcome. As a result, many results claimed in the literature may be overly optimistic. To remedy, we first formally define the utility guarantee of a defender strategy and provide examples to show that the utility of SSE can be higher than its utility guarantee. Second, inspired by the analysis of leader\u2019s payoff by Von Stengel and Zamir (2004), we provide the solution concept called the inducible Stackelberg equilibrium (ISE), which owns the highest utility guarantee and always exists. Third, we show the conditions when ISE coincides with SSE and the fact that in general case, SSE can be extremely worse with respect to utility guarantee. Moreover, introducing the ISE does not invalidate existing algorithmic results as the problem of computing an ISE polynomially reduces to that of computing an SSE. We also provide an algorithmic implementation for computing ISE, with which our experiments unveil the empirical advantage of the ISE over the SSE.", "total_citations": {"2019": 2, "2020": 2, "2021": 4, "2022": 5, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:9c2xU6iGI7YC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8620370/", "authors": ["Jiuchuan Jiang", "Bo An", "Yichuan Jiang", "Peng Shi", "Zhan Bu", "Jie Cao"], "publication_date": "2019/1/20", "journal": "IEEE Transactions on Parallel and Distributed Systems", "volume": "30", "issue": "8", "pages": "1722-1737", "publisher": "IEEE", "description": "Existing studies on crowdsourcing often adopt the retail-style allocation approach, in which tasks are allocated individually and independently. However, such retail-style task allocation has the following problems: 1) each task is executed independently from scratch, thus the execution of one task seldom utilize the results of other tasks and the requester must pay in full for the task; 2) many workers only undertake a very small number of tasks contemporaneously, thus the workers' skills and time may not be fully utilized. We observe that many complex tasks in real-world crowdsourcing platforms have similar skill requirements and long deadlines. Based on these real-world observations, this paper presents a novel batch allocation approach for tasks with overlapping skill requirements. Requesters' real payment can be discounted because the real execution cost of tasks can be reduced due to batch allocation and\u00a0\u2026", "total_citations": {"2020": 4, "2021": 5, "2022": 5, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:4hFrxpcac9AC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/11291", "authors": ["Xinrun Wang", "Bo An", "Martin Strobel", "Fookwai Kong"], "publication_date": "2018/4/25", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "32", "issue": "1", "description": "Pirate syndicates capturing tankers to siphon oil, causing an estimated cost of $5 billion a year, has become a serious security issue for maritime traffic. In response to the threat, coast guards and navies deploy patrol boats to protect international oil trade. However, given the vast area of the sea and the highly time and space dependent behaviors of both players, it remains a significant challenge to find efficient ways to deploy patrol resources. In this paper, we address the research challenges and provide four key contributions. First, we construct a Stackelberg model of the oil-siphoning problem based on incident reports of actual attacks; Second, we propose a compact formulation and a constraint generation algorithm, which tackle the exponentially growth of the defender\u2019s and attacker\u2019s strategy spaces, respectively, to compute efficient strategies of security agencies; Third, to further improve the scalability, we propose an abstraction method, which exploits the intrinsic similarity of defender\u2019s strategy space, to solve extremely large-scale games; Finally, we evaluate our approaches through extensive simulations and a detailed case study with real ship traffic data. The results demonstrate that our approach achieves a dramatic improvement of scalability with modest influence on the solution quality and can scale up to realistic-sized problems.", "total_citations": {"2018": 2, "2019": 6, "2020": 3, "2021": 0, "2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:MhiOAD_qIWkC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/10122995/", "authors": ["Jiaqi Lv", "Biao Liu", "Lei Feng", "Ning Xu", "Miao Xu", "Bo An", "Gang Niu", "Xin Geng", "Masashi Sugiyama"], "publication_date": "2023/5/11", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "publisher": "IEEE", "description": "Partial-label learning \u00a0(PLL) utilizes instances with PLs, where a PL includes several candidate labels but only one is the true label (TL). In PLL,  identification-based strategy \u00a0(IBS) purifies each PL on the fly to select the (most likely) TL for training;  average-based strategy \u00a0(ABS) treats all candidate labels equally for training and let trained models be able to predict TL. Although PLL research has focused on IBS for better performance, ABS is also worthy of study since  modern IBS behaves like ABS in the beginning of training  to prepare for PL purification and TL selection. In this paper, we analyze why ABS was unsatisfactory and propose how to improve it. Theoretically, we propose two problem settings of PLL and prove that  average PL  losses\u00a0(APLLs) with  bounded  multi-class losses are  always  robust, while APLLs with  unbounded  losses may be non-robust, which is the first robustness analysis for PLL\u00a0\u2026", "total_citations": {"2022": 4, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:DBa1UEJaJKAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9765651/", "authors": ["Hongxin Wei", "Renchunzi Xie", "Lei Feng", "Bo Han", "Bo An"], "publication_date": "2022/4/29", "journal": "IEEE transactions on neural networks and learning systems", "publisher": "IEEE", "description": "Crowdsourcing is a popular solution for large-scale data annotations. So far, various end-to-end deep learning methods have been proposed to improve the practical performance of learning from crowds. Despite their practical effectiveness, most of them have two major limitations--they do not hold learning consistency and suffer from computational inefficiency. In this article, we propose a novel method named UnionNet, which is not only theoretically consistent but also experimentally effective and efficient. Specifically, unlike existing methods that either fit a given label from each annotator independently or fuse all the labels into a reliable one, we concatenate the one-hot encoded vectors of crowdsourced labels provided by all the annotators, which takes all the labeling information as a union and coordinates multiple annotators. In this way, we can directly train an end-to-end deep neural network by maximizing\u00a0\u2026", "total_citations": {"2021": 1, "2022": 1, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:6bLC7aUMtPcC": {"external_link": "https://openreview.net/forum?id=18Ys0-PzyPI", "authors": ["Pengjie Gu", "Mengchen Zhao", "Jianye Hao", "Bo An"], "publication_date": "2021/10/6", "conference": "International Conference on Learning Representations", "description": "Autonomous agents often need to work together as a team to accomplish complex cooperative tasks. Due to privacy and other realistic constraints, agents might need to collaborate with previously unknown teammates on the fly. This problem is known as ad hoc teamwork, which remains a core research challenge. Prior works usually rely heavily on strong assumptions like full observability, fixed and predefined teammates' types. This paper relaxes these assumptions with a novel reinforcement learning framework called ODITS, which allows the autonomous agent to adapt to arbitrary teammates in an online fashion. Instead of limiting teammates into a finite set of predefined types, ODITS automatically learns latent variables of teammates' behaviors to infer how to cooperate with new teammates effectively. To overcome partial observability, we introduce an information-based regularizer to derive proxy representations of the learned variables from local observations. Extensive experimental results show that ODITS significantly outperforms various baselines in widely used ad hoc teamwork tasks.", "total_citations": {"2022": 3, "2023": 11}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:KbBQZpvPDL4C": {"external_link": "https://proceedings.neurips.cc/paper_files/paper/2019/hash/c4819d06b0ca810d38506453cfaae9d8-Abstract.html", "authors": ["Jiarui Gan", "Qingyu Guo", "Long Tran-Thanh", "Bo An", "Michael Wooldridge"], "publication_date": "2019", "journal": "Advances in Neural Information Processing Systems", "volume": "32", "description": "In Stackelberg security games when information about the attacker's payoffs is uncertain, algorithms have been proposed to learn the optimal defender commitment by interacting with the attacker and observing their best responses. In this paper, we show that, however, these algorithms can be easily manipulated if the attacker responds untruthfully. As a key finding, attacker manipulation normally leads to the defender learning a maximin strategy, which effectively renders the learning attempt meaningless as to compute a maximin strategy requires no additional information about the other player at all. We then apply a game-theoretic framework at a higher level to counteract such manipulation, in which the defender commits to a policy that specifies her strategy commitment according to the learned information. We provide a polynomial-time algorithm to compute the optimal such policy, and in addition, a heuristic approach that applies even when the attacker's payoff space is infinite or completely unknown. Empirical evaluation shows that our approaches can improve the defender's utility significantly as compared to the situation when attacker manipulation is ignored.", "total_citations": {"2020": 3, "2021": 4, "2022": 5, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:S16KYo8Pm5AC": {"external_link": "https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2749", "authors": ["Pradeep Varakantham", "Bo An", "Bryan Low", "Jie Zhang"], "publication_date": "2017/10/2", "journal": "Ai Magazine", "volume": "38", "issue": "3", "pages": "102-105", "description": "Artificial Intelligence (AI) research in Singapore is focused on accelerating the country\u2019s development into a Smart Nation. Specifically, AI has been employed extensively in either augmenting the intelligence of humans or in developing automated methods and systems to improve quality of life in Singapore.", "total_citations": {"2018": 1, "2019": 2, "2020": 1, "2021": 4, "2022": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:j8SEvjWlNXcC": {"external_link": "https://www.ijcai.org/Proceedings/16/Papers/082.pdf", "authors": ["Yue Yin", "Bo An"], "publication_date": "2016/7/9", "conference": "IJCAI", "pages": "531-537", "description": "Coral reefs are valuable and fragile ecosystems which are under threat from human activities like coral mining. Many countries have built marine protected areas (MPAs) and protect their ecosystems through boat patrol. However, it remains a significant challenge to efficiently patrol the MPAs given the limited patrol resources of the protection agency and potential destructors\u2019 strategic actions. In this paper, we view the problem of efficiently patrolling for protecting coral reef ecosystems from a game-theoretic perspective and propose 1) a new Stackelberg game model to formulate the problem of protecting MPAs, 2) two algorithms to compute the efficient protection agency\u2019s strategies: CLP in which the protection agency\u2019s strategies are compactly represented as fractional flows in a network, and CDOG which combines the techniques of compactly representing defender strategies and incrementally generating strategies. Experimental results show that our approach leads to significantly better solution quality than that of previous works.", "total_citations": {"2017": 4, "2018": 3, "2019": 2, "2020": 2, "2021": 1, "2022": 0, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Y0pCki6q_DkC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5285147/", "authors": ["Bo An", "Nicola Gatti", "Victor Lesser"], "publication_date": "2009/9/15", "conference": "2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology", "volume": "2", "pages": "403-410", "publisher": "IEEE", "description": "It is a challenging problem to find agents\u2019 rational strategies in bargaining with incomplete information. In this paper we perform a game theoretic analysis of agents\u2019 rational strategies in finite horizon bilateral bargaining with one-sided uncertainty regarding agents\u2019 reserve prices. The negotiation setting considered in this paper has four features: alternating-offers bargaining protocol, finite horizon, two-type uncertainty about agents\u2019 reserve prices, and discount factors. The main contribution of this paper is the development of a novel algorithm to find a pure strategy sequential equilibrium in the setting we study. Our algorithm is based on the combination of game theoretic analysis and search techniques which finds agents\u2019 equilibrium in pure strategies when they exist.", "total_citations": {"2010": 3, "2011": 0, "2012": 1, "2013": 1, "2014": 0, "2015": 4, "2016": 2, "2017": 0, "2018": 1, "2019": 1, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:kw52XkFRtyQC": {"external_link": "https://proceedings.mlr.press/v161/chen21b.html", "authors": ["Haipeng Chen", "Wei Qiu", "Han-Ching Ou", "Bo An", "Milind Tambe"], "publication_date": "2021/12/1", "conference": "Uncertainty in Artificial Intelligence", "pages": "1535-1545", "publisher": "PMLR", "description": "The influence maximization (IM) problem aims at finding a subset of seed nodes in a social network that maximize the spread of influence. In this study, we focus on a sub-class of IM problems, where whether the nodes are willing to be the seeds when being invited is uncertain, called contingency-aware IM. Such contingency aware IM is critical for applications for non-profit organizations in low resource communities (eg, spreading awareness of disease prevention). Despite the initial success, a major practical obstacle in promoting the solutions to more communities is the tremendous runtime of the greedy algorithms and the lack of high performance computing (HPC) for the non-profits in the field\u2013whenever there is a new social network, the non-profits usually do not have the HPCs to recalculate the solutions. Motivated by this and inspired by the line of works that use reinforcement learning (RL) to address combinatorial optimization on graphs, we formalize the problem as a Markov Decision Process (MDP), and use RL to learn an IM policy over historically seen networks, and generalize to unseen networks with negligible runtime at test phase. To fully exploit the properties of our targeted problem, we propose two technical innovations that improve the existing methods, including state-abstraction and theoretically grounded reward shaping. Empirical results show that our method achieves influence as high as the state-of-the-art methods for contingency-aware IM, while having negligible runtime at test phase.", "total_citations": {"2022": 4, "2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:uDGL6kOW6j0C": {"external_link": "http://proceedings.mlr.press/v139/feng21d.html", "authors": ["Lei Feng", "Senlin Shu", "Nan Lu", "Bo Han", "Miao Xu", "Gang Niu", "Bo An", "Masashi Sugiyama"], "publication_date": "2021/7/1", "conference": "International Conference on Machine Learning", "pages": "3252-3262", "publisher": "PMLR", "description": "To alleviate the data requirement for training effective binary classifiers in binary classification, many weakly supervised learning settings have been proposed. Among them, some consider using pairwise but not pointwise labels, when pointwise labels are not accessible due to privacy, confidentiality, or security reasons. However, as a pairwise label denotes whether or not two data points share a pointwise label, it cannot be easily collected if either point is equally likely to be positive or negative. Thus, in this paper, we propose a novel setting called pairwise comparison (Pcomp) classification, where we have only pairs of unlabeled data that we know one is more likely to be positive than the other. Firstly, we give a Pcomp data generation process, derive an unbiased risk estimator (URE) with theoretical guarantee, and further improve URE using correction functions. Secondly, we link Pcomp classification to noisy-label learning to develop a progressive URE and improve it by imposing consistency regularization. Finally, we demonstrate by experiments the effectiveness of our methods, which suggests Pcomp is a valuable and practically useful type of pairwise supervision besides the pairwise label.", "total_citations": {"2021": 4, "2022": 3, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:artPoR2Yc-kC": {"external_link": "http://proceedings.mlr.press/v119/zhang20c.html", "authors": ["Youzhi Zhang", "Bo An"], "publication_date": "2020/11/21", "conference": "International Conference on Machine Learning", "pages": "11033-11043", "publisher": "PMLR", "description": "Efficiently computing equilibria for multiplayer games is still an open challenge in computational game theory. This paper focuses on computing Team-Maxmin Equilibria (TMEs), which is an important solution concept for zero-sum multiplayer games where players in a team having the same utility function play against an adversary independently. Existing algorithms are inefficient to compute TMEs in large games, especially when the strategy space is too large to be represented due to limited memory. In two-player games, the Incremental Strategy Generation (ISG) algorithm is an efficient approach to avoid enumerating all pure strategies. However, the study of ISG for computing TMEs is completely unexplored. To fill this gap, we first study the properties of ISG for multiplayer games, showing that ISG converges to a Nash Equilibrium (NE) but may not converge to a TME. Second, we design an ISG variant for TMEs (ISGT) by exploiting that a TME is an NE maximizing the team\u2019s utility and show that ISGT converges to a TME and the impossibility of relaxing conditions in ISGT. Third, to further improve the scalability, we design an ISGT variant (CISGT) by using the strategy space for computing an equilibrium that is close to a TME but is easier to be computed as the initial strategy space of ISGT. Finally, extensive experimental results show that CISGT is orders of magnitude faster than ISGT and the state-of-the-art algorithm to compute TMEs in large games.", "total_citations": {"2020": 1, "2021": 4, "2022": 5, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:FPJr55Dyh1AC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S000437021830050X", "authors": ["Yue Yin", "Yevgeniy Vorobeychik", "Bo An", "Noam Hazon"], "publication_date": "2018/6/1", "journal": "Artificial Intelligence", "volume": "259", "pages": "32-51", "publisher": "Elsevier", "description": "Election control encompasses attempts from an external agent to alter the structure of an election in order to change its outcome. This problem is both a fundamental theoretical problem in social choice, and a major practical concern for democratic institutions. Consequently, this issue has received considerable attention, particularly as it pertains to different voting rules. In contrast, the problem of how election control can be prevented or deterred has been largely ignored. We introduce the problem of optimal defense against election control, including destructive and constructive control, where manipulation is allowed at the granularity of groups of voters (e.g., voting locations) through a denial-of-service attack, and the defender allocates limited protection resources to prevent control. We consider plurality voting, and show that it is computationally hard to prevent both types of control, though destructive control itself\u00a0\u2026", "total_citations": {"2019": 5, "2020": 3, "2021": 2, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:M3NEmzRMIkIC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-642-32729-2_13", "authors": ["Han Yu", "Zhiqi Shen", "Bo An"], "publication_date": "2012", "conference": "PRIMA 2012: Principles and Practice of Multi-Agent Systems: 15th International Conference, Kuching, Sarawak, Malaysia, September 3-7, 2012. Proceedings 15", "pages": "184-198", "publisher": "Springer Berlin Heidelberg", "description": "In distributed multi-agent systems where agents need to cooperate with each other in order to thrive, accurately estimating a potential partner\u2019s trustworthiness is vital to an agent\u2019s wellbeing. Many distributed reputation-based agent trust models have been proposed. It has been agreed that testimony sharing is a useful way for agents to gain knowledge about the reputation of potential interaction partners without having to expose themselves to the risk of actually interacting with them. However, the presence of unfair testimonies adversely affects an agent\u2019s long term wellbeing and has been an important problem in agent trust research. Many testimony filtering methods have been proposed, but they often rely on assumptions about the characteristics of the witness agent population or supporting facilities in the environment. In addition, some methods involve highly iterative approaches that consume\u00a0\u2026", "total_citations": {"2013": 1, "2014": 0, "2015": 2, "2016": 7, "2017": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:6_hjMsCP8ZoC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9779600/", "authors": ["Bo An", "Shuo Sun", "Rundong Wang"], "publication_date": "2022/5/20", "source": "IEEE Intelligent Systems", "volume": "37", "issue": "2", "pages": "23-26", "publisher": "IEEE", "description": "Quantitative trading (QT) has been a popular topic in both academia and the financial industry since the 1970s. In the last decade, deep reinforcement learning (DRL) has garnered significant research interest with stellar performance in solving complex sequential decision-making problems, such as Go and video games. The impact of DRL is pervasive, recently demonstrating its ability to conquer some challenging QT tasks. In this article, we outline several key challenges and opportunities that manifest in DRL-based QT to shed light on future research in this field.", "total_citations": {"2021": 1, "2022": 3, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:otzGkya1bYkC": {"external_link": "https://arxiv.org/abs/2012.00925", "authors": ["Zhuowei Wang", "Jing Jiang", "Bo Han", "Lei Feng", "Bo An", "Gang Niu", "Guodong Long"], "publication_date": "2020/12/2", "journal": "arXiv preprint arXiv:2012.00925", "description": "Deep learning with noisy labels is a challenging task. Recent prominent methods that build on a specific sample selection (SS) strategy and a specific semi-supervised learning (SSL) model achieved state-of-the-art performance. Intuitively, better performance could be achieved if stronger SS strategies and SSL models are employed. Following this intuition, one might easily derive various effective noisy-label learning methods using different combinations of SS strategies and SSL models, which is, however, reinventing the wheel in essence. To prevent this problem, we propose SemiNLL, a versatile framework that combines SS strategies and SSL models in an end-to-end manner. Our framework can absorb various SS strategies and SSL backbones, utilizing their power to achieve promising performance. We also instantiate our framework with different combinations, which set the new state of the art on benchmark-simulated and real-world datasets with noisy labels.", "total_citations": {"2021": 3, "2022": 6, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:gKiMpY-AVTkC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9120177/", "authors": ["Wanyuan Wang", "Bo An", "Yichuan Jiang"], "publication_date": "2020/6/18", "journal": "IEEE Transactions on Computational Social Systems", "volume": "7", "issue": "4", "pages": "940-955", "publisher": "IEEE", "description": "Peer grading is a natural crowdsourcing application, where dispersed students/peers resources are collected to evaluate others' assignments. Peer grading also offers a promising solution for scaling evaluation and learning to large-scale educational systems. A key challenge in peer grading is motivating peers to grade diligently and provide a high-quality evaluation. Spot-checking (SC) mechanisms, allowing instructors to check evaluations, can prevent peer collusion where peers grade arbitrarily and coordinate to report the uninformative grade. However, existing SC mechanisms unrealistically assume that peers have the same grading reliability and cost. This is limiting in practice, where we would expect peers to differ in reliability and cost. This article proposes the general Optimal SC (OptSC) model of determining the probability that each assignment needs to be checked to maximize assignments' evaluation\u00a0\u2026", "total_citations": {"2020": 1, "2021": 2, "2022": 5, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:JQOojiI6XY0C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/9824", "authors": ["Fei Fang", "Thanh Nguyen", "Rob Pickles", "Wai Lam", "Gopalasamy Clements", "Bo An", "Amandeep Singh", "Milind Tambe"], "publication_date": "2016/3/5", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "30", "issue": "1", "description": "The conservation of key wildlife species such as tigers and elephants are threatened by poaching activities. In many conservation areas, foot patrols are conducted to prevent poaching but they may not be well-planned to make the best use of the limited patrolling resources. While prior work has introduced PAWS (Protection Assistant for Wildlife Security) as a game-theoretic decision aid to design effective foot patrol strategies to protect wildlife, the patrol routes generated by PAWS may be difficult to follow in areas with complex terrain. Subsequent research has worked on the significant evolution of PAWS, from an emerging application to a regularly deployed software. A key advance of the deployed version of PAWS is that it incorporates the complex terrain information and generates a strategy consisting of easy-to-follow routes. In this demonstration, we provide 1) a video introducing the PAWS system; 2) an interactive visualization of the patrol routes generated by PAWS in an example area with complex terrain; and 3) a machine-human competition in designing patrol strategy given complex terrain and animal distribution.", "total_citations": {"2016": 2, "2017": 1, "2018": 5, "2019": 0, "2020": 2, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:0EnyYjriUFMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4810152/", "authors": ["Kwang Mong Sim", "Bo An"], "publication_date": "2009/4/3", "journal": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)", "volume": "39", "issue": "3", "pages": "284-298", "publisher": "IEEE", "description": "There are very few existing works that adopt genetic algorithms (GAs) for evolving the most successful strategies for different negotiation situations. Furthermore, these works did not explicitly model the influence of market dynamics. The contribution of this work is developing bargaining agents that can both: 1) react to different market situations by adjusting their amounts of concessions and 2) evolve their best-response strategies for different market situations and constraints using an aggregative fitness GA (AFGA). While many existing negotiation agents only optimize utilities, the AFGA in this work is used to evolve best-response strategies of negotiation agents that optimize their utilities, success rates, and negotiation speed in different market situations. Given different constraints and preferences of agents in optimizing utilities, success rates, and negotiation speed, different best-response strategies can be evolved\u00a0\u2026", "total_citations": {"2010": 2, "2011": 0, "2012": 2, "2013": 3, "2014": 1, "2015": 1, "2016": 1, "2017": 0, "2018": 0, "2019": 0, "2020": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:0CzhzZyukY4C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/16701", "authors": ["David Milec", "Jakub \u010cern\u00fd", "Viliam Lis\u00fd", "Bo An"], "publication_date": "2021/5/18", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "35", "issue": "6", "pages": "5575-5583", "description": "Solution concepts of traditional game theory assume entirely rational players; therefore, their ability to exploit subrational opponents is limited. One type of subrationality that describes human behavior well is the quantal response. While there exist algorithms for computing solutions against quantal opponents, they either do not scale or may provide strategies that are even worse than the entirely-rational Nash strategies. This paper aims to analyze and propose scalable algorithms for computing effective and robust strategies against a quantal opponent in normal-form and extensive-form games. Our contributions are:(1) we define two different solution concepts related to exploiting quantal opponents and analyze their properties;(2) we prove that computing these solutions is computationally hard;(3) therefore, we evaluate several heuristic approximations based on scalable counterfactual regret minimization (CFR); and (4) we identify a CFR variant that exploits the bounded opponents better than the previously used variants while being less exploitable by the worst-case perfectly-rational opponent.", "total_citations": {"2021": 3, "2022": 5, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:ZzlSgRqYykMC": {"external_link": "https://openreview.net/forum?id=ryeG924twB", "authors": ["Zhenyu Shi", "Runsheng Yu", "Xinrun Wang", "Rundong Wang", "Youzhi Zhang", "Hanjiang Lai", "Bo An"], "publication_date": "2019/9/23", "conference": "International Conference on Learning Representations", "description": "Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers' behaviors when assigning bonuses and ii) the complex interactions between followers make the training process hard to converge, especially when the leader's policy changes with time. In this work, we address this problem through an event-based deep RL approach. Our main contributions are threefold. (1) We model the leader's decision-making process as a semi-Markov Decision Process and propose a novel multi-agent event-based policy gradient to learn the leader's long-term policy. (2) We exploit the leader-follower consistency scheme to design a follower-aware module and a follower-specific attention module to predict the followers' behaviors and make accurate response to their behaviors. (3) We propose an action abstraction-based policy gradient algorithm to reduce the followers' decision space and thus accelerate the training process of followers. Experiments in resource collections, navigation, and the predator-prey game reveal that our approach outperforms the state-of-the-art methods dramatically.", "total_citations": {"2020": 4, "2021": 0, "2022": 2, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:oNZyr7d5Mn4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8029757/", "authors": ["Li Pan", "Bo An", "Shijun Liu", "Lizhen Cui"], "publication_date": "2017/6/25", "conference": "2017 IEEE International Conference on Web Services (ICWS)", "pages": "154-163", "publisher": "IEEE", "description": "QoS aware service composition necessitates an effective pricing mechanism in regulating service providers in public cloud computing environments. However, due to the fact that service providers are usually autonomous, strategic and self-motivated, it is far from trivial to deal with the pricing issues between them. In this paper we formulate a non-cooperative service pricing game to understand the performance of a QoS aware service composition model, for which multiple providers strategically bid how to provide and price their elementary services and establish the Nash equilibrium as the final service composition scheme. We also develop a proportional revenue division rule to incentivize elementary service providers to contribute in improving the QoS of the final composite service delivered to end users. Concerning privacy conservation, we develop a decentralized and recursive bidding algorithm, allowing\u00a0\u2026", "total_citations": {"2018": 7, "2019": 1, "2020": 2, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:35r97b3x0nAC": {"external_link": "https://link.springer.com/article/10.1007/s10472-016-9506-x", "authors": ["Bo An", "Nicola Gatti", "Victor Lesser"], "publication_date": "2016/6", "journal": "Annals of Mathematics and Artificial Intelligence", "volume": "77", "pages": "67-103", "publisher": "Springer International Publishing", "description": "Automating negotiations in markets where multiple buyers and sellers operate is a scientific challenge of extraordinary importance. One-to-one negotiations are classically studied as bilateral bargaining problems, while one-to-many and many-to-many negotiations are studied as auctioning problems. This paper aims at bridging together these two approaches, analyzing agents\u2019 strategic behavior in one-to-many and many-to-many negotiations when agents follow the alternating-offers bargaining protocol (Rubinstein Econometrica 50(1), 97\u2013109, 33). First, we extend this protocol, proposing a novel mechanism that captures the peculiarities of these settings. Then, we analyze agents\u2019 equilibrium strategies in complete information bargaining and we find that for a large subset of the space of the parameters, the equilibrium outcome depends on the values of a narrow number of parameters. Finally, we study\u00a0\u2026", "total_citations": {"2017": 2, "2018": 6, "2019": 2, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:KaMxkj08jr0C": {"external_link": "https://proceedings.mlr.press/v162/wei22c.html", "authors": ["Hongxin Wei", "Lue Tao", "Renchunzi Xie", "Lei Feng", "Bo An"], "publication_date": "2022/6/28", "conference": "International Conference on Machine Learning", "pages": "23615-23630", "publisher": "PMLR", "description": "Deep neural networks usually perform poorly when the training dataset suffers from extreme class imbalance. Recent studies found that directly training with out-of-distribution data (ie, open-set samples) in a semi-supervised manner would harm the generalization performance. In this work, we theoretically show that out-of-distribution data can still be leveraged to augment the minority classes from a Bayesian perspective. Based on this motivation, we propose a novel method called Open-sampling, which utilizes open-set noisy labels to re-balance the class priors of the training dataset. For each open-set instance, the label is sampled from our pre-defined distribution that is complementary to the distribution of original class priors. We empirically show that Open-sampling not only re-balances the class priors but also encourages the neural network to learn separable representations. Extensive experiments demonstrate that our proposed method significantly outperforms existing data re-balancing methods and can boost the performance of existing state-of-the-art methods.", "total_citations": {"2022": 2, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:wKETBy42zhYC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3447548.3467318", "authors": ["Lei Feng", "Senlin Shu", "Yuzhou Cao", "Lue Tao", "Hongxin Wei", "Tao Xiang", "Bo An", "Gang Niu"], "publication_date": "2021/8/14", "book": "proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining", "pages": "374-382", "description": "Multiple-instance learning (MIL) is an important weakly supervised binary classification problem, where training instances are arranged in bags, and each bag is assigned a positive or negative label. Most of the previous studies for MIL assume that training bags are fully labeled. However, in some real-world scenarios, it could be difficult to collect fully labeled bags, due to the expensive time and labor consumption of the labeling task. Fortunately, it could be much easier for us to collect similar and dissimilar bags (indicating whether two bags share the same label or not), because we do not need to figure out the underlying label of each bag in this case. Therefore, in this paper, we for the first time investigate MIL from only similar and dissimilar bags. To solve this new MIL problem, we propose a convex formulation to train a bag-level classifier based on empirical risk minimization and theoretically derive a\u00a0\u2026", "total_citations": {"2022": 5, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:7BrZ7Jt4UNcC": {"external_link": "https://arxiv.org/abs/2106.00897", "authors": ["Wanqi Xue", "Youzhi Zhang", "Shuxin Li", "Xinrun Wang", "Bo An", "Chai Kiat Yeo"], "publication_date": "2021/6/2", "journal": "arXiv preprint arXiv:2106.00897", "description": "Securing networked infrastructures is important in the real world. The problem of deploying security resources to protect against an attacker in networked domains can be modeled as Network Security Games (NSGs). Unfortunately, existing approaches, including the deep learning-based approaches, are inefficient to solve large-scale extensive-form NSGs. In this paper, we propose a novel learning paradigm, NSG-NFSP, to solve large-scale extensive-form NSGs based on Neural Fictitious Self-Play (NFSP). Our main contributions include: i) reforming the best response (BR) policy network in NFSP to be a mapping from action-state pair to action-value, to make the calculation of BR possible in NSGs; ii) converting the average policy network of an NFSP agent into a metric-based classifier, helping the agent to assign distributions only on legal actions rather than all actions; iii) enabling NFSP with high-level actions, which can benefit training efficiency and stability in NSGs; and iv) leveraging information contained in graphs of NSGs by learning efficient graph node embeddings. Our algorithm significantly outperforms state-of-the-art algorithms in both scalability and solution quality.", "total_citations": {"2022": 3, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:2tRrZ1ZAMYUC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3383313.3412234", "authors": ["Xu He", "Bo An", "Yanghua Li", "Haikai Chen", "Qingyu Guo", "Xin Li", "Zhirong Wang"], "publication_date": "2020/9/22", "book": "Proceedings of the 14th ACM Conference on Recommender Systems", "pages": "63-72", "description": "Online recommendation services recommend multiple commodities to users. Nowadays, a considerable proportion of users visit e-commerce platforms by mobile devices. Due to the limited screen size of mobile devices, positions of items have a significant influence on clicks: 1) Higher positions lead to more clicks for one commodity. 2) The \u2018pseudo-exposure\u2019 issue: Only a few recommended items are shown at first glance and users need to slide the screen to browse other items. Therefore, some recommended items ranked behind are not viewed by users and it is not proper to treat this kind of items as negative samples. While many works model the online recommendation as contextual bandit problems, they rarely take the influence of positions into consideration and thus the estimation of the reward function may be biased. In this paper, we aim at addressing these two issues to improve the performance of online\u00a0\u2026", "total_citations": {"2021": 5, "2022": 2, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:HbR8gkJAVGIC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/11591", "authors": ["Jiang Rong", "Tao Qin", "Bo An"], "publication_date": "2018/4/26", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "32", "issue": "1", "description": "The market for selling reusable products (eg, car rental, cloud services and network access resources) is growing rapidly over the last few years, where service providers maximize their revenues through setting optimal prices. While there has been lots of research on pricing optimization, existing works often ignore dynamic property of demand and the competition among providers. Thus, existing pricing solutions might be far from optimal in realistic markets. This paper provides the first study of service providers' dynamic pricing in consideration of market competition and makes three key contributions along this line. First, we propose a comprehensive model that takes into account the dynamic demand and interaction among providers, and formulate the optimal pricing policy in the competitive market as an equilibrium. Second, we propose an approximate Nash equilibrium to describe providers' behaviors, and design an efficient algorithm to compute the equilibrium which is guaranteed to converge. Third, we derive many properties of the model without any further constraints on demand functions, which can reduce the search space of policies in the algorithm. Finally, we conduct extensive experiments with different parameter settings, showing that the approximate equilibrium is very close to the Nash equilibrium and our proposed pricing policy outperforms existing strategies.", "total_citations": {"2019": 2, "2020": 3, "2021": 3, "2022": 0, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:hC7cP41nSMkC": {"external_link": "https://cdn.aaai.org/ocs/4260/4260-19482-1-PB.pdf", "authors": ["Milind Tambe", "Bo An"], "publication_date": "2012/3/23", "conference": "2012 AAAI Spring Symposium Series", "description": "The goal of this paper is to introduce a real-world challenge problem for researchers in multiagent systems and beyond, where our collective efforts may have a significant impact on activities in the real-world. The challenge is in applying game theory for security: Our goal is not only to introduce the problem, but also to provide exemplars of initial successes of deployed systems in this challenge problem arena, some key open research challenges and pointers to getting started in this research.", "total_citations": {"2012": 1, "2013": 0, "2014": 2, "2015": 1, "2016": 1, "2017": 1, "2018": 0, "2019": 2, "2020": 0, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:U4n9YNQMCAIC": {"external_link": "http://proceedings.mlr.press/v139/cao21b.html", "authors": ["Yuzhou Cao", "Lei Feng", "Yitian Xu", "Bo An", "Gang Niu", "Masashi Sugiyama"], "publication_date": "2021/7/1", "conference": "International Conference on Machine Learning", "pages": "1272-1282", "publisher": "PMLR", "description": "Weakly supervised learning has drawn considerable attention recently to reduce the expensive time and labor consumption of labeling massive data. In this paper, we investigate a novel weakly supervised learning problem of learning from similarity-confidence (Sconf) data, where only unlabeled data pairs equipped with confidence that illustrates their degree of similarity (two examples are similar if they belong to the same class) are needed for training a discriminative binary classifier. We propose an unbiased estimator of the classification risk that can be calculated from only Sconf data and show that the estimation error bound achieves the optimal convergence rate. To alleviate potential overfitting when flexible models are used, we further employ a risk correction scheme on the proposed risk estimator. Experimental results demonstrate the effectiveness of the proposed methods.", "total_citations": {"2021": 2, "2022": 1, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:kuK5TVdYjLIC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7933924/", "authors": ["Jiarui Gan", "Bo An"], "publication_date": "2017/5/25", "journal": "IEEE Intelligent Systems", "volume": "32", "issue": "3", "pages": "46-52", "publisher": "IEEE", "description": "Taxi service is an indispensable part of public transport in modern cities. To support its unique features, a taxi system adopts a decentralized operation mode in which thousands of taxis freely decide their working schedules and routes. Taxis compete with each other for individual profits regardless of system-level efficiency, making the taxi system inefficient and hard to optimize. Most research into the management and economics of taxi markets has focused on modeling from a macro level the effects of and relationships between various market factors. Less has been done regarding a more important component--drivers' strategic behavior under the decentralized operation mode. The authors propose looking at the problem from a game-theoretic perspective. Combining game-theoretic solution concepts with existing models of taxi markets, they model taxi drivers' strategy-making process as a game and transform\u00a0\u2026", "total_citations": {"2018": 1, "2019": 2, "2020": 0, "2021": 2, "2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:ZuybSZzF8UAC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/spearphishing_ijcai15workshop.pdf", "authors": ["Mengchen Zhao", "Bo An", "Christopher Kiekintveld"], "publication_date": "2015/7", "journal": "Proceedings of the 2015 IJCAI Workshop on Behavioral, Economic and Computational Intelligence for Security", "pages": "1-9", "description": "Different from spam and regular phishing attacks, spear phishing attacks target a small group of people, and the attackers usually make elaborate plans before attacking. There is existing work on classifying spear phishing emails where a threshold value is used to balance misclassified normal emails and misclassified malicious emails. However, most existing systems use a uniform threshold for all users, while in reality users may differ in how susceptible they are to phishing attacks and their access to critical information. Existing work on setting personalized thresholds assumes that the attacker compromises multiple users simultaneously to maximize his expected utility. However, an attacker may be only interested in specific credential information, which could be accessed by a group of users. In this situation, a sequential attack is more reasonable for the attacker to reduce the cost of launching attacks and the likelihood of detection. We propose a Stackelberg game model to calculate the optimal solution for the sequential attack situation and formulate a bilevel optimization problem for the defender. By exploiting the structure of the bilevel problem, we propose a single level formulation called PEDS that is equivalent to the bilevel problem. Experimental results show that PEDS can solved within 60 seconds even when the number of users is 70, and the thresholds computed by PEDS lead to significant higher defender utilities as compared with existing approaches.", "total_citations": {"2016": 5, "2017": 2, "2018": 0, "2019": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:aqlVkmm33-oC": {"external_link": "https://patents.google.com/patent/US8656403B2/en", "inventors": "Bo An, Frederick Douglis, Bradley William Fawcett, Anton V Riabov, Fan Ye", "publication_date": "2014/2/18", "patent_office": "US", "patent_number": "8656403", "application_number": "12112142", "description": "Techniques are disclosed for optimizing schedules used in implementing plans for performing tasks in data processing systems. For example, an automated method of negotiating for resources in a data processing system, wherein the data processing system comprises multiple sites, comprises a negotiation management component of a computer system at a given one of the sites performing the following steps. One or more tasks from at least one source of one or more plans are obtained. Each plan is annotated with one or more needed resources and one or more potential resource providers at one or more sites in the data processing system. An optimized resource negotiation schedule based on the one or more obtained tasks is computed. The schedule comprises an order in which resources are negotiated. In accordance with the optimized resource negotiation schedule, a request for each needed resource is\u00a0\u2026", "total_citations": {"2013": 1, "2014": 3, "2015": 0, "2016": 1, "2017": 1, "2018": 0, "2019": 0, "2020": 1, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:GnPB-g6toBAC": {"external_link": "https://search.ieice.org/bin/summary.php?id=e88-d_12_2672", "authors": ["MIAO Chunyan", "Daijie CHENG"], "publication_date": "2005/12/1", "journal": "IEICE transactions on information and systems", "volume": "88", "issue": "12", "pages": "2672-2680", "publisher": "The Institute of Electronics, Information and Communication Engineers", "description": "Coalition formation in multi-agent systems (MAS) is becoming increasingly important as it increases the ability of agents to execute tasks and maximize their payoffs. Dependence relations are regarded as the foundation of coalition formation. This paper proposes a novel dependence theory namely transitive dependence theory for dynamic coalition formation in multi-agent systems. Transitive dependence is an extension of direct dependence that supports an agent's reasoning about other social members during coalition formation. Based on the proposed transitive dependence theory, a dynamic coalition formation framework has been worked out which includes information gathering, transitive dependence based reasoning for coalition partners search and coalition resolution. The nested coalitions and how to deal with incomplete knowledge while forming coalitions are also discussed in the paper.", "total_citations": {"2007": 1, "2008": 0, "2009": 0, "2010": 0, "2011": 1, "2012": 1, "2013": 1, "2014": 1, "2015": 2, "2016": 0, "2017": 0, "2018": 1, "2019": 0, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:1Ye0OR6EYb4C": {"external_link": "https://arxiv.org/abs/2206.02620", "authors": ["Wanqi Xue", "Qingpeng Cai", "Ruohan Zhan", "Dong Zheng", "Peng Jiang", "Kun Gai", "Bo An"], "publication_date": "2022/6/1", "journal": "arXiv preprint arXiv:2206.02620", "description": "Long-term engagement is preferred over immediate engagement in sequential recommendation as it directly affects product operational metrics such as daily active users (DAUs) and dwell time. Meanwhile, reinforcement learning (RL) is widely regarded as a promising framework for optimizing long-term engagement in sequential recommendation. However, due to expensive online interactions, it is very difficult for RL algorithms to perform state-action value estimation, exploration and feature extraction when optimizing long-term engagement. In this paper, we propose ResAct which seeks a policy that is close to, but better than, the online-serving policy. In this way, we can collect sufficient data near the learned policy so that state-action values can be properly estimated, and there is no need to perform online exploration. ResAct optimizes the policy by first reconstructing the online behaviors and then improving it via a Residual Actor. To extract long-term information, ResAct utilizes two information-theoretical regularizers to confirm the expressiveness and conciseness of features. We conduct experiments on a benchmark dataset and a large-scale industrial dataset which consists of tens of millions of recommendation requests. Experimental results show that our method significantly outperforms the state-of-the-art baselines in various long-term engagement optimization tasks.", "total_citations": {"2022": 1, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:vDZJ-YLwNdEC": {"external_link": "https://www.ijcai.org/proceedings/2020/0005.pdf", "authors": ["Yanchen Deng", "Bo An"], "publication_date": "2021/1/7", "book": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence", "pages": "31-38", "description": "Incomplete GDL-based algorithms including Maxsum and its variants are important methods for multi-agent optimization. However, they face a significant scalability challenge as the computational overhead grows exponentially with respect to the arity of each utility function. Generic Domain Pruning (GDP) technique reduces the computational effort by performing a one-shot pruning to filter out suboptimal entries. Unfortunately, GDP could perform poorly when dealing with dense local utilities and ties which widely exist in many domains. In this paper, we present several novel sorting-based acceleration algorithms by alleviating the effect of densely distributed local utilities. Specifically, instead of one-shot pruning in GDP, we propose to integrate both search and pruning to iteratively reduce the search space. Besides, we cope with the utility ties by organizing the search space of tied utilities into AND/OR trees to enable branch-and-bound. Finally, we propose a discretization mechanism to offer a tradeoff between the reconstruction overhead and the pruning efficiency. We demonstrate the superiorities of our algorithms over the state-of-the-art from both theoretical and experimental perspectives.", "total_citations": {"2021": 2, "2022": 3, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:_OXeSy2IsFwC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8930604/", "authors": ["Wanyuan Wang", "Zichen Dong", "Bo An", "Yichuan Jiang"], "publication_date": "2019/12/10", "journal": "IEEE Transactions on Intelligent Transportation Systems", "volume": "22", "issue": "2", "pages": "747-757", "publisher": "IEEE", "description": "Motivated by the increasing need of the real-world patrolling, this paper studies a practical city-scale patrolling (CSP) variant. In CSP, the police are scheduled to patrol city regions, and the objective is not only to protect public security but also to respond to incidents timely. We use an integer program (IP) to formulate the CSP problem, with the objective of maximizing the police visibility rate (PVR) to improve public safety and the additional constraint of response time guarantee to handle incidents timely. For such an NP-hard problem, existing studies either cannot scale-up or do not provide a bound from optimum. To fill the research gap, we propose a decomposition and grafting approach. We first decompose the original CSP into two weakly-coupled subproblems, minimizing police problem (MinP) and maximizing PVR (MaxP) problem. By exploiting the subproblem structures, a polynomial time approximation\u00a0\u2026", "total_citations": {"2021": 3, "2022": 3, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:OR75R8vi5nAC": {"external_link": "https://link.springer.com/article/10.1007/s11390-019-1933-9", "authors": ["Jiang Rong", "Tao Qin", "Bo An"], "publication_date": "2019/5", "journal": "Journal of Computer Science and Technology", "volume": "34", "pages": "645-656", "publisher": "Springer US", "description": "We study the pricing policy optimization problem for cloud providers while considering three properties of the real-world market: 1) providers have only incomplete information about the market; 2) it is in evolution due to the increasing number of users and decreasing marginal cost of providers; 3) it is fully competitive because of providers\u2019 and users\u2019 revenuedriven nature. As far as we know, there is no existing work investigating the optimal pricing policies under such realistic settings. We first propose a comprehensive model for the real-world cloud market and formulate it as a stochastic game. Then we use the Markov perfect equilibrium (MPE) to describe providers\u2019 optimal policies. Next we decompose the problem of computing the MPE into two subtasks: 1) dividing the stochastic game into many normal-formal games and calculating their Nash equilibria, for which we develop an algorithm ensuring to\u00a0\u2026", "total_citations": {"2020": 3, "2021": 2, "2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Br1UauaknNIC": {"external_link": "https://cdn.aaai.org/ocs/ws/ws0017/10070-45935-1-PB.pdf", "authors": ["Jiarui Gan", "Bo An"], "publication_date": "2015/4/1", "conference": "Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence", "description": "Taxi service is an indispensable part of public transport in modern cities. The taxi system is operated by a large number of self-controlled drivers lacking of centralized scheduling and control, which makes it inefficient, difficult to analyze and optimize. It is thus important to take into account taxi drivers\u2019 strategic behavior in order to optimize taxi systems\u2019 efficiency. This paper reviews existing taxi system researches for modeling taxi system dynamics, introduces the taxi system efficiency optimization problem, and presents a game theoretic approach for optimizing the efficiency of taxi systems. 1 Challenges and open issues in the taxi system efficiency optimization problem are also discussed.", "total_citations": {"2015": 1, "2016": 0, "2017": 1, "2018": 0, "2019": 2, "2020": 1, "2021": 2, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:_xSYboBqXhAC": {"external_link": "https://patents.google.com/patent/US20130273514A1/en", "inventors": "Milind Tambe, Fernando Ord\u00f3\u00f1ez, Rong Yang, Zhengyu Yin, Matthew Brown, Bo An, Christopher Kiekintveld", "publication_date": "2013/10/17", "patent_office": "US", "application_number": "13838466", "description": "Different solution methodologies for addressing problems or issues when directing security domain patrolling strategies according to attacker-defender Stackelberg security games. One type of solution provides for computing optimal strategy against quantal response in security games, and includes two algorithms, the G OSAQ and P ASAQ algorithms. Another type of solution provides for a unified method for handling discrete and continuous uncertainty in Bayesian Stackelberg games, and introduces the HUNTER algorithm. Another solution type addresses multi-objective security games (MOSG), combining security games and multi-objective optimization. MOSGs have a set of Pareto optimal (non-dominated) solutions referred to herein as the Pareto frontier. The Pareto frontier can be generated by solving a sequence of constrained single-objective optimization problems (CSOP), where one objective is selected to\u00a0\u2026", "total_citations": {"2016": 2, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 3, "2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:isC4tDSrTZIC": {"external_link": "https://link.springer.com/chapter/10.1007/978-1-4614-5311-6_20", "authors": ["Eric Shieh", "Bo An", "Rong Yang", "Milind Tambe", "Craig Baldwin", "Joseph DiRenzo", "Ben Maule", "Garrett Meyer", "Kathryn Moretti"], "publication_date": "2013", "journal": "Handbook of computational approaches to counterterrorism", "pages": "441-463", "publisher": "Springer New York", "description": "The global need for security of key infrastructure with limited resources has led to significant interest in research conducted in multiagent systems towards game-theory for real-world security. As reported previously at AAMAS, three applications based on Stackelberg games have been transitioned to real-world deployment. This includes ARMOR, used by the Los Angeles International Airport to randomize checkpoints of roadways and canine patrols [16]; IRIS, which helps the US Federal Air Marshal Service [22] in scheduling air marshals on international flights; and GUARDS [17], which is under evaluation by the US Transportation Security Administration to allocate resources for airport protection. We as a community remain in the early stages of these deployments, and must continue to develop our understanding of core principles of innovative applications of game theory for security.", "total_citations": {"2013": 1, "2014": 0, "2015": 1, "2016": 0, "2017": 1, "2018": 1, "2019": 0, "2020": 3, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:uVUOdF_882EC": {"external_link": "https://arxiv.org/abs/2212.02779", "authors": ["Wanqi Xue", "Qingpeng Cai", "Zhenghai Xue", "Shuo Sun", "Shuchang Liu", "Dong Zheng", "Peng Jiang", "Bo An"], "publication_date": "2022/12/6", "journal": "arXiv preprint arXiv:2212.02779", "description": "Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Despite being a promising approach, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, Preference-based Recommender systems (PrefRec), which allows RL recommender systems to learn from preferences about users' historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they do not require any expert knowledge. With PrefRec, we can fully exploit the advantages of RL in optimizing long-term goals, while avoiding complex reward engineering. PrefRec uses the preferences to automatically train a reward function in an end-to-end manner. The reward function is then used to generate learning signals to train the recommendation policy. Furthermore, we design an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance. Extensive experiments are conducted on a variety of long-term user engagement optimization tasks\u00a0\u2026", "total_citations": {"2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:5icHVeHT4IsC": {"external_link": "https://proceedings.mlr.press/v124/he20a.html", "authors": ["Xu He", "Haipeng Chen", "Bo An"], "publication_date": "2020/8/27", "conference": "Conference on Uncertainty in Artificial Intelligence", "pages": "131-140", "publisher": "PMLR", "description": "Human feedback is widely used to train agents in many domains. However, previous works rarely consider the uncertainty when humans provide feedback, especially in cases that the optimal actions are not obvious to the trainers. For example, the reward of a sub-optimal action can be stochastic and sometimes exceeds that of the optimal action, which is common in games or real-world. Trainers are likely to provide positive feedback to sub-optimal actions, negative feedback to the optimal actions and even do not provide feedback in some confusing situations. Existing works, which utilize the Expectation Maximization (EM) algorithm and treat the feedback model as hidden parameters, do not consider uncertainties in the learning environment and human feedback. To address this challenge, we introduce a novel feedback model that considers the uncertainty of human feedback. However, this incurs intractable calculus in the EM algorithm. To this end, we propose a novel approximate EM algorithm, in which we approximate the expectation step with the Gradient Descent method. Experimental results in both synthetic scenarios and two real-world scenarios with human participants demonstrate the superior performance of our proposed approach.", "total_citations": {"2020": 1, "2021": 2, "2022": 3, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Dem6FJhTUoYC": {"external_link": "https://www.future-science.com/doi/full/10.4155/bio-2019-0191", "authors": ["John F Kellie", "Timothy W Sikorski", "Bo An", "Zhuo Chen", "Ahmed H Moghieb", "Maria G Busz", "Matthew E Szapacs", "Thomas E Angel"], "publication_date": "2019/8", "source": "Bioanalysis", "volume": "11", "issue": "19", "pages": "1731-1735", "publisher": "Newlands Press Ltd", "description": "BackgroundIdentification and quantification of the proteins and proteoforms present in a biological sample provide a necessary foundation to generate insights into the complexities and emergent properties of biological systems to support drug discovery and development activities. To fully understand these complex systems, researchers have turned to MS-based proteomic technologies as well as highly multiplexed affinity-based platforms to quantitate hundreds to thousands of proteins in a single biological sample. In the pharmaceutical industry, proteomics has long been utilized as a drug-discovery tool to help understand changes in protein profiles for disease states or protein expression in relation to genomic studies for target discovery or identification [1]. With technological advancements in both MS-and array-based proteomics, areas such as biomarker research and PD are poised for emerging applications by\u00a0\u2026", "total_citations": {"2020": 2, "2021": 2, "2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:zLWjf1WUPmwC": {"external_link": "https://dl.acm.org/doi/abs/10.3233/978-1-61499-672-9-515", "authors": ["Jiang Rong", "Tao Qin", "Bo An", "Tie-Yan Liu"], "publication_date": "2016/8/29", "book": "Proceedings of the Twenty-second European Conference on Artificial Intelligence", "pages": "515-523", "description": "Sponsored search auctions (SSAs) have attracted a lot of research attention in recent years and different equilibrium concepts have been studied in order to understand advertisers' bidding strategies. However, the assumption that advertisers are perfectly rational in these studies is unrealistic in the real world. In this work, we apply the quantal response equilibrium (QRE), which is powerful in modeling bounded rationality, to SSAs. Due to high computational complexity, existing methods for QRE computation have very poor scalability for SSAs. Through exploiting the structures of QRE for SSAs, this paper presents an efficient homotopy-based algorithm to compute the QRE for large-size SSAs, which features the following two novelties: 1) we represent the SSAs as an Action Graph Game (AGG) which can compute the expected utilities in polynomial time; 2) we further significantly reduce redundant calculations by\u00a0\u2026", "total_citations": {"2017": 2, "2018": 1, "2019": 1, "2020": 0, "2021": 2, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:OU6Ihb5iCvQC": {"external_link": "https://www.ifaamas.org/Proceedings/aamas2014/aamas/p1465.pdf", "authors": ["Jiarui Gan", "Bo An", "Chunyan Miao"], "publication_date": "2014/5/5", "book": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems", "pages": "1465-1466", "description": "Taxi service is an important mode of modern public transportation. However, operated by a large number of selfcontrolled and profit-driven taxi drivers, taxi systems are quite in efficient and difficult to analyze and regulate. While there has been some work on designing algorithms for improving taxi system efficiency, the state of the art algorithm, unfortunately, cannot scale up efficiently. To address the inadequacy, we propose a novel algorithm\u2014FLORA\u2014in this paper. Using convex polytope representation conversion techniques, FLORA provides a fully compact representation of taxi drivers\u2019 strategy space, and avoids enumerating any type of schedules. Experimental results show orders of magnitude improvement of FLORA in terms of the complexity.", "total_citations": {"2015": 2, "2016": 0, "2017": 0, "2018": 0, "2019": 3, "2020": 0, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:1sJd4Hv_s6UC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAAISS14b.pdf", "authors": ["Jiarui Gan", "Bo An"], "publication_date": "2014", "journal": "Proc. AAAI Spring Symp. on Appl. Computat. Game Theory", "description": "Stackelberg security games have been applied to address challenges in security resource allocation of realworld infrastructure protection tasks. The key to such an application is to efficiently compute the defender\u2019s optimal strategy in consideration of the attacker\u2019s surveillance capability and best response. Experimental results show that the defender\u2019s optimal strategy often uses only a small subset of pure strategies, as compared with the entire pure strategy set which can be exponentially large. A number of algorithms in the literature have already exploited this small support size observation. This paper analyzes a number of widely studied security games and provides bounds on the minimum support size of the defender\u2019s Strong Stackelberg Equilibrium (SSE) strategies in security games.", "total_citations": {"2016": 1, "2017": 1, "2018": 0, "2019": 1, "2020": 2, "2021": 0, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:RHpTSmoSYBkC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-642-34799-3_2", "authors": ["Bo An", "Milind Tambe"], "publication_date": "2012", "conference": "Multi-Agent Systems: 9th European Workshop, EUMAS 2011, Maastricht, The Netherlands, November 14-15, 2011. Revised Selected Papers 9", "pages": "17-30", "publisher": "Springer Berlin Heidelberg", "description": "The goal of this paper is to introduce a real-world challenge problem for researchers in multiagent systems and beyond, where our collective efforts may have a significant impact on activities in the real-world. The challenge is in applying game theory for security: Our goal is not only to introduce the problem, but also to provide exemplars of initial successes of deployed systems in this challenge problem arena, some key open research challenges and pointers to getting started in this research.", "total_citations": {"2013": 1, "2014": 1, "2015": 1, "2016": 1, "2017": 1, "2018": 0, "2019": 0, "2020": 0, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:yxmsSjX2EkcC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/20851", "authors": ["Renchunzi Xie", "Hongxin Wei", "Lei Feng", "Bo An"], "publication_date": "2022/6/28", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "36", "issue": "8", "pages": "8717-8725", "description": "This paper studies a weakly supervised domain adaptation (WSDA) problem, where we only have access to the source domain with noisy labels, from which we need to transfer useful information to the unlabeled target domain. Although there have been a few studies on this problem, most of them only exploit unidirectional relationships from the source domain to the target domain. In this paper, we propose a universal paradigm called GearNet to exploit bilateral relationships between the two domains. Specifically, we take the two domains as different inputs to train two models alternately, and a symmetrical Kullback-Leibler loss is used for selectively matching the predictions of the two models in the same domain. This interactive learning schema enables implicit label noise canceling and exploit correlations between the source and target domains. Therefore, our GearNet has the great potential to boost the performance of a wide range of existing WSDA methods. Comprehensive experimental results show that the performance of existing methods can be significantly improved by equipping with our GearNet.", "total_citations": {"2022": 2, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:IaI1MmNe2tcC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/16664", "authors": ["Jakub \u010cern\u00fd", "Viliam Lis\u00fd", "Branislav Bo\u0161ansk\u00fd", "Bo An"], "publication_date": "2021/5/18", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "35", "issue": "6", "pages": "5260-5268", "description": "Deployments of game-theoretic solution concepts in the real world have highlighted the necessity to consider human opponents' boundedly rational behavior. If subrationality is not addressed, the system can face significant losses in terms of expected utility. While there exist algorithms for computing optimal strategies to commit to when facing subrational decision-makers in one-shot interactions, these algorithms cannot be generalized for solving sequential scenarios because of the inherent curse of strategy-space dimensionality in sequential games and because humans act subrationally in each decision point separately. We study optimal strategies to commit to against subrational opponents in sequential games for the first time and make the following key contributions:(1) we prove the problem is NP-hard in general;(2) to enable further analysis, we introduce a non-fractional reformulation of the direct non-concave representation of the equilibrium;(3) we identify conditions under which the problem can be approximated in polynomial time in the size of the representation;(4) we show how an MILP can approximate the reformulation with a guaranteed bounded error, and (5) we experimentally demonstrate that our algorithm provides higher quality results several orders of magnitude faster than a baseline method for general non-linear optimization.", "total_citations": {"2022": 5, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:nRpfm8aw39MC": {"external_link": "https://arxiv.org/abs/2105.08440", "authors": ["Shuxin Li", "Youzhi Zhang", "Xinrun Wang", "Wanqi Xue", "Bo An"], "publication_date": "2021/5/18", "journal": "arXiv preprint arXiv:2105.08440", "description": "In many real-world scenarios, a team of agents coordinate with each other to compete against an opponent. The challenge of solving this type of game is that the team's joint action space grows exponentially with the number of agents, which results in the inefficiency of the existing algorithms, e.g., Counterfactual Regret Minimization (CFR). To address this problem, we propose a new framework of CFR: CFR-MIX. Firstly, we propose a new strategy representation that represents a joint action strategy using individual strategies of all agents and a consistency relationship to maintain the cooperation between agents. To compute the equilibrium with individual strategies under the CFR framework, we transform the consistency relationship between strategies to the consistency relationship between the cumulative regret values. Furthermore, we propose a novel decomposition method over cumulative regret values to guarantee the consistency relationship between the cumulative regret values. Finally, we introduce our new algorithm CFR-MIX which employs a mixing layer to estimate cumulative regret values of joint actions as a non-linear combination of cumulative regret values of individual actions. Experimental results show that CFR-MIX outperforms existing algorithms on various games significantly.", "total_citations": {"2022": 4, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:nVrZBo8bIpAC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3391403.3399517", "authors": ["Jakub \u010cern\u00fd", "Branislav Bosansk\u00fd", "Bo An"], "publication_date": "2020/7/13", "book": "Proceedings of the 21st ACM Conference on Economics and Computation", "pages": "509-533", "description": "Finite state machines are a well-known representation of strategies in (in)finitely repeated or stochastic games. Actions of players correspond to states in the machine and the transition between machine-states are caused by observations in the game. For extensive-form games (EFGs), machines can act as a formal grounding for abstraction methods used for solving large EFGs and as a domain-independent approach for generating sufficiently compact abstractions. We show that using machines of a restricted size in EFGs can both (i) reduce the theoretical complexity of computing some solution concepts, including Strong Stackelberg Equilibrium (SSE), (ii) as well as bring new practical algorithms that compute near-optimal equilibria considering only a fraction of strategy space. Our contributions include (1) formal definition and theoretical characterization of machine strategies in EFGs, (2) formal definitions and\u00a0\u2026", "total_citations": {"2021": 3, "2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:k8Z6L05lTy4C": {"external_link": "https://www.ijcai.org/proceedings/2017/0523.pdf", "authors": ["Shuxin Li", "Xiaohong Li", "Jianye Hao", "Bo An", "Zhiyong Feng", "Kangjie Chen", "Chengwei Zhang"], "publication_date": "2017/8/19", "conference": "IJCAI", "pages": "3742-3748", "description": "The Man-in-the-Middle (MITM) attack has become widespread in networks nowadays. The MITM attack would cause serious information leakage and result in tremendous loss to users. Previous work applies game theory to analyze the MITM attack-defense problem and computes the optimal defense strategy to minimize the total loss. It assumes that all defenders are cooperative and the attacker know defenders\u2019 strategies beforehand. However, each individual defender is rational and may not have the incentive to cooperate. Furthermore, the attacker can hardly know defenders\u2019 strategies ahead of schedule in practice. To this end, we assume that all defenders are self-interested and model the MITM attack-defense scenario as a simultaneous-move game. Nash equilibrium is adopted as the solution concept which is proved to be always unique. Given the impracticability of computing Nash equilibrium directly, we propose practical adaptive algorithms for the defenders and the attacker to learn towards the unique Nash equilibrium through repeated interactions. Simulation results show that the algorithms are able to converge to Nash equilibrium strategy efficiently.", "total_citations": {"2017": 1, "2018": 1, "2019": 2, "2020": 1, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:tuHXwOkdijsC": {"external_link": "https://www.ijcai.org/proceedings/2017/0732.pdf", "authors": ["Bo An"], "publication_date": "2017/8/19", "conference": "IJCAI", "pages": "5111-5115", "description": "Computational game theory has become a powerful tool to address critical issues in security and sustainability. Casting the security resource allocation problem as a Stackelberg game, novel algorithms have been developed to provide randomized security resource allocations. These algorithms have led to deployed security-game based decision aids for many real-world security domains including infrastructure security and wildlife protection. We contribute to this community by addressing several major research challenges in complex security resource allocation, including dynamic payoffs, uncertainty, protection externality, games on networks, and strategic secrecy. We also analyze optimal security resource allocation in many potential application domains including cyber security. Furthermore, we apply game theory to reasoning optimal policy in deciding taxi pricing scheme and EV charging placement and pricing.", "total_citations": {"2018": 1, "2019": 3, "2020": 1, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:ML0RJ9NH7IQC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10604", "authors": ["Jiang Rong", "Tao Qin", "Bo An", "Tie-Yan Liu"], "publication_date": "2017/2/10", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "31", "issue": "1", "description": "Reserve price is an effective tool for revenue maximization in ad auctions. The optimal reserve price depends on bidders' value distributions, which, however, are generally unknown to auctioneers. A common practice for auctioneers is to first collect information about the value distributions by a sampling procedure and then apply the reserve price estimated with the sampled bids to the following auctions. In order to maximize the total revenue over finite auctions, it is important for the auctioneer to find a proper sample size to trade off between the cost of the sampling procedure and the optimality of the estimated reserve price. We investigate the sample size optimization problem for Generalized Second Price auctions, which is the most widely-used mechanism in ad auctions, and make three main contributions along this line. First, we bound the revenue losses in the form of competitive ratio during and after sampling. Second, we formulate the problem of finding the optimal sample size as a non-convex mixed integer optimization problem. Then we characterize the properties of the problem and prove the uniqueness of the optimal sample size. Third, we relax the integer optimization problem to a continuous form and develop an efficient algorithm based on the properties to solve it. Experimental results show that our approach can significantly improve the revenue for the auctioneer in finitely repeated ad auctions.", "total_citations": {"2017": 1, "2018": 2, "2019": 2, "2020": 0, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:SP6oXDckpogC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/aamas14-curiosity.pdf", "authors": ["Qiong Wu", "Chunyan Miao", "Bo An"], "publication_date": "2014/5/5", "book": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems", "pages": "1401-1402", "description": "Curiosity is an emotional motivation that drives knowledge acquisition in learning context. Studies have shown that curiosity positively influences social learning and a peer learner\u2019s curiosity may elicit the curiosity of other learners. Hence, modeling curiosity in learning companions may improve human learners\u2019 learning experience in virtual environments. However, curiosity has not been explored as one key personality trait in existing learning companions. In this paper, we propose a novel model of curiosity for learning companions to capture salient curiosity stimuli based on human psychology. Our model is based on Silvia\u2019s theory and considers three most salient appraisal variables in virtual learning environments, including novelty, surprise, and uncertainty.", "total_citations": {"2014": 1, "2015": 3, "2016": 0, "2017": 1, "2018": 0, "2019": 0, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:O0nohqN1r9EC": {"external_link": "https://arxiv.org/abs/2212.04055", "authors": ["Hongxin Wei", "Huiping Zhuang", "Renchunzi Xie", "Lei Feng", "Gang Niu", "Bo An", "Yixuan Li"], "publication_date": "2022/12/8", "journal": "arXiv preprint arXiv:2212.04055", "description": "In the presence of noisy labels, designing robust loss functions is critical for securing the generalization performance of deep neural networks. Cross Entropy (CE) loss has been shown to be not robust to noisy labels due to its unboundedness. To alleviate this issue, existing works typically design specialized robust losses with the symmetric condition, which usually lead to the underfitting issue. In this paper, our key idea is to induce a loss bound at the logit level, thus universally enhancing the noise robustness of existing losses. Specifically, we propose logit clipping (LogitClip), which clamps the norm of the logit vector to ensure that it is upper bounded by a constant. In this manner, CE loss equipped with our LogitClip method is effectively bounded, mitigating the overfitting to examples with noisy labels. Moreover, we present theoretical analyses to certify the noise-tolerant ability of LogitClip. Extensive experiments show that LogitClip not only significantly improves the noise robustness of CE loss, but also broadly enhances the generalization performance of popular robust losses.", "total_citations": {"2021": 1, "2022": 0, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:ce2CqMG-AY4C": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3511808.3557283", "authors": ["Shuo Sun", "Wanqi Xue", "Rundong Wang", "Xu He", "Junlei Zhu", "Jian Li", "Bo An"], "publication_date": "2022/10/17", "book": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management", "pages": "1858-1867", "description": "Reinforcement learning (RL) techniques have shown great success in many challenging quantitative trading tasks, such as portfolio management and algorithmic trading. Especially, intraday trading is one of the most profitable and risky tasks because of the intraday behaviors of the financial market that reflect billions of rapidly fluctuating capitals. However, a vast majority of existing RL methods focus on the relatively low frequency trading scenarios (e.g., day-level) and fail to capture the fleeting intraday investment opportunities due to two major challenges: 1) how to effectively train profitable RL agents for intraday investment decision-making, which involves high-dimensional fine-grained action space; 2) how to learn meaningful multi-modality market representation to understand the intraday behaviors of the financial market at tick-level. Motivated by the efficient workflow of professional human intraday traders, we\u00a0\u2026", "total_citations": {"2021": 1, "2022": 0, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:RoXSNcbkSzsC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3511808.3557140", "authors": ["Junning Liu", "Xinjian Li", "Bo An", "Zijie Xia", "Xu Wang"], "publication_date": "2022/10/17", "book": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management", "pages": "3332-3341", "description": "There have been many studies on improving the efficiency of shared learning in Multi-Task Learning (MTL). Previous works focused on the \"micro\" sharing perspective for a small number of tasks, while in Recommender Systems (RS) and many other AI applications, we often need to model a large number of tasks. For example, when using MTL to model various user behaviors in RS, if we differentiate new users and new items from old ones, the number of tasks will increase exponentially with multidimensional relations. This work proposes a Multi-Faceted Hierarchical MTL model (MFH) that exploits the multidimensional task relations in large scale MTLs with a nested hierarchical tree structure. MFH maximizes the shared learning through multi-facets of sharing and improves the performance with heterogeneous task tower design. For the first time, MFH addresses the \"macro\" perspective of shared learning and\u00a0\u2026", "total_citations": {"2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:XUvXOeBm_78C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/21164", "authors": ["Yanchen Deng", "Shufeng Kong", "Bo An"], "publication_date": "2022/6/28", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "36", "issue": "9", "pages": "9331-9340", "description": "Distributed Constraint Optimization Problems (DCOPs) are an important subclass of combinatorial optimization problems, where information and controls are distributed among multiple autonomous agents. Previously, Machine Learning (ML) has been largely applied to solve combinatorial optimization problems by learning effective heuristics. However, existing ML-based heuristic methods are often not generalizable to different search algorithms. Most importantly, these methods usually require full knowledge about the problems to be solved, which are not suitable for distributed settings where centralization is not realistic due to geographical limitations or privacy concerns. To address the generality issue, we propose a novel directed acyclic graph representation schema for DCOPs and leverage the Graph Attention Networks (GATs) to embed graph representations. Our model, GAT-PCM, is then pretrained with optimally labelled data in an offline manner, so as to construct effective heuristics to boost a broad range of DCOP algorithms where evaluating the quality of a partial assignment is critical, such as local search or backtracking search. Furthermore, to enable decentralized model inference, we propose a distributed embedding schema of GAT-PCM where each agent exchanges only embedded vectors, and show its soundness and complexity. Finally, we demonstrate the effectiveness of our model by combining it with a local search or a backtracking search algorithm. Extensive empirical evaluations indicate that the GAT-PCM-boosted algorithms significantly outperform the state-of-the-art methods in various benchmarks.", "total_citations": {"2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:WC9gN4BGCRcC": {"external_link": "https://openreview.net/forum?id=naoQDOYsHnS", "authors": ["Pengjie Gu", "Mengchen Zhao", "Chen Chen", "Dong Li", "Jianye Hao", "Bo An"], "publication_date": "2021/10/6", "description": "Offline reinforcement learning is a promising approach for practical applications since it does not require interactions with real-world environments. However, existing offline RL methods only work well in environments with continuous or small discrete action spaces. In environments with large and discrete action spaces, such as recommender systems and dialogue systems, the performance of existing methods decreases drastically because they suffer from inaccurate value estimation for a large proportion of o.o.d. actions. While recent works have demonstrated that online RL benefits from incorporating semantic information in action representations, unfortunately, they fail to learn reasonable relative distances between action representations, which is key to offline RL to reduce the influence of out-of-distribution (o.o.d.) actions. This paper proposes an action representation learning framework for offline RL based on a pseudometric, which measures both the behavioral relation and the data-distributional relation between actions.  We provide theoretical analysis on the continuity and the bounds of the expected Q-values using the learned action representations. Experimental results show that our methods significantly improve the performance of two typical offline RL methods in environments with large and discrete action spaces.", "total_citations": {"2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:j7_hQOaDUrUC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/IJCAI21_Deep_DCOP.pdf", "authors": ["Yanchen Deng", "Runsheng Yu", "Xinrun Wang", "Bo An"], "publication_date": "2021", "conference": "IJCAI", "pages": "146-153", "description": "Distributed constraint optimization problems (DCOPs) are a powerful model for multi-agent coordination and optimization, where information and controls are distributed among multiple agents by nature. However, most of incomplete algorithms for DCOPs are context-free, ie, agents make a decision purely based on the state of their neighbors, which makes them prone to get trapped in poor local convergence. On the other hand, context-based algorithms use tables to exactly store all the information (eg, costs, confidence bounds), which limits their scalability. This paper tackles the limitation by incorporating deep neural networks in solving DCOPs for the first time and presents a neural context-based sampling scheme built upon regret-matching. In the algorithm, each agent trains a neural network to approximate the regret related to its local problem under current context and performs sampling according to the estimated regret. Furthermore, to ensure exploration, we propose a regret rounding scheme that rounds small regret values to positive numbers. We theoretically show the regret bound of our algorithm and extensive evaluations indicate that our algorithm can scale up to large-scale DCOPs and significantly outperform the state-of-the-art methods.", "total_citations": {"2022": 2, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:48xauSegjOkC": {"external_link": "https://arxiv.org/abs/2012.05273", "authors": ["Hongxin Wei", "Lei Feng", "Rundong Wang", "Bo An"], "publication_date": "2020/12/9", "journal": "arXiv preprint arXiv:2012.05273", "description": "Deep neural networks have been shown to easily overfit to biased training data with label noise or class imbalance. Meta-learning algorithms are commonly designed to alleviate this issue in the form of sample reweighting, by learning a meta weighting network that takes training losses as inputs to generate sample weights. In this paper, we advocate that choosing proper inputs for the meta weighting network is crucial for desired sample weights in a specific task, while training loss is not always the correct answer. In view of this, we propose a novel meta-learning algorithm, MetaInfoNet, which automatically learns effective representations as inputs for the meta weighting network by emphasizing task-related information with an information bottleneck strategy. Extensive experimental results on benchmark datasets with label noise or class imbalance validate that MetaInfoNet is superior to many state-of-the-art methods.", "total_citations": {"2021": 1, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:P7Ujq4OLJYoC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-32430-8_3", "authors": ["Yoav Ben Yaakov", "Xinrun Wang", "Joachim Meyer", "Bo An"], "publication_date": "2019", "conference": "Decision and Game Theory for Security: 10th International Conference, GameSec 2019, Stockholm, Sweden, October 30\u2013November 1, 2019, Proceedings 10", "pages": "33-44", "publisher": "Springer International Publishing", "description": "Firewalls, Intrusion Detection Systems (IDS), and cyber-insurance are widely used to protect against cyber-attacks and their consequences. The optimal investment in each of these security measures depends on the likelihood of threats and the severity of the damage they cause, on the user\u2019s ability to distinguish between malicious and non-malicious content, and on the properties of the different security measures and their costs. We present a model of the optimal investment in the security measures, given that the effectiveness of each measure depends partly on the performance of the others. We also conducted an online experiment in which participants classified events as malicious or non-malicious, based on the value of an observed variable. They could protect themselves by investing in a firewall, an IDS or insurance. Four experimental conditions differed in the optimal investment in the different\u00a0\u2026", "total_citations": {"2020": 2, "2021": 0, "2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:ILKRHgRFtOwC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAMAS17_Container.pdf", "authors": ["Xinrun Wang", "Qingyu Guo", "Bo An"], "publication_date": "2017/5/8", "book": "Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems", "pages": "669-677", "description": "Since 2003, the US government has spent $850 million on the Megaport Initiative which aims at stopping the nuclear smuggling in international container shipping through advanced inspection facilities including Non-Intrusive Inspection (NII) and Mobile Radiation Detection and Identification System (MRDIS). Unfortunately, it remains a significant challenge to efficiently inspect more than 11.7 million containers imported to the US due to the limited inspection resources. Moreover, existing work in container inspection neglects the sophisticated behavior of the smuggler who can surveil the inspector\u2019s strategy and decide the optimal (sequential) smuggling plan. This paper is the first to tackle this challenging container inspection problem, where a novel Container Inspection Model (CIM) is proposed, which models the interaction between the inspector and the smuggler as a leader-follower Stackelberg game and formulates the smuggler\u2019s sequential decision behavior as a Markov Decision Process (MDP). The special structure of the CIM results in a non-convex optimization problem, which cannot be addressed by existing approaches. We make several key contributions including: i) a linear relaxation approximation with guarantee of solution quality which reformulates the model as a bilinear optimization problem, ii) an algorithm inspired by the Multipleparametric Disaggregation Technique (MDT) to solve the reformulated bilinear optimization, and iii) a novel iterative algorithm to further improve the scalability. Extensive experimental evaluation shows that our approach can scale up to realistic-sized problems with robust enough solutions\u00a0\u2026", "total_citations": {"2017": 2, "2018": 0, "2019": 1, "2020": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:lmc2jWPfTJgC": {"external_link": "https://eprints.soton.ac.uk/411950/", "authors": ["Qingyu Guo", "Bo An", "Long Tran-Thanh"], "publication_date": "2017/4/23", "description": "We study repeated network interdiction games with no prior knowledge of the adversary and the environment, which can model many real world network security domains. Existing works often require plenty of available information for the defender and neglect the frequent interactions between both players, which are unrealistic and impractical, and thus, are not suitable for our settings. As such, we provide the first defender strategy, that enjoys nice theoretical and practical performance guarantees, by applying the adversarial online learning approach. In particular, we model the repeated network interdiction game with no prior knowledge as an online linear optimization problem, for which a novel and efficient online learning algorithm, SBGA, is proposed, which exploits the unique semi-bandit feedback in network security domains. We prove that SBGA achieves sublinear regret against adaptive adversary, compared with both the best fixed strategy in hindsight and a near optimal adaptive strategy. Extensive experiments also show that SBGA significantly outperforms existing approaches with fast convergence rate.", "total_citations": {"2017": 1, "2018": 0, "2019": 1, "2020": 1, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:JoZmwDi-zQgC": {"external_link": "https://projects.iq.harvard.edu/files/teamcore/files/2015_37_teamcore_aamas15_ala_debarunkar_crc.pdf", "authors": ["Debarun Kar", "Fei Fang", "Francesco Delle Fave", "Nicole Sintov", "Arunesh Sinha", "Aram Galstyan", "Bo An", "Milind Tambe"], "publication_date": "2015/5/5", "journal": "Retrieved from Nanyang Technological University: http://www3. ntu. edu. sg/home/boan/papers/ALA15_Debarun. pdf", "description": "Several competing human behavior models have been proposed to model and protect against boundedly rational adversaries in repeated Stackelberg security games (SSGs). However, these existing models fail to address three main issues which are extremely detrimental to defender performance. First, while they attempt to learn adversary behavior models from adversaries\u2019 past actions (\u201cattacks on targets\u201d), they fail to take into account adversaries\u2019 future adaptation based on successes or failures of these past actions. Second, they assume that sufficient data in the initial rounds will lead to a reliable model of the adversary. However, our analysis reveals that the issue is not the amount of data, but that there just is not enough of the attack surface exposed to the adversary to learn a reliable model. Third, current leading approaches have failed to include probability weighting functions, even though it is well known that human beings\u2019 weighting of probability is typically nonlinear. Moreover, the performances of these models may be critically dependent on the learning algorithm used to learn the parameters of these models. The first contribution of this paper is a new human behavior model, SHARP, which mitigates these three limitations as follows:(i) SHARP reasons based on success or failure of the adversary\u2019s past actions on exposed portions of the attack surface to model adversary adaptiveness;(ii) SHARP reasons about similarity between exposed and unexposed areas of the attack surface, and also incorporates a discounting parameter to mitigate adversary\u2019s lack of exposure to enough of the attack surface; and (iii) SHARP integrates a non\u00a0\u2026", "total_citations": {"2015": 1, "2016": 1, "2017": 1, "2018": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:9vf0nzSNQJEC": {"external_link": "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e528e96f1499784926afc781c3b26dd1b4ddafe4", "authors": ["Jiang Rong", "Tao Qin", "Bo An"], "publication_date": "2015/5/4", "conference": "AAMAS", "pages": "1803-1804", "description": "Sponsored search auctions (SSAs) have attracted much research attention in recent years and different equilibrium concepts have been studied to understand advertisers\u2019 behaviors. However, the assumption that bidders are perfectly rational in these studies is unrealistic in the real world. In this work, we investigate the quantal response equilibrium (QRE) for SSAs. QRE is powerful in characterizing the bounded rationality in the sense that it only assumes that an advertiser chooses a better strategy with a larger probability instead of choosing the best strategy deterministically. We propose a homotopy-based method to compute the QRE of SSAs. We further show that there are many nice properties of the SSAs compared with general normal formal games, which can be used to improve the computational performance. Our experimental results indicate that our algorithm outperforms the basic traversal method.", "total_citations": {"2016": 2, "2017": 1, "2018": 1, "2019": 0, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:l7t_Zn2s7bgC": {"external_link": "https://ifaamas.org/AAMAS/aamas2014/proceedings/aamas/p1379.pdf", "authors": ["Yuan Liu", "Jie Zhang", "Bo An", "Sandip Sen"], "publication_date": "2014/5/5", "book": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems", "pages": "1379-1380", "description": "In this abstract, we propose a general robustness definition for a desired equilibrium and a practical robustness measure against a nonequilibrium strategy, inspired by the studies of evolutionary game theory. We also propose a framework to quantitatively evaluate the robustness, and provide theoretical analysis of the framework.", "total_citations": {"2014": 1, "2015": 1, "2016": 1, "2017": 0, "2018": 1, "2019": 0, "2020": 0, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:08ZZubdj9fEC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/qre-adauction.pdf", "authors": ["Jiang Rong", "Tao Qin", "Bo An"], "publication_date": "2014", "journal": "Tenth ad auction workshop", "description": "Sponsored search auctions have attracted much research attention in recent years and different equilibrium concepts have been studied to understand advertisers\u2019 bidding strategies. However, the assumption that bidders are perfectly rational in these studies is unrealistic in the real world. In this work, we investigate the quantal response equilibrium (QRE) solution concept for sponsored search auctions, in which advertisers choose strategies based on their relative expected utilities. QRE is powerful in characterizing the bounded rationality in the sense that it only assumes that an advertiser chooses a better strategy with a larger probability instead assuming that the advertiser chooses the best strategy deterministically. We first propose a homotopy-based method, which is potential to be globally convergent, to compute QRE for sponsored search auctions. We show that this method is effective in finding a QRE. Then we fit the model into the datasets from a commercial search engine and develop an estimator to infer the values of advertisers and click-through rates of their advertisements. Our experiments on several search phrases indicate that the model works quite well for certain queries and the values we estimated are consistent with the basic property of sponsored search auctions.", "total_citations": {"2015": 2, "2016": 1, "2017": 1, "2018": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:HhcuHIWmDEUC": {"external_link": "https://arxiv.org/abs/2301.11774", "authors": ["Wanqi Xue", "Bo An", "Shuicheng Yan", "Zhongwen Xu"], "publication_date": "2023/1/27", "journal": "arXiv preprint arXiv:2301.11774", "description": "The complexity of designing reward functions has been a major obstacle to the wide application of deep reinforcement learning (RL) techniques. Describing an agent's desired behaviors and properties can be difficult, even for experts. A new paradigm called reinforcement learning from human preferences (or preference-based RL) has emerged as a promising solution, in which reward functions are learned from human preference labels among behavior trajectories. However, existing methods for preference-based RL are limited by the need for accurate oracle preference labels. This paper addresses this limitation by developing a method for crowd-sourcing preference labels and learning from diverse human preferences. The key idea is to stabilize reward learning through regularization and correction in a latent space. To ensure temporal consistency, a strong constraint is imposed on the reward model that forces its latent space to be close to the prior distribution. Additionally, a confidence-based reward model ensembling method is designed to generate more stable and reliable predictions. The proposed method is tested on a variety of tasks in DMcontrol and Meta-world and has shown consistent and significant improvements over existing preference-based RL algorithms when learning from diverse feedback, paving the way for real-world applications of RL methods.", "total_citations": {"2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:v1_lew4L6wgC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Aung_DO-GAN_A_Double_Oracle_Framework_for_Generative_Adversarial_Networks_CVPR_2022_paper.html", "authors": ["Aye Phyu Phyu Aung", "Xinrun Wang", "Runsheng Yu", "Bo An", "Senthilnath Jayavelu", "Xiaoli Li"], "publication_date": "2022", "conference": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition", "pages": "11275-11284", "description": "In this paper, we propose a new approach to train Generative Adversarial Networks (GANs) where we deploy a double-oracle framework using the generator and discriminator oracles. GAN is essentially a two-player zero-sum game between the generator and the discriminator. Training GANs is challenging as a pure Nash equilibrium may not exist and even finding the mixed Nash equilibrium is difficult as GANs have a large-scale strategy space. In DO-GAN, we extend the double oracle framework to GANs. We first generalize the players' strategies as the trained models of generator and discriminator from the best response oracles. We then compute the meta-strategies using a linear program. For scalability of the framework where multiple generators and discriminator best responses are stored in the memory, we propose two solutions: 1) pruning the weakly-dominated players' strategies to keep the oracles from becoming intractable; 2) applying continual learning to retain the previous knowledge of the networks. We apply our framework to established GAN architectures such as vanilla GAN, Deep Convolutional GAN, Spectral Normalization GAN and Stacked GAN. Finally, we conduct experiments on MNIST, CIFAR-10 and CelebA datasets and show that DO-GAN variants have significant improvements in both subjective qualitative evaluation and quantitative metrics, compared with their respective GAN architectures.", "total_citations": {"2021": 1, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:tH6gc1N1XXoC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0377221720306147", "authors": ["Xiaoxuan Hu", "Waiming Zhu", "Huawei Ma", "Bo An", "Yanling Zhi", "Yi Wu"], "publication_date": "2021/2/16", "journal": "European Journal of Operational Research", "volume": "289", "issue": "1", "pages": "254-269", "publisher": "North-Holland", "description": "In this article, we study a challenging optimization problem, namely the orientational variable-length strip covering problem. Given a large rectangular region and multiple orientational strips with variable lengths, the strips should be positioned and their lengths be determined such that their union can cover the large region with the minimal total area. This problem is motivated by the real-world problem in which multiple Earth observation satellites are used to image a large region in a cooperative pattern. It is a challenging nonlinear combinatorial optimization problem in continuous space. We propose a set-covering-problem-like integer programming formulation for the problem based on grid discretization and prove that the optimal solutions can be achieved when the strips take discrete values. As there is a large number of columns, we use a column generation method to solve the linear relaxation problem and an\u00a0\u2026", "total_citations": {"2021": 1, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:CB2v5VPnA5kC": {"external_link": "https://dl.acm.org/doi/abs/10.5555/3491440.3491475", "authors": ["Jakub \u010cern\u00fd", "Viliam Lis\u00fd", "Branislav Bo\u0161ansk\u00fd", "Bo An"], "publication_date": "2021/1/7", "book": "Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence", "pages": "246-253", "description": "Stackelberg security games (SSGs) have been deployed in many real-world situations to optimally allocate scarce resource to protect targets against attackers. However, actual human attackers are not perfectly rational and there are several behavior models that attempt to predict subrational behavior. Quantal response is among the most commonly used such models and Quantal Stackelberg Equilibrium (QSE) describes the optimal strategy to commit to when facing a subrational opponent. Nonconcavity makes computing QSE computationally challenging and while there exist algorithms for computing QSE for SSGs, they cannot be directly used for solving an arbitrary game in the normal form. We (1) present a transformation of the primal problem for computing QSE using a Dinkelbach's method for any general-sum normal-form game, (2) provide a gradient-based and a MILP-based algorithm, give the\u00a0\u2026", "total_citations": {"2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:UxriW0iASnsC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-319-05579-4_32", "authors": ["Ya\u2019akov Gal", "Avi Rosenfeld", "Sarit Kraus", "Michele Gelfand", "Bo An", "Jun Lin"], "publication_date": "2014", "conference": "Social Computing, Behavioral-Cultural Modeling and Prediction: 7th International Conference, SBP 2014, Washington, DC, USA, April 1-4, 2014. Proceedings 7", "pages": "261-268", "publisher": "Springer International Publishing", "description": "Corruption frequently occurs in many aspects of multi-party interaction between private agencies and government employees. Past works studying corruption in a lab context have explicitly included covert or illegal activities in participants\u2019 strategy space or have relied on surveys like the Corruption Perception Index (CPI). This paper studies corruption in ecologically realistic settings in which corruption is not suggested to the players a priori but evolves during repeated interaction. We ran studies involving hundreds of subjects in three countries: China, Israel, and the United States. Subjects interacted using a four-player board game in which three bidders compete to win contracts by submitting bids in repeated auctions, and a single auctioneer determines the winner of each auction. The winning bid was paid to an external \u201cgovernment\u201d entity, and was not distributed among the players. The game logs were\u00a0\u2026", "total_citations": {"2013": 1, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:hFOr9nPyWt4C": {"external_link": "https://cdn.aaai.org/ocs/4262/4262-19483-1-PB.pdf", "authors": ["Bo An", "David Kempe", "Christopher Kiekintveld", "Eric Shieh", "Satinder Singh", "Milind Tambe", "Yevgeniy Vorobeychik"], "publication_date": "2012/3/23", "conference": "2012 AAAI Spring Symposium Series", "description": "Stackelberg games have been used in several deployed applications of game theory to make recommendations for allocating limited resources for protecting critical infrastructure. The resource allocation strategies are randomized to prevent a strategic attacker from using surveillance to learn and exploit patterns in the allocation. An important limitation of previous work on security games is that it typically assumes that attackers have perfect surveillance capabilities, and can learn the exact strategy of the defender. We introduce a new model that explicitly models the process of an attacker observing a sequence of resource allocation decisions and updating his beliefs about the defender\u2019s strategy. For this model we present computational techniques for updating the attacker\u2019s beliefs and computing optimal strategies for both the attacker and defender, given a specific number of observations. We provide multiple formulations for computing the defender\u2019s optimal strategy, including non-convex programming and a convex approximation. We also present an approximate method for computing the optimal length of time for the attacker to observe the defender\u2019s strategy before attacking. Finally, we present experimental results comparing the efficiency and runtime of our methods.", "total_citations": {"2013": 1, "2014": 1, "2015": 0, "2016": 1, "2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:M3ejUd6NZC8C": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-540-74769-7_14", "authors": ["Liang Gui Tang", "Bo An", "Dai Jie Cheng"], "publication_date": "2007/9/14", "book": "International Conference on Life System Modeling and Simulation", "pages": "117-127", "publisher": "Springer Berlin Heidelberg", "description": "This paper thoroughly analyzes the transfer and construction of the state-action space of the agent decision-making process, discusses the optimal strategy of agent\u2019s action selection based on Markov decision-making process, designs a neural networks model for the agent reinforcement learning, and designs the agent reinforcement learning based on neural networks. By the simulation experiment of agent\u2019s bid price in Multi-Agent Electronic Commerce System, validated the Agent Reinforcement Learning Algorithm Based on Neural Networks has very good performance and the action impending ability.", "total_citations": {"2012": 1, "2013": 1, "2014": 1, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:MXK_kJrjxJIC": {"external_link": "https://content.iospress.com/articles/multiagent-and-grid-systems/mgs00033", "authors": ["Chunyan Miao", "Jianshu Weng", "Angela Goh", "Zhiqi Shen", "Bo An"], "publication_date": "2006/1/1", "journal": "Multiagent and Grid Systems", "volume": "2", "issue": "2", "pages": "101-114", "publisher": "IOS Press", "description": "The grid is moving from the scientific grid to a pervasive and economic/business grid. Service trading, in which service provider and service consumer negotiate for a mutually acceptable agreement on multi-issues such as service performance, access cost etc., is one of the most important components in building the Economic Grid. In view of Pervasive Grid, a new challenging issue is that participants on pervasive devices usually have limited computational capacity. And it is also desirable that a multi-issue negotiation agreement can be reached as quickly as possible since the wireless communication to exchange the offers is generally unreliable and power-consuming. Hence, an agile, automated, but lightweight multi-issue decision-making model is needed to facilitate service negotiation in Pervasive Grid. More over, existing methods for multi-issue negotiation only regard each issue as a separate issue, though\u00a0\u2026", "total_citations": {"2008": 1, "2009": 0, "2010": 0, "2011": 0, "2012": 0, "2013": 1, "2014": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:qxL8FJ1GzNcC": {"external_link": "https://link.springer.com/chapter/10.1007/11596448_54", "authors": ["Bo An", "Chunyan Miao", "Zhiqi Shen", "Yuan Miao", "Daijie Cheng"], "publication_date": "2005", "conference": "Computational Intelligence and Security: International Conference, CIS 2005, Xi\u2019an, China, December 15-19, 2005, Proceedings Part I", "pages": "375-380", "publisher": "Springer Berlin Heidelberg", "description": "This paper first proposes a novel transitive dependence theory. Based on the proposed transitive dependence theory, a dynamic virtual organization formation framework has been worked out which includes service discovery, transitive dependence based reasoning for organization partners search and organization resolution.", "total_citations": {"2011": 1, "2012": 0, "2013": 1, "2014": 1, "2015": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:ULOm3_A8WrAC": {"external_link": "https://link.springer.com/chapter/10.1007/11508069_63", "authors": ["Bo An", "Chunyan Miao", "Lianggui Tang", "Shuangqing Li", "Daijie Cheng"], "publication_date": "2005", "conference": "Intelligent Data Engineering and Automated Learning-IDEAL 2005: 6th International Conference, Brisbane, Australia, July 6-8, 2005. Proceedings 6", "pages": "486-493", "publisher": "Springer Berlin Heidelberg", "description": "This research investigates transitive dependence relations, an extension of direct dependence relations, in multi-agent systems. In this paper, action dependence relations are employed to deduct transitive dependence relations from direct dependence relations. Transitive dependence is useful in representation, analysis, and social relations reasoning between agents, groups, organizations, etc. Furthermore, in this paper, dependence relations are differentiated by both dependence property and dependence degree, which is useful in quantitative social reasoning.", "total_citations": {"2011": 1, "2012": 1, "2013": 0, "2014": 0, "2015": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:2l5NCbZemmgC": {"external_link": "https://openreview.net/forum?id=g0ofsq1NRL", "authors": ["Hongxin Wei", "Huiping Zhuang", "Renchunzi Xie", "Lei Feng", "Gang Niu", "Bo An", "Yixuan Li"], "publication_date": "2023/6/15", "description": "In the presence of noisy labels, designing robust loss functions is critical for securing the generalization performance of deep neural networks. Cross Entropy (CE) loss has been shown to be not robust to noisy labels due to its unboundedness. To alleviate this issue, existing works typically design specialized robust losses with the symmetric condition, which usually lead to the underfitting issue. In this paper, our key idea is to induce a loss bound at the logit level, thus universally enhancing the noise robustness of existing losses. Specifically, we propose logit clipping (LogitClip), which clamps the norm of the logit vector to ensure that it is upper bounded by a constant. In this manner, CE loss equipped with our LogitClip method is effectively bounded, mitigating the overfitting to examples with noisy labels. Moreover, we present theoretical analyses to certify the noise-tolerant ability of LogitClip. Extensive experiments show that LogitClip not only significantly improves the noise robustness of CE loss, but also broadly enhances the generalization performance of popular robust losses.", "total_citations": {"2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=100&pagesize=100&citation_for_view=PEEpuNwAAAAJ:mUJArPsKIAAC": {"external_link": "https://proceedings.mlr.press/v206/liu23g.html", "authors": ["Shuqi Liu", "Yuzhou Cao", "Qiaozhen Zhang", "Lei Feng", "Bo An"], "publication_date": "2023/4/11", "conference": "International Conference on Artificial Intelligence and Statistics", "pages": "8734-8748", "publisher": "PMLR", "description": "In contrast to ordinary supervised classification tasks that require massive data with high-quality labels, complementary-label learning (CLL) deals with the weakly-supervised learning scenario where each instance is equipped with a complementary label, which specifies a class the instance does not belong to. However, most of the existing statistically consistent CLL methods suffer from overfitting intrinsically, due to the negative empirical risk issue. In this paper, we aim to propose overfitting-resistant and theoretically grounded methods for CLL. Considering the unique property of the distribution of complementarily labeled samples, we provide a risk estimator via order-preserving losses, which are naturally non-negative and thus can avoid overfitting caused by negative terms in risk estimators. Moreover, we provide classifier-consistency analysis and statistical guarantee for this estimator. Furthermore, we provide a weighed version of the proposed risk estimator to further enhance its generalization ability and prove its statistical consistency. Experiments on benchmark datasets demonstrate the effectiveness of our proposed methods.", "total_citations": {"2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:jE2MZjpN3IcC": {"external_link": "https://arxiv.org/abs/2207.05285", "authors": ["Shuxin Li", "Xinrun Wang", "Youzhi Zhang", "Jakub Cerny", "Pengdeng Li", "Hau Chan", "Bo An"], "publication_date": "2022/7/12", "journal": "arXiv preprint arXiv:2207.05285", "description": "Offline reinforcement learning (offline RL) is an emerging field that has recently begun gaining attention across various application domains due to its ability to learn strategies from earlier collected datasets. Offline RL proved very successful, paving a path to solving previously intractable real-world problems, and we aim to generalize this paradigm to a multiplayer-game setting. To this end, we introduce a problem of offline equilibrium finding (OEF) and construct multiple types of datasets across a wide range of games using several established methods. To solve the OEF problem, we design a model-based framework that can directly apply any online equilibrium finding algorithm to the OEF setting while making minimal changes. The three most prominent contemporary online equilibrium finding algorithms are adapted to the context of OEF, creating three model-based variants: OEF-PSRO and OEF-CFR, which generalize the widely-used algorithms PSRO and Deep CFR to compute Nash equilibria (NEs), and OEF-JPSRO, which generalizes the JPSRO to calculate (Coarse) Correlated equilibria ((C)CEs). We also combine the behavior cloning policy with the model-based policy to further improve the performance and provide a theoretical guarantee of the solution quality. Extensive experimental results demonstrate the superiority of our approach over offline RL algorithms and the importance of using model-based methods for OEF problems. We hope our work will contribute to advancing research in large-scale equilibrium finding.", "total_citations": {"2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Ade32sEp0pkC": {"external_link": "https://ifaamas.org/Proceedings/aamas2022/pdfs/p1364.pdf", "authors": ["Wanyuan Wang", "Gerong Wu", "Weiwei Wu", "Yichuan Jiang", "Bo An"], "publication_date": "2022/5/9", "book": "Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems", "pages": "1364-1372", "description": "The popularity of mobility-on-demand (MoD) systems boosts the need for online collective multiagent planning, where spatially distributed servicing agents are planned to meet dynamically arriving demands. For city-scale MoDs with a population of agents, it is necessary to find a balance between computation time (ie, realtime) and solution quality (ie, the number of demands served). Directly using an offline policy can guarantee real-time, but cannot be dynamically adjusted to real agent and demand distributions. On the other hand, search-based online planning methods are adaptive. However, they are computationally expensive and cannot scale up. In this paper, we propose a principled online multiagent planning method, which reuses and improves the offline policy in an anytime manner. We first model MoDs as a collective Markov Decision Process (C-MDP) where the history collective behavior of agents affects the joint reward. We propose a novel state value function to evaluate the policy, and a gradient ascent (GA) technique to improve the policy. We show that GA-based policy iteration (GA-PI) on local policy can converge. Finally, given real-time information, the offline policy is used as the default plan and GA-PI is used to improve it and generate an online plan. Experimentally, the proposed offline policy reuse method significantly outperforms standard online multiagent planning methods on MoD systems like ride-sharing and security traffic patrolling in terms of computation time and solution quality.", "total_citations": {"2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:rHJHxKgnXwkC": {"external_link": "https://arxiv.org/abs/2204.04930", "authors": ["Xinrun Wang", "Jakub Cerny", "Shuxin Li", "Chang Yang", "Zhuyun Yin", "Hau Chan", "Bo An"], "publication_date": "2022/4/11", "journal": "arXiv preprint arXiv:2204.04930", "description": "Extensive-form games provide a versatile framework for modeling interactions of multiple agents subjected to imperfect observations and stochastic events. In recent years, two paradigms, policy space response oracles (PSRO) and counterfactual regret minimization (CFR), showed that extensive-form games may indeed be solved efficiently. Both of them are capable of leveraging deep neural networks to tackle the scalability issues inherent to extensive-form games and we refer to them as deep equilibrium-finding algorithms. Even though PSRO and CFR share some similarities, they are often regarded as distinct and the answer to the question of which is superior to the other remains ambiguous. Instead of answering this question directly, in this work we propose a unified perspective on deep equilibrium finding that generalizes both PSRO and CFR. Our four main contributions include: i) a novel response oracle (RO) which computes Q values as well as reaching probability values and baseline values; ii) two transform modules -- a pre-transform and a post-transform -- represented by neural networks transforming the outputs of RO to a latent additive space (LAS), and then the LAS to action probabilities for execution; iii) two average oracles -- local average oracle (LAO) and global average oracle (GAO) -- where LAO operates on LAS and GAO is used for evaluation only; and iv) a novel method inspired by fictitious play that optimizes the transform modules and average oracles, and automatically selects the optimal combination of components of the two frameworks. Experiments on Leduc poker game demonstrate that our approach can\u00a0\u2026", "total_citations": {"2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:SjuI4pbJlxcC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9739859/", "authors": ["Jiuchuan Jiang", "Kai Di", "Bo An", "Yichuan Jiang", "Zhan Bu", "Jie Cao"], "publication_date": "2022/3/22", "journal": "IEEE Transactions on Parallel and Distributed Systems", "volume": "33", "issue": "12", "pages": "3600-3615", "publisher": "IEEE", "description": "Team formation has been extensively studied for complex task crowdsourcing in E-markets, in which a set of workers are hired to form a team to complete a complex task collaboratively. However, existing studies have two typical drawbacks: 1) each team is created for only one task, which may be costly and cannot accommodate crowdsourcing markets with a large number of tasks; and 2) most existing studies form teams in a centralized manner by the requesters, which may place a heavy burden on requesters. In fact, we observe that many complex tasks at real-world crowdsourcing platforms have similar skill requirements and workers are often connected through social networks. Therefore, this paper explores distributed team formation-based batch crowdsourcing for complex tasks to address the drawbacks in existing studies, in which similar tasks can be addressed in a batch to reduce computational costs and\u00a0\u2026", "total_citations": {"2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:LPtt_HFRSbwC": {"external_link": "https://scholar.google.com/scholar?cluster=15037254428627280581&hl=en&oi=scholarr", "authors": ["Shuo Sun", "Rundong Wang", "Xu He", "Junlei Zhu", "Jian Li", "Bo An"], "publication_date": "2022", "journal": "arXiv preprint arXiv:2201.09058", "total_citations": {"2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:FiytvqdAVhgC": {"external_link": "https://link.springer.com/article/10.1007/s10458-021-09511-z", "authors": ["Yanchen Deng", "Bo An"], "publication_date": "2021/10", "journal": "Autonomous Agents and Multi-Agent Systems", "volume": "35", "issue": "2", "pages": "24", "publisher": "Springer US", "description": "Belief propagation algorithms including Max-sum and its variants are important methods for multi-agent optimization. However, they face a significant scalability challenge as the computational overhead grows exponentially with respect to the arity of each utility function. To date, a number of acceleration algorithms for belief propagation algorithms were proposed. These algorithms maintain a lower bound on total utility and employ either a domain pruning technique or branch and bound to reduce the search space. However, these algorithms still suffer from low-quality bounds and the inability of filtering out suboptimal tied entries. In this paper, we first show that these issues are exacerbated and can considerably degenerate the performance of the state-of-the-art methods when dealing with the problems with dense utility functions, which widely exist in many real-world domains. Built on this observation, we then\u00a0\u2026", "total_citations": {"2021": 1, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:WHdLCjDvYFkC": {"external_link": "https://arxiv.org/abs/2106.08864", "authors": ["Yuzhou Cao", "Lei Feng", "Senlin Shu", "Yitian Xu", "Bo An", "Gang Niu", "Masashi Sugiyama"], "publication_date": "2021/6/16", "journal": "arXiv preprint arXiv:2106.08864", "description": "Can we learn a multi-class classifier from only data of a single class? We show that without any assumptions on the loss functions, models, and optimizers, we can successfully learn a multi-class classifier from only data of a single class with a rigorous consistency guarantee when confidences (i.e., the class-posterior probabilities for all the classes) are available. Specifically, we propose an empirical risk minimization framework that is loss-/model-/optimizer-independent. Instead of constructing a boundary between the given class and other classes, our method can conduct discriminative classification between all the classes even if no data from the other classes are provided. We further theoretically and experimentally show that our method can be Bayes-consistent with a simple modification even if the provided confidences are highly noisy. Then, we provide an extension of our method for the case where data from a subset of all the classes are available. Experimental results demonstrate the effectiveness of our methods.", "total_citations": {"2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:QD3KBmkZPeQC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9252133/", "authors": ["Lei Feng", "Hongxin Wei", "Qingyu Guo", "Zhuoyi Lin", "Bo An"], "publication_date": "2020/11/9", "journal": "IEEE Intelligent Systems", "volume": "36", "issue": "6", "pages": "32-41", "publisher": "IEEE", "description": "Learning effective representations of users and items is crucially important to recommendation with implicit feedback. Matrix factorization is the basic idea to derive the representations of users and items by decomposing the given interaction matrix. However, existing matrix factorization based approaches share the limitation in that the interaction between user embedding and item embedding is only weakly enforced by fitting the given individual rating value, which may lose potentially useful information. In this article, we propose a novel augmented generalized matrix factorization approach that is able to incorporate the historical interaction information of users and items for learning effective representations of users and items. Despite the simplicity of our proposed approach, extensive experiments on four public implicit feedback datasets demonstrate that our approach outperforms state-of-the-art counterparts\u00a0\u2026", "total_citations": {"2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:PVgj2kMGcgYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9209426/", "authors": ["Yoav Ben-Yaakov", "Joachim Meyer", "Xinrun Wang", "Bo An"], "publication_date": "2020/9/7", "conference": "2020 IEEE International Conference on Human-Machine Systems (ICHMS)", "pages": "1-6", "publisher": "IEEE", "description": "Cyber attacks and the associated costs made cybersecurity a vital part of any system. User behavior and decisions are still a major part in the coping with these risks. We developed a model of optimal investment and human decisions with security measures, given that the effectiveness of each measure depends partly on the performance of the others. In an online experiment, participants classified events as malicious or non-malicious, based on the value of an observed variable. Prior to making the decisions, they had invested in three security measures - a firewall, an IDS or insurance. In three experimental conditions, maximal investment in only one of the measures was optimal, while in a fourth condition, participants should not have invested in any of the measures. A previous paper presents the analysis of the investment decisions. This paper reports users' classifications of events when interacting with these\u00a0\u2026", "total_citations": {"2021": 1, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:buQ7SEKw-1sC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-64096-5_9", "authors": ["Yanchen Deng", "Bo An", "Zongmin Qiu", "Liuxi Li", "Yong Wang", "Yinghui Xu"], "publication_date": "2020", "conference": "Distributed Artificial Intelligence: Second International Conference, DAI 2020, Nanjing, China, October 24\u201327, 2020, Proceedings 2", "pages": "126-139", "publisher": "Springer International Publishing", "description": "Automated warehouses are widely deployed in large-scale distribution centers due to their ability of reducing operational cost and improving throughput capacity. In an automated warehouse, orders are fulfilled by battery-powered AGVs transporting movable shelves or boxes. Therefore, battery management is crucial to the productivity since recovering depleted batteries can be time-consuming and seriously affect the overall performance of the system by reducing the number of available robots. In this paper, we propose to solve the battery management problem by using deep reinforcement learning (DRL). We first formulate the battery management problem as a Markov Decision Process (MDP). Then we show the state-of-the-art DRL method which uses Gaussian noise to enforce exploration could perform poorly in the formulated MDP, and present a novel algorithm called TD3-ARL that performs effective\u00a0\u2026", "total_citations": {"2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Ak0FvsSvgGUC": {"external_link": "https://arxiv.org/abs/1911.07712", "authors": ["Runsheng Yu", "Zhenyu Shi", "Xinrun Wang", "Rundong Wang", "Buhong Liu", "Xinwen Hou", "Hanjiang Lai", "Bo An"], "publication_date": "2019/11/18", "journal": "arXiv preprint arXiv:1911.07712", "description": "Existing value-factorized based Multi-Agent deep Reinforce-ment Learning (MARL) approaches are well-performing invarious multi-agent cooperative environment under thecen-tralized training and decentralized execution(CTDE) scheme,where all agents are trained together by the centralized valuenetwork and each agent execute its policy independently. How-ever, an issue remains open: in the centralized training process,when the environment for the team is partially observable ornon-stationary, i.e., the observation and action informationof all the agents cannot represent the global states, existingmethods perform poorly and sample inefficiently. Regret Min-imization (RM) can be a promising approach as it performswell in partially observable and fully competitive settings.However, it tends to model others as opponents and thus can-not work well under the CTDE scheme. In this work, wepropose a novel team RM based Bayesian MARL with threekey contributions: (a) we design a novel RM method to traincooperative agents as a team and obtain a team regret-basedpolicy for that team; (b) we introduce a novel method to de-compose the team regret to generate the policy for each agentfor decentralized execution; (c) to further improve the perfor-mance, we leverage a differential particle filter (a SequentialMonte Carlo method) network to get an accurate estimation ofthe state for each agent. Experimental results on two-step ma-trix games (cooperative game) and battle games (large-scalemixed cooperative-competitive games) demonstrate that ouralgorithm significantly outperforms state-of-the-art methods.", "total_citations": {"2021": 2, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:OP4eGU-M3BUC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/IJCAI19_Cyber.pdf", "authors": ["Xinrun Wang", "Bo An", "Hau Chan"], "publication_date": "2019", "conference": "IJCAI", "pages": "6020-6027", "description": "Due to recent cyber attacks, cybersecurity is becoming more critical. A single attack (eg, WannaCry ransomware attack) can cause as much as $4 billion in damage. However, the cybersecurity investment by companies is far from satisfactory. Therefore, governments (eg, in the UK) launch grants and subsidies to help companies to boost their cybersecurity to create a safer national cyber environment. Computing the optimal allocation is challenging due to limited subsidies, the interdependence between companies and the presence of strategic cyber attackers. To tackle the government\u2019s allocation problem, we introduce a Stackelberg game model where the government first commits to an allocation and the companies/users and attacker simultaneously determine their protection and attack (pure or mixed) strategies, respectively. For the pure-strategy case, while there may not be a feasible allocation in general, we prove that computing an optimal allocation is NP-hard and propose a linear reverse convex program when the attacker can attack all users. For the mixed-strategy case, we show that there is a polynomial time algorithm to find an optimal allocation when the attacker has a single-attack capability. We then provide a heuristic algorithm, based on best-responsegradient dynamics, to find an effective allocation in general settings. Experimentally, we show that our heuristic algorithm is effective and significantly outperforms baselines on synthetic and real data.", "total_citations": {"2020": 1, "2021": 0, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:HeT0ZceujKMC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-32430-8_32", "authors": ["Xinrun Wang", "Milind Tambe", "Branislav Bo\u0161ansk\u00fd", "Bo An"], "publication_date": "2019", "conference": "Decision and Game Theory for Security: 10th International Conference, GameSec 2019, Stockholm, Sweden, October 30\u2013November 1, 2019, Proceedings 10", "pages": "542-562", "publisher": "Springer International Publishing", "description": "Most of the current security models assume that the values of targets/areas are static or the changes (if any) are scheduled and known to the defender. Unfortunately, such models are not sufficient for many domains, where actions of the players modify the values of the targets. Examples include wildlife scenarios, where the attacker can increase value of targets by secretly building supporting facilities. To address such security game domains with player-affected values, we first propose DPOS3G, a novel partially observable stochastic Stackelberg game where target values are determined by the players\u2019 actions; the defender can only partially observe these targets\u2019 values, while the attacker can fully observe the targets\u2019 values and the defender\u2019s strategy. Second, we propose RITA (Reduced game Iterative Transfer Algorithm), which is based on the heuristic search value iteration algorithm for partially\u00a0\u2026", "total_citations": {"2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:XoXfffV-tXoC": {"external_link": "https://dl.acm.org/doi/pdf/10.1145/3154942", "authors": ["Bo An", "Nick Jennings", "Zhenhui Jessie Li"], "publication_date": "2017/11/24", "source": "ACM Transactions on Intelligent Systems and Technology (TIST)", "volume": "9", "issue": "3", "pages": "1-4", "publisher": "ACM", "description": "Past decades saw the rapid development of cities and the boom of urban population, but also the rise of many urban issues, such as traffic congestion, energy shortage, and pollution. The situation is continuing: as predicted by the United Nations, the world\u2019s urban population will add another 2.5 billion by 2050, an increase of 66% over today\u2019s total population. While the pressure on resources is unprecedented, increasing volume and diversity of data is at the same time being generated and collected with the help of new technologies. This calls for the integration of advanced information and computational technologies to develop intelligent solutions for urban issues. Indeed, there has been a variety of research in the AI community, pioneering in applying AI and data science to the practice of urban computing [1, 2]. These include the development of smart communities, smart home automation, intelligent transport\u00a0\u2026", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:w1MjKQ0l0TYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/10045747/", "authors": ["Shifei Ding", "Wei Du", "Ling Ding", "Jian Zhang", "Lili Guo", "Bo An"], "publication_date": "2023/2/16", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "publisher": "IEEE", "description": "Communication learning is an important research direction in the multiagent reinforcement learning (MARL) domain. Graph neural networks (GNNs) can aggregate the information of neighbor nodes for representation learning. In recent years, several MARL methods leverage GNN to model information interactions between agents to coordinate actions and complete cooperative tasks. However, simply aggregating the information of neighboring agents through GNNs may not extract enough useful information, and the topological relationship information is ignored. To tackle this difficulty, we investigate how to efficiently extract and utilize the rich information of neighbor agents as much as possible in the graph structure, so as to obtain high-quality expressive feature representation to complete the cooperation task. To this end, we present a novel GNN-based MARL method with graphical mutual information (MI\u00a0\u2026", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:jSAVyFp_754C": {"external_link": "https://www.degruyter.com/document/doi/10.1515/nanoph-2022-0612/html", "authors": ["Eng Aik Chan", "Carolina Rend\u00f3n-Barraza", "Benquan Wang", "Tanchao Pu", "Jun-Yu Ou", "Hongxin Wei", "Giorgio Adamo", "Bo An", "Nikolay I Zheludev"], "publication_date": "2023/1/18", "journal": "Nanophotonics", "issue": "0", "publisher": "De Gruyter", "description": "Particle counting is of critical importance for nanotechnology, environmental monitoring, pharmaceutical, food and semiconductor industries. Here we introduce a super-resolution single-shot optical method for counting and mapping positions of subwavelength particles on a surface. The method is based on the deep learning analysis of the intensity profile of the coherent light scattered on the group of particles. In a proof of principle experiment, we demonstrated particle counting accuracies of more than 90%. We also demonstrate that the particle locations can be mapped on a 4 \u00d7 4 grid with a nearly perfect accuracy (16-pixel binary imaging of the particle ensemble). Both the retrieval of number of particles and their mapping is achieved with super-resolution: accuracies are similar for sets with closely located optically unresolvable particles and sets with sparsely located particles. As the method does not require\u00a0\u2026", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:rTD5ala9j4wC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/KDD23_Stock.pdf", "authors": ["Shuo Sun", "Xinrun Wang", "Wanqi Xue", "Xiaoxuan Lou", "Bo An"], "publication_date": "2023", "description": "Quantitative stock investment is a fundamental financial task that highly relies on accurate prediction of market status and profitable investment decision making. Despite recent advances in deep learning (DL) have shown stellar performance on capturing trading opportunities in the stochastic stock market, the performance of existing DL methods is unstable with sensitivity to network initialization and hyperparameter selection. One major limitation of existing works is that investment decisions are made based on one individual neural network predictor with high uncertainty, which is inconsistent with the workflow in real-world trading firms. To tackle this limitation, we propose AlphaMix, a novel three-stage mixture-of-experts (MoE) framework for quantitative investment to mimic the efficient bottom-up hierarchical trading strategy design workflow of successful trading companies. In Stage one, we introduce an efficient ensemble learning method, whose computational and memory costs are significantly lower comparing to traditional ensemble methods, to train multiple groups of trading experts with personalised market understanding and trading styles. In Stage two, we collect diversified investment suggestions through building a pool of trading experts utilizing hyperparameter level and initialization level diversity of neural networks for post hoc ensemble construction. In Stage three, we design three different mechanisms, namely as-needed router, with-replacement selection and integrated expert soup, to dynamically pick experts from the expert pool, which takes the responsibility of a portfolio manager. Through extensive experiments on US and\u00a0\u2026", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:mKu_rENv82IC": {"external_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a375e3cb803e0d78fda4bb3933bd3a3a-Abstract-Conference.html", "authors": ["Yanchen Deng", "Shufeng Kong", "Caihua Liu", "Bo An"], "publication_date": "2022/12/6", "journal": "Advances in Neural Information Processing Systems", "volume": "35", "pages": "25436-25449", "description": "Belief Propagation (BP) is an important message-passing algorithm for various reasoning tasks over graphical models, including solving the Constraint Optimization Problems (COPs). It has been shown that BP can achieve state-of-the-art performance on various benchmarks by mixing old and new messages before sending the new one, ie, damping. However, existing methods on tuning a static damping factor for BP not only is laborious but also harms their performance. Moreover, existing BP algorithms treat each variable node's neighbors equally when composing a new message, which also limits their exploration ability. To address these issues, we seamlessly integrate BP, Gated Recurrent Units (GRUs), and Graph Attention Networks (GATs) within the massage-passing framework to reason about dynamic weights and damping factors for composing new BP messages. Our model, Deep Attentive Belief Propagation (DABP), takes the factor graph and the BP messages in each iteration as the input and infers the optimal weights and damping factors through GRUs and GATs, followed by a multi-head attention layer. Furthermore, unlike existing neural-based BP variants, we propose a novel self-supervised learning algorithm for DABP with a smoothed solution cost, which does not require expensive training labels and also avoids the common out-of-distribution issue through efficient online learning. Extensive experiments show that our model significantly outperforms state-of-the-art baselines.", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:-mN3Mh-tlDkC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/20389", "authors": ["Wanqi Xue", "Bo An", "Chai Kiat Yeo"], "publication_date": "2022/6/28", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "36", "issue": "4", "pages": "4646-4653", "description": "How resources are deployed to secure critical targets in networks can be modelled by Network Security Games (NSGs). While recent advances in deep learning (DL) provide a powerful approach to dealing with large-scale NSGs, DL methods such as NSG-NFSP suffer from the problem of data inefficiency. Furthermore, due to centralized control, they cannot scale to scenarios with a large number of resources. In this paper, we propose a novel DL-based method, NSGZero, to learn a non-exploitable policy in NSGs. NSGZero improves data efficiency by performing planning with neural Monte Carlo Tree Search (MCTS). Our main contributions are threefold. First, we design deep neural networks (DNNs) to perform neural MCTS in NSGs. Second, we enable neural MCTS with decentralized control, making NSGZero applicable to NSGs with many resources. Third, we provide an efficient learning paradigm, to achieve joint training of the DNNs in NSGZero. Compared to state-of-the-art algorithms, our method achieves significantly better data efficiency and scalability.", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:sszUF3NjhM4C": {"external_link": "https://arxiv.org/abs/2207.07578", "authors": ["Shuo Sun", "Rundong Wang", "Bo An"], "publication_date": "2022/6/7", "journal": "arXiv preprint arXiv:2207.07578", "description": "Quantitative investment is a fundamental financial task that highly relies on accurate stock prediction and profitable investment decision making. Despite recent advances in deep learning (DL) have shown stellar performance on capturing trading opportunities in the stochastic stock market, we observe that the performance of existing DL methods is sensitive to random seeds and network initialization. To design more profitable DL methods, we analyze this phenomenon and find two major limitations of existing works. First, there is a noticeable gap between accurate financial predictions and profitable investment strategies. Second, investment decisions are made based on only one individual predictor without consideration of model uncertainty, which is inconsistent with the workflow in real-world trading firms. To tackle these two limitations, we first reformulate quantitative investment as a multi-task learning problem. Later on, we propose AlphaMix, a novel two-stage mixture-of-experts (MoE) framework for quantitative investment to mimic the efficient bottom-up trading strategy design workflow of successful trading firms. In Stage one, multiple independent trading experts are jointly optimized with an individual uncertainty-aware loss function. In Stage two, we train neural routers (corresponding to the role of a portfolio manager) to dynamically deploy these experts on an as-needed basis. AlphaMix is also a universal framework that is applicable to various backbone network architectures with consistent performance gains. Through extensive experiments on long-term real-world data spanning over five years on two of the most influential financial\u00a0\u2026", "total_citations": {"2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:AXkvAH5U_nMC": {"external_link": "https://www.ijcai.org/proceedings/2022/0086.pdf", "authors": ["Youzhi Zhang", "Bo An", "VS Subrahmanian"], "publication_date": "2022", "conference": "31st International Joint Conference on Artificial Intelligence, IJCAI 2022", "pages": "606-612", "publisher": "International Joint Conferences on Artificial Intelligence", "description": "Efficient algorithms computing a Nash equilibrium have been successfully applied to large zerosum two-player extensive-form games (eg, poker). However, in multiplayer games, computing a Nash equilibrium is generally hard, and the equilibria are not exchangeable, which makes players face the problem of selecting one of many different Nash equilibria. In this paper, we focus on an alternative solution concept in zero-sum multiplayer extensive-form games called Team-Maxmin Equilibrium (TME). It is a Nash equilibrium that maximizes each team member\u2019s utility. As TME is unique in general, it avoids the equilibrium selection problem. However, it is still difficult (FNP-hard) to find a TME. Computing it can be formulated as a non-convex program, but existing algorithms are capable of solving this program for only very small games. In this paper, we first refine the complexity result for computing a TME by using a correlation plan to show that a TME can be found in polynomial time in a specific class of games according to our boundary for complexity. Second, we propose an efficient correlation-based algorithm to solve the non-convex program for TME in games not belonging to this class. The algorithm combines two special correlation plans based on Mc-Cormick envelopes for convex relaxation and von Stengel-Forges polytope for correlated equilibria. We show that restricting the feasible solution space to von Stengel-Forges polytope will strictly reduce the feasible solution space after convex relaxation of nonlinear terms. Finally, experiments show that our algorithm is about four orders of magnitude faster than the prior state of the art and can\u00a0\u2026", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:CaZNVDsoPx4C": {"external_link": "https://arxiv.org/abs/2012.03822", "authors": ["Xinrun Wang", "Tarun Nair", "Haoyang Li", "Yuh Sheng Reuben Wong", "Nachiket Kelkar", "Srinivas Vaidyanathan", "Rajat Nayak", "Bo An", "Jagdish Krishnaswamy", "Milind Tambe"], "publication_date": "2020/12/7", "journal": "arXiv preprint arXiv:2012.03822", "description": "Dams impact downstream river dynamics through flow regulation and disruption of upstream-downstream linkages. However, current dam operation is far from satisfactory due to the inability to respond the complicated and uncertain dynamics of the upstream-downstream system and various usages of the reservoir. Even further, the unsatisfactory dam operation can cause floods in downstream areas. Therefore, we leverage reinforcement learning (RL) methods to compute efficient dam operation guidelines in this work. Specifically, we build offline simulators with real data and different mathematical models for the upstream inflow, i.e., generalized least square (GLS) and dynamic linear model (DLM), then use the simulator to train the state-of-the-art RL algorithms, including DDPG, TD3 and SAC. Experiments show that the simulator with DLM can efficiently model the inflow dynamics in the upstream and the dam operation policies trained by RL algorithms significantly outperform the human-generated policy.", "total_citations": {"2021": 1, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:mNrWkgRL2YcC": {"external_link": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/C768D10A51110317C873CA6F83036971/stamped-9781316512920c6_103-117.pdf/protecting-coral-reef-ecosystems-via-efficient-patrols.pdf", "authors": ["Yue Yin", "Bo An"], "publication_date": "2019/3/28", "journal": "Artificial Intelligence and Conservation; Cambridge University Press: Cambridge, UK", "pages": "103", "description": "Coral reefs are valuable and fragile ecosystems that are under threat from human activities like coral mining. Many countries have built marine protected areas (MPAs) and protect their ecosystems through boat patrols. However, it remains a significant challenge to efficiently patrol the MPAs given the limited patrol resources of the protection agency and potential destructors\u2019 strategic actions. In this chapter, we view the problem of efficiently patrolling for protecting coral reef ecosystems from a game-theoretic perspective and propose (1) a new Stackelberg game model to formulate the problem of protecting MPAs,(2) two algorithms to compute the efficient protection agency\u2019s strategies: CLP, in which the protection agency\u2019s strategies are compactly represented as fractional flows in a network, and CDOG, which combines the techniques of compactly representing defender strategies and incrementally generating\u00a0\u2026", "total_citations": {"2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Ug5p-4gJ2f0C": {"external_link": "https://www.cais.usc.edu/wp-content/uploads/2018/01/equilibrium-refinement-security_0128.pdf", "authors": ["Kai Wang", "Qingyu Guo", "Phebe Vayanos", "Milind Tambe", "Bo An"], "publication_date": "2018/7/9", "conference": "AAMAS", "pages": "919-927", "description": "Stackelberg Security Games (SSG) have been successfuly applied in a variety of domains to optimize the use of limited security resources against a strategic adversary, with examples such as ARMOR for airport security [16], IRIS for security of flights [8], ports [20] and border [3, 11] patrolling, traffic enforcement [17, 18], and transit network [23]. In SSG, the defender (security agencies) protects targets using limited security resources, but allocation of resources to targets must obey many scheduling constraints. For example, some resources may be prohibited from being assigned to certain targets or may be able to cover several targets at the same time.", "total_citations": {"2019": 1, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:NDuN12AVoxsC": {"external_link": "https://books.google.com/books?hl=en&lr=&id=dHJgDwAAQBAJ&oi=fnd&pg=PR6&dq=info:YEibE7JOy0wJ:scholar.google.com&ots=cMJKNCig-p&sig=MGWPjQTlCbT2CtT83dzX_4J95cs", "authors": ["Yves Demazeau", "Bo An", "Javier Bajo", "Antonio Fern\u00e1ndez-Caballero"], "publication_date": "2018/6/19", "volume": "10978", "publisher": "Springer", "description": "This book constitutes the proceedings of the 16th International Conference on Practical Applications of Agents and Multi-Agent Systems, PAAMS 2018, held in Toledo, Spain, in June 2018. The 20 regular and 19 demo papers presented in this volume were carefully reviewed and selected from 57 submissions. They deal with the application and validation of agent-based models, methods, and technologies in a number of key applications areas, such as: energy and security; engineering and tools; evaluation and ethics; negotiation and organisations; personalization and learning; simulation applications; simulation platforms; social networks and humans. The book also contains two invited talks in full paper length.", "total_citations": {"2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:hkOj_22Ku90C": {"external_link": "https://scholar.google.com/scholar?cluster=1452902174540796792&hl=en&oi=scholarr", "authors": ["Fei Fang", "Thanh H Nguyen", "Bo An", "Milind Tambe", "Rob Pickles", "Wai Y Lam", "Gopalasamy R Clements"], "publication_date": "2015", "journal": "Workshop of Behavioral, Economic and Computational Intelligence for Security (BECIS) held at IJCAI", "description": "Based on the successful deployment of gametheoretic decision support systems in protecting critical infrastructure such as ports, air ports and trains, recent research have started focusing on Green Security Games, where the law enforcement agencies aim to protect forest, wildlife and fishery with limited patrol resources. This paper (i) lays out the challenges in Green Security Games in the wild, including the challenges in planning patrol strategies, in learning from data and in real-world deployment;(ii) outlines the lessons learned from an initial test in an protected area in Southeast Asia during which patrol routes are generated based on game-theoretic models;(iii) outlines our on-going research of address some of the challenges.", "total_citations": {"2015": 1, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Wp0gIr-vW9MC": {"external_link": "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ae68281bdff1f62fafeeaad63451cb4313eda2b9", "authors": ["Bo An", "Victor Lesser"], "publication_date": "2011/5/2", "book": "The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 3", "pages": "1101-1102", "description": "We consider the role of negotiation in deciding decommitment penalties. In our model, agents simultaneously negotiate over both the contract price and decommitment penalty in the contracting game and then decide whether to decommit from contracts in the decommitment game. Experimental results show that setting penalties through negotiation achieved higher social welfare than other exogenous penalty setting mechanisms.", "total_citations": {"2013": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:LkGwnXOMwfcC": {"external_link": "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6ceb34b9c71f1809705dca623ed010b5d8168e74", "authors": ["Bo An", "Nicola Gatti", "Victor Lesser"], "publication_date": "2010/5/10", "book": "Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume 1", "pages": "1607-1608", "description": "The problem of finding agents\u2019 rational strategies in bargaining with incomplete information is well known to be challenging. The literature provides a collection of results for very narrow uncertainty settings, but no generally applicable algorithm. In this paper, we focus on the alternating-offers finite horizon bargaining protocol with onesided uncertainty regarding agents\u2019 reserve prices. We provide an algorithm based on the combination of game theoretic analysis and search techniques which finds agents\u2019 equilibrium in pure strategies when they exist. Our approach is sound, complete and, in principle, can be applied to other uncertainty settings.", "total_citations": {"2010": 1, "2011": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:Se3iqnhoufwC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4424584/", "authors": ["Bo An", "Kwang Mong Sim", "Victor Lesser"], "publication_date": "2007/9/25", "conference": "2007 IEEE Congress on Evolutionary Computation", "pages": "1035-1042", "publisher": "IEEE", "description": "This paper designed and developed negotiation agents with the distinguishing features of 1) conducting continuous time negotiation rather than discrete time negotiation, 2) learning the response times of trading parties using Bayesian learning and, 3) deciding when to make a proposal using a multi-objective genetic algorithm (MOGA) to evolve their best-response proposing time strategies for different negotiation environments and constraints. Results from a series of experiments suggest that 1) learning trading parties\u2019 response times helps agents achieve more favorable trading results, and 2) on average, when compared with SSAs (Static Strategy Agents), BRSAs (Best-Response proposing time Strategy Agents) achieved higher average utilities, higher success rates in reaching deals, and smaller average negotiation time.", "total_citations": {"2010": 1, "2011": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:5nxA0vEk-isC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4053404/", "authors": ["Bo An", "Zhiqi Shen", "Chunyan Miao", "Lianggui Tang", "Daijie Cheng"], "publication_date": "2006/8/16", "conference": "2006 4th IEEE International Conference on Industrial Informatics", "pages": "299-304", "publisher": "IEEE", "description": "This paper presents a new fuzzy constraint based multi-issue negotiation model for agent mediated collaborative manufacturing. Unlike the related work in fuzzy constraint based negotiation models, the pressure of deadline is taken into account while designing agents' demand relaxation strategies. We give an analysis of the optimal concession strategies and the analysis shows the existence of equilibrium even when all the trading parties have incomplete information about one another. Moreover, outside options are specifically considered while analyzing agents' optimal strategies and equilibrium solutions.", "total_citations": {"2008": 1, "2009": 0, "2010": 0, "2011": 0, "2012": 0, "2013": 0, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:oi2SiIJ9l4AC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320323001371", "authors": ["Shifei Ding", "Wei Du", "Ling Ding", "Lili Guo", "Jian Zhang", "Bo An"], "publication_date": "2023/7/1", "journal": "Pattern Recognition", "volume": "139", "pages": "109436", "publisher": "Pergamon", "description": "A great deal of multi agent reinforcement learning(MARL) work has investigated how multiple agents effectively accomplish cooperative tasks utilizing value function decomposition methods. However, existing value decomposition methods can only handle cooperative tasks with shared reward, due to these methods factorize the value function from a global perspective. To tackle the competitive tasks and mixed cooperative-competitive tasks with differing individual reward setting, we design the Multi-agent Dueling Q-learning (MDQ) method based on mean-filed theory and individual value decomposition. Specifically, we integrate the mean-field theory with the value decomposition to factorize the value function at the individual level, which can deal with mixed cooperative-competitive tasks. Besides, we take a dueling network architecture to distinguish which states are valuable, eliminating the need to learn the\u00a0\u2026", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:DkZNVXde3BIC": {"external_link": "https://arxiv.org/abs/2306.10944", "authors": ["Dong Xing", "Pengjie Gu", "Qian Zheng", "Xinrun Wang", "Shanqi Liu", "Longtao Zheng", "Bo An", "Gang Pan"], "publication_date": "2023/6/19", "journal": "arXiv preprint arXiv:2306.10944", "description": "Ad hoc teamwork requires an agent to cooperate with unknown teammates without prior coordination. Many works propose to abstract teammate instances into high-level representation of types and then pre-train the best response for each type. However, most of them do not consider the distribution of teammate instances within a type. This could expose the agent to the hidden risk of \\emph{type confounding}. In the worst case, the best response for an abstract teammate type could be the worst response for all specific instances of that type. This work addresses the issue from the lens of causal inference. We first theoretically demonstrate that this phenomenon is due to the spurious correlation brought by uncontrolled teammate distribution. Then, we propose our solution, CTCAT, which disentangles such correlation through an instance-wise teammate feedback rectification. This operation reweights the interaction of teammate instances within a shared type to reduce the influence of type confounding. The effect of CTCAT is evaluated in multiple domains, including classic ad hoc teamwork tasks and real-world scenarios. Results show that CTCAT is robust to the influence of type confounding, a practical issue that directly hazards the robustness of our trained agents but was unnoticed in previous works.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:SGW5VrABaM0C": {"external_link": "https://arxiv.org/abs/2306.10458", "authors": ["Xin Cheng", "Yuzhou Cao", "Ximing Li", "Bo An", "Lei Feng"], "publication_date": "2023/6/18", "journal": "arXiv preprint arXiv:2306.10458", "description": "This paper investigates an interesting weakly supervised regression setting called regression with interval targets (RIT). Although some of the previous methods on relevant regression settings can be adapted to RIT, they are not statistically consistent, and thus their empirical performance is not guaranteed. In this paper, we provide a thorough study on RIT. First, we proposed a novel statistical model to describe the data generation process for RIT and demonstrate its validity. Second, we analyze a simple selection method for RIT, which selects a particular value in the interval as the target value to train the model. Third, we propose a statistically consistent limiting method for RIT to train the model by limiting the predictions to the interval. We further derive an estimation error bound for our limiting method. Finally, extensive experiments on various datasets demonstrate the effectiveness of our proposed method.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:3bvyWxjaHKcC": {"external_link": "https://arxiv.org/abs/2306.08968", "authors": ["Xin Cheng", "Deng-Bao Wang", "Lei Feng", "Min-Ling Zhang", "Bo An"], "publication_date": "2023/6/15", "journal": "arXiv preprint arXiv:2306.08968", "description": "Partial-label learning is a popular weakly supervised learning setting that allows each training example to be annotated with a set of candidate labels. Previous studies on partial-label learning only focused on the classification setting where candidate labels are all discrete, which cannot handle continuous labels with real values. In this paper, we provide the first attempt to investigate partial-label regression, where each training example is annotated with a set of real-valued candidate labels. To solve this problem, we first propose a simple baseline method that takes the average loss incurred by candidate labels as the predictive loss. The drawback of this method lies in that the loss incurred by the true label may be overwhelmed by other false labels. To overcome this drawback, we propose an identification method that takes the least loss incurred by candidate labels as the predictive loss. We further improve it by proposing a progressive identification method to differentiate candidate labels using progressively updated weights for incurred losses. We prove that the latter two methods are model-consistent and provide convergence analyses. Our proposed methods are theoretically grounded and can be compatible with any models, optimizers, and losses. Experiments validate the effectiveness of our proposed methods.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:An6A6Jpfc1oC": {"external_link": "https://arxiv.org/abs/2306.03552", "authors": ["Zhenghai Xue", "Qingpeng Cai", "Shuchang Liu", "Dong Zheng", "Peng Jiang", "Kun Gai", "Bo An"], "publication_date": "2023/6/6", "journal": "arXiv preprint arXiv:2306.03552", "description": "In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \\textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\\textbf{S}tate \\textbf{R}egularized \\textbf{P}olicy \\textbf{O}ptimization) algorithm. To conduct theoretical analyses, the intuition of similar environment structures is characterized by the notion of homomorphous MDPs. We then demonstrate a lower-bound performance guarantee on policies regularized by the stationary state distribution. In practice, SRPO can be an add-on module to context-based algorithms in both online and offline RL settings. Experimental results show that SRPO can make several context-based algorithms far more data efficient and significantly improve their overall performance.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:zGdJYJv2LkUC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/10098283/", "authors": ["Xiao Liu", "Shuyang Liu", "Bo An", "Yang Gao", "Shangdong Yang", "Wenbin Li"], "publication_date": "2023/4/10", "journal": "IEEE Intelligent Systems", "publisher": "IEEE", "description": "Interpretable policy distillation aims to imitate a deep reinforcement learning (DRL) policy into a self-explainable model. However, the distilled policy usually does not generalize well to complex tasks. To investigate this phenomenon, we examine the experience pools of DRL tasks and find that these interactive experience distributions are heavy tailed. However, this critical issue is largely ignored by existing approaches, and, thus, they do not fully unitize the less frequent but very critical experience points. To address this issue, we propose characterizing decision boundaries via the minimum experience retention to deal with the heavy-tailed experience distributions. Our method identifies critical experience points that are close to the model\u2019s decision boundaries, and such experience points are more critical because they portray the prerequisite of a model to take an action. As a result, our method distills the DRL\u00a0\u2026", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:w0F2JDEymm0C": {"external_link": "https://arxiv.org/abs/2302.03364", "authors": ["Pengdeng Li", "Xinrun Wang", "Shuxin Li", "Hau Chan", "Bo An"], "publication_date": "2023/2/7", "journal": "arXiv preprint arXiv:2302.03364", "description": "In this work, we attempt to bridge the two fields of finite-agent and infinite-agent games, by studying how the optimal policies of agents evolve with the number of agents (population size) in mean-field games, an agent-centric perspective in contrast to the existing works focusing typically on the convergence of the empirical distribution of the population. To this end, the premise is to obtain the optimal policies of a set of finite-agent games with different population sizes. However, either deriving the closed-form solution for each game is theoretically intractable, training a distinct policy for each game is computationally intensive, or directly applying the policy trained in a game to other games is sub-optimal. We address these challenges through the Population-size-Aware Policy Optimization (PAPO). Our contributions are three-fold. First, to efficiently generate efficient policies for games with different population sizes, we propose PAPO, which unifies two natural options (augmentation and hypernetwork) and achieves significantly better performance. PAPO consists of three components: i) the population-size encoding which transforms the original value of population size to an equivalent encoding to avoid training collapse, ii) a hypernetwork to generate a distinct policy for each game conditioned on the population size, and iii) the population size as an additional input to the generated policy. Next, we construct a multi-task-based training procedure to efficiently train the neural networks of PAPO by sampling data from multiple games with different population sizes. Finally, extensive experiments on multiple environments show the significant superiority of\u00a0\u2026", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:FiDNX6EVdGUC": {"external_link": "https://arxiv.org/abs/2302.03429", "authors": ["Rundong Wang", "Longtao Zheng", "Wei Qiu", "Bowei He", "Bo An", "Zinovi Rabinovich", "Yujing Hu", "Yingfeng Chen", "Tangjie Lv", "Changjie Fan"], "publication_date": "2023/2/7", "journal": "arXiv preprint arXiv:2302.03429", "description": "Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolving them is automatic curriculum learning (ACL). ACL involves a student (curriculum learner) training on tasks of increasing difficulty controlled by a teacher (curriculum generator). Despite its success, ACL's applicability is limited by (1) the lack of a general student framework for dealing with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity of the teacher's task due to ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Skilled Population Curriculum (SPC), which adapts curriculum learning to multi-agent coordination. Specifically, we endow the student with population-invariant communication and a hierarchical skill set, allowing it to learn cooperation and behavior skills from distinct tasks with varying numbers of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies, enabling a team of agents to change its size while still retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem and provide a corresponding regret bound. Empirical results show that our method improves the performance, scalability and sample efficiency in several MARL environments.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:VN7nJs4JPk0C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/10021858/", "authors": ["Lei Feng", "Senlin Shu", "Yuzhou Cao", "Lue Tao", "Hongxin Wei", "Tao Xiang", "Bo An", "Gang Niu"], "publication_date": "2023/1/19", "journal": "IEEE Transactions on Knowledge and Data Engineering", "publisher": "IEEE", "description": "In  multiple-instance learning  (MIL), each training example is represented by a bag of instances. A training bag is either negative if it contains no positive instances or positive if it has at least one positive instance. Previous MIL methods generally assume that training bags are fully labeled. However, the exact labels of training examples may not be accessible, due to security, confidentiality, and privacy concerns. Fortunately, it could be easier for us to access the pairwise similarity between two bags (indicating whether two bags share the same label or not) and unlabeled bags, as we do not need to know the underlying label of each bag. In this paper, we provide the first attempt to investigate MIL from only similar-dissimilar-unlabeled bags. To solve this new MIL problem, we first propose a strong baseline method that trains an instance-level classifier by employing an unlabeled-unlabeled learning strategy. Then, we\u00a0\u2026", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:-7ulzOJl1JYC": {"external_link": "https://arxiv.org/abs/2302.00586", "authors": ["Shuo Sun", "Molei Qin", "Xinrun Wang", "Bo An"], "publication_date": "2023/1/14", "journal": "arXiv preprint arXiv:2302.00586", "description": "The financial markets, which involve more than $90 trillion in market capitalization, attract the attention of innumerable investors around the world. Recently, reinforcement learning in financial markets (FinRL) emerges as a promising direction to train agents for making profitable investment decisions. However, the evaluation of most FinRL methods only focus on profit-related measures, which are far from satisfactory for practitioners to deploy these methods into real-world financial markets. Therefore, we introduce PRUDEX-Compass, which has 6 axes, i.e., Profitability, Risk-control, Universality, Diversity, rEliability, and eXplainability, with a total of 17 measures for a systematic evaluation. Specifically, i) we propose AlphaMix+ as a strong FinRL baseline, which leverages Mixture-of-Experts (MoE) and risk-10 sensitive approaches to make diversified risk-aware investment decisions, ii) we11 evaluate 8 widely used FinRL methods in 4 long-term real-world datasets of influential financial markets to demonstrate the usage of our PRUDEX-Compass, iii) PRUDEX-Compass1 together with 4 real-world datasets, standard implementation of 8 FinRL methods and a portfolio management RL environment is released as public resources to facilitate the design and comparison of new FinRL methods. We hope that PRUDEX-Compass can shed light on future FinRL research to prevent untrustworthy results from stagnating FinRL into successful industry deployment.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:1DsIQWDZLl8C": {"external_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/03a90e1bb2ceb2ea165424f2d96aa3a1-Abstract-Conference.html", "authors": ["Yuzhou Cao", "Tianchi Cai", "Lei Feng", "Lihong Gu", "Jinjie Gu", "Bo An", "Gang Niu", "Masashi Sugiyama"], "publication_date": "2022/12/6", "journal": "Advances in Neural Information Processing Systems", "volume": "35", "pages": "521-534", "description": "\\emph {Classification with rejection}(CwR) refrains from making a prediction to avoid critical misclassification when encountering test samples that are difficult to classify. Though previous methods for CwR have been provided with theoretical guarantees, they are only compatible with certain loss functions, making them not flexible enough when the loss needs to be changed with the dataset in practice. In this paper, we derive a novel formulation for CwR that can be equipped with arbitrary loss functions while maintaining the theoretical guarantees. First, we show that -class CwR is equivalent to a -class classification problem on the original data distribution with an augmented class, and propose an empirical risk minimization formulation to solve this problem with an estimation error bound. Then, we find necessary and sufficient conditions for the learning\\emph {consistency} of the surrogates constructed on our proposed formulation equipped with any classification-calibrated multi-class losses, where consistency means the surrogate risk minimization implies the target risk minimization for CwR. Finally, experiments on benchmark datasets validate the effectiveness of our proposed method.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:jU7OWUQzBzMC": {"external_link": "https://openreview.net/forum?id=D9hpqJyXAi", "authors": ["Hongxin Wei", "Lue Tao", "RENCHUNZI XIE", "Lei Feng", "Bo An"], "publication_date": "2021/10/6", "description": "Deep neural networks usually perform poorly when the training dataset suffers from extreme class imbalance. To handle this issue, popular re-sampling methods generally require in-distribution data to balance the class priors. However, obtaining suitable in-distribution data with precise labels for selected classes is challenging. In this paper, we theoretically show that out-of-distribution data (i.e., open-set samples) could be leveraged to augment the minority classes from a Bayesian perspective. Based on this motivation, we propose a novel method called Open-sampling, which utilizes open-set noisy labels to re-balance the class priors of the training dataset. For each open-set instance, the label is sampled from our pre-defined distribution that is complementary to the original class priors. Furthermore, class-dependent weights are generated to provide stronger regularization on the minority classes than on the majority classes.  We empirically show that Open-sampling not only re-balances the class prior but also encourages the neural network to learn separable representations. Extensive experiments on benchmark datasets demonstrate that our proposed method significantly outperforms existing data re-balancing methods and can be easily incorporated into existing state-of-the-art methods to enhance their performance.", "total_citations": {"2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:lvd772isFD0C": {"external_link": "https://aaai.org/ojs/index.php/ICAPS/article/view/6745", "authors": ["Aye Phyu Phyu Aung", "Xinrun Wang", "Bo An", "Xiaoli Li"], "publication_date": "2020/6/1", "journal": "Proceedings of the International Conference on Automated Planning and Scheduling", "volume": "30", "pages": "499-507", "description": "Mental health has become a major concern according to WHO who estimates that more than 350 million people worldwide are affected by depression. Studies have shown that interventions and social support can reduce stress and depression. However, counselling centers do not have enough resources to provide counselling and social support to all the participants in their interest. This paper helps social support organizations (eg, university counselling centers) sequentially select the participants for interventions. Unfortunately, previous works do not consider emotion propagation from other neighbours of the influencees and initial uncertainties of mental states and influence. Moreover, they fail to scale up to solve problems with a large number of participants due to the huge state space. Our contributions in this paper are fourfold. Firstly, we propose a new model that addresses the sequential intervention of participants while considering the propagation of emotions and formulate it as a Partially Observable Markov Decision Process (POMDP) to handle uncertainties about their mental states and the influence between them. Secondly, we apply reasoning to refine belief to improve solution quality for the lack of initial information on mental state values. Thirdly, we improve the scalability by the abstraction of states to reduce the number of states by representing the mental states with an abstracted discrete set. We further improve the scalability by multi-level partitioning to get smaller POMDPs. Finally, we conduct extensive experiments on both synthetic and real networks to show that our algorithm significantly improves scalability with comparable\u00a0\u2026", "total_citations": {"2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:NyGDZy8z5eUC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-64096-5_8", "authors": ["Feifei Lin", "Xu He", "Bo An"], "publication_date": "2020", "conference": "Distributed Artificial Intelligence: Second International Conference, DAI 2020, Nanjing, China, October 24\u201327, 2020, Proceedings 2", "pages": "103-125", "publisher": "Springer International Publishing", "description": "Coordination between multiple agents can be found in many areas of industry or society. Despite a few recent advances, this problem remains challenging due to its combinatorial nature. First, with an exponentially scaling action set, it is challenging to search effectively and find the right balance between exploration and exploitation. Second, performing maximization over all agents\u2019 actions jointly is computationally intractable. To tackle these challenges, we exploit the side information and loose couplings, i.e., conditional independence between agents, which is often available in coordination tasks. We make several key contributions in this paper. First, the repeated multi-agent coordination problem is formulated as a multi-agent contextual bandit problem to balance the exploration-exploitation trade-off. Second, a novel algorithm called MACUCB is proposed, which uses a modified zooming technique to improve\u00a0\u2026", "total_citations": {"2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:HtS1dXgVpQUC": {"external_link": "https://www.ifaamas.org/Proceedings/aamas2019/pdfs/p2259.pdf", "authors": ["Wanyuan Wang", "Zichen Dong", "Bo An", "Yichuan Jiang"], "publication_date": "2019/5/8", "conference": "AAMAS", "pages": "2259-2261", "description": "This paper uses an integer program (IP) to formulate the city-scale patrolling (CSP) problem, with the objective of maximizing the police visibility rate (PVR) and the constraint of incident response time guarantee. We decompose the original CSP into two subproblems: minimizing police problem (MinP) and maximizing PVR (MaxP) problem. A polynomial time approximation algorithm is proposed for MinP, and a polynomial time optimal algorithm is proposed for MaxP. We conduct experiments to demonstrate the efficiency of the proposed algorithm.", "total_citations": {"2019": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:5qfkUJPXOUwC": {"external_link": "https://www.cais.usc.edu/wp-content/uploads/2018/04/inducible-equilibrium-security.pdf", "authors": ["Qingyu Guo", "Jiarui Gan", "Fei Fang", "Long Tran-Thanh", "Milind Tambe", "Bo An"], "publication_date": "2018/7/9", "conference": "AAMAS", "pages": "1947-1949", "description": "Strong Stackelberg equilibrium (SSE) is the standard solution concept of Stackelberg security games. The SSE assumes that the follower breaks ties in favor of the leader and this is widely acknowledged and justified by the assertion that the defender can often induce the attacker to choose a preferred action by making an infinitesimal adjustment to her strategy. Unfortunately, in security games with resource assignment constraints, the assertion might not be valid. To overcome this issue, inspired by the notion of inducibility and the pessimistic Stackelberg equilibrium [20, 21], this paper presents the inducible Stackelberg equilibrium (ISE), which is guaranteed to exist and avoids overoptimism as the outcome can always be induced with infinitesimal strategy deviation. Experimental evaluation unveils the significant overoptimism and sub-optimality of SSE and thus, verifies the advantage of the ISE as an alternative solution concept.", "total_citations": {"2019": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:g3aElNc5_aQC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-319-51563-2_14", "authors": ["Bhargav Sosale", "Swarup Satish", "Bo An"], "publication_date": "2017", "journal": "Modern Approaches to Agent-based Complex Automated Negotiation", "pages": "191-199", "publisher": "Springer International Publishing", "description": "The 2015 edition of the Automated Negotiation Agents Competition (ANAC) was the first in its history to introduce multi-party negotiation. To this end, we present the strategy of Agent Buyog, a finalist of the competition. The strategy is based on determining which of the opponent agents is harder to strike a deal with and conceding just enough to please that opponent. This paper aims at outlining various aspects of the strategy such as opponent modeling, concession strategies, bidding strategies and acceptance criteria. It further discusses the limitations of the strategy and discusses possible improvements.", "total_citations": {"2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:XD-gHx7UXLsC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAMAS16_adauction.pdf", "authors": ["Jiang Rong", "Tao Qin", "Bo An", "Tie-Yan Liu"], "publication_date": "2016/5/9", "book": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems", "pages": "1459-1460", "description": "Generalized Second Price (GSP) mechanism is widely used in ad auctions and reserve price is an effective tool for revenue maximization. The optimal reserve price depends on bidders\u2019 value distribution, which, however, is generally unknown to auctioneers. A common practice for auctioneers is to first collect information about the value distribution by a sampling procedure and then apply the reserve price estimated with the sampled bids to the following auctions. In order to maximize his/her total revenue over finite GSP ad auctions, it is important for the auctioneer to find a proper sample size to trade off between the cost of the sampling procedure and the optimality of the estimated reserve price. We first propose the revenue bounds during and after sampling. Then we formulate the problem of finding the optimal sample size that maximizes the auctioneer\u2019s worse-case total revenue as an constrained optimization problem, the solution of which is independent of the value distribution.", "total_citations": {"2016": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:BwyfMAYsbu0C": {"external_link": "https://scholar.google.com/scholar?cluster=384064697218348428&hl=en&oi=scholarr", "authors": ["Galit Haim", "Y Gal", "S Kraus", "B An"], "publication_date": "2014", "journal": "Twenty First European Conference on AI (ECAI 2014)", "description": "This paper studies commitment strategies in a three-player game involving human players and computer agents playing equilibrium strategies. We defined a new game called the Contract Game which is analogous to a market setting in which participants need to reach agreement and commit or renege from contracts over time in order to succeed. The game comprises three players, two service providers and one customer. The service providers compete to make repeated contract offers to the customer consisting of resource exchanges in the game. We formally analyzed the game and defined equilibrium strategies for the customer and service provider in the game that are based on making contracts containing commitment offers. To evaluate computer agents that use the equilibrium strategies, we conducted extensive empirical studies in three different countries, the US, Israel and China. We ran several\u00a0\u2026", "total_citations": {"2015": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:L8Ckcad2t8MC": {"external_link": "https://link.springer.com/chapter/10.1007/11508069_66", "authors": ["Bo An", "Chunyan Miao", "Lianggui Tang", "Shuangqing Li", "Daijie Cheng"], "publication_date": "2005", "conference": "Intelligent Data Engineering and Automated Learning-IDEAL 2005: 6th International Conference, Brisbane, Australia, July 6-8, 2005. Proceedings 6", "pages": "507-514", "publisher": "Springer Berlin Heidelberg", "description": "Coalition formation in multi-agent systems (MAS) is becoming increasingly important as it increases the ability of agents to execute tasks and maximize their payoffs. This paper proposes a novel dependence theory namely transitive dependence theory for dynamic coalition formation in multi-agent system. Based on the proposed transitive dependence theory, a reasoning mechanism for searching coalition partners has been worked out which includes dependence tree generation, dependence tree reduction, plan optimization and action optimization.", "total_citations": {"2011": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:roLk4NBRz8UC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/1342970/", "authors": ["Bo An", "Lianggui Tang", "Shuangqing Li", "Daijie Cheng"], "publication_date": "2004/9/24", "conference": "Proceedings. IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2004.(IAT 2004).", "pages": "357-360", "publisher": "IEEE", "description": "This work introduces a negotiation strategy based on uncompromising degree. Agents get information of the negotiation opponents in each iteration by means of Bayesian learning mechanism, and then bring forward the proposals for the next iteration according to the negotiation strategy based on uncompromising degree. We analyze Bayesian learning mechanism and use it to get the opponents' information; introduce the negotiation strategy based on uncompromising degree; discuss how the remaining time affects negotiation strategy. Our strategy regards the whole negotiation process as a dynamic interaction process, which enhances the usage of MAS in complex and dynamic environment. The experiments show that our strategy has good negotiation performance.", "total_citations": {"2007": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:65Yg0jNCQDAC": {"external_link": "https://arxiv.org/abs/2310.03984", "authors": ["Zhenghai Xue", "Qingpeng Cai", "Tianyou Zuo", "Bin Yang", "Lantao Hu", "Peng Jiang", "Kun Gai", "Bo An"], "publication_date": "2023/10/6", "journal": "arXiv preprint arXiv:2310.03984", "description": "Growing attention has been paid to Reinforcement Learning (RL) algorithms when optimizing long-term user engagement in sequential recommendation tasks. One challenge in large-scale online recommendation systems is the constant and complicated changes in users' behavior patterns, such as interaction rates and retention tendencies. When formulated as a Markov Decision Process (MDP), the dynamics and reward functions of the recommendation system are continuously affected by these changes. Existing RL algorithms for recommendation systems will suffer from distribution shift and struggle to adapt in such an MDP. In this paper, we introduce a novel paradigm called Adaptive Sequential Recommendation (AdaRec) to address this issue. AdaRec proposes a new distance-based representation loss to extract latent information from users' interaction trajectories. Such information reflects how RL policy fits to current user behavior patterns, and helps the policy to identify subtle changes in the recommendation system. To make rapid adaptation to these changes, AdaRec encourages exploration with the idea of optimism under uncertainty. The exploration is further guarded by zero-order action optimization to ensure stable recommendation quality in complicated environments. We conduct extensive empirical analyses in both simulator-based and live sequential recommendation tasks, where AdaRec exhibits superior long-term performance compared to all baseline algorithms."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:DrR-2ekChdkC": {"external_link": "https://link.springer.com/article/10.1007/s10458-023-09620-x", "authors": ["Shijie Han", "Siyuan Li", "Bo An", "Wei Zhao", "Peng Liu"], "publication_date": "2023/10", "journal": "Autonomous Agents and Multi-Agent Systems", "volume": "37", "issue": "2", "pages": "35", "publisher": "Springer US", "description": "Multi-agent reinforcement learning (MARL) is a prevalent learning paradigm for solving stochastic games. In most MARL studies, agents in a game are defined as teammates or enemies beforehand, and the relationships among the agents (i.e., their identities) remain fixed throughout the game. However, in real-world problems, the agent relationships are commonly unknown in advance or dynamically changing. Many multi-party interactions start off by asking: who is on my team? This question arises whether it is the first day at the stock exchange or the kindergarten. Therefore, training policies for such situations in the face of imperfect information and ambiguous identities is an important problem that needs to be addressed. In this work, we develop a novel identity detection reinforcement learning (IDRL) framework that allows an agent to dynamically infer the identities of nearby agents and select an appropriate\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:PkcyUWeTMh0C": {"external_link": "https://arxiv.org/abs/2309.12891", "authors": ["Molei Qin", "Shuo Sun", "Wentao Zhang", "Haochong Xia", "Xinrun Wang", "Bo An"], "publication_date": "2023/9/22", "journal": "arXiv preprint arXiv:2309.12891", "description": "High-frequency trading (HFT) uses computer algorithms to make trading decisions in short time scales (e.g., second-level), which is widely used in the Cryptocurrency (Crypto) market (e.g., Bitcoin). Reinforcement learning (RL) in financial research has shown stellar performance on many quantitative trading tasks. However, most methods focus on low-frequency trading, e.g., day-level, which cannot be directly applied to HFT because of two challenges. First, RL for HFT involves dealing with extremely long trajectories (e.g., 2.4 million steps per month), which is hard to optimize and evaluate. Second, the dramatic price fluctuations and market trend changes of Crypto make existing algorithms fail to maintain satisfactory performance. To tackle these challenges, we propose an Efficient hieArchical Reinforcement learNing method for High Frequency Trading (EarnHFT), a novel three-stage hierarchical RL framework for HFT. In stage I, we compute a Q-teacher, i.e., the optimal action value based on dynamic programming, for enhancing the performance and training efficiency of second-level RL agents. In stage II, we construct a pool of diverse RL agents for different market trends, distinguished by return rates, where hundreds of RL agents are trained with different preferences of return rates and only a tiny fraction of them will be selected into the pool based on their profitability. In stage III, we train a minute-level router which dynamically picks a second-level agent from the pool to achieve stable performance across different markets. Through extensive experiments in various market trends on Crypto markets in a high-fidelity simulation trading\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:JTqpx9DYBaYC": {"external_link": "https://arxiv.org/abs/2309.07708", "authors": ["Haochong Xia", "Shuo Sun", "Xinrun Wang", "Bo An"], "publication_date": "2023/9/14", "journal": "arXiv preprint arXiv:2309.07708", "description": "Financial simulators play an important role in enhancing forecasting accuracy, managing risks, and fostering strategic financial decision-making. Despite the development of financial market simulation methodologies, existing frameworks often struggle with adapting to specialized simulation context. We pinpoint the challenges as i) current financial datasets do not contain context labels; ii) current techniques are not designed to generate financial data with context as control, which demands greater precision compared to other modalities; iii) the inherent difficulties in generating context-aligned, high-fidelity data given the non-stationary, noisy nature of financial data. To address these challenges, our contributions are: i) we proposed the Contextual Market Dataset with market dynamics, stock ticker, and history state as context, leveraging a market dynamics modeling method that combines linear regression and Dynamic Time Warping clustering to extract market dynamics; ii) we present Market-GAN, a novel architecture incorporating a Generative Adversarial Networks (GAN) for the controllable generation with context, an autoencoder for learning low-dimension features, and supervisors for knowledge transfer; iii) we introduce a two-stage training scheme to ensure that Market-GAN captures the intrinsic market distribution with multiple objectives. In the pertaining stage, with the use of the autoencoder and supervisors, we prepare the generator with a better initialization for the adversarial training stage. We propose a set of holistic evaluation metrics that consider alignment, fidelity, data usability on downstream tasks, and market facts. We evaluate\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:wvYxNZNCP7wC": {"external_link": "https://arxiv.org/abs/2308.11256", "authors": ["Linjian Meng", "Zhenxing Ge", "Wenbin Li", "Bo An", "Yang Gao"], "publication_date": "2023/8/22", "journal": "arXiv preprint arXiv:2308.11256", "description": "No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, which holds for both continuous and discrete-time feedback. We demonstrate that the essence of the RT framework is to transform the problem of learning NE in the original game into a series of strongly convex-concave optimization problems (SCCPs). We show that the bottleneck of RT-based algorithms is the speed of solving SCCPs. To improve the their empirical performance, we design a novel transformation method to enable the SCCPs can be solved by Regret Matching+ (RM+), a no-regret algorithm with better empirical performance, resulting in Reward Transformation RM+ (RTRM+). RTRM+ enjoys last\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:HJSXoJQnj-YC": {"external_link": "https://arxiv.org/abs/2308.08918", "authors": ["Hui Niu", "Siyuan Li", "Jiahao Zheng", "Zhouchi Lin", "Jian Li", "Jian Guo", "Bo An"], "publication_date": "2023/8/17", "journal": "arXiv preprint arXiv:2308.08918", "description": "Market making (MM) has attracted significant attention in financial trading owing to its essential function in ensuring market liquidity. With strong capabilities in sequential decision-making, Reinforcement Learning (RL) technology has achieved remarkable success in quantitative trading. Nonetheless, most existing RL-based MM methods focus on optimizing single-price level strategies which fail at frequent order cancellations and loss of queue priority. Strategies involving multiple price levels align better with actual trading scenarios. However, given the complexity that multi-price level strategies involves a comprehensive trading action space, the challenge of effectively training profitable RL agents for MM persists. Inspired by the efficient workflow of professional human market makers, we propose Imitative Market Maker (IMM), a novel RL framework leveraging both knowledge from suboptimal signal-based experts and direct policy interactions to develop multi-price level MM strategies efficiently. The framework start with introducing effective state and action representations adept at encoding information about multi-price level orders. Furthermore, IMM integrates a representation learning unit capable of capturing both short- and long-term market trends to mitigate adverse selection risk. Subsequently, IMM formulates an expert strategy based on signals and trains the agent through the integration of RL and imitation learning techniques, leading to efficient learning. Extensive experimental results on four real-world market datasets demonstrate that IMM outperforms current RL-based market making strategies in terms of several financial criteria. The\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:TlpoogIpr_IC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/25722", "authors": ["Linjian Meng", "Zhenxing Ge", "Pinzhuo Tian", "Bo An", "Yang Gao"], "publication_date": "2023/6/26", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "37", "issue": "5", "pages": "5823-5831", "description": "One of the most popular methods for learning Nash equilibrium (NE) in large-scale imperfect information extensive-form games (IIEFGs) is the neural variants of counterfactual regret minimization (CFR). CFR is a special case of Follow-The-Regularized-Leader (FTRL). At each iteration, the neural variants of CFR update the agent's strategy via the estimated counterfactual regrets. Then, they use neural networks to approximate the new strategy, which incurs an approximation error. These approximation errors will accumulate since the counterfactual regrets at iteration t are estimated using the agent's past approximated strategies. Such accumulated approximation error causes poor performance. To address this accumulated approximation error, we propose a novel FTRL algorithm called FTRL-ORW, which does not utilize the agent's past strategies to pick the next iteration strategy. More importantly, FTRL-ORW can update its strategy via the trajectories sampled from the game, which is suitable to solve large-scale IIEFGs since sampling multiple actions for each information set is too expensive in such games. However, it remains unclear which algorithm to use to compute the next iteration strategy for FTRL-ORW when only such sampled trajectories are revealed at iteration t. To address this problem and scale FTRL-ORW to large-scale games, we provide a model-free method called Deep FTRL-ORW, which computes the next iteration strategy using model-free Maximum Entropy Deep Reinforcement Learning. Experimental results on two-player zero-sum IIEFGs show that Deep FTRL-ORW significantly outperforms existing model-free neural\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:QUX0mv85b1cC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/26369", "authors": ["Shuxin Li", "Xinrun Wang", "Youzhi Zhang", "Wanqi Xue", "Jakub \u010cern\u00fd", "Bo An"], "publication_date": "2023/6/26", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "37", "issue": "10", "pages": "11586-11594", "description": "Pursuit-evasion games on graphs model the coordination of police forces chasing a fleeing felon in real-world urban settings, using the standard framework of imperfect-information extensive-form games (EFGs). In recent years, solving EFGs has been largely dominated by the Policy-Space Response Oracle (PSRO) methods due to their modularity, scalability, and favorable convergence properties. However, even these methods quickly reach their limits when facing large combinatorial strategy spaces of the pursuit-evasion games. To improve their efficiency, we integrate the pre-training and fine-tuning paradigm into the core module of PSRO--the repeated computation of the best response. First, we pre-train the pursuer's policy base model against many different strategies of the evader. Then we proceed with the PSRO loop and fine-tune the pre-trained policy to attain the pursuer's best responses. The empirical evaluation shows that our approach significantly outperforms the baselines in terms of speed and scalability, and can solve even games on street maps of megalopolises with tens of thousands of crossroads--a scale beyond the effective reach of previous methods."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:5bg8sr1QxYwC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAMAS23-Correlation.pdf", "authors": ["Youzhi Zhang", "Bo An", "VS Subrahmanian"], "publication_date": "2023/5/30", "book": "Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems", "pages": "2712-2714", "description": "Designing efficient algorithms to compute a Nash Equilibrium (NE) in multiplayer games is still an open challenge. In this paper, we focus on computing an NE that optimizes a given objective function. Finding an optimal NE in multiplayer games can be formulated as a mixed-integer bilinear program by introducing auxiliary variables to represent bilinear terms, leading to a huge number of bilinear terms, making it hard to solve. To overcome this challenge, we propose a novel algorithm called CRM based on a novel mixedinteger bilinear program with correlation plans for some subsets of players, which uses Correlation plans with their Relations to strictly reduce the feasible solution space after the convex relaxation of bilinear terms while Minimizing the number of correlation plans to significantly reduce the number of bilinear terms. Experimental results show that our algorithm can be several orders of magnitude faster than the state-of-the-art baseline."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:kzcSZmkxUKAC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAMAS23-Planning.pdf", "authors": ["Qian Che", "Wanyuan Wang", "Fengchen Wang", "Tianchi Qiao", "Xiang Liu", "Jiuchuan Jiang", "Bo An", "Yichuan Jiang"], "publication_date": "2023/5/30", "book": "Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems", "pages": "543-551", "description": "Online planning has been widely focused in many areas, such as industry chain and collective intelligence. Due to the trade-off nature of trading computation time for solution quality, Monte-Carlo tree search (MCTS) methods have shown great success in online planning. However, the exponential growth of global joint-action space makes it challenging to apply MCTS to online multiagent planning (MAP). Our goal in this paper is to design an efficient and scalable coordinated MCTS method for online MAP. Combining with coordination graphs, recent Factored Value MCTS (FV-MCTS) has attempted to recover the trade-off property for MCTS-based online MAP. However, FV-MCTS directly uses the global payoff to reward each agent, and has difficulty in finding coordination actions in multiagent MCTS settings where other agents are also taking exploratory actions. We overcome this limitation by designing a generalized structural credit assignment (SCA)-guided coordinated MCTS, where SCA is used to promote coordination and MCTS is used to search promising global joint-actions. Specially, we use the Shapley value to provide a fair SCA, which can be efficiently computed by exploiting locality of interaction between agents. Moreover, theoretical analysis shows that the proposed method can bound the bias of the estimated value of the global join-action under certain conditions. Finally, we conduct extensive experiments in some typical sequential multiagent coordination domains such as multi-robot warehouse patrolling in industry chain, etc. to validate the efficiency and scalability of the proposed method over other benchmarks.\u2217\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:kF1pexMAQbMC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAMAS23-IM.pdf", "authors": ["Haipeng Chen", "Bryan Wilder", "Wei Qiu", "Bo An", "Eric Rice", "Milind Tambe"], "publication_date": "2023/5/30", "book": "Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems", "pages": "2622-2624", "description": "Influence maximization (IM) aims to find a set of seed nodes in a social network that maximizes the influence spread. While most IM problems focus on classical influence cascades (eg, Independent Cascade and Linear Threshold) which assume individual influence cascade probability is independent of the number of neighbors, recent studies by sociologists show that many influence cascades follow a pattern called complex contagion (CC), where influence cascade probability is much higher when more neighbors are influenced. Nonetheless, there are very limited studies on complex contagion influence maximization (CCIM) problems. This is partly because CC is non-submodular, the solution of which has been an open challenge. In this study, we propose the first reinforcement learning (RL) approach to CCIM. We find that a key obstacle in applying existing RL approaches to CCIM is the reward sparseness issue, which comes from two distinct sources. We then design a new RL algorithm that uses the CCIM problem structure to address the issue. Empirical results show that our approach achieves the state-of-the-art performance on four real-world networks."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:WAzi4Gm8nLoC": {"external_link": "https://arxiv.org/abs/2303.15488", "authors": ["Renchunzi Xie", "Hongxin Wei", "Yuzhou Cao", "Lei Feng", "Bo An"], "publication_date": "2023/3/27", "journal": "arXiv preprint arXiv:2303.15488", "description": "Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability, and propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:pS0ncopqnHgC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/KDD23_PrefRec.pdf", "authors": ["Wanqi Xue", "Qingpeng Cai", "Zhenghai Xue", "Shuo Sun", "Shuchang Liu", "Dong Zheng", "Peng Jiang", "Kun Gai", "Bo An"], "publication_date": "2023", "description": "Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, longterm user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users\u2019 historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they do not require any expert knowledge. With PrefRec, we can fully exploit the advantages of RL in optimizing long-term goals, while avoiding complex reward engineering. PrefRec uses the preferences to automatically train a reward function in an end-to-end manner. The reward function is then used to generate learning signals to train the recommendation policy. Furthermore, we design an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance. We conduct experiments on a variety of long-term user engagement\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:F2UWTTQJPOcC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAAI23-FTRL.pdf", "authors": ["Linjian Meng", "Zhenxing Ge", "Pinzhuo Tian", "Bo An", "Yang Gao"], "publication_date": "2023", "description": "One of the most popular methods for learning Nash equilibrium (NE) in large-scale imperfect information extensiveform games (IIEFGs) is the neural variants of counterfactual regret minimization (CFR). CFR is a special case of Follow-The-Regularized-Leader (FTRL). At each iteration, the neural variants of CFR update the agent\u2019s strategy via the estimated counterfactual regrets. Then, they use neural networks to approximate the new strategy, which incurs an approximation error. These approximation errors will accumulate since the counterfactual regrets at iteration t are estimated using the agent\u2019s past approximated strategies. Such accumulated approximation error causes poor performance. To address this accumulated approximation error, we propose a novel FTRL algorithm called FTRL-ORW, which does not utilize the agent\u2019s past strategies to pick the next iteration strategy. More importantly, FTRL-ORW can update its strategy via the trajectories sampled from the game, which is suitable to solve large-scale IIEFGs since sampling multiple actions for each information set is too expensive in such games. However, it remains unclear which algorithm to use to compute the next iteration strategy for FTRL-ORW when only such sampled trajectories are revealed at iteration t. To address this problem and scale FTRL-ORW to large-scale games, we provide a model-free method called Deep FTRL-ORW, which computes the next iteration strategy using model-free Maximum Entropy Deep Reinforcement Learning. Experimental results on two-player zero-sum IIEFGs show that Deep FTRL-ORW significantly outperforms existing model-free neural\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:raTqNPD5sRQC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-031-25549-6_2", "authors": ["Tu Gu", "Bo An"], "publication_date": "2022/12/15", "book": "International Conference on Distributed Artificial Intelligence", "pages": "15-28", "publisher": "Springer Nature Switzerland", "description": "There has been extensive research on social dilemmas. Many models and mechanisms have been proposed to promote cooperation. In this work, we propose a three-stage social dilemma game, the Flexi Partner Selection (FPS) mechanism that can promote cooperative behaviour among agents that are trained to maximize an absolutely selfish objective function. Compared with previous works, our settings are more general and flexible as the number of players in each game is not fixed. Specifically, agents can vote out players based on their past behaviours or stay out of the game if playing the game makes them worse off. Moreover, we consider social dilemmas with both linear and non-linear payoffs. Using reinforcement learning (RL), self-interested agents are able to learn to punish defectors by consistently excluding them and cooperate with others in a number of different settings."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:-95Q15plzcUC": {"external_link": "https://openreview.net/forum?id=jv1bis_HYBL", "authors": ["Rundong Wang", "Longtao Zheng", "Wei Qiu", "Bowei He", "Bo An", "Zinovi Rabinovich", "Yujing Hu", "Yingfeng Chen", "Tangjie Lv", "Changjie Fan"], "publication_date": "2022/10/31", "description": "Recent advances in multi-agent reinforcement learning (MARL) allow agents to coordinate their behaviors in complex environments. However, common MARL algorithms still suffer from scalability and sparse reward issues. One promising approach to resolve them is automated curriculum learning (ACL), where a student (curriculum learner) train on tasks of increasing difficulty controlled by a teacher (curriculum generator). Unfortunately, in spite of its success, ACL's applicability is restricted due to: (1) lack of a general student framework to deal with the varying number of agents across tasks and the sparse reward problem, and (2) the non-stationarity in the teacher's task due to the ever-changing student strategies. As a remedy for ACL, we introduce a novel automatic curriculum learning framework, Curriculum Oriented Skills and Tactics (COST), adapting curriculum learning to multi-agent coordination. To be specific, we endow the student with population-invariant communication and a hierarchical skill set. Thus, the student can learn cooperation and behavior skills from distinct tasks with a varying number of agents. In addition, we model the teacher as a contextual bandit conditioned by student policies. As a result, a team of agents can change its size while retaining previously acquired skills. We also analyze the inherent non-stationarity of this multi-agent automatic curriculum teaching problem, and provide a corresponding regret bound. Empirical results show that our method improves scalability, sample efficiency, and generalization in MPE and Google Research Football. The source code and the video can be found at https://sites.google\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:kWvqk_afx_IC": {"external_link": "https://arxiv.org/abs/2210.09646", "authors": ["Wei Qiu", "Xiao Ma", "Bo An", "Svetlana Obraztsova", "Shuicheng Yan", "Zhongwen Xu"], "publication_date": "2022/10/18", "journal": "arXiv preprint arXiv:2210.09646", "description": "Despite the recent advancement in multi-agent reinforcement learning (MARL), the MARL agents easily overfit the training environment and perform poorly in the evaluation scenarios where other agents behave differently. Obtaining generalizable policies for MARL agents is thus necessary but challenging mainly due to complex multi-agent interactions. In this work, we model the problem with Markov Games and propose a simple yet effective method, ranked policy memory (RPM), to collect diverse multi-agent trajectories for training MARL policies with good generalizability. The main idea of RPM is to maintain a look-up memory of policies. In particular, we try to acquire various levels of behaviors by saving policies via ranking the training episode return, i.e., the episode return of agents in the training environment; when an episode starts, the learning agent can then choose a policy from the RPM as the behavior policy. This innovative self-play training framework leverages agents' past policies and guarantees the diversity of multi-agent interaction in the training data. We implement RPM on top of MARL algorithms and conduct extensive experiments on Melting Pot. It has been demonstrated that RPM enables MARL agents to interact with unseen agents in multi-agent generalization evaluation scenarios and complete given tasks, and it significantly boosts the performance up to 402% on average."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:69ZgNCALVd0C": {"external_link": "https://openreview.net/forum?id=rgp4_59eC0", "authors": ["Lipeng Wan", "Xu He", "Zeyang Liu", "Kai Li", "Mengchen Zhao", "Dong Li", "Bo An", "HAO Jianye", "Xuguang Lan"], "publication_date": "2022/9/29", "description": "Value factorization is an efficient approach for centralized training with decentralized execution in cooperative multi-agent reinforcement learning tasks. As the simplest implementation of value factorization, Linear Value Factorization (LVF) attracts wide attention. In this paper, firstly, we investigate the applicable conditions of LVF, which is important but usually neglected by previous works. We prove that due to the representation limitation, LVF is only perfectly applicable to an extremely narrow class of tasks, which we define as the decomposable Markov game. Secondly, to handle the indecomposable Markov game where the LVF is inapplicable, we turn to value factorization with complete representation capability (CRC) and explore the general form of the value factorization function that satisfies both Independent Global Max (IGM) and CRC conditions. A common problem of these value factorization functions is the representation interference among true Q values with shared local Q value functions. As a result, the policy could be trapped in local optimums due to the representation interference on the optimal true Q values. Thirdly, to address the problem, we propose a novel value factorization method, namely Q Factorization with Representation Interference Suppression (QFRIS).  QFRIS adaptively reduces the gradients of the local Q value functions contributed by the non-optimal true Q values. Our method is evaluated on various benchmarks. Experimental results demonstrate the good convergence of QFIRS."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:mWEH9CqjF64C": {"external_link": "https://openreview.net/forum?id=GX0uI5T8kd", "authors": ["Pengjie Gu", "Mengchen Zhao", "HAO Jianye", "Bo An"], "publication_date": "2022/9/29", "description": "Off-policy evaluation (OPE) aims to estimate the online performance of target policies given dataset collected by some behavioral policies. OPE is crucial in many applications where online policy evaluation is expensive. However, existing OPE methods are far from reliable. Fortunately, in many real-world scenarios, we care only about the ranking of the evaluating policies, rather than their exact online performance. Existing works on off-policy ranking (OPR) adopt a supervised training paradigm, which assumes that there are plenty of deployed policies and the labels of their performance are available. However, this assumption does not apply to most OPE scenarios because collecting such training data might be highly expensive. In this paper, we propose a novel OPR framework called SOCCER, where the existing OPE methods are modeled as workers in a crowdsourcing system. SOCCER can be trained in a self-supervised way as it does not require any ground-truth labels of policies. Moreover, in order to capture the relative discrepancies between policies, we propose a novel transformer-based architecture to learn effective pairwise policy representations. Experimental results show that SOCCER achieves significantly high accuracy in a variety of OPR tasks. Surprisingly, SOCCER even performs better than baselines trained in a supervised way using additional labeled data, which further demonstrates the superiority of SOCCER in OPR tasks."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:ziOE8S1-AIUC": {"external_link": "https://openreview.net/forum?id=HnSceSzlfrY", "authors": ["Wei Qiu", "Xiao Ma", "Bo An", "Svetlana Obraztsova", "YAN Shuicheng", "Zhongwen Xu"], "publication_date": "2022/9/29", "conference": "The Eleventh International Conference on Learning Representations", "description": "Despite the recent advancement in multi-agent reinforcement learning (MARL), the MARL agents easily overfit the training environment and perform poorly in evaluation scenarios where other agents behave differently. Obtaining generalizable policies for MARL agents is thus necessary but challenging mainly due to complex multi-agent interactions. In this work, we model the MARL problem with Markov Games and propose a simple yet effective method, called ranked policy memory (RPM), i.e., to maintain a look-up memory of policies to achieve good generalizability. The main idea of RPM is to train MARL policies via gathering massive multi-agent interaction data. In particular, we first rank each agent\u2019s policies by its training episode return, i.e., the episode return of each agent in the training environment; we then save the ranked policies in the memory; when an episode starts, each agent can randomly select a policy from the RPM as the behavior policy. Each agent uses the behavior policy to gather multi-agent interaction data for MARL training. This innovative self-play framework guarantees the diversity of multi-agent interaction in the training data. Experimental results on Melting Pot demonstrate that RPM enables MARL agents to interact with unseen agents in multi-agent generalization evaluation scenarios and complete given tasks. It significantly boosts the performance up to 818% on average."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:-jrNzM816MMC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3490486.3538350", "authors": ["Jakub \u010cern\u00fd", "Bo An", "Allan N Zhang"], "publication_date": "2022/7/12", "book": "Proceedings of the 23rd ACM Conference on Economics and Computation", "pages": "210-239", "description": "Correlated equilibrium is an established solution concept in game theory describing a situation when players condition their strategies on external signals produced by a correlation device. In recent years, the concept has begun gaining traction also in general artificial intelligence because of its suitability for studying coordinated multi-agent systems. Yet the original formulation of correlated equilibrium assumes entirely rational players and hence fails to capture the subrational behavior of human decision-makers. We investigate the analogue of quantal response for correlated equilibrium, which is among the most commonly used models of bounded rationality. We coin the solution concept the quantal correlated equilibrium and study its relation to quantal response and correlated equilibria. The definition corroborates with prior conception as every quantal response equilibrium is a quantal correlated equilibrium\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:jFemdcug13IC": {"external_link": "https://arxiv.org/abs/2205.13718", "authors": ["Wei Qiu", "Weixun Wang", "Rundong Wang", "Bo An", "Yujing Hu", "Svetlana Obraztsova", "Zinovi Rabinovich", "Jianye Hao", "Yingfeng Chen", "Changjie Fan"], "publication_date": "2022/5/27", "journal": "arXiv preprint arXiv:2205.13718", "description": "We investigate model-free multi-agent reinforcement learning (MARL) in environments where off-beat actions are prevalent, i.e., all actions have pre-set execution durations. During execution durations, the environment changes are influenced by, but not synchronised with, action execution. Such a setting is ubiquitous in many real-world problems. However, most MARL methods assume actions are executed immediately after inference, which is often unrealistic and can lead to catastrophic failure for multi-agent coordination with off-beat actions. In order to fill this gap, we develop an algorithmic framework for MARL with off-beat actions. We then propose a novel episodic memory, LeGEM, for model-free MARL algorithms. LeGEM builds agents' episodic memories by utilizing agents' individual experiences. It boosts multi-agent learning by addressing the challenging temporal credit assignment problem raised by the off-beat actions via our novel reward redistribution scheme, alleviating the issue of non-Markovian reward. We evaluate LeGEM on various multi-agent scenarios with off-beat actions, including Stag-Hunter Game, Quarry Game, Afforestation Game, and StarCraft II micromanagement tasks. Empirical results show that LeGEM significantly boosts multi-agent coordination and achieves leading performance and improved sample efficiency."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:3NQIlFlcGxIC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0925231222000467", "authors": ["Aye Phyu Phyu Aung", "Senthilnath Jayavelu", "Xiaoli Li", "Bo An"], "publication_date": "2022/4/7", "journal": "Neurocomputing", "volume": "481", "pages": "182-192", "publisher": "Elsevier", "description": "Studies, with the increasing concern for mental health, have shown that interventions along with social support can reduce stress and depression. However, counselling centers do not have enough resources to provide counselling and social support to all the participants in their interest. This paper helps social support organizations (e.g., university counselling centers) sequentially select the participants for interventions. Meanwhile, Deep Reinforcement Learning (DRL) has shown significant success in learning an efficient policy for sequential decision-making problems in both fully observable environments and partially observable environments with small action space. In this paper, we consider emotion propagation from other neighbours of the influencees, initial uncertainties of mental states and influence in the student network. We propose a new architecture called DRLPSO (Deep Reinforcement Learning with\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:anf4URPfarAC": {"external_link": "https://scholar.google.com/scholar?cluster=6105452282317105735&hl=en&oi=scholarr", "authors": ["Bo An"], "publication_date": "2018/10/1", "source": "Artificial Intelligence", "volume": "263", "pages": "1-2", "publisher": "Elsevier", "description": "The last few years have witnessed significant AI research progress in many domains including vision, natural language processing, security, and games such as Go and Poker. Though many impressive milestones have been achieved, current AI successes mainly focus on tasks where the interaction with human beings plays a marginal role (if any) in the system's design and operation. Given that part of the ultimate goal of AI is helping people (eg, to make better decisions), effective AI systems need to proficiently interact with human beings (eg, virtual assistants Siri and Cortana, intelligent smart home assistants, robotic assistants for elderly, just to name a few). Thus, it is imperative to develop appropriate AI techniques and methodologies necessary for modeling and predicting human decisions and behavior. Unfortunately, many challenges arise when considering the above challenge as people's decisions and\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:L7CI7m0gUJcC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/AAMAS17_cloud.pdf", "authors": ["Jiang Rong", "Tao Qin", "Bo An", "Tie-Yan Liu"], "publication_date": "2017/5/8", "book": "Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems", "pages": "1719-1721", "description": "The market for selling reusable products is growing rapidly. Existing works for policy optimization often ignore the dynamic property of demand and the competition among providers. This paper studies service providers\u2019 dynamic pricing in consideration of market competition and dynamics, which makes two key contributions. First, we propose a comprehensive model that takes into account the dynamic demand under market competition and formulate the optimal pricing policy as an equilibrium. Second, as it is difficult to compute the Nash equilibrium due to incomplete information and implicit revenue function, we develop an efficient algorithm to calculate an approximate equilibrium, which is more practical in the real world. The experiments show that the proposed policy outperforms existing strategies and the incentive to deviate the approximate equilibrium is small."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:t7zJ5fGR-2UC": {"external_link": "https://www.academia.edu/download/77683875/QingyuGuo_SecMAS.pdf", "authors": ["Qingyu Guo", "Bo An", "Yevgeniy Vorobeychik", "Long Tran-Thanh", "Jiarui Gan", "Chunyan Miao"], "publication_date": "2016", "description": "Game theoretic models of security, and associated computational methods, have emerged as critical components of security posture across a broad array of domains, including airport security and coast guard. These approaches consider terrorists as motivated but independent entities. There is, however, increasing evidence that attackers, be it terrorists or cyber attackers, communicate extensively and form coalitions that can dramatically increase their ability to achieve malicious goals. To date, such cooperative decision making among attackers has been ignored in the security games literature. To address the issue of cooperation among attackers, we introduce a novel coalitional security game (CSG) model. A CSG consists of a set of attackers connected by a (communication or trust) network who can form coalitions as connected subgraphs of this network so as to attack a collection of targets. A defender in a CSG can delete a set of edges, incurring a cost for deleting each edge, with the goal of optimally limiting the attackers\u2019 ability to form effective coalitions (in terms of successfully attacking high value targets). We first show that a CSG is, in general, hard to approximate. Nevertheless, we develop a novel branch and price algorithm, leveraging a combination of column generation, relaxation, greedy approximation, and stabilization methods to enable scalable high-quality approximations of CSG solutions on realistic problem instances."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:uWQEDVKXjbEC": {"external_link": "https://aamas.csc.liv.ac.uk/Proceedings/aamas2014/aamas/p1473.pdf", "authors": ["Yue Yin", "Bo An", "Manish Jain"], "publication_date": "2014/5/5", "book": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems", "pages": "1473-1474", "description": "Large scale public events are attractive targets for terrorist attacks. It is of great significance to intelligently allocate limited security resources to protect such events. In most public events, the impact of an attack at different targets changes over time. For instance, in marathon, the impact of an attack at different locations changes over time as the participants move along the race course. In addition, the police can relocate security resources among potential attacked targets at any time during the event and an attacker may act at any time, thus the strategy spaces of both agents are continuous. Furthermore, a certain kind of public events, eg, the Olympic Games, is usually held infrequently. Thus the attacker does not get an opportunity to conduct surveillance and respond to a distribution of defender strategies. In this paper, we aim to address the security resource allocation problem in public events domain with time-critical payoff, continuous strategy spaces, and low frequency."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:HIFyuExEbWQC": {"external_link": "https://www.osti.gov/servlets/purl/1118356", "authors": ["Yevgeniy Vorobeychik", "Bo An", "Milind Tambe"], "publication_date": "2012/1/1", "issue": "SAND2012-0353C", "publisher": "Sandia National Lab.(SNL-CA), Livermore, CA (United States)", "description": "Defender-Attacker Stackelberg games are the foundations of tools deployed for computing optimal patrolling strategies in adversarial domains such as the US Federal Air Marshals Service and the US Coast Guard, among others. In Stackelberg security game models the attacker knows only the probability that each target is covered by the defender, but is oblivious to the detailed timing of the coverage schedule. In many real-world situations, however, the attacker can observe the current location of the defender and can exploit this knowledge to reason about the defender\u2019s future moves. We study Stackelberg security games in which the defender sequentially moves between targets, with moves constrained by a graph, while the attacker can observe the defender\u2019s current location and his (stochastic) policy concerning future moves. We offer six contributions:(1) We model this adversarial patrolling game as a stochastic game and present a non-linear programming (NLP) formulation. We show that our formulation yields significantly better solutions than previous approaches.(2) We extend the initial formulation to allow arbitrary number of defender resources and an arbitrary number of steps for attacks to unfold.(3) We provide a MILP approximation that uses discrete defender move probabilities.(4) We experimentally demonstrate the efficacy of our NLP-based approach.(5) We extend our model to allow the defender to construct the graph constraining his moves, at some cost, and offer NLP and MILP formulations for this setting.(6) We present an alternative model in which we replace graph constraints on defender moves with transition costs, and\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:hqOjcs7Dif8C": {"external_link": null, "authors": ["Bo An", "Lianggui Tang", "Shuangqing Li", "Daijie Cheng"], "publication_date": "2005/9", "journal": "Computer Engineering. Vol31 No17"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:IsPWOBWtZBwC": {"external_link": null}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:SnGPuo6Feq8C": {"external_link": "https://scholar.google.com/scholar?cluster=10174281121055216485&hl=en&oi=scholarr", "authors": ["P Serafino", "C Ventre", "L Tran-thanh", "J Zhang", "B An", "N Jennings", "Smart Route Guidance In AC Nayak", "A Sharma"], "description": "We model and study the problem of assigning traffic in an urban road network infrastructure. In our model, each driver submits their intended destination and is assigned a route to follow that minimizes the social cost (ie, travel distance of all the drivers). We assume drivers are strategic and try to manipulate the system (ie, misreport their intended destination and/or deviate from the assigned route) if they can reduce their travel distance by doing so. Such strategic behavior is highly undesirable as it can lead to an overall suboptimal traffic assignment and cause congestion. To alleviate this problem, we develop moneyless mechanisms that are resilient to manipulation by the agents and offer provable approximation guarantees on the social cost obtained by the solution. We then empirically test the mechanisms studied in the paper, showing that they can be effectively used in practice in order to compute manipulation\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:eO3_k5sD8BwC": {"external_link": "https://personal.ntu.edu.sg/boan/papers/IJCAI23_BLO.pdf", "authors": ["Hao Cheng", "Shufeng Kong", "Yanchen Deng", "Caihua Liu", "Xiaohu Wu", "Bo An", "Chongjun Wang"], "description": "Core-selecting combinatorial auctions (CAs) restrict the auction result in the core such that no coalitions could improve their utilities by engaging in collusion. The minimum-revenue-core (MRC) rule is a widely used core-selecting payment rule to maximize the total utilities of all bidders. However, the MRC rule can suffer from severe unfairness since it ignores individuals\u2019 utilities. To address this limitation, we propose to explore the leximin principle to achieve fairness in core-selecting CAs since the leximin principle prefers to maximize the utility of the worst-off; the resulting bidderleximin-optimal (BLO) payment rule is then theoretically analyzed and an effective algorithm is further provided to compute the BLO outcome. Moreover, we conduct extensive experiments to show that our algorithm returns fairer utility distributions and is faster than existing algorithms of coreselecting payment rules."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:EPG8bYD4jVwC": {"external_link": "https://teamcore.seas.harvard.edu/sites/projects.iq.harvard.edu/files/teamcore/files/ijcai_23_rl4ccim.pdf", "authors": ["Haipeng Chen", "Bryan Wilder", "Wei Qiu", "Bo An", "Eric Rice", "Milind Tambe"], "description": "In influence maximization (IM), the goal is to find a set of seed nodes in a social network that maximizes the influence spread. While most IM problems focus on classical influence cascades (eg, Independent Cascade and Linear Threshold) which assume individual influence cascade probability is independent of the number of neighbors, recent studies by sociologists show that many influence cascades follow a pattern called complex contagion (CC), where influence cascade probability is much higher when more neighbors are influenced. Nonetheless, there are very limited studies for complex contagion influence maximization (CCIM) problems. This is partly because CC is non-submodular, the solution of which has been an open challenge. In this study, we propose the first reinforcement learning (RL) approach to CCIM. We find that a key obstacle in applying existing RL approaches to CCIM is the reward sparseness issue, which comes from two distinct sources. We then design a new RL algorithm that uses the CCIM problem structure to address the issue. Empirical results show that our approach achieves the state-of-the-art performance on 9 realworld networks."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:QyXJ3EUuO1IC": {"external_link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4089384", "authors": ["Bo An", "Timothy Sikorsiki", "John F Kellie", "Zhuo Chen", "Nicole Schneck", "John Mehl", "Huaping Tang", "Jun Qu", "Tujin Shi", "Yuqian Gao", "Jon M Jacobs", "Eshani Nandita", "Remco van Soest", "Elliott Jones"], "journal": "Sensitive Quantification of Protein Biomarkers in Complex Biomatrices", "description": "Sensitive, multiplexed protein quantification remains challenging despite recent advancements in LC-MS assays for targeted protein biomarker quantification. High-sensitivity protein biomarker measurements usually require immuno-affinity enrichment of target protein; a process which is highly dependent on capture reagent and limited in capability to measure multiple analytes. Herein, we report a novel antibody-free platform, which measures multiple biomarkers from complex matrices employing a strategically optimized solid-phase extraction cleanup and orthogonal multidimensional LC-MS. Eight human protein biomarkers with different specifications were spiked into dog plasma as a model investigation system. The developed strategy achieved the desired sensitivity, robustness, and throughput via the following steps:(1) post digestion mixed-mode cation exchange-reverse phase SPE enrichment to remove the majority of non-peptide components;(2) rapid, high-pH peptide fractionation to further eliminate background components efficiently while selectively enriching signature peptides (SP) to provide sufficient sensitivity for multiple targets; and (3) trapping-micro-LC-MS analysis, which achieves high sensitivity comparable to a nano-LC-MS method but with much better robustness and throughput for the final analysis. Compared with a conventional LC-MS assay with direct protein digestion and limited clean-up, analysis with this antibody-free platform improved the LLOQ by 1-2 orders of magnitude for the eight protein biomarkers, reaching as low as 5 ng/mL in plasma, with feasible robustness and throughput. This platform was applied for the\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=PEEpuNwAAAAJ&cstart=200&pagesize=100&citation_for_view=PEEpuNwAAAAJ:HGTzPopzzJcC": {"external_link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4156476", "authors": ["Dong Hao", "Qi Shi", "Jinyan Su", "Bo An"], "journal": "Available at SSRN 4156476", "description": "Revision game is a very new model formulating the real-time situation where players dynamically prepare and revise their actions in advance before a deadline when payoffs are realized. It is at the cutting edge of dynamic game theory and can be applied in many real-world scenarios, such as eBay auction, stock market, election, online games, etc. In this work, we novelly identify a class of strategies for revision games which are called Limited Retaliation strategies. An limited retaliation strategy stipulates that,(i) players first follow a recommended cooperative plan;(ii) if anyone deviates from the plan, the limited retaliation player retaliates by using the defection action for a limited duration;(iii) after the retaliation, the limited retaliation player returns to the cooperative plan. A limited retaliation strategy has three key features. It is cooperative, sustaining a high level of social welfare. It is vengeful, deterring the opponent from betrayal by threatening with a future retaliation. It is yet forgiving, since it resumes cooperation after a proper retaliation. The cooperativeness and vengefulness make it constitute cooperative subgame perfect equilibrium, while the forgiveness makes it tolerate occasional mistakes. limited retaliation strategies show significant advantages over Grim Trigger, which is currently the only known strategy for revision games. Besides its contribution as a new robust and welfare-optimizing equilibrium strategy, our results about limited retaliation strategy can also be used to explain how easy cooperation can happen, and why forgiveness emerges in real-world multi-agent interactions. In the application aspect, limited retaliation strategies are\u00a0\u2026"}}