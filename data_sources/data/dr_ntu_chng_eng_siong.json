{"full_name": "Chng Eng Siong", "designation": "Associate Professor, School of Computer Science and Engineering", "name_card": "Assoc Prof Chng Eng Siong", "email": "aseschng@ntu.edu.sg", "keywords": ["Computational Intelligence", "Smart Cities", "Artificial and Augmented Intelligence"], "biography": "Dr Chng Eng Siong is currently an Associate Professor in the School of Computer Science and Engineering (SCSE), Nanyang Technological University (NTU), Singapore. Prior to joining NTU in 2003, he has worked in several research centers/companies, namely: Knowles Electronics (USA), Lernout and Hauspie (Belgium), Institute of Infocomm Research (I2R, Singapore), and RIKEN (Japan). He received both PhD and BEng (Hons) from Edinburgh University, U.K. in 1996 and 1991 respectively. His area of focus is in speech research, machine learning and signal processing. To date, he has been Principal Investigator of research grants awarded by AI-SG, NTU-Rolls Royce, Mindef, MOE and AStar with a total funding amount of over S$9 million under the \u201cSpeech and Language Technology Program (SLTP)\u201d at SCSE. He has graduated more than 15 PhD students and 8 Masters Engineering students. His publications include 2 edited books and over 100 journal/conference papers. He has served as the publication chair for 5 international conferences (Human Agent Interaction 2016, INTERSPEECH 2014, APSIPA-2010, APSIPA-2011, ISCSLP-2006) and local organizing committee in ASRU 2019.", "interests": "Specialisations include: speech  recognition/enhancement, end-to-end ASR, signal processing, microphone arrays and classifications using machine learning", "grants": "Artificial Intelligence-based advisory system for blood sugar management in elderly diabeticsDevelopment of Medical AI Speech-to-Text Engine for Implementation in the Emergency Department and Acute WardISSACMulti-Channel Far-field Speaker Diarization for Meeting Rooms with Overlap Speaker DetectionProject ANPASSENSASEAF FY22 Dr Luong Hieu Thi (Voice Anti-Spoofing, Fake Speech Detection)SingaKids Pic2Speak: Multilingual AI Tutor \u2013 Uplifting Singapore\u2019s Bilingual EdgeSingle-Channel Far-field Speaker Diarization for Interview Rooms with Far-field Voice Activity DetectionStrategic Centre For Research In Privacy-Preserving Technologies & Systems (SCRIPTS)Voice command recognition under noisy conditions", "articles": ["Wu, Z., Virtanen, T., Chng, E. S., & Li, H. (2014). Exemplar-based sparse representation with residual compensation for voice conversion. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(10), 1506-1521.", "Chng, E. S., Chen, S., & Mulgrew, B. (1996). Gradient radial basis function networks for nonlinear and nonstationary time series prediction. IEEE Transactions on Neural Networks, 7(1), 190-194.", "Dennis, J., Tran, H. D., & Chng, E. S. (2012). Image feature representation of the subband power distribution for robust sound event classification. IEEE Transactions on Audio, Speech, and Language Processing, 21(2), 367-377.", "Xiao, X., Chng, E. S., & Li, H. (2008). Normalization of the speech modulation spectra for robust speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 16(8), 1662-1674.", "Dennis, J., Tran, H. D., & Chng, E. S. (2013). Overlapping sound event recognition using local spectrogram features and the generalised hough transform. Pattern Recognition Letters, 34(9), 1085-1093.", "Xu, C., Rao, W., Chng, E. S., & Li, H. (2020). Spex: Multi-scale time domain speaker extraction network. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28, 1370-1384.", "Tian, X., Lee, S. W., Wu, Z., Chng, E. S., & Li, H. (2017). An exemplar-based approach to frequency warping for voice conversion. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25(10), 1863-1876.", "Lyu, D. C., Tan, T. P., Chng, E. S., & Li, H. (2015). Mandarin\u2013English code-switching speech corpus in South-East Asia: SEAME. Language Resources and Evaluation, 49(3), 581-600.", "Mustafa, M. B., Salim, S. S., Mohamed, N., Al-Qatab, B., & Siong, C. E. (2014). Severity-based adaptation with limited data for ASR to aid dysarthric speakers. PLoS ONE, 9(1), e86285.", "Xiao, X., Chng, E. S., & Li, H. (2007). Temporal structure normalization of speech feature for robust speech recognition. IEEE Signal Processing Letters, 14(7), 500-503.", "Click  here for more publications.", "Liu, H., Perera, L. P. G., Khong, A. W., Chng, E. S., Styles, S. J., & Khudanpur, S. (2022). Efficient Self-supervised Learning Representations for Spoken Language Identification. IEEE Journal of Selected Topics in Signal Processing.", "Guo, L., Wang, L., Dang, J., Chng, E. S., & Nakagawa, S. (2022). Learning affective representations based on magnitude and dynamic relative phase information for speech emotion recognition. Speech Communication, 136, 118-127.", "Ghose, U., Srinivasan, A. A., Boyce, W. P., Xu, H., & Chng, E. S. (2020). PyTrack: An end-to-end analysis toolkit for eye tracking. Behavior Research Methods, 52(6), 2588-2603.", "Xu, C., Rao, W., Chng, E. S., & Li, H. (2020). Spex: Multi-scale time domain speaker extraction network. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28, 1370-1384.", "Xu, H., Xiao, X., Chen, N. F., Chng, E. S., & Li, H. (2018). Re-ranking spoken term detection with acoustic exemplars of keywords. Speech Communication, 104, 12-23."], "books": [], "book_chapters": [], "conferences": [], "bibliometrics": {"google_scholar": "https://scholar.google.com/citations?user=FJodrCcAAAAJ", "scopus": "https://www.scopus.com/authid/detail.uri?authorId=24474196000"}, "google_scholar": "https://scholar.google.com/citations?user=FJodrCcAAAAJ", "orcid": "https://orcid.org/0000-0001-6257-7399", "github": null, "scopus": "https://www.scopus.com/authid/detail.uri?authorId=24474196000", "web_of_science": null, "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp00098", "other_websites": ["https://personal.ntu.edu.sg/aseschng/default.html"]}