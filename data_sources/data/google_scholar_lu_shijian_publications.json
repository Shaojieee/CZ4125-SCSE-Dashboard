{"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:YB4bud6kWLwC": {"external_link": "https://arxiv.org/abs/2306.10502", "authors": ["Gongjie Zhang", "Jiahao Lin", "Shuang Wu", "Yilin Song", "Zhipeng Luo", "Yang Xue", "Shijian Lu", "Zuoguan Wang"], "publication_date": "2023/12/11", "journal": "Neural Information Processing Systems (NeurIPS)", "description": "Vectorized high-definition (HD) map is essential for autonomous driving, providing detailed and precise environmental information for advanced perception and planning. However, current map vectorization methods often exhibit deviations, and the existing evaluation metric for map vectorization lacks sufficient sensitivity to detect these deviations. To address these limitations, we propose integrating the philosophy of rasterization into map vectorization. Specifically, we introduce a new rasterization-based evaluation metric, which has superior sensitivity and is better suited to real-world autonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization via Rasterization), a novel framework that applies differentiable rasterization to vectorized outputs and then performs precise and geometry-aware supervision on rasterized HD maps. Notably, MapVR designs tailored rasterization strategies for various geometric shapes, enabling effective adaptation to a wide range of map elements. Experiments show that incorporating rasterization into map vectorization greatly enhances performance with no extra computational cost during inference, leading to more accurate map perception and ultimately promoting safer autonomous driving.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:mel-f30kHHgC": {"external_link": "https://arxiv.org/abs/2305.14093", "authors": ["Kunhao Liu", "Fangneng Zhan", "Jiahui Zhang", "Muyu Xu", "Yingchen Yu", "Abdulmotaleb El Saddik", "Christian Theobalt", "Eric Xing", "Shijian Lu"], "publication_date": "2023/12/11", "journal": "Neural Information Processing Systems (NeurIPS)", "description": "Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.", "total_citations": {"2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:RuPIJ_LgqDgC": {"external_link": "https://arxiv.org/abs/2304.08938", "authors": ["Rongliang Wu", "Yingchen Yu", "Fangneng Zhan", "Jiahui Zhang", "Shengcai Liao", "Shijian Lu"], "publication_date": "2023/10/18", "journal": "IEEE Transactions on Image Processing", "description": "Facial expression editing has attracted increasing attention with the advance of deep neural networks in recent years. However, most existing methods suffer from compromised editing fidelity and limited usability as they either ignore pose variations (unrealistic editing) or require paired training data (not easy to collect) for pose controls. This paper presents POCE, an innovative pose-controllable expression editing network that can generate realistic facial expressions and head poses simultaneously with just unpaired training images. POCE achieves the more accessible and realistic pose-controllable expression editing by mapping face images into UV space, where facial expressions and head poses can be disentangled and edited separately. POCE has two novel designs. The first is self-supervised UV completion that allows to complete UV maps sampled under different head poses, which often suffer from self-occlusions and missing facial texture. The second is weakly-supervised UV editing that allows to generate new facial expressions with minimal modification of facial identity, where the synthesized expression could be controlled by either an expression label or directly transplanted from a reference UV map via feature transfer. Extensive experiments show that POCE can learn from unpaired face images effectively, and the learned model can generate realistic and high-fidelity facial expressions under various new poses."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:BJrgspguQaEC": {"external_link": "https://arxiv.org/abs/2304.08799", "authors": ["Siyuan Yang", "Jun Liu", "Shijian Lu", "Er Meng Hwa", "Yongjian Hu", "Alex C Kot"], "publication_date": "2023/10/12", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in press", "description": "3D Skeleton-based human action recognition has attracted increasing attention in recent years. Most of the existing work focuses on supervised learning which requires a large number of labeled action sequences that are often expensive and time-consuming to annotate. In this paper, we address self-supervised 3D action representation learning for skeleton-based action recognition. We investigate self-supervised representation learning and design a novel skeleton cloud colorization technique that is capable of learning spatial and temporal skeleton representations from unlabeled skeleton sequence data. We represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. Specifically, we design a two-steam pretraining network that leverages fine-grained and coarse-grained colorization to learn multi-scale spatial-temporal features.In addition, we design a Masked Skeleton Cloud Repainting task that can pretrain the designed auto-encoder framework to learn informative representations. We evaluate our skeleton cloud colorization approach with linear classifiers trained under different configurations, including unsupervised, semi-supervised, fully-supervised, and transfer learning settings. Extensive experiments on NTU RGB+D, NTU RGB+D 120, PKU-MMD, NW-UCLA, and UWA3D datasets show that the proposed method\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:zUl2_INMlC4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/10268350/", "authors": ["Yingjian Li", "Jiaxing Huang", "Shijian Lu", "Zheng Zhang", "Guangming Lu"], "publication_date": "2023/9/29", "journal": "IEEE Transactions on Image Processing", "publisher": "IEEE", "description": "Unsupervised cross-domain Facial Expression Recognition (FER) aims to transfer the knowledge from a labeled source domain to an unlabeled target domain. Existing methods strive to reduce the discrepancy between source and target domain, but cannot effectively explore the abundant semantic information of the target domain due to the absence of target labels. To this end, we propose a novel framework via Contrastive Warm up and Complexity-aware Self-Training (namely CWCST), which facilitates source knowledge transfer and target semantic learning jointly. Specifically, we formulate a contrastive warm up strategy via features, momentum features, and learnable category centers to concurrently learn discriminative representations and narrow the domain gap, which benefits domain adaptation by generating more accurate target pseudo labels. Moreover, to deal with the inevitable noise in pseudo labels\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:nPT8s1NX_-sC": {"external_link": "https://arxiv.org/abs/2309.13505", "authors": ["Yun Xing", "Jian Kang", "Aoran Xiao", "Jiahao Nie", "Ling Shao", "Shijian Lu"], "publication_date": "2023/9/24", "journal": "Neural Information Processing Systems (NeurIPS)", "description": "Vision-Language Pre-training has demonstrated its remarkable zero-shot recognition ability and potential to learn generalizable visual representations from language supervision. Taking a step ahead, language-supervised semantic segmentation enables spatial localization of textual inputs by learning pixel grouping solely from image-text pairs. Nevertheless, the state-of-the-art suffers from clear semantic gaps between visual and textual modality: plenty of visual concepts appeared in images are missing in their paired captions. Such semantic misalignment circulates in pre-training, leading to inferior zero-shot performance in dense predictions due to insufficient visual concepts captured in textual representations. To close such semantic gap, we propose Concept Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing semantics. For each image-text pair, we establish a concept archive that maintains potential visually-matched concepts with our proposed vision-driven expansion and text-to-vision-guided ranking. Relevant concepts can thus be identified via cluster-guided sampling and fed into pre-training, thereby bridging the gap between visual and textual semantics. Extensive experiments over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb zero-shot transfer performance and greatly boosts language-supervised segmentation baseline by a large margin, suggesting the value of bridging semantic gap in pre-training data."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hQUaER0FWQ4C": {"external_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Domain_Generalization_via_Balancing_Training_Difficulty_and_Model_Capability_ICCV_2023_paper.html", "authors": ["Xueying Jiang", "Jiaxing Huang", "Sheng Jin", "Shijian Lu"], "publication_date": "2023/9/2", "journal": "International Conference on Computer Vision (ICCV)", "description": "Domain generalization (DG) aims to learn domaingeneralizable models from one or multiple source domains that can perform well in unseen target domains. Despite its recent progress, most existing work suffers from the misalignment between the difficulty level of training samples and the capability of contemporarily trained models, leading to over-fitting or under-fitting in the trained generalization model. We design MoDify, a Momentum Difficulty framework that tackles the misalignment by balancing the seesaw between the model's capability and the samples' difficulties along the training process. MoDify consists of two novel designs that collaborate to fight against the misalignment while learning domain-generalizable models. The first is MoDify-based Data Augmentation which exploits an RGB Shuffle technique to generate difficulty-aware training samples on the fly. The second is MoDify-based Network Optimization which dynamically schedules the training samples for balanced and smooth learning with appropriate difficulty. Without bells and whistles, a simple implementation of MoDify achieves superior performance across multiple benchmarks. In addition, MoDify can complement existing methods as a plug-in, and it is generic and can work for different visual recognition tasks.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:oPLKW5k6eA4C": {"external_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Pose-Free_Neural_Radiance_Fields_via_Implicit_Pose_Regularization_ICCV_2023_paper.html", "authors": ["Jiahui Zhang", "Fangneng Zhan", "Yingchen Yu", "Kunhao Liu", "Rongliang Wu", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "publication_date": "2023/8/29", "journal": "International Conference on Computer Vision (ICCV)", "description": "Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed multi-view images and it has achieved very impressive success in recent years. Most existing works share the pipeline of training a coarse pose estimator with rendered images at first, followed by a joint optimization of estimated poses and neural radiance field. However, as the pose estimator is trained with only rendered images, the pose estimation is usually biased or inaccurate for real images due to the domain gap between real images and rendered images, leading to poor robustness for the pose estimation of real images and further local min-ima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF that introduces implicit pose regularization to refine pose estimator with unposed real images and improve the robustness of the pose estimation for real images. With a collection of 2D images of a specific scene, IR-NeRF constructs a scene codebook that stores scene features and captures the scene-specific pose distribution implicitly as priors. Thus, the robustness of pose estimation can be promoted with the scene priors according to the rationale that a 2D real image can be well reconstructed from the scene codebook only when its estimated pose lies within the pose distribution. Extensive experiments show that IR-NeRF achieves superior novel view synthesis and outperforms the state-of-the-art consistently across multiple synthetic and real datasets."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LGlY6t8CeOMC": {"external_link": "https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Black-Box_Unsupervised_Domain_Adaptation_with_Bi-Directional_Atkinson-Shiffrin_Memory_ICCV_2023_paper.html", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Xueying Jiang", "Shijian Lu"], "publication_date": "2023/8/25", "journal": "International Conference on Computer Vision (ICCV)", "description": "Black-box unsupervised domain adaptation (UDA) learns with source predictions of target data without accessing either source data or source models during training, and it has clear superiority in data privacy and flexibility in target network selection. However, the source predictions of target data are often noisy and training with them is prone to learning collapses. We propose BiMem, a bi-directional memorization mechanism that learns to remember useful and representative information to correct noisy pseudo labels on the fly, leading to robust black-box UDA that can generalize across different visual recognition tasks. BiMem constructs three types of memory, including sensory memory, short-term memory, and long-term memory, which interact in a bi-directional manner for comprehensive and robust memorization of learnt features. It includes a forward memorization flow that identifies and stores useful features and a backward calibration flow that rectifies features' pseudo labels progressively. Extensive experiments show that BiMem achieves superior domain adaptation performance consistently across various visual recognition tasks such as image classification, semantic segmentation and object detection."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:YTuZlYwrTOUC": {"external_link": "https://scholar.google.com/scholar?cluster=8425404867119684977&hl=en&oi=scholarr", "authors": ["Fangneng Zhan", "Yingchen Yu", "Rongliang Wu", "Jiahui Zhang", "Shijian Lu", "Lingjie Liu", "Adam Kortylewski", "Christian Theobalt", "Eric Xing"], "publication_date": "2023/8/11", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "total_citations": {"2021": 1, "2022": 32, "2023": 50}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:t-hv7AR41mYC": {"external_link": "http://openaccess.thecvf.com/content/ICCV2023/html/Xu_WaveNeRF_Wavelet-based_Generalizable_Neural_Radiance_Fields_ICCV_2023_paper.html", "authors": ["Muyu Xu", "Fangneng Zhan", "Jiahui Zhang", "Yingchen Yu", "Xiaoqin Zhang", "Christian Theobalt", "Ling Shao", "Shijian Lu"], "publication_date": "2023/8/9", "journal": "International Conference on Computer Vision (ICCV)", "description": "Neural Radiance Field (NeRF) has shown impressive performance in novel view synthesis via implicit scene representation. However, it usually suffers from poor scalability as requiring densely sampled images for each new scene. Several studies have attempted to mitigate this problem by integrating Multi-View Stereo (MVS) technique into NeRF while they still entail a cumbersome fine-tuning process for new scenes. Notably, the rendering quality will drop severely without this fine-tuning process and the errors mainly appear around the high-frequency features. In the light of this observation, we design WaveNeRF, which integrates wavelet frequency decomposition into MVS and NeRF to achieve generalizable yet high-quality synthesis without any per-scene optimization. To preserve high-frequency information when generating 3D feature volumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integrating the discrete wavelet transform into the classical cascade MVS, which disentangles high-frequency information explicitly. With that, disentangled frequency features can be injected into classic NeRF via a novel hybrid neural renderer to yield faithful high-frequency details, and an intuitive frequency-guided sampling strategy can be designed to suppress artifacts around high-frequency regions. Extensive experiments over three widely studied benchmarks show that WaveNeRF achieves superior generalizable radiance field modeling when only given three images as input.", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:AzKEL7Gb_04C": {"external_link": "https://arxiv.org/abs/2304.08945", "authors": ["Rongliang Wu", "Yingchen Yu", "Fangneng Zhan", "Jiahui Zhang", "Xiaoqin Zhang", "Shijian Lu"], "publication_date": "2023/7/31", "journal": "Pattern Recognition", "description": "Audio-driven talking face generation, which aims to synthesize talking faces with realistic facial animations (including accurate lip movements, vivid facial expression details and natural head poses) corresponding to the audio, has achieved rapid progress in recent years. However, most existing work focuses on generating lip movements only without handling the closely correlated facial expressions, which degrades the realism of the generated faces greatly. This paper presents DIRFA, a novel method that can generate talking faces with diverse yet realistic facial animations from the same driving audio. To accommodate fair variation of plausible facial animations for the same audio, we design a transformer-based probabilistic mapping network that can model the variational facial animation distribution conditioned upon the input audio and autoregressively convert the audio signals into a facial animation sequence. In addition, we introduce a temporally-biased mask into the mapping network, which allows to model the temporal dependency of facial animations and produce temporally smooth facial animation sequence. With the generated facial animation sequence and a source image, photo-realistic talking faces can be synthesized with a generic generation network. Extensive experiments show that DIRFA can generate talking faces with realistic facial animations effectively.", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hGdtkIFZdKAC": {"external_link": "https://arxiv.org/abs/2307.07286", "authors": ["Siyuan Yang", "Jun Liu", "Shijian Lu", "Er Meng Hwa", "Alex C Kot"], "publication_date": "2023/7/14", "journal": "arXiv preprint arXiv:2307.07286", "description": "One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatial-temporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multiple scales. Extensive experiments over three large-scale datasets (NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior one-shot skeleton action recognition, and it outperforms the state-of-the-art consistently by large margins."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:SxCCDk4iOpsC": {"external_link": "https://arxiv.org/abs/2306.16658", "authors": ["Jiaxing Huang", "Jingyi Zhang", "Han Qiu", "Sheng Jin", "Shijian Lu"], "publication_date": "2023/6/29", "journal": "arXiv preprint arXiv:2306.16658", "description": "Traditional domain adaptation assumes the same vocabulary across source and target domains, which often struggles with limited transfer flexibility and efficiency while handling target domains with different vocabularies. Inspired by recent vision-language models (VLMs) that enable open-vocabulary visual recognition by reasoning on both images and texts, we study open-vocabulary domain adaptation (OVDA), a new unsupervised domain adaptation framework that positions a pre-trained VLM as the source model and transfers it towards arbitrary unlabelled target domains. To this end, we design a Prompt Ensemble Self-training (PEST) technique that exploits the synergy between vision and language to mitigate the domain discrepancies in image and text distributions simultaneously. Specifically, PEST makes use of the complementary property of multiple prompts within and across vision and language modalities, which enables joint exploitation of vision and language information and effective learning of image-text correspondences in the unlabelled target domains. Additionally, PEST captures temporal information via temporal prompt ensemble which helps memorize previously learnt target information. Extensive experiments show that PEST outperforms the state-of-the-art consistently across 10 image recognition tasks."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:silx2ntsSuwC": {"external_link": "https://arxiv.org/abs/2105.08383", "authors": ["Chuhui Xue", "Shijian Lu", "Song Bai", "Wenqing Zhang", "Changhu Wang"], "publication_date": "2023/6", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in press", "description": "Leveraging the advances of natural language processing, most recent scene text recognizers adopt an encoder-decoder architecture where text images are first converted to representative features and then a sequence of characters via `sequential decoding'. However, scene text images suffer from rich noises of different sources such as complex background and geometric distortions which often confuse the decoder and lead to incorrect alignment of visual features at noisy decoding time steps. This paper presents I2C2W, a novel scene text recognition technique that is tolerant to geometric and photometric degradation by decomposing scene text recognition into two inter-connected tasks. The first task focuses on image-to-character (I2C) mapping which detects a set of character candidates from images based on different alignments of visual features in an non-sequential way. The second task tackles character-to-word (C2W) mapping which recognizes scene text by decoding words from the detected character candidates. The direct learning from character semantics (instead of noisy image features) corrects falsely detected character candidates effectively which improves the final text recognition accuracy greatly. Extensive experiments over nine public datasets show that the proposed I2C2W outperforms the state-of-the-art by large margins for challenging scene text datasets with various curvature and perspective distortions. It also achieves very competitive recognition performance over multiple normal scene text datasets.", "total_citations": {"2021": 2, "2022": 7, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:G1UMdFYMoxkC": {"external_link": "https://arxiv.org/abs/2305.19812", "authors": ["Aoran Xiao", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "publication_date": "2023/5/31", "journal": "arXiv preprint arXiv:2305.19812", "description": "In the past decade, deep neural networks have achieved significant progress in point cloud learning. However, collecting large-scale precisely-annotated training data is extremely laborious and expensive, which hinders the scalability of existing point cloud datasets and poses a bottleneck for efficient exploration of point cloud data in various tasks and applications. Label-efficient learning offers a promising solution by enabling effective deep network training with much-reduced annotation efforts. This paper presents the first comprehensive survey of label-efficient learning of point clouds. We address three critical questions in this emerging research field: i) the importance and urgency of label-efficient learning in point cloud processing, ii) the subfields it encompasses, and iii) the progress achieved in this area. To achieve this, we propose a taxonomy that organizes label-efficient learning methods based on the data prerequisites provided by different types of labels. We categorize four typical label-efficient learning approaches that significantly reduce point cloud annotation efforts: data augmentation, domain transfer learning, weakly-supervised learning, and pretrained foundation models. For each approach, we outline the problem setup and provide an extensive literature review that showcases relevant progress and challenges. Finally, we share insights into current research challenges and potential future directions. A project associated with this survey has been built at \\url{https://github.com/xiaoaoran/3D_label_efficient_learning}."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:8dzOF9BpDQoC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/10102322/", "authors": ["Qinghua Ren", "Qirong Mao", "Shijian Lu"], "publication_date": "2023/4/13", "journal": "IEEE Transactions on Multimedia", "publisher": "IEEE", "description": "Cross-domain semantic segmentation, which aims to address the distribution shift while adapting from a labeled source domain to an unlabeled target domain, has achieved great progress in recent years. However, most existing work adopts a source-to-target adaptation path, which often suffers from clear class mismatching or class imbalance issues. We design PBAL, a prototypical bidirectional adaptation and learning technique that introduces bidirectional prototype learning and prototypical self-training for optimal inter-domain alignment and adaptation. We perform bidirectional alignments in a complementary and cooperative manner which balances both dominant and tail categories as well as easy and hard samples effectively. In addition, We derive prototypes efficiently from a source-trained classifier, which enables class-aware adaptation as well as synchronous prototype updating and network optimization\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Zh0EY9V9P6UC": {"external_link": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Cui_Face_Transformer_Towards_High_Fidelity_and_Accurate_Face_Swapping_CVPRW_2023_paper.html", "authors": ["Kaiwen Cui", "Rongliang Wu", "Fangneng Zhan", "Shijian Lu"], "publication_date": "2023/4/5", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)", "description": "Face swapping aims to generate swapped images that fuse the identity of source faces and the attributes of target faces. Most existing works address this challenging task through 3D modelling or generation using generative adversarial networks (GANs), but 3D modelling suffers from limited reconstruction accuracy and GANs often struggle in preserving subtle yet important identity details of source faces (eg, skin colors, face features) and structural attributes of target faces (eg, face shapes, facial expressions). This paper presents Face Transformer, a novel face swapping network that can accurately preserve source identities and target attributes simultaneously in the swapped face images. We introduce a transformer network for the face swapping task, which learns high-quality semantic-aware correspondence between source and target faces and maps identity features of source faces to the corresponding region in target faces. The high-quality semantic-aware correspondence enables smooth and accurate transfer of source identity information with minimal modification of target shapes and expressions. In addition, our Face Transformer incorporates a multi-scale transformation mechanism for preserving the rich fine facial details. Extensive experiments show that our Face Transformer achieves superior face swapping performance qualitatively and quantitatively.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:4aZ_i-5WJEQC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Xiao_3D_Semantic_Segmentation_in_the_Wild_Learning_Generalized_Models_for_CVPR_2023_paper.html", "authors": ["Aoran Xiao", "Jiaxing Huang", "Weihao Xuan", "Ruijie Ren", "Kangcheng Liu", "Dayan Guan", "Abdulmotaleb El Saddik", "Shijian Lu", "Eric Xing"], "publication_date": "2023/4/3", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We investigate universal 3DSS modeling with two tasks: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalized 3DSS that learns a generalizable model from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their encoded embeddings, ultimately leading to a generalizable model that effectively improves 3DSS under various adverse weather. The SemanticSTF and related codes are available at https://github. com/xiaoaoran/SemanticSTF.", "total_citations": {"2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:KWzIFqRkAKkC": {"external_link": "https://arxiv.org/abs/2304.00685", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Sheng Jin", "Shijian Lu"], "publication_date": "2023/4/3", "journal": "arXiv preprint arXiv:2304.00685", "description": "Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition. A project associated with this survey has been created at https://github.com/jingyi0000/VLM_survey.", "total_citations": {"2023": 18}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:-uzm3Y7AvW0C": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Cui_KD-DLGAN_Data_Limited_Image_Generation_via_Knowledge_Distillation_CVPR_2023_paper.html", "authors": ["Kaiwen Cui", "Yingchen Yu", "Fangneng Zhan", "Shengcai Liao", "Shijian Lu", "Eric Xing"], "publication_date": "2023/3/30", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-GAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited image generation models. KD-GAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-GAN achieves superior image generation with limited training data. In addition, KD-GAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.", "total_citations": {"2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:JP7YXuLIOvAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/10086697/", "authors": ["Aoran Xiao", "Jiaxing Huang", "Dayan Guan", "Xiaoqin Zhang", "Shijian Lu", "Ling Shao"], "publication_date": "2023/3/29", "source": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in press", "publisher": "IEEE", "description": "Point cloud data have been widely explored due to its superior accuracy and robustness under various adverse situations. Meanwhile, deep neural networks (DNNs) have achieved very impressive success in various applications such as surveillance and autonomous driving. The convergence of point cloud and DNNs has led to many deep point cloud models, largely trained under the supervision of large-scale and densely-labelled point cloud data. Unsupervised point cloud representation learning, which aims to learn general and useful point cloud representations from unlabelled point cloud data, has recently attracted increasing attention due to the constraint in large-scale point cloud labelling. This paper provides a comprehensive review of unsupervised point cloud representation learning using DNNs. It first describes the motivation, general pipelines as well as terminologies of the recent studies. Relevant\u00a0\u2026", "total_citations": {"2022": 7, "2023": 27}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:H_jBuBxbQIAC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.html", "authors": ["Kunhao Liu", "Fangneng Zhan", "Yiwen Chen", "Jiahui Zhang", "Yingchen Yu", "Abdulmotaleb El Saddik", "Shijian Lu", "Eric Xing"], "publication_date": "2023/3/19", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu. github. io/StyleRF/", "total_citations": {"2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:wE-fMHVdjMkC": {"external_link": "https://arxiv.org/abs/2206.15083", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Xiaoqin Zhang", "Shijian Lu"], "publication_date": "2023/3/19", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Domain adaptive panoptic segmentation aims to mitigate data annotation challenge by leveraging off-the-shelf annotated data in one or multiple related source domains. However, existing studies employ two networks for instance segmentation and semantic segmentation separately which lead to a large amount of network parameters with complicated and computationally intensive training and inference processes. We design UniDAPS, a Unified Domain Adaptive Panoptic Segmentation network that is simple but capable of achieving domain adaptive instance segmentation and semantic segmentation simultaneously within a single network. UniDAPS introduces Hierarchical Mask Calibration (HMC) that rectifies the predicted pseudo masks, pseudo superpixels and pseudo pixels and performs network re-training via an online self-training process on the fly. It has three unique features: 1) it enables unified domain adaptive panoptic adaptation; 2) it mitigates false predictions and improves domain adaptive panoptic segmentation effectively; 3) it is end-to-end trainable with much less parameters and simpler training and inference pipeline. Extensive experiments over multiple public benchmarks show that UniDAPS achieves superior domain adaptive panoptic segmentation as compared with the state-of-the-art.", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:CNPyR2KL9-0C": {"external_link": "https://arxiv.org/abs/2303.07605", "authors": ["Zhipeng Luo", "Gongjie Zhang", "Changqing Zhou", "Zhonghua Wu", "Qingyi Tao", "Lewei Lu", "Shijian Lu"], "publication_date": "2023/3/14", "journal": "arXiv preprint arXiv:2303.07605", "description": "The task of 3D single object tracking (SOT) with LiDAR point clouds is crucial for various applications, such as autonomous driving and robotics. However, existing approaches have primarily relied on appearance matching or motion modeling within only two successive frames, thereby overlooking the long-range continuous motion property of objects in 3D space. To address this issue, this paper presents a novel approach that views each tracklet as a continuous stream: at each timestamp, only the current frame is fed into the network to interact with multi-frame historical features stored in a memory bank, enabling efficient exploitation of sequential information. To achieve effective cross-frame message passing, a hybrid attention mechanism is designed to account for both long-range relation modeling and local geometric feature extraction. Furthermore, to enhance the utilization of multi-frame features for robust tracking, a contrastive sequence enhancement strategy is designed, which uses ground truth tracklets to augment training sequences and promote discrimination against false positives in a contrastive manner. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art method by significant margins (approximately 8%, 6%, and 12% improvements in the success performance on KITTI, nuScenes, and Waymo, respectively)."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ZqE1mSdD_DYC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Liu_FAC_3D_Representation_Learning_via_Foreground_Aware_Feature_Contrast_CVPR_2023_paper.html", "authors": ["Kangcheng Liu", "Aoran Xiao", "Xiaoqin Zhang", "Shijian Lu", "Ling Shao"], "publication_date": "2023/3/11", "journal": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Contrastive learning has recently demonstrated great potential for unsupervised pre-training in 3D scene understanding tasks. However, most existing work randomly selects point features as anchors while building contrast, leading to a clear bias toward background points that often dominate in 3D scenes. Also, object awareness and foreground-to-background discrimination are neglected, making contrastive learning less effective. To tackle these issues, we propose a general foreground-aware feature contrast (FAC) framework to learn more effective point cloud representations in pre-training. FAC consists of two novel contrast designs to construct more effective and informative contrast pairs. The first is building positive pairs within the same foreground segment where points tend to have the same semantics. The second is that we prevent over-discrimination between 3D segments/objects and encourage foreground-to-background distinctions at the segment level with adaptive feature learning in a Siamese correspondence network, which adaptively learns feature correlations within and across point cloud views effectively. Visualization with point activation maps shows that our contrast pairs capture clear correspondences among foreground regions during pre-training. Quantitative experiments also show that FAC achieves superior knowledge transfer and data efficiency in various downstream 3D semantic segmentation and object detection tasks. All codes, data, and models are available at: https://github. com/KangchengLiu/FAC_Foreground_Aware_Contrast.", "total_citations": {"2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:zwpXiJ37cpgC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Regularized_Vector_Quantization_for_Tokenized_Image_Synthesis_CVPR_2023_paper.html", "authors": ["Jiahui Zhang", "Fangneng Zhan", "Christian Theobalt", "Shijian Lu"], "publication_date": "2023/3/11", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misaligned inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantizers consistently across different generative models including auto-regressive models and diffusion models.", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:NxmKEeNBbOMC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Tan_Backdoor_Attacks_Against_Deep_Image_Compression_via_Adaptive_Frequency_Trigger_CVPR_2023_paper.html", "authors": ["Yi Yu", "Yufei Wang", "Wenhan Yang", "Shijian Lu", "Yap-peng Tan", "Alex C Kot"], "publication_date": "2023/2/28", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Recent deep-learning-based compression methods have achieved superior performance compared with traditional approaches. However, deep learning models have proven to be vulnerable to backdoor attacks, where some specific trigger patterns added to the input can lead to malicious behavior of the models. In this paper, we present a novel backdoor attack with multiple triggers against learned image compression models. Motivated by the widely used discrete cosine transform (DCT) in existing compression systems and standards, we propose a frequency-based trigger injection model that adds triggers in the DCT domain. In particular, we design several attack objectives for various attacking scenarios, including: 1) attacking compression quality in terms of bit-rate and reconstruction quality; 2) attacking task-driven measures, such as down-stream face recognition and semantic segmentation. Moreover, a novel simple dynamic loss is designed to balance the influence of different loss terms adaptively, which helps achieve more efficient training. Extensive experiments show that with our trained trigger injection models and simple modification of encoder parameters (of the compression model), the proposed attack can successfully inject several backdoors with corresponding triggers in a single image compression model.", "total_citations": {"2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:T_ojBgVMvoEC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Towards_Efficient_Use_of_Multi-Scale_Features_in_Transformer-Based_Object_Detectors_CVPR_2023_paper.html", "authors": ["Gongjie Zhang", "Zhipeng Luo", "Zichen Tian", "Jingyi Zhang", "Xiaoqin Zhang", "Shijian Lu"], "publication_date": "2023/2/24", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA)-a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA boosts the performance of multiple Transformer-based object detectors significantly yet with only slight computational overhead.", "total_citations": {"2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:DrR-2ekChdkC": {"external_link": "https://arxiv.org/abs/2103.17084", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Zhipeng Luo", "Gongjie Zhang", "Xiaoqin Zhang", "Shijian Lu"], "publication_date": "2023/2/19", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "The prevalent approach in domain adaptive object detection adopts a two-stage architecture (Faster R-CNN) that involves a number of hyper-parameters and hand-crafted designs such as anchors, region pooling, non-maximum suppression, etc. Such architecture makes it very complicated while adopting certain existing domain adaptation methods with different ways of feature alignment. In this work, we adopt a one-stage detector and design DA-DETR, a simple yet effective domain adaptive object detection network that performs inter-domain alignment with a single discriminator. DA-DETR introduces a hybrid attention module that explicitly pinpoints the hard-aligned features for simple yet effective alignment across domains. It greatly simplifies traditional domain adaptation pipelines by eliminating sophisticated routines that involve multiple adversarial learning frameworks with different types of features. Despite its simplicity, extensive experiments show that DA-DETR demonstrates superior accuracy as compared with highly-optimized state-of-the-art approaches.", "total_citations": {"2021": 6, "2022": 15, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:RtRctb2lSbAC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_UniDAformer_Unified_Domain_Adaptive_Panoptic_Segmentation_Transformer_via_Hierarchical_Mask_CVPR_2023_paper.html", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Xiaoqin Zhang", "Shijian Lu"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "pages": "11227-11237", "description": "Domain adaptive panoptic segmentation aims to mitigate data annotation challenge by leveraging off-the-shelf annotated data in one or multiple related source domains. However, existing studies employ two separate networks for instance segmentation and semantic segmentation which lead to excessive network parameters as well as complicated and computationally intensive training and inference processes. We design UniDAformer, a unified domain adaptive panoptic segmentation transformer that is simple but can achieve domain adaptive instance segmentation and semantic segmentation simultaneously within a single network. UniDAformer introduces Hierarchical Mask Calibration (HMC) that rectifies inaccurate predictions at the level of regions, superpixels and pixels via online self-training on the fly. It has three unique features: 1) it enables unified domain adaptive panoptic adaptation; 2) it mitigates false predictions and improves domain adaptive panoptic segmentation effectively; 3) it is end-to-end trainable with a much simpler training and inference pipeline. Extensive experiments over multiple public benchmarks show that UniDAformer achieves superior domain adaptive panoptic segmentation as compared with the state-of-the-art.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:-yGd096yOn8C": {"external_link": "https://dayan-guan.github.io/pub/CIR.pdf", "authors": ["Rumeng Yi", "Dayan Guan", "Yaping Huang", "Shijian Lu"], "publication_date": "2023", "journal": "AAAI Conference on Artificial Intelligence (AAAI)", "description": "Training deep neural networks (DNNs) with noisy labels often leads to poorly generalized models as DNNs tend to memorize the noisy labels in training. Various strategies have been developed for improving sample selection precision and mitigating the noisy label memorization issue. However, most existing works adopt a class-dependent softmax classifier that is vulnerable to noisy labels by entangling the classification of multi-class features. This paper presents a class-independent regularization (CIR) method that can effectively alleviate the negative impact of noisy labels in DNN training. CIR regularizes the class-dependent softmax classifier by introducing multi-binary classifiers each of which takes care of one class only. Thanks to its class-independent nature, CIR is tolerant to noisy labels as misclassification by one binary classifier does not affect others. For effective training of CIR, we design a heterogeneous adaptive co-teaching strategy that forces the class-independent and class-dependent classifiers to focus on sample selection and image classification, respectively, in a cooperative manner. Extensive experiments show that CIR achieves superior performance consistently across multiple benchmarks with both synthetic and real images. Code is available at https://github. com/RumengYi/CIR."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:D_tqNUsBuKoC": {"external_link": "https://openaccess.thecvf.com/content/WACV2023/html/Luo_TransPillars_Coarse-To-Fine_Aggregation_for_Multi-Frame_3D_Object_Detection_WACV_2023_paper.html", "authors": ["Zhipeng Luo", "Gongjie Zhang", "Changqing Zhou", "Tianrui Liu", "Shijian Lu", "Liang Pan"], "publication_date": "2023", "conference": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision", "pages": "4230-4239", "description": "3D object detection using point clouds has attracted increasing attention due to its wide applications in autonomous driving and robotics. However, most existing studies focus on single point cloud frames without harnessing the temporal information in point cloud sequences. In this paper, we design TransPillars, a novel transformer-based feature aggregation technique that exploits temporal features of consecutive point cloud frames for multi-frame 3D object detection. TransPillars aggregates spatial-temporal point cloud features from two perspectives. First, it fuses voxel-level features directly from multi-frame feature maps instead of pooled instance features to preserve instance details with contextual information that are essential to accurate object localization. Second, it introduces a hierarchical coarse-to-fine strategy to fuse multi-scale features progressively to effectively capture the motion of moving objects and guide the aggregation of fine features. Besides, a variant of deformable transformer is introduced to improve the effectiveness of cross-frame feature matching. Extensive experiments show that our proposed TransPillars achieves state-of-art performance as compared to existing multi-frame detection approaches.", "total_citations": {"2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Wq2b2clWBLsC": {"external_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/475b85eb74d201bead9927807e713e95-Abstract-Conference.html", "authors": ["Aoran Xiao", "Jiaxing Huang", "Dayan Guan", "Kaiwen Cui", "Shijian Lu", "Ling Shao"], "publication_date": "2022/12/30", "journal": "Advances in Neural Information Processing Systems", "description": "LiDAR point clouds, which are usually scanned by rotating LiDAR sensors continuously, capture precise geometry of the surrounding environment and are crucial to many autonomous detection and navigation tasks. Though many 3D deep architectures have been developed, efficient collection and annotation of large amounts of point clouds remain one major challenge in the analytics and understanding of point cloud data. This paper presents PolarMix, a point cloud augmentation technique that is simple and generic but can mitigate the data constraint effectively across various perception tasks and scenarios. PolarMix enriches point cloud distributions and preserves point cloud fidelity via two cross-scan augmentation strategies that cut, edit, and mix point clouds along the scanning direction. The first is scene-level swapping which exchanges point cloud sectors of two LiDAR scans that are cut along the LiDAR scanning direction. The second is instance-level rotation and paste which crops point instances from one LiDAR scan, rotates them by multiple angles (to create multiple copies), and paste the rotated point instances into other scans. Extensive experiments show that PolarMix achieves superior performance consistently across different perception tasks and scenarios. In addition, it can work as a plug-and-play for various 3D deep architectures and also performs well for unsupervised domain adaptation.", "total_citations": {"2022": 1, "2023": 25}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:MIg0yeAD4ggC": {"external_link": "https://arxiv.org/abs/2212.07849", "authors": ["Zhipeng Luo", "Changqing Zhou", "Gongjie Zhang", "Shijian Lu"], "publication_date": "2022/12/15", "journal": "arXiv preprint arXiv:2212.07849", "description": "3D object detection with surround-view images is an essential task for autonomous driving. In this work, we propose DETR4D, a Transformer-based framework that explores sparse attention and direct feature query for 3D object detection in multi-view images. We design a novel projective cross-attention mechanism for query-image interaction to address the limitations of existing methods in terms of geometric cue exploitation and information loss for cross-view objects. In addition, we introduce a heatmap generation technique that bridges 3D and 2D spaces efficiently via query initialization. Furthermore, unlike the common practice of fusing intermediate spatial features for temporal aggregation, we provide a new perspective by introducing a novel hybrid approach that performs cross-frame fusion over past object queries and image features, enabling efficient and robust modeling of temporal information. Extensive experiments on the nuScenes dataset demonstrate the effectiveness and efficiency of the proposed DETR4D.", "total_citations": {"2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:A8cqit5AE6sC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320322004356", "authors": ["Zhipeng Luo", "Xiaobing Zhang", "Shijian Lu", "Shuai Yi"], "publication_date": "2022/12/1", "journal": "Pattern Recognition", "volume": "132", "pages": "108955", "publisher": "Pergamon", "description": "Deep learning-based multi-source unsupervised domain adaptation (MUDA) has been actively studied in recent years. Compared with single-source unsupervised domain adaptation (SUDA), domain shift in MUDA exists not only between the source and target domains but also among multiple source domains. Most existing MUDA algorithms focus on extracting domain-invariant representations among all domains whereas the task-specific decision boundaries among classes are largely neglected. In this paper, we propose an end-to-end trainable network that exploits domain Consistency Regularization for unsupervised Multi-source domain Adaptive classification (CRMA). CRMA aligns not only the distributions of each pair of source and target domains but also that of all domains. For each pair of source and target domains, we employ an intra-domain consistency to regularize a pair of domain-specific classifiers\u00a0\u2026", "total_citations": {"2021": 1, "2022": 1, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:z6xuaG2dYH0C": {"external_link": "https://arxiv.org/abs/2212.00377", "authors": ["Zichen Tian", "Chuhui Xue", "Jingyi Zhang", "Shijian Lu"], "publication_date": "2022/12/1", "journal": "arXiv preprint arXiv:2212.00377", "description": "Most existing scene text detectors require large-scale training data which cannot scale well due to two major factors: 1) scene text images often have domain-specific distributions; 2) collecting large-scale annotated scene text images is laborious. We study domain adaptive scene text detection, a largely neglected yet very meaningful task that aims for optimal transfer of labelled scene text images while handling unlabelled images in various new domains. Specifically, we design SCAST, a subcategory-aware self-training technique that mitigates the network overfitting and noisy pseudo labels in domain adaptive scene text detection effectively. SCAST consists of two novel designs. For labelled source data, it introduces pseudo subcategories for both foreground texts and background stuff which helps train more generalizable source models with multi-class detection objectives. For unlabelled target data, it mitigates the network overfitting by co-regularizing the binary and subcategory classifiers trained in the source domain. Extensive experiments show that SCAST achieves superior detection performance consistently across multiple public benchmarks, and it also generalizes well to other domain adaptive detection tasks such as vehicle detection.", "total_citations": {"2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:-6RzNnnwWf8C": {"external_link": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/0efcb1885b8534109f95ca82a5319d25-Abstract-Conference.html", "authors": ["Jiaxing Huang", "Kaiwen Cui", "Dayan Guan", "Aoran Xiao", "Fangneng Zhan", "Shijian Lu", "Shengcai Liao", "Eric Xing"], "publication_date": "2022/12", "conference": "Advances in Neural Information Processing Systems", "description": "This paper shows that masked generative adversarial network (MaskedGAN) is robust image generation learners with limited training data. The idea of MaskedGAN is simple: it randomly masks out certain image information for effective GAN training with limited data. We develop two masking strategies that work along orthogonal dimensions of training images, including a shifted spatial masking that masks the images in spatial dimensions with random shifts, and a balanced spectral masking that masks certain image spectral bands with self-adaptive probabilities. The two masking strategies complement each other which together encourage more challenging holistic learning from limited training data, ultimately suppressing trivial solutions and failures in GAN training. Albeit simple, extensive experiments show that MaskedGAN achieves superior performance consistently across different network architectures (eg, CNNs including BigGAN and StyleGAN-v2 and Transformers including TransGAN and GANformer) and datasets (eg, CIFAR-10, CIFAR-100, ImageNet, 100-shot, AFHQ, FFHQ and Cityscapes).", "total_citations": {"2022": 1, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:C33y2ycGS3YC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-031-19815-1_22", "authors": ["Chuhui Xue", "Jiaxing Huang", "Shijian Lu", "Changhu Wang", "Song Bai"], "publication_date": "2022/10/25", "journal": "Proceedings of the European Conference on Computer Vision (ECCV)", "pages": "arXiv: 2207.12955", "description": "Most existing scene text detectors focus on detecting characters or words that only capture partial text messages due to missing contextual information. For a better understanding of text in scenes, it is more desired to detect contextual text blocks (CTBs) which consist of one or multiple integral text units (e.g., characters, words, or phrases) in natural reading order and transmit certain complete text messages. This paper presents contextual text detection, a new setup that detects CTBs for better understanding of texts in scenes. We formulate the new setup by a dual detection task which first detects integral text units and then groups them into a CTB. To this end, we design a novel scene text clustering technique that treats integral text units as tokens and groups them (belonging to the same CTB) into an ordered token sequence. In addition, we create two datasets SCUT-CTW-Context and ReCTS-Context to facilitate\u00a0\u2026", "total_citations": {"2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:isU91gLudPYC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_7", "authors": ["Fangneng Zhan", "Yingchen Yu", "Rongliang Wu", "Jiahui Zhang", "Kaiwen Cui", "Changgong Zhang", "Shijian Lu"], "publication_date": "2022/10/25", "journal": "Proceedings of the European Conference on Computer Vision (ECCV), 2022", "pages": "arXiv: 2207.10776", "description": "Deep generative models have achieved conspicuous progress in realistic image synthesis with multifarious conditional inputs, while generating diverse yet high-fidelity images remains a grand challenge in conditional image generation. This paper presents a versatile framework for conditional image generation which incorporates the inductive bias of CNNs and powerful sequence modeling of auto-regression that naturally leads to diverse image generation. Instead of independently quantizing the features of multiple domains as in prior research, we design an integrated quantization scheme with a variational regularizer that mingles the feature discretization in multiple domains, and markedly boosts the auto-regressive modeling performance. Notably, the variational regularizer enables to regularize feature distributions in incomparable latent spaces by penalizing the intra-domain variations of distributions. In\u00a0\u2026", "total_citations": {"2021": 1, "2022": 1, "2023": 15}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:0aBXIfxlw9sC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-031-20056-4_36", "authors": ["Yun Xing", "Dayan Guan", "Jiaxing Huang", "Shijian Lu"], "publication_date": "2022/10/25", "journal": "Proceedings of the European Conference on Computer Vision (ECCV), 2022", "description": "Video semantic segmentation has achieved great progress under the supervision of large amounts of labelled training data. However, domain adaptive video segmentation, which can mitigate data labelling constraints by adapting from a labelled source domain toward an unlabelled target domain, is largely neglected. We design temporal pseudo supervision (TPS), a simple and effective method that explores the idea of consistency training for learning effective representations from unlabelled target videos. Unlike traditional consistency training that builds consistency in spatial space, we explore consistency training in spatiotemporal space by enforcing model consistency across augmented video frames which helps learn from more diverse target data. Specifically, we design cross-frame pseudo labelling to provide pseudo supervision from previous video frames while learning from the augmented current video\u00a0\u2026", "total_citations": {"2021": 1, "2022": 1, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:27LrP4qxOz0C": {"external_link": "https://scholar.google.com/scholar?cluster=16315689731337251244&hl=en&oi=scholarr", "authors": ["Chuhui Xue", "Yu Hao", "Shijian Lu", "Philip Torr", "Song Bai"], "publication_date": "2022/10/25", "journal": "Proceedings of the European Conference on Computer Vision (ECCV), 2022", "total_citations": {"2022": 4, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:SnGPuo6Feq8C": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-031-19787-1_13", "authors": ["Fangneng Zhan", "Yingchen Yu", "Rongliang Wu", "Kaiwen Cui", "Aoran Xiao", "Shijian Lu", "Ling Shao"], "publication_date": "2022/10/25", "journal": "Proceedings of the European Conference on Computer Vision (ECCV), 2022", "description": "Generative adversarial networks (GANs) have achieved great success in image translation and manipulation. However, high-fidelity image generation with faithful style control remains a grand challenge in computer vision. This paper presents a versatile image translation and manipulation framework that achieves accurate semantic and style guidance in image generation by explicitly building a correspondence. To handle the quadratic complexity incurred by building the dense correspondences, we introduce a bi-level feature alignment strategy that adopts a top-k operation to rank block-wise features followed by dense attention between block features which reduces memory cost substantially. As the top-k operation involves index swapping which precludes the gradient propagation, we approximate the non-differentiable top-k operation with a regularized earth mover\u2019s problem so that its gradient can be\u00a0\u2026", "total_citations": {"2021": 4, "2022": 22, "2023": 11}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:c1e4I3QdEKYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9981388/", "authors": ["Kangcheng Liu", "Aoran Xiao", "Jiaxing Huang", "Kaiwen Cui", "Yun Xing", "Shijian Lu"], "publication_date": "2022/10/23", "conference": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "pages": "12212-12218", "publisher": "IEEE", "description": "The current LiDAR SLAM (Simultaneous Localization and Mapping) system suffers greatly from low accuracy and limited robustness when faced with complicated circumstances. From our experiments, we find that current LiDAR SLAM systems have limited performance when the noise level in the obtained point clouds is large. Therefore, in this work, we propose a general framework to tackle the problem of denoising and loop closure for LiDAR SLAM in complex environments with many noises and outliers caused by reflective materials. Current approaches for point clouds denoising are mainly designed for small-scale point clouds and can not be extended to large-scale point clouds scenes. In this work, we firstly proposed a lightweight network for large-scale point clouds denoising. Subsequently, we have also designed an efficient loop closure network for place recognition in global optimization to improve the\u00a0\u2026", "total_citations": {"2022": 3, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:tHtfpZlB6tUC": {"external_link": "https://arxiv.org/abs/2208.05216", "authors": ["Zhipeng Luo", "Changqing Zhou", "Liang Pan", "Gongjie Zhang", "Tianrui Liu", "Yueru Luo", "Haiyu Zhao", "Ziwei Liu", "Shijian Lu"], "publication_date": "2022/8/10", "journal": "arXiv preprint arXiv:2208.05216", "description": "With the prevalence of LiDAR sensors in autonomous driving, 3D object tracking has received increasing attention. In a point cloud sequence, 3D object tracking aims to predict the location and orientation of an object in consecutive frames given an object template. Motivated by the success of transformers, we propose Point Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D tracking results in a coarse-to-fine manner with the help of transformer operations. PTTR consists of three novel designs. 1) Instead of random sampling, we design Relation-Aware Sampling to preserve relevant points to the given template during subsampling. 2) We propose a Point Relation Transformer for effective feature aggregation and feature matching between the template and search region. 3) Based on the coarse tracking results, we employ a novel Prediction Refinement Module to obtain the final refined prediction through local feature pooling. In addition, motivated by the favorable properties of the Bird's-Eye View (BEV) of point clouds in capturing object motion, we further design a more advanced framework named PTTR++, which incorporates both the point-wise view and BEV representation to exploit their complementary effect in generating high-quality tracking results. PTTR++ substantially boosts the tracking performance on top of PTTR with low computational overhead. Extensive experiments over multiple datasets show that our proposed approaches achieve superior 3D tracking accuracy and efficiency.", "total_citations": {"2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:L_l9e5I586QC": {"external_link": "https://arxiv.org/abs/2208.02861", "authors": ["Jiahui Zhang", "Fangneng Zhan", "Yingchen Yu", "Rongliang Wu", "Xiaoqin Zhang", "Shijian Lu"], "publication_date": "2022/8/4", "journal": "arXiv preprint arXiv:2208.02861", "description": "Recently, single image super-resolution (SR) under large scaling factors has witnessed impressive progress by introducing pre-trained generative adversarial networks (GANs) as priors. However, most GAN-Priors based SR methods are constrained by an attribute disentanglement problem in inverted latent codes which directly leads to mismatches of visual attributes in the generator layers and further degraded reconstruction. In addition, stochastic noises fed to the generator are employed for unconditional detail generation, which tends to produce unfaithful details that compromise the fidelity of the generated SR image. We design LAREN, a LAtent multi-Relation rEasoNing technique that achieves superb large-factor SR through graph-based multi-relation reasoning in latent space. LAREN consists of two innovative designs. The first is graph-based disentanglement that constructs a superior disentangled latent space via hierarchical multi-relation reasoning. The second is graph-based code generation that produces image-specific codes progressively via recursive relation reasoning which enables prior GANs to generate desirable image details. Extensive experiments show that LAREN achieves superior large-factor image SR and outperforms the state-of-the-art consistently across multiple benchmarks.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:qE4H1tSSYIIC": {"external_link": "https://www.researchgate.net/profile/Gongjie-Zhang-3/publication/350312366_Meta-DETR_Few-Shot_Object_Detection_via_Unified_Image-Level_Meta-Learning/links/60616c15458515e8347bb36b/Meta-DETR-Few-Shot-Object-Detection-via-Unified-Image-Level-Meta-Learning.pdf", "authors": ["Gongjie Zhang", "Zhipeng Luo", "Kaiwen Cui", "Shijian Lu", "Eric P Xing"], "publication_date": "2022/8/2", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)", "volume": "2", "issue": "6", "description": "Few-shot object detection aims at detecting novel objects with only a few annotated examples. Prior works have proved meta-learning a promising solution, and most of them essentially address detection by meta-learning over regions for their classification and location fine-tuning. However, these methods substantially rely on initially welllocated region proposals, which are usually hard to obtain under the few-shot settings. This paper presents a novel meta-detector framework, namely Meta-DETR, which eliminates region-wise prediction and instead meta-learns object localization and classification at image level in a unified and complementary manner. Specifically, it first encodes both support and query images into category-specific features and then feeds them into a category-agnostic decoder to directly generate predictions for specific categories. To facilitate meta-learning with deep networks, we design a simple but effective Semantic Alignment Mechanism (SAM), which aligns high-level and low-level feature semantics to improve the generalization of meta-learned representations. Experiments over multiple few-shot object detection benchmarks show that Meta-DETR outperforms state-of-the-art methods by large margins.", "total_citations": {"2021": 11, "2022": 49, "2023": 52}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:AYInfyleIOsC": {"external_link": "https://arxiv.org/abs/2207.14172", "authors": ["Gongjie Zhang", "Zhipeng Luo", "Yingchen Yu", "Jiaxing Huang", "Kaiwen Cui", "Shijian Lu", "Eric P Xing"], "publication_date": "2022/7/28", "journal": "arXiv preprint arXiv:2207.14172", "description": "The recently proposed DEtection TRansformer (DETR) has established a fully end-to-end paradigm for object detection. However, DETR suffers from slow training convergence, which hinders its applicability to various detection tasks. We observe that DETR's slow convergence is largely attributed to the difficulty in matching object queries to relevant regions due to the unaligned semantics between object queries and encoded image features. With this observation, we design Semantic-Aligned-Matching DETR++ (SAM-DETR++) to accelerate DETR's convergence and improve detection performance. The core of SAM-DETR++ is a plug-and-play module that projects object queries and encoded image features into the same feature embedding space, where each object query can be easily matched to relevant regions with similar semantics. Besides, SAM-DETR++ searches for multiple representative keypoints and exploits their features for semantic-aligned matching with enhanced representation capacity. Furthermore, SAM-DETR++ can effectively fuse multi-scale features in a coarse-to-fine manner on the basis of the designed semantic-aligned matching. Extensive experiments show that the proposed SAM-DETR++ achieves superior convergence speed and competitive detection accuracy. Additionally, as a plug-and-play method, SAM-DETR++ can complement existing DETR convergence solutions with even better performance, achieving 44.8% AP with merely 12 training epochs and 49.1% AP with 50 training epochs on COCO val2017 with ResNet-50. Codes are available at https://github.com/ZhangGongjie/SAM-DETR .", "total_citations": {"2022": 2, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:YlPif8NxrbYC": {"external_link": "https://arxiv.org/abs/2112.01806", "authors": ["Shuang Wu", "Shijian Lu", "Li Cheng"], "publication_date": "2022/7/23", "journal": "IJCAI", "description": "Dance choreography for a piece of music is a challenging task, having to be creative in presenting distinctive stylistic dance elements while taking into account the musical theme and rhythm. It has been tackled by different approaches such as similarity retrieval, sequence-to-sequence modeling and generative adversarial networks, but their generated dance sequences are often short of motion realism, diversity and music consistency. In this paper, we propose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning to generate 3D dance choreographies from music. We introduce an optimal transport distance for evaluating the authenticity of the generated dance distribution and a Gromov-Wasserstein distance to measure the correspondence between the dance distribution and the input music. This gives a well defined and non-divergent training objective that mitigates the limitation of standard GAN training which is frequently plagued with instability and divergent generator loss issues. Extensive experiments demonstrate that our MDOT-Net can synthesize realistic and diverse dances which achieve an organic unity with the input music, reflecting the shared intentionality and matching the rhythmic articulation. Sample results are found at https://www.youtube.com/watch?v=dErfBkrlUO8.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:eGYfIraVYiQC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3503161.3548078", "authors": ["Jiahui Zhang", "Fangneng Zhan", "Rongliang Wu", "Yingchen Yu", "Wenqing Zhang", "Bai Song", "Xiaoqin Zhang", "Shijian Lu"], "publication_date": "2022/7/6", "journal": "ACM Multimedia", "description": "Neural Radiance Fields (NeRF) has demonstrated very impressive performance in novel view synthesis via implicitly modelling 3D representations from multi-view 2D images. However, most existing studies train NeRF models with either reasonable camera pose initialization or manually-crafted camera pose distributions which are often unavailable or hard to acquire in various real-world data. We design VMRF, an innovative view matching NeRF that enables effective NeRF training without requiring prior knowledge in camera poses or camera pose distributions. VMRF introduces a view matching scheme, which exploits unbalanced optimal transport to produce a feature transport plan for mapping a rendered image with randomly initialized camera pose to the corresponding real image. With the feature transport plan as the guidance, a novel pose calibration technique is designed which rectifies the initially\u00a0\u2026", "total_citations": {"2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:1tZ8xJnm2c8C": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3503161.3547935", "authors": ["Yingchen Yu", "Fangneng Zhan", "Rongliang Wu", "Jiahui Zhang", "Shijian Lu", "Miaomiao Cui", "Xuansong Xie", "Xian-Sheng Hua", "Chunyan Miao"], "publication_date": "2022/7/6", "journal": "ACM Multimedia", "description": "Leveraging StyleGAN's expressivity and its disentangled latent codes, existing methods can achieve realistic editing of different visual attributes such as age and gender of facial images. An intriguing yet challenging problem arises: Can generative models achieve counterfactual editing against their learnt priors? Due to the lack of counterfactual samples in natural datasets, we investigate this problem in a text-driven manner with Contrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic knowledge even for various counterfactual concepts. Different from in-domain manipulation, counterfactual manipulation requires more comprehensive exploitation of semantic knowledge encapsulated in CLIP as well as more delicate handling of editing directions for avoiding being stuck in local minimum or undesired editing. To this end, we design a novel contrastive loss that exploits predefined CLIP-space\u00a0\u2026", "total_citations": {"2021": 1, "2022": 7, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:unp9ATQDT5gC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9815287/", "authors": ["Koteswar Rao Jerripothula", "Prerana Mukherjee", "Jianfei Cai", "Shijian Lu", "Junsong Yuan"], "publication_date": "2022/7/5", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "volume": "32", "issue": "12", "pages": "8261-8274", "publisher": "IEEE", "description": "Various types of saliency cues exist, all of which can be instrumental in the foreground extraction. It brings us to an interesting problem of effectively combining them. Note that earlier works either fuse them in the spatial domain or introduce dedicated terms in the energy functions to cater to multiple cues. In contrast, this paper investigates the appearance domain and proposes a novel appearance fusion framework, which we refer to as AppFuse. It is an intuitive framework for fusing candidate appearance models into the desired one for an energy function. Thus, we do not require any alterations in the energy function anymore. Like any fusion strategy, the proposed framework also requires guidance, which we facilitate through reliability and mutual consensus phenomena. To demonstrate the efficacy, we leverage it to solve a foreground extraction problem named video co-localization, where we propose two novel\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hSRAE-fF4OAC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhan_Modulated_Contrast_for_Versatile_Image_Synthesis_CVPR_2022_paper.html", "authors": ["Fangneng Zhan", "Jiahui Zhang", "Yingchen Yu", "Rongliang Wu", "Shijian Lu"], "publication_date": "2022/6/17", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Perceiving the similarity between images has been a long-standing and fundamental problem underlying various visual generation tasks. Predominant approaches measure the inter-image distance by computing pointwise absolute deviations, which tends to estimate the median of instance distributions and leads to blurs and artifacts in the generated images. This paper presents MoNCE, a versatile metric that introduces image contrast to learn a calibrated metric for the perception of multifaceted inter-image distances. Unlike vanilla contrast which indiscriminately pushes negative samples from the anchor regardless of their similarity, we propose to re-weight the pushing force of negative samples adaptively according to their similarity to the anchor, which facilitates the contrastive learning from informative negative samples. Since multiple patch-level contrastive objectives are involved in image distance measurement, we introduce optimal transport in MoNCE to modulate the pushing force of negative samples collaboratively across multiple contrastive objectives. Extensive experiments over multiple image translation tasks show that the proposed MoNCE outperforms various prevailing metrics substantially.", "total_citations": {"2021": 1, "2022": 30, "2023": 28}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:2v_ZtQDX9iAC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhou_PTTR_Relational_3D_Point_Cloud_Object_Tracking_With_Transformer_CVPR_2022_paper.html", "authors": ["Changqing Zhou", "Zhipeng Luo", "Yueru Luo", "Tianrui Liu", "Liang Pan", "Zhongang Cai", "Haiyu Zhao", "Shijian Lu"], "publication_date": "2022/6/12", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "In a point cloud sequence, 3D object tracking aims to predict the location and orientation of an object in the current search point cloud given a template point cloud. Motivated by the success of transformers, we propose Point Tracking TRansformer (PTTR), which efficiently predicts high-quality 3D tracking results in a coarse-to-fine manner with the help of transformer operations. PTTR consists of three novel designs. 1) Instead of random sampling, we design Relation-Aware Sampling to preserve relevant points to given templates during subsampling. 2) Furthermore, we propose a Point Relation Transformer (PRT) consisting of a self-attention and a cross-attention module. The global self-attention operation captures long-range dependencies to enhance encoded point features for the search area and the template, respectively. Subsequently, we generate the coarse tracking results by matching the two sets of point features via cross-attention. 3) Based on the coarse tracking results, we employ a novel Prediction Refinement Module to obtain the final refined prediction. In addition, we create a large-scale point cloud single object tracking benchmark based on the Waymo Open Dataset. Extensive experiments show that PTTR achieves superior point cloud tracking in both accuracy and efficiency. Our code and dataset will be released upon acceptance.", "total_citations": {"2022": 15, "2023": 37}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:UuEBAcK4md4C": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Spectral_Unsupervised_Domain_Adaptation_for_Visual_Recognition_CVPR_2022_paper.html", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Shijian Lu"], "publication_date": "2022/6/11", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Though unsupervised domain adaptation (UDA) has achieved very impressive progress recently, it remains a great challenge due to missing target annotations and the rich discrepancy between source and target distributions. We propose Spectral UDA (SUDA), an effective and efficient UDA technique that works in the spectral space and can generalize across different visual recognition tasks. SUDA addresses the UDA challenges from two perspectives. First, it introduces a spectrum transformer (ST) that mitigates inter-domain discrepancies by enhancing domain-invariant spectra while suppressing domain-variant spectra of source and target samples simultaneously. Second, it introduces multi-view spectral learning that learns useful unsupervised representations by maximizing mutual information among multiple ST-generated spectral views of each target sample. Extensive experiments show that SUDA achieves superior accuracy consistently across different visual tasks in image classification, semantic segmentation, and object detection. Additionally, SUDA also works with the transformer-based network and achieves state-of-the-art performance on object detection.", "total_citations": {"2021": 3, "2022": 9, "2023": 30}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:SIv7DqKytYAC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Huang_Category_Contrast_for_Unsupervised_Domain_Adaptation_in_Visual_Tasks_CVPR_2022_paper.html", "authors": ["Jiaxing Huang", "Dayan Guan", "Aoran Xiao", "Shijian Lu", "Ling Shao"], "publication_date": "2022/6/5", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Instance contrast for unsupervised representation learning has achieved great success in recent years. In this work, we explore the idea of instance contrastive learning in unsupervised domain adaptation (UDA) and propose a novel Category Contrast technique (CaCo) that introduces semantic priors on top of instance discrimination for visual UDA tasks. By considering instance contrastive learning as a dictionary look-up operation, we construct a semantics-aware dictionary with samples from both source and target domains where each target sample is assigned a (pseudo) category label based on the category priors of source samples. This allows category contrastive learning (between target queries and the category-level dictionary) for category-discriminative yet domain-invariant feature representations: samples of the same category (from either source or target domain) are pulled closer while those of different categories are pushed apart simultaneously. Extensive UDA experiments in multiple visual tasks (eg, segmentation, classification and detection) show that CaCo achieves superior performance as compared with state-of-the-art methods. The experiments also demonstrate that CaCo is complementary to existing UDA methods and generalizable to other learning setups such as unsupervised model adaptation, open-/partial-set adaptation etc.", "total_citations": {"2021": 5, "2022": 18, "2023": 57}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:X9ykpCP0fEIC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhan_Marginal_Contrastive_Correspondence_for_Guided_Image_Generation_CVPR_2022_paper.html", "authors": ["Fangneng Zhan", "Yingchen Yu", "Rongliang Wu", "Jiahui Zhang", "Shijian Lu", "Changgong Zhang"], "publication_date": "2022/6/1", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Exemplar-based image translation establishes dense correspondences between a conditional input and an exemplar (from two different domains) for leveraging detailed exemplar styles to achieve realistic image translation. Existing work builds the cross-domain correspondences implicitly by minimizing feature-wise distances across the two domains. Without explicit exploitation of domain-invariant features, this approach may not reduce the domain gap effectively which often leads to sub-optimal correspondences and image translation. We design a Marginal Contrastive Learning Network (MCL-Net) that explores contrastive learning to learn domain-invariant features for realistic exemplar-based image translation. Specifically, we design an innovative marginal contrastive loss that guides to establish dense correspondences explicitly. Nevertheless, building correspondence with domain-invariant semantics alone may impair the texture patterns and lead to degraded texture generation. We thus design a Self-Correlation Map (SCM) that incorporates scene structures as auxiliary information which improves the built correspondences substantially. Quantitative and qualitative experiments on multifarious image translation tasks show that the proposed method outperforms the state-of-the-art consistently.", "total_citations": {"2021": 1, "2022": 18, "2023": 21}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:-DxkuPiZhfEC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Xue_Fourier_Document_Restoration_for_Robust_Document_Dewarping_and_Recognition_CVPR_2022_paper.html", "authors": ["Chuhui Xue", "Zichen Tian", "Fangneng Zhan", "Shijian Lu", "Song Bai"], "publication_date": "2022/6", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "State-of-the-art document dewarping techniques learn to predict 3-dimensional information of documents which are prone to errors while dealing with documents with irregular distortions or large variations in depth. This paper presents FDRNet, a Fourier Document Restoration Network that can restore documents with different distortions and improve document recognition in a reliable and simpler manner. FDRNet focuses on high-frequency components in the Fourier space that capture most structural information but are largely free of degradation in appearance. It dewarps documents by a flexible Thin-Plate Spline transformation which can handle various deformations effectively without requiring deformation annotations in training. These features allow FDRNet to learn from a small amount of simply labeled training images, and the learned model can dewarp documents with complex geometric distortion and recognize the restored texts accurately. To facilitate document restoration research, we create a benchmark dataset consisting of over one thousand camera documents with different types of geometric and photometric distortion. Extensive experiments show that FDRNet outperforms the state-of-the-art by large margins on both dewarping and text recognition tasks. In addition, FDRNet requires a small amount of simply labeled training data and is easy to deploy.", "total_citations": {"2021": 1, "2022": 4, "2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:v6i8RKmR8ToC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Guan_Unbiased_Subclass_Regularization_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html", "authors": ["Dayan Guan", "Jiaxing Huang", "Aoran Xiao", "Shijian Lu"], "publication_date": "2022/6", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "Semi-supervised semantic segmentation learns from small amounts of labelled images and large amounts of unlabelled images, which has witnessed impressive progress with the recent advance of deep neural networks. However, it often suffers from severe class-bias problem while exploring the unlabelled images, largely due to the clear pixel-wise class imbalance in the labelled images. This paper presents an unbiased subclass regularization network (USRN) that alleviates the class imbalance issue by learning class-unbiased segmentation from balanced subclass distributions. We build the balanced subclass distributions by clustering pixels of each original class into multiple subclasses of similar sizes, which provide class-balanced pseudo supervision to regularize the class-biased segmentation. In addition, we design an entropy-based gate mechanism to coordinate learning between the original classes and the clustered subclasses which facilitates subclass regularization effectively by suppressing unconfident subclass predictions. Extensive experiments over multiple public benchmarks show that USRN achieves superior performance as compared with the state-of-the-art. The code will be made available on Github.", "total_citations": {"2022": 8, "2023": 14}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:qwy9JoKyICEC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.html", "authors": ["Gongjie Zhang", "Zhipeng Luo", "Yingchen Yu", "Kaiwen Cui", "Shijian Lu"], "publication_date": "2022/6", "journal": "EEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "description": "The recently developed DEtection TRansformer (DETR) establishes a new object detection paradigm by eliminating a series of hand-crafted components. However, DETR suffers from extremely slow convergence, which increases the training cost significantly. We observe that the slow convergence is largely attributed to the complication in matching object queries with target features in different feature embedding spaces. This paper presents SAM-DETR, a Semantic-Aligned-Matching DETR that greatly accelerates DETR's convergence without sacrificing its accuracy. SAM-DETR addresses the convergence issue from two perspectives. First, it projects object queries into the same embedding space as encoded image features, where the matching can be accomplished efficiently with aligned semantics. Second, it explicitly searches salient points with the most discriminative features for semantic-aligned matching, which further speeds up the convergence and boosts detection accuracy as well. Being like a plug and play, SAM-DETR complements existing convergence solutions well yet only introduces slight computational overhead. Extensive experiments show that the proposed SAM-DETR achieves superior convergence as well as competitive detection accuracy. The implementation codes are publicly available at https://github. com/ZhangGongjie/SAM-DETR.", "total_citations": {"2022": 16, "2023": 40}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:wvYxNZNCP7wC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9725240/", "authors": ["Fangneng Zhan", "Yingchen Yu", "Rongliang Wu", "Changgong Zhang", "Shijian Lu", "Ling Shao", "Feiying Ma", "Xuansong Xie"], "publication_date": "2022/6", "journal": "IEEE Transactions on Image Processing", "description": "Inferring the scene illumination from a single image is an essential yet challenging task in computer vision and computer graphics. Existing works estimate lighting by regressing representative illumination parameters or generating illumination maps directly. However, these methods often suffer from poor accuracy and generalization. This paper presents Geometric Mover\u2019s Light (GMLight), a lighting estimation framework that employs a regression network and a generative projector for effective illumination estimation. We parameterize illumination scenes in terms of the geometric light distribution, light intensity, ambient term, and auxiliary depth, which can be estimated by a regression network. Inspired by the earth mover\u2019s distance, we design a novel geometric mover\u2019s loss to guide the accurate regression of light distribution parameters. With the estimated light parameters, the generative projector synthesizes\u00a0\u2026", "total_citations": {"2021": 12, "2022": 28, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:DyXnQzXoVgIC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9674853/", "authors": ["Mengxi Jia", "Xinhua Cheng", "Shijian Lu", "Jian Zhang"], "publication_date": "2022/3/6", "journal": "IEEE Transaction on Multimedia", "description": "Person re-IDentification (re-ID) under various occlusions has been a long-standing challenge as person images with different types of occlusions often suffer from misalignment in image matching and ranking. Most existing methods tackle this challenge by aligning spatial features of body parts according to external semantic cues or feature similarities but this alignment approach is complicated and sensitive to noises. We design DRL-Net, a disentangled representation learning network that handles occluded re-ID without requiring strict person image alignment or any additional supervision. Leveraging transformer architectures, DRL-Net achieves alignment-free re-ID via global reasoning of local features of occluded person images. It measures image similarity by automatically disentangling the representation of undefined semantic components, e.g., human body parts or obstacles, under the guidance of semantic\u00a0\u2026", "total_citations": {"2022": 25, "2023": 40}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:QsaTk4IG4EwC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320321006701", "authors": ["Chuhui Xue", "Shijian Lu", "Steven Hoi"], "publication_date": "2022/3/1", "journal": "Pattern Recognition", "description": "Detection and recognition of scene texts of arbitrary shapes remain a grand challenge due to the super-rich text shape variation in text line orientations, lengths, curvatures, etc. This paper presents a mask-guided multi-task network that detects and rectifies scene texts of arbitrary shapes reliably. Three types of keypoints are detected which specify the centre line and so the shape of text instances accurately. In addition, four types of keypoint links are detected of which the horizontal links associate the detected keypoints of each text instance and the vertical links predict a pair of landmark points (for each keypoint) along the upper and lower text boundary, respectively. Scene texts can be located and rectified by linking up the associated landmark points (giving localization polygon boxes) and transforming the polygon boxes via thin plate spline, respectively. Extensive experiments over several public datasets show\u00a0\u2026", "total_citations": {"2021": 1, "2022": 7, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LXmCCkuhhTsC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320321005641", "authors": ["Jiaxing Huang", "Dayan Guan", "Aoran Xiao", "Shijian Lu"], "publication_date": "2022/3/1", "journal": "Pattern Recognition", "pages": "108384", "publisher": "Pergamon", "description": "Recent progresses in domain adaptive semantic segmentation demonstrate the effectiveness of adversarial learning (AL) in unsupervised domain adaptation. However, most adversarial learning based methods align source and target distributions at a global image level but neglect the inconsistency around local image regions. This paper presents a novel multi-level adversarial network (MLAN) that aims to address inter-domain inconsistency at both global image level and local region level optimally. MLAN has two novel designs, namely, region-level adversarial learning (RL-AL) and co-regularized adversarial learning (CR-AL). Specifically, RL-AL models prototypical regional context-relations explicitly in the feature space of a labelled source domain and transfers them to an unlabelled target domain via adversarial learning. CR-AL fuses region-level AL and image-level AL optimally via mutual regularization. In\u00a0\u2026", "total_citations": {"2021": 7, "2022": 12, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:qe6vwMD2xtsC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0950705121010558", "authors": ["Xian Li", "Xiaofei Yang", "Xutao Li", "Shijian Lu", "Yunming Ye", "Yifang Ban"], "publication_date": "2022/2/28", "journal": "Knowledge-Based Systems", "volume": "238", "pages": "107890", "publisher": "Elsevier", "description": "Cloud detection is a prerequisite in many remote sensing applications, and it has been tackled through different approaches from simple thresholding to complicated deep network training. On the other hand, existing approaches are susceptible to failures while handling thin clouds, largely because of their small sizes, sparse distributions, as well as high transparency and similarity to the non-cloud background regions. This paper presents global context dense block U-Net (GCDB-UNet), a robust cloud detection network that embeds global context dense block (GCDB) into the U-Net framework and is capable of detecting thin clouds effectively. GCDB consists of two feature extraction units for addressing the challenges in thin cloud detection, namely, a non-local self-attention unit that extracts sample correlation features by aggregating the sparsely distributed thin clouds and a squeeze excitation unit that extracts\u00a0\u2026", "total_citations": {"2022": 10, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:sA9dB-pw3HoC": {"external_link": "https://scholar.google.com/scholar?cluster=11178553343016853105&hl=en&oi=scholarr", "authors": ["Aoran Xiao", "Jiaxing Huang", "Dayan Guan", "Fangneng Zhan", "Shijian Lu"], "publication_date": "2022/2/22", "journal": "AAAI Conference on Artificial Intelligence (AAAI)", "total_citations": {"2021": 1, "2022": 15, "2023": 40}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:k_7cPK9k7w8C": {"external_link": "https://scholar.google.com/scholar?cluster=11867165632284327891&hl=en&oi=scholarr", "authors": ["Kaiwen Cui", "Jiaxing Huang", "Zhipeng Luo", "Gongjie Zhang", "Fangneng Zhan", "Shijian Lu"], "publication_date": "2022/2/22", "conference": "AAAI Conference on Artificial Intelligence (AAAI)", "total_citations": {"2022": 5, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:8Xgff_V0N9gC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9669004/", "authors": ["Zhenguang Liu", "Shuang Wu", "Shuyuan Jin", "Shouling Ji", "Qi Liu", "Shijian Lu", "Li Cheng"], "publication_date": "2022/1/4", "journal": "IEEE transactions on pattern analysis and machine intelligence", "volume": "45", "issue": "1", "pages": "681-697", "publisher": "IEEE", "description": "Predicting human motion from historical pose sequence is crucial for a machine to succeed in intelligent interactions with humans. One aspect that has been obviated so far, is the fact that how we represent the skeletal pose has a critical impact on the prediction results. Yet there is no effort that investigates across different pose representation schemes. We conduct an indepth study on various pose representations with a focus on their effects on the motion prediction task. Moreover, recent approaches build upon off-the-shelf RNN units for motion prediction. These approaches process input pose sequence sequentially and inherently have difficulties in capturing long-term dependencies. In this paper, we propose a novel RNN architecture termed AHMR (Attentive Hierarchical Motion Recurrent network) for motion prediction which simultaneously models local motion contexts and a global context. We further explore a\u00a0\u2026", "total_citations": {"2022": 24, "2023": 22}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:65Yg0jNCQDAC": {"external_link": "https://proceedings.neurips.cc/paper/2021/hash/1dba5eed8838571e1c80af145184e515-Abstract.html", "authors": ["Jiaxing Huang", "Dayan Guan", "Aoran Xiao", "Shijian Lu"], "publication_date": "2021/12/6", "journal": "Advances in Neural Information Processing Systems", "volume": "34", "pages": "3635-3649", "description": "Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups.", "total_citations": {"2021": 5, "2022": 38, "2023": 83}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:jE2MZjpN3IcC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9439889/", "authors": ["Dayan Guan", "Jiaxing Huang", "Aoran Xiao", "Shijian Lu", "Yanpeng Cao"], "publication_date": "2021/12", "journal": "IEEE Transaction on Multimedia", "description": "Unsupervised domain adaptive object detection aims to adapt detectors from a labelled source domain to an unlabelled target domain. Most existing works take a two-stage strategy that first generates region proposals and then detects objects of interest, where adversarial learning is widely adopted to mitigate the inter-domain discrepancy in both stages. However, adversarial learning may impair the alignment of well-aligned samples as it merely aligns the global distributions across domains. To address this issue, we design an uncertainty-aware domain adaptation network (UaDAN) that introduces conditional adversarial learning to align well-aligned and poorly-aligned samples separately in different manners. Specifically, we design an uncertainty metric that assesses the alignment of each sample and adjusts the strength of adversarial learning for well-aligned and poorly-aligned samples adaptively. In addition\u00a0\u2026", "total_citations": {"2021": 12, "2022": 31, "2023": 41}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Hck25ST_3aIC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3474085.3475180", "authors": ["Shuang Wu", "Zhenguang Liu", "Shijian Lu", "Li Cheng"], "publication_date": "2021/10/17", "book": "Proceedings of the 29th ACM International Conference on Multimedia", "pages": "3746-3754", "description": "Music and dance have always co-existed as pillars of human activities, contributing immensely to the cultural, social, and entertainment functions in virtually all societies. Notwithstanding the gradual systematization of music and dance into two independent disciplines, their intimate connection is undeniable and one art-form often appears incomplete without the other. Recent research works have studied generative models for dance sequences conditioned on music. The dual task of composing music for given dances, however, has been largely overlooked. In this paper, we propose a novel extension, where we jointly model both tasks in a dual learning approach. To leverage the duality of the two modalities, we introduce an optimal transport objective to align feature embeddings, as well as a cycle consistency loss to foster overall consistency. Experimental results demonstrate that our dual learning framework\u00a0\u2026", "total_citations": {"2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:O0nohqN1r9EC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/3474085.3475436", "authors": ["Yingchen Yu", "Fangneng Zhan", "Rongliang Wu", "Jianxiong Pan", "Kaiwen Cui", "Shijian Lu", "Feiying Ma", "Xuansong Xie", "Chunyan Miao"], "publication_date": "2021/10/17", "journal": "Proceedings of the 29th ACM International Conference on Multimedia", "pages": "69 - 78", "description": "Image inpainting is an underdetermined inverse problem, which naturally allows diverse contents to fill up the missing or corrupted regions realistically. Prevalent approaches using convolutional neural networks (CNNs) can synthesize visually pleasant contents, but CNNs suffer from limited perception fields for capturing global features. With image-level attention, transformers enable to model long-range dependencies and generate diverse contents with autoregressive modeling of pixel-sequence distributions. However, the unidirectional attention in autoregressive transformers is suboptimal as corrupted image regions may have arbitrary shapes with contexts from any direction. We propose BAT-Fill, an innovative image inpainting framework that introduces a novel bidirectional autoregressive transformer (BAT) for image inpainting. BAT utilizes the transformers to learn autoregressive distributions, which naturally\u00a0\u2026", "total_citations": {"2021": 9, "2022": 44, "2023": 31}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:PyEswDtIyv0C": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0925231221002502", "authors": ["Chun-Mei Feng", "Kai Wang", "Shijian Lu", "Yong Xu", "Xuelong Li"], "publication_date": "2021/10/7", "journal": "Neurocomputing", "volume": "456", "pages": "190-199", "publisher": "Elsevier", "description": "Magnetic Resonance Imaging (MRI) has been widely used in clinical application and pathology research to help doctors provide better diagnoses. However, accurate diagnosis by MRI remains a great challenge, as images obtained via current MRI techniques usually have low resolutions. Improving MRI image quality and resolution has thus become a critically important task. This paper presents an innovative Coupled-Projection Residual Network (CPRN) for MRI super-resolution. CPRN consists of two complementary sub-networks: a shallow network and a deep one, which maintain content consistency while learning high frequency differences between low-resolution and high-resolution images. The shallow sub-network employs coupled-projection to better retain the MR image details, where a novel feedback mechanism is introduced to guide the reconstruction of high-resolution images. The deep sub-network\u00a0\u2026", "total_citations": {"2020": 2, "2021": 6, "2022": 9, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:PkcyUWeTMh0C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9524559/", "authors": ["Tianjiao Li", "Wei Zhang", "Ran Song", "Zhiheng Li", "Jun Liu", "Xiaolei Li", "Shijian Lu"], "publication_date": "2021/8/27", "journal": "IEEE Transactions on Image Processing", "publisher": "IEEE", "description": "Pose-based person image synthesis aims to generate a new image containing a person with a target pose conditioned on a source image containing a person with a specified pose. It is challenging as the target pose is arbitrary and often significantly differs from the specified source pose, which leads to large appearance discrepancy between the source and the target images. This paper presents the Pose Transform Generative Adversarial Network (PoT-GAN) for person image synthesis where the generator explicitly learns the transform between the two poses by manipulating the corresponding multi-scale feature maps. By incorporating the learned pose transform information into the multi-scale feature maps of the source image in a GAN architecture, our method reliably transfers the appearance of the person in the source image to the target pose with no need for any hard-coded spatial information depicting the\u00a0\u2026", "total_citations": {"2021": 1, "2022": 6, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:mWEH9CqjF64C": {"external_link": "http://openaccess.thecvf.com/content/ICCV2021/html/Yang_Skeleton_Cloud_Colorization_for_Unsupervised_3D_Action_Representation_Learning_ICCV_2021_paper.html", "authors": ["Siyuan Yang", "Jun Liu", "Shijian Lu", "Meng Hwa Er", "Alex C Kot"], "publication_date": "2021/8/4", "journal": "ICCV", "description": "Skeleton-based human action recognition has attracted increasing attention in recent years. However, most of the existing works focus on supervised learning which requiring a large number of annotated action sequences that are often expensive to collect. We investigate unsupervised representation learning for skeleton action recognition, and design a novel skeleton cloud colorization technique that is capable of learning skeleton representations from unlabeled skeleton sequence data. Specifically, we represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. We evaluate our skeleton cloud colorization approach with action classifiers trained under different configurations, including unsupervised, semi-supervised and fully-supervised settings. Extensive experiments on NTU RGB+ D and NW-UCLA datasets show that the proposed method outperforms existing unsupervised and semi-supervised 3D action recognition methods by large margins, and it achieves competitive performance in supervised 3D action recognition as well.", "total_citations": {"2021": 2, "2022": 25, "2023": 38}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:rTD5ala9j4wC": {"external_link": "http://openaccess.thecvf.com/content/ICCV2021/html/Luo_Unsupervised_Domain_Adaptive_3D_Detection_With_Multi-Level_Consistency_ICCV_2021_paper.html", "authors": ["Zhipeng Luo", "Zhongang Cai", "Changqing Zhou", "Gongjie Zhang", "Haiyu Zhao", "Shuai Yi", "Shijian Lu", "Hongsheng Li", "Shanghang Zhang", "Ziwei Liu"], "publication_date": "2021/7/23", "journal": "ICCV", "description": "Deep learning-based 3D object detection has achieved unprecedented success with the advent of large-scale autonomous driving datasets. However, drastic performance degradation remains a critical challenge for cross-domain deployment. In addition, existing 3D domain adaptive detection methods often assume prior access to the target domain annotations, which is rarely feasible in the real world. To address this challenge, we study a more realistic setting, unsupervised 3D domain adaptive detection, which only utilizes source domain annotations. 1) We first comprehensively investigate the major underlying factors of the domain gap in 3D detection. Our key insight is that geometric mismatch is the key factor of domain shift. 2) Then, we propose a novel and unified framework, Multi-Level Consistency Network (MLC-Net), which employs a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net exploits point-, instance-and neural statistics-level consistency to facilitate cross-domain transfer. Extensive experiments demonstrate that MLC-Net outperforms existing state-of-the-art methods (including those using additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic, which achieves consistent gains on both single-and two-stage 3D detectors. Code will be released.", "total_citations": {"2021": 5, "2022": 27, "2023": 30}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:pS0ncopqnHgC": {"external_link": "https://openaccess.thecvf.com/content/ICCV2021/html/Yu_WaveFill_A_Wavelet-Based_Generation_Network_for_Image_Inpainting_ICCV_2021_paper.html?ref=https://githubhelp.com", "authors": ["Yingchen Yu", "Fangneng Zhan", "Shijian Lu", "Jianxiong Pan", "Feiying Ma", "Xuansong Xie", "Chunyan Miao"], "publication_date": "2021/7/23", "journal": "ICCV", "description": "Image inpainting aims to complete the missing or corrupted regions of images with realistic contents. The prevalent approaches adopt a hybrid objective of reconstruction and perceptual quality by using generative adversarial networks. However, the reconstruction loss and adversarial loss focus on synthesizing contents of different frequencies and simply applying them together often leads to inter-frequency conflicts and compromised inpainting. This paper presents WaveFill, a wavelet-based inpainting network that decomposes images into multiple frequency bands and fills the missing regions in each frequency band separately and explicitly. WaveFill decomposes images by using discrete wavelet transform (DWT) that preserves spatial information naturally. It applies L1 reconstruction loss to the decomposed low-frequency bands and adversarial loss to high-frequency bands, hence effectively mitigate inter-frequency conflicts while completing images in spatial domain. To address the inpainting inconsistency in different frequency bands and fuse features with distinct statistics, we design a novel normalization scheme that aligns and fuses the multi-frequency features effectively. Extensive experiments over multiple datasets show that WaveFill achieves superior image inpainting qualitatively and quantitatively.", "total_citations": {"2021": 1, "2022": 26, "2023": 33}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:eO3_k5sD8BwC": {"external_link": "http://openaccess.thecvf.com/content/ICCV2021/html/Guan_Domain_Adaptive_Video_Segmentation_via_Temporal_Consistency_Regularization_ICCV_2021_paper.html", "authors": ["Dayan Guan", "Jiaxing Huang", "Aoran Xiao", "Shijian Lu"], "publication_date": "2021/7/23", "journal": "ICCV", "description": "Video semantic segmentation is an essential task for the analysis and understanding of videos. Recent efforts largely focus on supervised video segmentation by learning from fully annotated data, but the learnt models often experience clear performance drop while applied to videos of a different domain. This paper presents DA-VSN, a domain adaptive video segmentation network that addresses domain gaps in videos by temporal consistency regularization (TCR) for consecutive frames of target-domain videos. DA-VSN consists of two novel and complementary designs. The first is cross-domain TCR that guides the prediction of target frames to have similar temporal consistency as that of source frames (learnt from annotated source data) via adversarial learning. The second is intra-domain TCR that guides unconfident predictions of target frames to have similar temporal consistency as confident predictions of target frames. Extensive experiments demonstrate the superiority of our proposed domain adaptive video segmentation network which outperforms multiple baselines consistently by large margins.", "total_citations": {"2021": 2, "2022": 16, "2023": 14}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:__bU50VfleQC": {"external_link": "https://arxiv.org/abs/2107.03166", "authors": ["Kaiwen Cui", "Gongjie Zhang", "Fangneng Zhan", "Jiaxing Huang", "Shijian Lu"], "publication_date": "2021/7/7", "journal": "arXiv preprint arXiv:2107.03166", "description": "Generative Adversarial Networks (GANs) have become the de-facto standard in image synthesis. However, without considering the foreground-background decomposition, existing GANs tend to capture excessive content correlation between foreground and background, thus constraining the diversity in image generation. This paper presents a novel Foreground-Background Composition GAN (FBC-GAN) that performs image generation by generating foreground objects and background scenes concurrently and independently, followed by composing them with style and geometrical consistency. With this explicit design, FBC-GAN can generate images with foregrounds and backgrounds that are mutually independent in contents, thus lifting the undesirably learned content correlation constraint and achieving superior diversity. It also provides excellent flexibility by allowing the same foreground object with different background scenes, the same background scene with varying foreground objects, or the same foreground object and background scene with different object positions, sizes and poses. It can compose foreground objects and background scenes sampled from different datasets as well. Extensive experiments over multiple datasets show that FBC-GAN achieves competitive visual realism and superior diversity as compared with state-of-the-art methods.", "total_citations": {"2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:IsPWOBWtZBwC": {"external_link": "https://arxiv.org/abs/2107.00708", "authors": ["Jiahui Zhang", "Shijian Lu", "Fangneng Zhan", "Yingchen Yu"], "publication_date": "2021/7/1", "journal": "arXiv preprint arXiv:2107.00708", "description": "Image super-resolution (SR) research has witnessed impressive progress thanks to the advance of convolutional neural networks (CNNs) in recent years. However, most existing SR methods are non-blind and assume that degradation has a single fixed and known distribution (e.g., bicubic) which struggle while handling degradation in real-world data that usually follows a multi-modal, spatially variant, and unknown distribution. The recent blind SR studies address this issue via degradation estimation, but they do not generalize well to multi-source degradation and cannot handle spatially variant degradation. We design CRL-SR, a contrastive representation learning network that focuses on blind SR of images with multi-modal and spatially variant distributions. CRL-SR addresses the blind SR challenges from two perspectives. The first is contrastive decoupling encoding which introduces contrastive learning to extract resolution-invariant embedding and discard resolution-variant embedding under the guidance of a bidirectional contrastive loss. The second is contrastive feature refinement which generates lost or corrupted high-frequency details under the guidance of a conditional contrastive loss. Extensive experiments on synthetic datasets and real images show that the proposed CRL-SR can handle multi-modal and spatially variant degradation effectively under blind settings and it also outperforms state-of-the-art SR methods qualitatively and quantitatively.", "total_citations": {"2021": 1, "2022": 15, "2023": 11}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:-nhnvRiOwuoC": {"external_link": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhan_Sparse_Needlets_for_Lighting_Estimation_With_Spherical_Transport_Loss_ICCV_2021_paper.html", "authors": ["Fangneng Zhan", "Changgong Zhang", "Wenbo Hu", "Shijian Lu", "Feiying Ma", "Xuansong Xie", "Ling Shao"], "publication_date": "2021/6/24", "journal": "ICCV", "description": "Accurate lighting estimation is challenging yet critical to many computer vision and computer graphics tasks such as high-dynamic-range (HDR) relighting. Existing approaches model lighting in either frequency domain or spatial domain which is insufficient to represent the complex lighting conditions in scenes and tends to produce inaccurate estimation. This paper presents NeedleLight, a new lighting estimation model that represents illumination with needlets and allows lighting estimation in both frequency domain and spatial domain jointly. An optimal thresholding function is designed to achieve sparse needlets which trims redundant lighting parameters and demonstrates superior localization properties for illumination representation. In addition, a novel spherical transport loss is designed based on optimal transport theory which guides to regress lighting representation parameters with consideration of the spatial information. Furthermore, we propose a new metric that is concise yet effective by directly evaluating the estimated illumination maps rather than rendered images. Extensive experiments show that NeedleLight achieves superior lighting estimation consistently across multiple evaluation metrics as compared with state-of-the-art methods.", "total_citations": {"2020": 2, "2021": 5, "2022": 38, "2023": 25}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:UmS_249rOGwC": {"external_link": "https://arxiv.org/abs/2106.02845", "authors": ["Jiaxing Huang", "Dayan Guan", "Aoran Xiao", "Shijian Lu"], "publication_date": "2021/6/5", "journal": "arXiv preprint arXiv:2106.02845", "description": "Contemporary domain adaptive semantic segmentation aims to address data annotation challenges by assuming that target domains are completely unannotated. However, annotating a few target samples is usually very manageable and worthwhile especially if it improves the adaptation performance substantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptive image Segmentation network that employs a few labeled target samples as anchors for adaptive and progressive feature alignment between labeled source samples and unlabeled target samples. We position the few labeled target samples as references that gauge the similarity between source and target features and guide adaptive inter-domain alignment for learning more similar source features. In addition, we replace the dissimilar source features by high-confidence target features continuously during the iterative training process, which achieves progressive intra-domain alignment between confident and unconfident target features. Extensive experiments show the proposed SSDAS greatly outperforms a number of baselines, i.e., UDA-based semantic segmentation and SSDA-based image classification. In addition, SSDAS is complementary and can be easily incorporated into UDA-based methods with consistent improvements in domain adaptive semantic segmentation.", "total_citations": {"2021": 2, "2022": 3, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:pAkWuXOU-OoC": {"external_link": "http://openaccess.thecvf.com/content/ICCV2021/html/Huang_RDA_Robust_Domain_Adaptation_via_Fourier_Adversarial_Attacking_ICCV_2021_paper.html", "authors": ["Jiaxing Huang", "Dayan Guan", "Aoran Xiao", "Shijian Lu"], "publication_date": "2021/6/5", "journal": "ICCV", "description": "Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled source domain and an unsupervised loss in an unlabeled target domain, which often faces more severe overfitting (than classical supervised learning) as the supervised source loss has clear domain gap and the unsupervised target loss is often noisy due to the lack of annotations. This paper presents RDA, a robust domain adaptation technique that introduces adversarial attacking to mitigate overfitting in UDA. We achieve robust domain adaptation by a novel Fourier adversarial attacking (FAA) method that allows large magnitude of perturbation noises but has minimal modification of image semantics, the former is critical to the effectiveness of its generated adversarial samples due to the existence of domain gaps. Specifically, FAA decomposes images into multiple frequency components (FCs) and generates adversarial samples by just perturbating certain FCs that capture little semantic information. With FAA-generated samples, the training can continue the random walk and drift into an area with a flat loss landscape, leading to more robust domain adaptation. Extensive experiments over multiple domain adaptation tasks show that RDA can work with different computer vision tasks with superior performance.", "total_citations": {"2021": 8, "2022": 18, "2023": 26}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:1DsIQWDZLl8C": {"external_link": "http://openaccess.thecvf.com/content/CVPR2021/html/Zhan_Unbalanced_Feature_Transport_for_Exemplar-Based_Image_Translation_CVPR_2021_paper.html", "authors": ["Fangneng Zhan", "Yingchen Yu", "Kaiwen Cui", "Gongjie Zhang", "Shijian Lu", "Jianxiong Pan", "Changgong Zhang", "Feiying Ma", "Xuansong Xie", "Chunyan Miao"], "publication_date": "2021/6", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "volume": "21", "description": "Despite the great success of GANs in images translation with different conditioned inputs such as semantic segmentation and edge map, generating high-fidelity images with reference styles from exemplars remains a grand challenge in conditional image-to-image translation. This paper presents a general image translation framework that incorporates optimal transport for feature alignment between conditional inputs and style exemplars in translation. The introduction of optimal transport mitigates the constraint of many-to-one feature matching significantly while building up semantic correspondences between conditional inputs and exemplars. We design a novel unbalanced optimal transport to address the transport between features with deviational distributions which exists widely between conditional inputs and exemplars. In addition, we design a semantic-aware normalization scheme that injects style and semantic features of exemplars into the image translation process successfully. Extensive experiments over multiple image translation tasks show that our proposed technique achieves superior image translation qualitatively and quantitatively as compared with the state-of-the-art.", "total_citations": {"2020": 2, "2021": 29, "2022": 73, "2023": 28}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:OBSaB-F7qqsC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2021/html/Huang_FSDR_Frequency_Space_Domain_Randomization_for_Domain_Generalization_CVPR_2021_paper.html", "authors": ["Jiaxing Huang", "Dayan Guan", "Aoran Xiao", "Shijian Lu"], "publication_date": "2021/6", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "volume": "21", "description": "Domain generalization aims to learn a generalizable model from aknown'source domain for variousunknown'target domains. It has been studied widely by domain randomization that transfers source images to different styles in spatial space for learning domain-agnostic features. However, most existing randomization methods use GANs that often lack of controls and even alter semantic structures of images undesirably. Inspired by the idea of JPEG that converts spatial images into multiple frequency components (FCs), we propose Frequency Space Domain Randomization (FSDR) that randomizes images in frequency space by keeping domain-invariant FCs (DIFs) and randomizing domain-variant FCs (DVFs) only. FSDR has two unique features: 1) it decomposes images into DIFs and DVFs which allows explicit access and manipulation of them and more controllable randomization; 2) it has minimal effects on semantic structures of images and domain-invariant features. We examined domain variance and invariance property of FCs statistically and designed a network that can identify and fuse DIFs and DVFs dynamically through iterative learning. Extensive experiments over multiple domain generalizable segmentation tasks show that FSDR achieves superior segmentation and its performance is even on par with domain adaptation methods that access target data in training.", "total_citations": {"2021": 16, "2022": 55, "2023": 71}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:HGTzPopzzJcC": {"external_link": "http://openaccess.thecvf.com/content/CVPR2021/html/Huang_Cross-View_Regularization_for_Domain_Adaptive_Panoptic_Segmentation_CVPR_2021_paper.html", "authors": ["Jiaxing Huang", "Dayan Guan", "Aoran Xiao", "Shijian Lu"], "publication_date": "2021/6", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "volume": "21", "description": "Panoptic segmentation unifies semantic segmentation and instance segmentation which has been attracting increasing attention in recent years. On the other hand, most existing research was conducted under a supervised learning setup whereas domain adaptive panoptic segmentation which is critical in different tasks and applications is largely neglected. We design a domain adaptive panoptic segmentation network that exploits inter-style consistency and inter-task regularization for optimal domain adaptive panoptic segmentation. The inter-style consistency leverages geometric invariance across the same image of the different styles whichfabricates' certain self-supervisions to guide the network to learn domain-invariant features. The inter-task regularization exploits the complementary nature of instance segmentation and semantic segmentation and uses it as a constraint for better feature alignment across domains. Extensive experiments over multiple domain adaptive panoptic segmentation tasks (eg synthetic-to-real and real-to-real) show that our proposed network achieves superior segmentation performance as compared with the state-of-the-art.", "total_citations": {"2021": 12, "2022": 20, "2023": 17}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Dem6FJhTUoYC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S092427162100112X", "authors": ["Aoran Xiao", "Xiaofei Yang", "Shijian Lu", "Dayan Guan", "Jiaxing Huang"], "publication_date": "2021/6", "journal": "ISPRS Journal of Photogrammetry and Remote Sensing", "volume": "176", "pages": "237-249", "publisher": "ScienceDirect", "description": "Scene understanding based on LiDAR point cloud is an essential task for autonomous cars to drive safely, which often employs spherical projection to map 3D point cloud into multi-channel 2D images for semantic segmentation. Most existing methods simply stack different point attributes/modalities (e.g. coordinates, intensity, depth, etc.) as image channels to increase information capacity, but ignore distinct characteristics of point attributes in different image channels. We design FPS-Net, a convolutional fusion network that exploits the uniqueness and discrepancy among the projected image channels for optimal point cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead of simply stacking multiple channel images as a single input, we group them into different modalities to first learn modality-specific features separately and then map the learnt features into a common high-dimensional feature\u00a0\u2026", "total_citations": {"2021": 1, "2022": 12, "2023": 19}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:kWvqk_afx_IC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9424971/", "authors": ["Chong Zhang", "Zongxian Li", "Jingjing Liu", "Peixi Peng", "Qixiang Ye", "Shijian Lu", "Tiejun Huang", "Yonghong Tian"], "publication_date": "2021/5/6", "journal": "IEEE Transactions on Multimedia", "volume": "24", "pages": "2246-2258", "publisher": "IEEE", "description": "Unsupervised domain adaptation (UDA) has achieved unprecedented success in improving the cross-domain robustness of object detection models. However, existing UDA methods largely ignore the instantaneous data distribution and the sampling strategy during model learning, which could deteriorate the feature representation given large domain shift. In this work, we propose a Self-Guided Adaptation (SGA) model, targeting at aligning feature representation and transferring object detection models across domains while considering the instantaneous alignment difficulty. The core of SGA is to calculate \u201chardness\u201d factors for sample pairs indicating domain distance in a kernel space. With the hardness factor, the proposed SGA adaptively indicates the importance of samples and assigns them different constrains. Indicated by these hardness factors, Self-Guided Progressive Sampling (SPS) is implemented in an\u00a0\u2026", "total_citations": {"2021": 1, "2022": 8, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Ade32sEp0pkC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320320305677", "authors": ["Dayan Guan", "Jiaxing Huang", "Shijian Lu", "Aoran Xiao"], "publication_date": "2021/4/1", "journal": "Pattern Recognition", "volume": "112", "pages": "107764", "publisher": "Pergamon", "description": "We focus on unsupervised domain adaptation (UDA) in image segmentation. Existing works address this challenge largely by aligning inter-domain representations, which may lead over-alignment that impairs the semantic structures of images and further target-domain segmentation performance. We design a scale variance minimization (SVMin) method by enforcing the intra-image semantic structure consistency in the target domain. Specifically, SVMin leverages an intrinsic property that simple scale transformation has little effect on the semantic structures of images. It thus introduces certain supervision in the target domain by imposing a scale-invariance constraint while learning to segment an image and its scale-transformation concurrently. Additionally, SVMin is complementary to most existing UDA techniques and can be easily incorporated with consistent performance boost but little extra parameters\u00a0\u2026", "total_citations": {"2021": 14, "2022": 27, "2023": 19}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:RoXSNcbkSzsC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9370169/", "authors": ["Zhenhuan Rao", "Yuechen Wu", "Zifei Yang", "Wei Zhang", "Shijian Lu", "Weizhi Lu", "ZhengJun Zha"], "publication_date": "2021/3/5", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "volume": "32", "issue": "12", "pages": "5445-5455", "publisher": "IEEE", "description": "Learning to adapt to a series of different goals in visual navigation is challenging. In this work, we present a model-embedded actor\u2013critic architecture for the multigoal visual navigation task. To enhance the task cooperation in multigoal learning, we introduce two new designs to the reinforcement learning scheme:  inverse dynamics model (InvDM)  and  multigoal colearning (MgCl) . Specifically, InvDM is proposed to capture the navigation-relevant association between state and goal and provide additional training signals to relieve the sparse reward issue. MgCl aims at improving the sample efficiency and supports the agent to learn from unintentional positive experiences. Besides, to further improve the scene generalization capability of the agent, we present an enhanced navigation model that consists of two self-supervised auxiliary task modules. The first module, which is named path closed-loop detection\u00a0\u2026", "total_citations": {"2021": 2, "2022": 8, "2023": 13}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:1Ye0OR6EYb4C": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/16260", "authors": ["Mengxi Jia", "Xinhua Cheng", "Yunpeng Zhai", "Shijian Lu", "Siwei Ma", "Jian Zhang"], "publication_date": "2021/2", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "description": "Occluded person re-identification (re-ID) is a challenging task as different human parts may become invisible in cluttered scenes, making it hard to match person images of different identities. Most existing methods address this challenge by aligning spatial features of body parts according to semantic information (eg human poses) or feature similarities but this approach is complicated and sensitive to noises. This paper presents Matching on Sets (MoS), a novel method that positions occluded person re-ID as a set matching task without requiring spatial alignment. MoS encodes a person image by a pattern set as represented by aglobal vector\u2019with each element capturing one specific visual pattern, and it introduces Jaccard distance as a metric to compute the distance between pattern sets and measure image similarity. To enable Jaccard distance over continuous real numbers, we employ minimization and maximization to approximate the operations of intersection and union, respectively. In addition, we design a Jaccard triplet loss that enhances the pattern discrimination and allows to embed set matching into deep neural networks for end-to-end training. In the inference stage, we introduce a conflict penalty mechanism that detects mutually exclusive patterns in the pattern union of image pairs and decreases their similarities accordingly. Extensive experiments over three widely used datasets (Market1501, DukeMTMC and Occluded-DukeMTMC) show that MoS achieves superior re-ID performance. Additionally, it is tolerant of occlusions and outperforms the state-of-the-art by large margins for Occluded-DukeMTMC.", "total_citations": {"2021": 6, "2022": 24, "2023": 21}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:rHJHxKgnXwkC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/16440", "authors": ["Fangneng Zhan", "Changgong Zhang", "Yingchen Yu", "Yuan Chang", "Shijian Lu", "Feiying Ma", "Xuansong Xie"], "publication_date": "2021/2", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "description": "Illumination estimation from a single image is critical in 3D rendering and it has been investigated extensively in the computer vision and computer graphic research community. On the other hand, existing works estimate illumination by either regressing light parameters or generating illumination maps that are often hard to optimize or tend to produce inaccurate predictions. We propose Earth Mover\u2019s Light (EMLight), an illumination estimation framework that leverages a regression network and a neural projector for accurate illumination estimation. We decompose the illumination map into spherical light distribution, light intensity and the ambient term, and define the illumination estimation as a parameter regression task for the three illumination components. Motivated by the Earth Mover's distance, we design a novel spherical mover's loss that guides to regress light distribution parameters accurately by taking advantage of the subtleties of spherical distribution. Under the guidance of the predicted spherical distribution, light intensity and ambient term, the neural projector synthesizes panoramic illumination maps with realistic light frequency. Extensive experiments show that EMLight achieves accurate illumination estimation and the generated relighting in 3D object embedding exhibits superior plausibility and fidelity as compared with state-of-the-art methods.", "total_citations": {"2020": 3, "2021": 17, "2022": 53, "2023": 25}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LPtt_HFRSbwC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9412465/", "authors": ["Fangneng Zhan", "Shijian Lu", "Aoran Xiao"], "publication_date": "2021/1/10", "conference": "2020 25th International Conference on Pattern Recognition (ICPR)", "pages": "6889-6896", "publisher": "IEEE", "description": "The recent person re-identification research has achieved great success by learning from a large number of labeled person images. On the other hand, the learned models often experience significant performance drops when applied to images collected in a different environment. Unsupervised domain adaptation (UDA) has been investigated to mitigate this constraint, but most existing systems adapt images at pixel level only and ignore obvious discrepancies at spatial level. This paper presents an innovative UDA-based person re-identification network that is capable of adapting images at both spatial and pixel levels simultaneously. A novel disentangled cycle-consistency loss is designed which guides the learning of spatial-level and pixel-level adaptation in a collaborative manner. In addition, a novel multi-modal mechanism is incorporated which is capable of generating images of different geometry views and\u00a0\u2026", "total_citations": {"2020": 2, "2021": 32, "2022": 9, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:q-HalDI95KYC": {"external_link": "http://openaccess.thecvf.com/content/WACV2021/html/Zhang_Defect-GAN_High-Fidelity_Defect_Synthesis_for_Automated_Defect_Inspection_WACV_2021_paper.html", "authors": ["Gongjie Zhang", "Kaiwen Cui", "Tzu-Yi Hung", "Shijian Lu"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision", "pages": "2524-2534", "description": "Automated defect inspection is critical for effective and efficient maintenance, repair, and operations in advanced manufacturing. On the other hand, automated defect inspection is often constrained by the lack of defect samples, especially when we adopt deep neural networks for this task. This paper presents Defect-GAN, an automated defect synthesis network that generates realistic and diverse defect samples for training accurate and robust defect inspection networks. Defect-GAN learns through defacement and restoration processes, where the defacement generates defects on normal surface images while the restoration removes defects to generate normal images. It employs a novel compositional layer-based architecture for generating realistic defects within various image backgrounds with different textures and appearances. It can also mimic the stochastic variations of defects and offer flexible control over the locations and categories of the generated defects within the image background. Extensive experiments show that Defect-GAN is capable of synthesizing various defects with superior diversity and fidelity. In addition, the synthesized defect samples demonstrate their effectiveness in training better defect inspection networks.", "total_citations": {"2021": 12, "2022": 24, "2023": 24}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:DBa1UEJaJKAC": {"external_link": "http://openaccess.thecvf.com/content/WACV2021/html/Zhang_PNPDet_Efficient_Few-Shot_Detection_Without_Forgetting_via_Plug-and-Play_Sub-Networks_WACV_2021_paper.html", "authors": ["Gongjie Zhang", "Kaiwen Cui", "Rongliang Wu", "Shijian Lu", "Yonghong Tian"], "publication_date": "2021", "conference": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision", "pages": "3823-3832", "description": "The human visual system can detect objects of unseen categories from merely a few examples. However, such capability remains absent in state-of-the-art detectors. To bridge this gap, several attempts have been proposed to perform few-shot detection by incorporating meta-learning techniques. Such methods can improve detection performance on unseen categories, but also add huge computational burden, and usually degrade detection performance on seen categories. In this paper, we present PNPDet, a novel Plug-and-Play Detector, for efficient few-shot detection without forgetting. It introduces a simple but effective architecture with separate sub-networks that disentangles the recognition of base and novel categories and prevents hurting performance on known categories while learning new concepts. Distance metric learning is further incorporated into sub-networks, consistently boosting detection performance for both base and novel categories. Experiments show that the proposed PNPDet can achieve comparable few-shot detection performance on unseen categories while not losing accuracy on seen categories, and also remain efficient and flexible at the same time.", "total_citations": {"2021": 8, "2022": 14, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:1yWc8FF-_SYC": {"external_link": "https://openaccess.thecvf.com/content/ACCV2020/html/Zhan_Adversarial_Image_Composition_with_Auxiliary_Illumination_ACCV_2020_paper.html", "authors": ["Fangneng Zhan", "Shijian Lu", "Changgong Zhang", "Feiying Ma", "Xuansong Xie"], "publication_date": "2020/9/17", "conference": "Asian Conference on Computer Vision (ACCV)", "description": "Dealing with the inconsistency between a foreground object and a background image is a challenging task in high-fidelity image composition. State-of-the-art methods strive to harmonize the composed image by adapting the style of foreground objects to be compatible with the background image, whereas the potential shadow of foreground objects within the composed image which is critical to the composition realism is largely neglected. In this paper, we propose an Adversarial Image Composition Net (AIC-Net) that achieves realistic image composition by considering potential shadows that the foreground object projects in the composed image. A novel branched generation mechanism is proposed, which disentangles the generation of shadows and the transfer of foreground styles for optimal accomplishment of the two tasks simultaneously. A differentiable spatial transformation module is designed which bridges the local harmonization and the global harmonization to achieve their joint optimization effectively. Extensive experiments on pedestrian and car composition tasks show that the proposed AIC-Net achieves superior composition performance qualitatively and quantitatively.", "total_citations": {"2019": 2, "2020": 2, "2021": 22, "2022": 24, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:CdxZDUztZiMC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-58610-2_10", "authors": ["Xiaobing Zhang", "Shijian Lu", "Haigang Gong", "Zhipeng Luo", "Ming Liu"], "publication_date": "2020/8/23", "book": "European Conference on Computer Vision", "pages": "158-173", "publisher": "Springer International Publishing", "description": "Online knowledge distillation has attracted increasing interest recently, which jointly learns teacher and student models or an ensemble of student models simultaneously and collaboratively. On the other hand, existing works focus more on outcome-driven learning according to knowledge like classification probabilities whereas the distilling processes which capture rich and useful intermediate features and information are largely neglected. In this work, we propose an innovative adversarial-based mutual learning network (AMLN) that introduces process-driven learning beyond outcome-driven learning for augmented online knowledge distillation. A block-wise training module is designed which guides the information flow and mutual learning among peer networks adversarially throughout different learning stages, and this spreads until the final network layer which captures more high-level information\u00a0\u2026", "total_citations": {"2020": 1, "2021": 2, "2022": 3, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:yxmsSjX2EkcC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-58610-2_46", "authors": ["Rongliang Wu", "Shijian Lu"], "publication_date": "2020/7/17", "conference": "Proceedings of the European Conference on Computer Vision", "description": "Recent studies on facial expression editing have obtained very promising progress. On the other hand, existing methods face the constraint of requiring a large amount of expression labels which are often expensive and time-consuming to collect. This paper presents an innovative label-free expression editing via disentanglement (LEED) framework that is capable of editing the expression of both frontal and profile facial images without requiring any expression label. The idea is to disentangle the identity and expression of a facial image in the expression manifold, where the neutral face captures the identity attribute and the displacement between the neutral image and the expressive image captures the expression attribute. Two novel losses are designed for optimal expression disentanglement and consistent synthesis, including a mutual expression information loss that aims to extract pure expression\u00a0\u2026", "total_citations": {"2021": 13, "2022": 9, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hsZV8lGYWTMC": {"external_link": "https://arxiv.org/abs/2007.07066", "authors": ["Changgong Zhang", "Fangneng Zhan", "Shijian Lu", "Feiying Ma", "Xuansong Xie"], "publication_date": "2020/7/14", "journal": "arXiv preprint arXiv:2007.07066", "description": "Recent advances in generative adversarial networks (GANs) have achieved great success in automated image composition that generates new images by embedding interested foreground objects into background images automatically. On the other hand, most existing works deal with foreground objects in two-dimensional (2D) images though foreground objects in three-dimensional (3D) models are more flexible with 360-degree view freedom. This paper presents an innovative View Alignment GAN (VA-GAN) that composes new images by embedding 3D models into 2D background images realistically and automatically. VA-GAN consists of a texture generator and a differential discriminator that are inter-connected and end-to-end trainable. The differential discriminator guides to learn geometric transformation from background images so that the composed 3D models can be aligned with the background images with realistic poses and views. The texture generator adopts a novel view encoding mechanism for generating accurate object textures for the 3D models under the estimated views. Extensive experiments over two synthesis tasks (car synthesis with KITTI and pedestrian synthesis with Cityscapes) show that VA-GAN achieves high-fidelity composition qualitatively and quantitatively as compared with state-of-the-art generation methods.", "total_citations": {"2019": 1, "2020": 2, "2021": 16, "2022": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:L1USKYWJimsC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-58555-6_42", "authors": ["Jiaxing Huang", "Shijian Lu", "Dayan Guan", "Xiaobing Zhang"], "publication_date": "2020/7/5", "conference": "Proceedings of the European Conference on Computer Vision", "description": "Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via backpropagation-based adversarial learning. An adaptive\u00a0\u2026", "total_citations": {"2020": 1, "2021": 43, "2022": 55, "2023": 33}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:prdVHNxh-e8C": {"external_link": "https://arxiv.org/abs/2007.01504", "authors": ["Mengxi Jia", "Yunpeng Zhai", "Shijian Lu", "Siwei Ma", "Jian Zhang"], "publication_date": "2020/7/3", "conference": "International Joint Conference on Artificial Intelligence", "description": "RGB-Infrared (IR) cross-modality person re-identification (re-ID), which aims to search an IR image in RGB gallery or vice versa, is a challenging task due to the large discrepancy between IR and RGB modalities. Existing methods address this challenge typically by aligning feature distributions or image styles across modalities, whereas the very useful similarities among gallery samples of the same modality (i.e. intra-modality sample similarities) is largely neglected. This paper presents a novel similarity inference metric (SIM) that exploits the intra-modality sample similarities to circumvent the cross-modality discrepancy targeting optimal cross-modality image matching. SIM works by successive similarity graph reasoning and mutual nearest-neighbor reasoning that mine cross-modality sample similarities by leveraging intra-modality sample similarities from two different perspectives. Extensive experiments over two cross-modality re-ID datasets (SYSU-MM01 and RegDB) show that SIM achieves significant accuracy improvement but with little extra training as compared with the state-of-the-art.", "total_citations": {"2020": 2, "2021": 12, "2022": 22, "2023": 17}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:XUvXOeBm_78C": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-58571-6_35", "authors": ["Yunpeng Zhai", "Qixiang Ye", "Shijian Lu", "Mengxi Jia", "Rongrong Ji", "Yonghong Tian"], "publication_date": "2020/7/3", "conference": "Proceedings of the European Conference on Computer Vision", "description": "Often the best performing deep neural models are ensembles of multiple base-level networks, nevertheless, ensemble learning with respect to domain adaptive person re-ID remains unexplored. In this paper, we propose a multiple expert brainstorming network (MEB-Net) for domain adaptive person re-ID, opening up a promising direction about model ensemble problem under unsupervised conditions. MEB-Net adopts a mutual learning strategy, where multiple networks with different architectures are pre-trained within a source domain as expert models equipped with specific features and knowledge, while the adaptation is then accomplished through brainstorming (mutual learning) among expert models. MEB-Net accommodates the heterogeneity of experts learned with different architectures and enhances discrimination capability of the adapted re-ID model, by introducing a regularization scheme\u00a0\u2026", "total_citations": {"2020": 2, "2021": 47, "2022": 72, "2023": 60}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:owLR8QvbtFgC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9115868/", "authors": ["Fan Yang", "Ke Yan", "Shijian Lu", "Huizhu Jia", "Don Xie", "Zongqiao Yu"], "publication_date": "2020/7", "journal": "IEEE Transaction on Multimedia", "description": "Unsupervised domain adaptation (UDA) aims to mitigate the domain shift that occurs when transferring knowledge from a labeled source domain to an unlabeled target domain. While it has been studied for application in unsupervised person re-identification (ReID), the relations of feature distribution across the source and target domains remain underexplored, as they either ignore the local relations or omit the in-depth consideration of negative transfer when two domains do not share identical label spaces. In light of the above, this paper presents an innovative part-aware progressive adaptation network (PPAN) that exploits global and local relations for UDA-based ReID across domains. A multi-branch network is developed that explicitly learns discriminative feature representation from both whole-body images and body-part images under the supervision of a labeled source domain. Within each network branch\u00a0\u2026", "total_citations": {"2020": 2, "2021": 21, "2022": 17, "2023": 21}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:wKETBy42zhYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9099416/", "authors": ["Qinghua Ren", "Shijian Lu", "Jinxia Zhang", "Renjie Hu"], "publication_date": "2020/5/25", "journal": "IEEE Transactions on Multimedia", "volume": "23", "pages": "1442-1453", "publisher": "IEEE", "description": "Benefiting from the powerful discriminative feature learning capability of convolutional neural networks (CNNs), deep learning techniques have achieved remarkable performance improvement for the task of salient object detection (SOD) in recent years. However, most existing deep SOD models do not fully exploit informative contextual features, which often leads to suboptimal detection performance in the presence of a cluttered background. This paper presents a context-aware attention module that detects salient objects by simultaneously constructing connections between each image pixel and its local and global contextual pixels. Specifically, each pixel and its neighbors bidirectionally exchange semantic information by computing their correlation coefficients, and this process aggregates contextual attention features both locally and globally. In addition, an attention-guided hierarchical network architecture is\u00a0\u2026", "total_citations": {"2021": 8, "2022": 22, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:WC9gN4BGCRcC": {"external_link": "https://openaccess.thecvf.com/content/ACCV2020/html/Koksal_RF-GAN_A_Light_and_Reconfigurable_Network_for_Unpaired_Image-to-Image_Translation_ACCV_2020_paper.html", "authors": ["Ali Koksal", "Shijian Lu"], "publication_date": "2020", "conference": "Proceedings of the Asian Conference on Computer Vision", "description": "Generative adversarial networks (GANs) have been widely studied for unpaired image-to-image translation in recent years. On the other hand, state-of-the-art translation GANs are often constrained by large model sizes and inflexibility in translating across various domains. Inspired by the observation that the mappings between two domains are often approximately invertible, we design an innovative reconfigurable GAN (RF-GAN) that has a small size but is versatile in high-fidelity image translation either across two domains or among multiple domains. One unique feature of RF-GAN lies with its single generator which is reconfigurable and can perform bidirectional image translations by swapping its parameters. In addition, a multi-domain discriminator is designed which allows joint discrimination of original and translated samples in multiple domains. Experiments over eight unpaired image translation datasets (on various tasks such as object transfiguration, season transfer, and painters' style transfer, etc.) show that RF-GAN reduces the model size by up to 75% as compared with state-of-the-art translation GANs but produces superior image translation performance with lower Frechet Inception Distance consistently.", "total_citations": {"2021": 3, "2022": 2, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Br1UauaknNIC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-030-58580-8_45", "authors": ["Siyuan Yang", "Jun Liu", "Shijian Lu", "Meng Hwa Er", "Alex C Kot"], "publication_date": "2020", "conference": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16", "pages": "769-786", "publisher": "Springer International Publishing", "description": "Gesture recognition and 3D hand pose estimation are two highly correlated tasks, yet they are often handled separately. In this paper, we present a novel collaborative learning network for joint gesture recognition and 3D hand pose estimation. The proposed network exploits joint-aware features that are crucial for both tasks, with which gesture recognition and 3D hand pose estimation boost each other to learn highly discriminative features. In addition, a novel multi-order multi-stream feature analysis method is introduced which learns posture and multi-order motion information from the intermediate feature maps of videos effectively and efficiently. Due to the exploitation of joint-aware features in common, the proposed technique is capable of learning gesture recognition and 3D hand pose estimation even when only gesture or pose labels are available, and this enables weakly supervised network learning\u00a0\u2026", "total_citations": {"2019": 1, "2020": 3, "2021": 9, "2022": 16, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:CB2v5VPnA5kC": {"external_link": "https://www.mdpi.com/2072-4292/12/12/2033", "authors": ["Xiaofei Yang", "Xiaofeng Zhang", "Yunming Ye", "Raymond YK Lau", "Shijian Lu", "Xutao Li", "Xiaohui Huang"], "publication_date": "2020/1", "journal": "Remote Sensing", "volume": "12", "issue": "12", "pages": "2033", "publisher": "Multidisciplinary Digital Publishing Institute", "description": "Accurate hyperspectral image classification has been an important yet challenging task for years. With the recent success of deep learning in various tasks, 2-dimensional (2D)/3-dimensional (3D) convolutional neural networks (CNNs) have been exploited to capture spectral or spatial information in hyperspectral images. On the other hand, few approaches make use of both spectral and spatial information simultaneously, which is critical to accurate hyperspectral image classification. This paper presents a novel Synergistic Convolutional Neural Network (SyCNN) for accurate hyperspectral image classification. The SyCNN consists of a hybrid module that combines 2D and 3D CNNs in feature learning and a data interaction module that fuses spectral and spatial hyperspectral information. Additionally, it introduces a 3D attention mechanism before the fully-connected layer which helps filter out interfering features and information effectively. Extensive experiments over three public benchmarking datasets show that our proposed SyCNNs clearly outperform state-of-the-art techniques that use 2D/3D CNNs.", "total_citations": {"2020": 1, "2021": 10, "2022": 15, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:7H_MAutzIkAC": {"external_link": "http://openaccess.thecvf.com/content_CVPR_2020/html/Zhai_AD-Cluster_Augmented_Discriminative_Clustering_for_Domain_Adaptive_Person_Re-Identification_CVPR_2020_paper.html", "authors": ["Yunpeng Zhai", "Shijian Lu", "Qixiang Ye", "Xuebo Shan", "Jie Chen", "Rongrong Ji", "Yonghong Tian"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "pages": "9021-9030", "description": "Domain adaptive person re-identification (re-ID) is a challenging task, especially when person identities in target domains are unknown. Existing methods attempt to address this challenge by transferring image styles or aligning feature distributions across domains, whereas the rich unlabeled samples in target domains are not sufficiently exploited. This paper presents a novel augmented discriminative clustering (AD-Cluster) technique that estimates and augments person clusters in target domains and enforces the discrimination ability of re-ID models with the augmented clusters. AD-Cluster is trained by iterative density-based clustering, adaptive sample augmentation, and discriminative feature learning. It learns an image generator and a feature encoder which aim to maximize the intra-cluster diversity in the sample space and minimize the intra-cluster distance in the feature space in an adversarial min-max manner. Finally, AD-Cluster increases the diversity of sample clusters and improves the discrimination capability of re-ID models greatly. Extensive experiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms the state-of-the-art with large margins.", "total_citations": {"2020": 11, "2021": 90, "2022": 112, "2023": 68}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:WHdLCjDvYFkC": {"external_link": "http://openaccess.thecvf.com/content_CVPR_2020/html/Wu_Cascade_EF-GAN_Progressive_Facial_Expression_Editing_With_Local_Focuses_CVPR_2020_paper.html", "authors": ["Rongliang Wu", "Gongjie Zhang", "Shijian Lu", "Tao Chen"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "pages": "5021-5030", "description": "Recent advances in Generative Adversarial Nets (GANs) have shown remarkable improvements for facial expression editing. However, current methods are still prone to generate artifacts and blurs around expression-intensive regions, and often introduce undesired overlapping artifacts while handling large-gap expression transformations such as transformation from furious to laughing. To address these limitations, we propose Cascade Expression Focal GAN (Cascade EF-GAN), a novel network that performs progressive facial expression editing with local expression focuses. The introduction of the local focus enables the Cascade EF-GAN to better preserve identity-related features and details around eyes, noses and mouths, which further helps reduce artifacts and blurs within the generated facial images. In addition, an innovative cascade transformation strategy is designed by dividing a large facial expression transformation into multiple small ones in cascade, which helps suppress overlapping artifacts and produce more realistic editing while dealing with large-gap expression transformations. Extensive experiments over two publicly available facial expression datasets show that our proposed Cascade EF-GAN achieves superior performance for facial expression editing.", "total_citations": {"2020": 4, "2021": 29, "2022": 32, "2023": 31}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:MhiOAD_qIWkC": {"external_link": "http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Suppressing_Uncertainties_for_Large-Scale_Facial_Expression_Recognition_CVPR_2020_paper.html", "authors": ["Kai Wang", "Xiaojiang Peng", "Jianfei Yang", "Shijian Lu", "Yu Qiao"], "publication_date": "2020", "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition", "pages": "6897-6906", "description": "Annotating a qualitative large-scale facial expression dataset is extremely difficult due to the uncertainties caused by ambiguous facial expressions, low-quality facial images, and the subjectiveness of annotators. These uncertainties suspend the progress of large-scale Facial Expression Recognition (FER) in data-driven deep learning era. To address this problelm, this paper proposes to suppress the uncertainties by a simple yet efficient Self-Cure Network (SCN). Specifically, SCN suppresses the uncertainty from two different aspects: 1) a self-attention mechanism over FER dataset to weight each sample in training with a ranking regularization, and 2) a careful relabeling mechanism to modify the labels of these samples in the lowest-ranked group. Experiments on synthetic FER datasets and our collected WebEmotion dataset validate the effectiveness of our method. Results on public benchmarks demonstrate that our SCN outperforms current state-of-the-art methods with 88.14% on RAF-DB, 60.23% on AffectNet, and 89.35% on FERPlus.", "total_citations": {"2020": 13, "2021": 112, "2022": 173, "2023": 170}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:7BrZ7Jt4UNcC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8946591/", "authors": ["Hongyuan Zhu", "Yi Cheng", "Xi Peng", "Joey Tianyi Zhou", "Zhao Kang", "Shijian Lu", "Zhiwen Fang", "Liyuan Li", "Joo-Hwee Lim"], "publication_date": "2019/12/31", "journal": "IEEE Transactions on Cybernetics", "volume": "51", "issue": "2", "pages": "829-838", "publisher": "IEEE", "description": "Single-image dehazing has been an important topic given the commonly occurred image degradation caused by adverse atmosphere aerosols. The key to haze removal relies on an accurate estimation of global air-light and the transmission map. Most existing methods estimate these two parameters using separate pipelines which reduces the efficiency and accumulates errors, thus leading to a suboptimal approximation, hurting the model interpretability, and degrading the performance. To address these issues, this article introduces a novel generative adversarial network (GAN) for single-image dehazing. The network consists of a novel compositional generator and a novel deeply supervised discriminator. The compositional generator is a densely connected network, which combines fine-scale and coarse-scale information. Benefiting from the new generator, our method can directly learn the physical\u00a0\u2026", "total_citations": {"2020": 5, "2021": 6, "2022": 5, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:PYBJJbyH-FwC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8978135/", "authors": ["Rui Zhang", "Yongsheng Zhou", "Qianyi Jiang", "Qi Song", "Nan Li", "Kai Zhou", "Lei Wang", "Dong Wang", "Minghui Liao", "Mingkun Yang", "Xiang Bai", "Baoguang Shi", "Dimosthenis Karatzas", "Shijian Lu", "CV Jawahar"], "publication_date": "2019/9/20", "conference": "2019 international conference on document analysis and recognition (ICDAR)", "pages": "1577-1581", "publisher": "IEEE", "description": "Chinese scene text reading is one of the most challenging problems in computer vision and has attracted great interest. Different from English text, Chinese has more than 6000 commonly used characters and Chinese characters can be arranged in various layouts with numerous fonts. The Chinese signboards in street view are a good choice for Chinese scene text images since they have different backgrounds, fonts and layouts. We organized a competition called ICDAR2019-ReCTS, which mainly focuses on reading Chinese text on signboard. This report presents the final results of the competition. A large-scale dataset of 25,000 annotated signboard images, in which all the text lines and characters are annotated with locations and transcriptions, were released. Four tasks, namely character recognition, text line recognition, text line detection and end-to-end recognition were set up. Besides, considering the\u00a0\u2026", "total_citations": {"2020": 7, "2021": 24, "2022": 32, "2023": 28}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:1taIhTC69MYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8977955/", "authors": ["Zheng Huang", "Kai Chen", "Jianhua He", "Xiang Bai", "Dimosthenis Karatzas", "Shijian Lu", "CV Jawahar"], "publication_date": "2019/9/20", "conference": "2019 International Conference on Document Analysis and Recognition (ICDAR)", "pages": "1516-1520", "publisher": "IEEE", "description": "The ICDAR 2019 Challenge on \"Scanned receipts OCR and key information extraction\" (SROIE) covers important aspects related to the automated analysis of scanned receipts. The SROIE tasks play a key role in many document analysis systems and hold significant commercial potential. Although a lot of work has been published over the years on administrative document analysis, the community has advanced relatively slowly, as most datasets have been kept private. One of the key contributions of SROIE to the document analysis community is to offer a first, standardized dataset of 1000 whole scanned receipt images and annotations, as well as an evaluation procedure for such tasks. The Challenge is structured around three tasks, namely Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). The competition opened on\u00a0\u2026", "total_citations": {"2020": 14, "2021": 43, "2022": 61, "2023": 79}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:lgwcVrK6X84C": {"external_link": "https://arxiv.org/abs/1901.02596", "authors": ["Chuhui Xue", "Shijian Lu", "Wei Zhang"], "publication_date": "2019/8/16", "journal": "International Joint Conference on Artificial Intelligence (IJCAI)", "description": "State-of-the-art scene text detection techniques predict quadrilateral boxes that are prone to localization errors while dealing with straight or curved text lines of different orientations and lengths in scenes. This paper presents a novel multi-scale shape regression network (MSR) that is capable of locating text lines of different lengths, shapes and curvatures in scenes. The proposed MSR detects scene texts by predicting dense text boundary points that inherently capture the location and shape of text lines accurately and are also more tolerant to the variation of text line length as compared with the state of the arts using proposals or segmentation. Additionally, the multi-scale network extracts and fuses features at different scales which demonstrates superb tolerance to the text scale variation. Extensive experiments over several public datasets show that the proposed MSR obtains superior detection performance for both curved and straight text lines of different lengths and orientations.", "total_citations": {"2019": 2, "2020": 17, "2021": 43, "2022": 39, "2023": 27}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:7Hz3ACDFbsoC": {"external_link": "https://www.researchgate.net/profile/Shijian-Lu/publication/334843748_Exploring_the_Task_Cooperation_in_Multi-goal_Visual_Navigation/links/5e9ec0ac299bf13079adafa8/Exploring-the-Task-Cooperation-in-Multi-goal-Visual-Navigation.pdf", "authors": ["Yuechen Wu", "Zhenhuan Rao", "Wei Zhang", "Shijian Lu", "Weizhi Lu", "Zheng-Jun Zha"], "publication_date": "2019/8", "conference": "IJCAI", "pages": "609-615", "description": "Learning to adapt to a series of different goals in visual navigation is challenging. In this work, we present a model-embedded actor-critic architecture for the multi-goal visual navigation task. To enhance the task cooperation in multi-goal learning, we introduce two new designs to the reinforcement learning scheme: inverse dynamics model (InvDM) and multi-goal co-learning (MgCl). Specifically, InvDM is proposed to capture the navigation-relevant association between state and goal, and provide additional training signals to relieve the sparse reward issue. MgCl aims at improving the sample efficiency and supports the agent to learn from unintentional positive experiences. Extensive results on the interactive platform AI2-THOR demonstrate that the proposed method converges faster than state-of-theart methods while producing more direct routes to navigate to the goal. The video demonstration is available at: https://youtube. com/channel/UCtpTMOsctt3yPzXqe JMD3w/videos.", "total_citations": {"2020": 3, "2021": 9, "2022": 3, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:kVjdVfd2voEC": {"external_link": "http://openaccess.thecvf.com/content_ICCV_2019/html/Zhan_GA-DAN_Geometry-Aware_Domain_Adaptation_Network_for_Scene_Text_Detection_and_ICCV_2019_paper.html", "authors": ["Fangneng Zhan", "Chuhui Xue", "Shijian Lu"], "publication_date": "2019/7/23", "journal": "International Conference on Computer Vision (ICCV)", "description": "Recent adversarial learning research has achieved very impressive progress for modelling cross-domain data shifts in appearance space but its counterpart in modelling cross-domain shifts in geometry space lags far behind. This paper presents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that is capable of modelling cross-domain shifts concurrently in both geometry space and appearance space and realistically converting images across domains with very different characteristics. In the proposed GA-DAN, a novel multi-modal spatial learning structure is designed which can convert a source-domain image into multiple images of different spatial views as in the target domain. A new disentangled cycle-consistency loss is introduced which balances the cycle consistency and greatly improves the concurrent learning in both appearance and geometry spaces. The proposed GA-DAN has been evaluated for the classic scene text detection and recognition tasks, and experiments show that the domain-adapted images achieve superior scene text detection and recognition performance while applied to network training.", "total_citations": {"2019": 3, "2020": 17, "2021": 55, "2022": 80, "2023": 54}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LgRImbQfgY4C": {"external_link": "https://patents.google.com/patent/US10332266B2/en", "inventors": "Tao Chen, Shijian Lu", "publication_date": "2019/6/25", "patent_office": "US", "patent_number": "US10332266B2", "application_number": "US15/527,833", "description": "Embodiments provide a method for recognizing a traffic sign from an input image. The method may include extracting image features from the input image; modifying the image features based on a predetermined image saliency map to determine modified image features; determining a plurality of traffic sign candidates by applying an adaptive boosting algorithm on the modified image features; determining a confidence score for each traffic sign candidate by applying a support vector regression algorithm; and recognizing a traffic sign based on the confidence score for each traffic sign candidate."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:v1_lew4L6wgC": {"external_link": "http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Towards_Natural_and_Accurate_Future_Motion_Prediction_of_Humans_and_CVPR_2019_paper.html", "authors": ["Zhenguang Liu", "Shuang Wu", "Shuyuan Jin", "Qi Liu", "Shijian Lu", "Roger Zimmermann", "Li Cheng"], "publication_date": "2019/6/20", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "pages": "10004-10012", "description": "Anticipating the future motions of 3D articulate objects is challenging due to its non-linear and highly stochastic nature. Current approaches typically represent the skeleton of an articulate object as a set of 3D joints, which unfortunately ignores the relationship between joints, and fails to encode fine-grained anatomical constraints. Moreover, conventional recurrent neural networks, such as LSTM and GRU, are employed to model motion contexts, which inherently have difficulties in capturing long-term dependencies. To address these problems, we propose to explicitly encode anatomical constraints by modeling their skeletons with a Lie algebra representation. Importantly, a hierarchical recurrent network structure is developed to simultaneously encodes local contexts of individual frames and global contexts of the sequence. We proceed to explore the applications of our approach to several distinct quantities including human, fish, and mouse. Extensive experiments show that our approach achieves more natural and accurate predictions over state-of-the-art methods.", "total_citations": {"2019": 2, "2020": 16, "2021": 24, "2022": 27, "2023": 31}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:CaZNVDsoPx4C": {"external_link": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhan_ESIR_End-To-End_Scene_Text_Recognition_via_Iterative_Image_Rectification_CVPR_2019_paper.html", "authors": ["Fangneng Zhan", "Shijian Lu"], "publication_date": "2019/6/20", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "Automated recognition of texts in scenes has been a research challenge for years, largely due to the arbitrary text appearance variation in perspective distortion, text line curvature, text styles and different types of imaging artifacts. The recent deep networks are capable of learning robust representations with respect to imaging artifacts and text style changes, but still face various problems while dealing with scene texts with perspective and curvature distortions. This paper presents an end-to-end trainable scene text recognition system (ESIR) that iteratively removes perspective distortion and text line curvature as driven by better scene text recognition performance. An innovative rectification network is developed, where a line-fitting transformation is designed to estimate the pose of text lines in scenes. Additionally, an iterative rectification framework is developed which corrects scene text distortions iteratively towards a fronto-parallel view. The ESIR is also robust to parameter initialization and easy to train, where the training needs only scene text images and word-level annotations as required by most scene text recognition systems. Extensive experiments over a number of public datasets show that the proposed ESIR is capable of rectifying scene text distortions accurately, achieving superior recognition performance for both normal scene text images and those suffering from perspective and curvature distortions.", "total_citations": {"2019": 18, "2020": 60, "2021": 96, "2022": 113, "2023": 89}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:otzGkya1bYkC": {"external_link": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhan_Spatial_Fusion_GAN_for_Image_Synthesis_CVPR_2019_paper.html", "authors": ["Fangneng Zhan", "Hongyuan Zhu", "Shijian Lu"], "publication_date": "2019/6/20", "journal": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "description": "Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjust the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is incorporated for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end with little supervision. The SF-GAN has been evaluated in two tasks:(1) realistic scene text image synthesis for training better recognition models;(2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN.", "total_citations": {"2019": 11, "2020": 28, "2021": 64, "2022": 103, "2023": 68}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:zCSUwVk65WsC": {"external_link": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0214389", "authors": ["Yonghong Tian", "Lan Wei", "Shijian Lu", "Tiejun Huang"], "publication_date": "2019/4/16", "journal": "PloS one", "volume": "14", "issue": "4", "pages": "e0214389", "publisher": "Public Library of Science", "description": "Human gait has been shown to be an effective biometric measure for person identification at a distance. On the other hand, changes in the view angle pose a major challenge for gait recognition as human gait silhouettes are usually different from different view angles. Traditionally, such a multi-view gait recognition problem can be tackled by View Transformation Model (VTM) which transforms gait features from multiple gallery views to the probe view so as to evaluate the gait similarity. In the real-world environment, however, gait sequences may be captured from an uncontrolled scene and the view angle is often unknown, dynamically changing, or does not belong to any predefined views (thus VTM becomes inapplicable). To address this free-view gait recognition problem, we propose an innovative view-adaptive mapping (VAM) approach. The VAM employs a novel walking trajectory fitting (WTF) to estimate the view angles of a gait sequence, and a joint gait manifold (JGM) to find the optimal manifold between the probe data and relevant gallery data for gait similarity evaluation. Additionally, a RankSVM-based algorithm is developed to supplement the gallery data for subjects whose gallery features are only available in predefined views. Extensive experiments on both indoor and outdoor datasets demonstrate that the VAM outperforms several reference methods remarkably in free-view gait recognition.", "total_citations": {"2019": 1, "2020": 7, "2021": 2, "2022": 4, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ubry08Y2EpUC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8804364/", "authors": ["Gongjie Zhang", "Shijian Lu", "Wei Zhang"], "publication_date": "2019/3/3", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "description": "Accurate and robust detection of multi-class objects in optical remote sensing images is essential to many real-world applications, such as urban planning, traffic control, searching, and rescuing. However, the state-of-the-art object detection techniques designed for images captured using ground-level sensors usually experience a sharp performance drop when directly applied to remote sensing images, largely due to the object appearance differences in remote sensing images in terms of sparse texture, low contrast, arbitrary orientations, and large-scale variations. This paper presents a novel object detection network [(context-aware detection network (CAD-Net)] that exploits attention-modulated features as well as global and local contexts to address the new challenges in detecting objects from remote sensing images. The proposed CAD-Net learns global and local contexts of objects by capturing their\u00a0\u2026", "total_citations": {"2019": 2, "2020": 29, "2021": 85, "2022": 112, "2023": 86}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:QD3KBmkZPeQC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320318303583", "authors": ["Dinh NguyenVan", "Shijian Lu", "Shangxuan Tian", "Nizar Ouarti", "Mounir Mokhtari"], "publication_date": "2019/3/1", "journal": "Pattern Recognition", "volume": "87", "pages": "118-129", "publisher": "Pergamon", "description": "Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi\u00a0\u2026", "total_citations": {"2019": 3, "2020": 8, "2021": 8, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:wMgC3FpKEyYC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320318303133", "authors": ["Fan Yang", "Ke Yan", "Shijian Lu", "Huizhu Jia", "Xiaodong Xie", "Wen Gao"], "publication_date": "2019/2/1", "journal": "Pattern Recognition", "volume": "86", "pages": "143-155", "publisher": "Pergamon", "description": "Person re-identification (ReID) is a challenging task due to arbitrary human pose variations, background clutters, etc. It has been studied extensively in recent years, but the multifarious local and global features are still not fully exploited by either ignoring the interplay between whole-body images and body-part images or missing in-depth examination of specific body-part images. In this paper, we propose a novel attention-driven multi-branch network that learns robust and discriminative human representation from global whole-body images and local body-part images simultaneously. Within each branch, an intra-attention network is designed to search for informative and discriminative regions within the whole-body or body-part images, where attention is elegantly decomposed into spatial-wise attention and channel-wise attention for effective and efficient learning. In addition, a novel inter-attention module is\u00a0\u2026", "total_citations": {"2018": 1, "2019": 18, "2020": 28, "2021": 35, "2022": 34, "2023": 34}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ALROH1vI_8AC": {"external_link": "https://arxiv.org/abs/1901.09193", "authors": ["Changgong Zhang", "Fangneng Zhan", "Hongyuan Zhu", "Shijian Lu"], "publication_date": "2019/1/26", "journal": "arXiv preprint arXiv:1901.09193", "description": "A large amount of annotated training images is critical for training accurate and robust deep network models but the collection of a large amount of annotated training images is often time-consuming and costly. Image synthesis alleviates this constraint by generating annotated training images automatically by machines which has attracted increasing interest in the recent deep learning research. We develop an innovative image synthesis technique that composes annotated training images by realistically embedding foreground objects of interest (OOI) into background images. The proposed technique consists of two key components that in principle boost the usefulness of the synthesized images in deep network training. The first is context-aware semantic coherence which ensures that the OOI are placed around semantically coherent regions within the background image. The second is harmonious appearance adaptation which ensures that the embedded OOI are agreeable to the surrounding background from both geometry alignment and appearance realism. The proposed technique has been evaluated over two related but very different computer vision challenges, namely, scene text detection and scene text recognition. Experiments over a number of public datasets demonstrate the effectiveness of our proposed image synthesis technique - the use of our synthesized images in deep network training is capable of achieving similar or even better scene text detection and scene text recognition performance as compared with using real images.", "total_citations": {"2019": 5, "2020": 3, "2021": 29, "2022": 6, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:zGdJYJv2LkUC": {"external_link": "https://scholar.google.com/scholar?cluster=1205117725395904163&hl=en&oi=scholarr", "authors": ["Fangneng Zhan", "Jiaxing Huang", "Shijian Lu"], "publication_date": "2019", "journal": "arXiv preprint arXiv:1905.04693", "volume": "2", "description": "Despite the rapid progress of generative adversarial networks (GANs) in image synthesis in recent years, current approaches work in either geometry domain or appearance domain which tend to introduce various synthesis artifacts. This paper presents an innovative Adaptive Composition GAN (AC-GAN) that incorporates image synthesis in geometry and appearance domains into an end-to-end trainable network and achieves synthesis realism in both domains simultaneously. An innovative hierarchical synthesis mechanism is designed which is capable of generating realistic geometry and composition when multiple foreground objects with or without occlusions are involved in synthesis. In addition, a novel attention mask is introduced to guide the appearance adaptation to the embedded foreground objects which helps preserve image details and resolution and also provide better reference for synthesis in geometry domain. Extensive experiments on scene text image synthesis, automated portrait editing and indoor rendering tasks show that the proposed AC-GAN achieves superior synthesis performance qualitatively and quantitatively.", "total_citations": {"2019": 2, "2020": 3, "2021": 5, "2022": 3, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:48xauSegjOkC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8576611/", "authors": ["Tao Chen", "Shijian Lu", "Jiayuan Fan"], "publication_date": "2018/12/14", "journal": "IEEE Transactions on Image Processing", "volume": "28", "issue": "5", "pages": "2389 - 2398", "publisher": "IEEE", "description": "The availability of large-scale annotated data and the uneven separability of different data categories have become two major impediments of deep learning for image classification. In this paper, we present a semi-supervised hierarchical convolutional neural network (SS-HCNN) to address these two challenges. A large-scale unsupervised maximum margin clustering technique is designed, which splits images into a number of hierarchical clusters iteratively to learn cluster-level CNNs at parent nodes and category-level CNNs at leaf nodes. The splitting uses the similarity of CNN features to group visually similar images into the same cluster, which relieves the uneven data separability constraint. With the hierarchical cluster-level CNNs capturing certain high-level image category information, the category-level CNNs can be trained with a small amount of labeled images, and this relieves the data annotation\u00a0\u2026", "total_citations": {"2019": 5, "2020": 7, "2021": 3, "2022": 6, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ce2CqMG-AY4C": {"external_link": "https://www.taylorfrancis.com/chapters/edit/10.1201/9781351061223-18/dictionary-learning-applications-hep-2-cell-classification-sadaf-monajemi-shahab-ensafi-shijian-lu-ashraf-kassim-chew-lim-tan-saeid-sanei-sim-heng-ong", "authors": ["Sadaf Monajemi", "Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan", "Saeid Sanei", "Sim-Heng Ong"], "publication_date": "2018/7/4", "book": "Signal Processing and Machine Learning for Biomedical Big Data", "pages": "369-380", "publisher": "CRC Press", "description": "Autoimmune diseases (ADs) are often diagnosed via indirect immunofluorescence (IIF) with human epithelial type-2 (HEp-2) cells. Computer aided diagnosis systems and automatic classification of HEp-2 cells can improve the diagnostic process in terms of lower cost, faster response, and better repeatability. In this chapter, we propose an adaptive distributed dictionary learning (ADDL) method where the dictionary learning problem is reformulated as a distributed learning task. Using this approach, we develop an automatic and robust method that effectively handles the complexity of the problem in terms of memory and computational cost. To improve the classification accuracy, we combine SURF (speeded-up robust features) and SIFT (scale-invariant feature transform) in a complementary fashion. The performance of our method is evaluated on two data sets and is shown to outperform state-of-the-art techniques\u00a0\u2026", "total_citations": {"2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:_OXeSy2IsFwC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8291617/", "authors": ["Hongyuan Zhu", "Romain Vial", "Shijian Lu", "Xi Peng", "Huazhu Fu", "Yonghong Tian", "Xianbin Cao"], "publication_date": "2018/2/14", "journal": "IEEE Transactions on Image Processing", "volume": "27", "issue": "6", "pages": "2609-2622", "publisher": "IEEE", "description": "In this paper, we propose YoTube-a novel deep learning framework for generating action proposals in untrimmed videos, where each action proposal corresponds to a spatial-temporal tube that potentially locates one human action. Most of the existing works generate proposals by clustering low-level features or linking image proposals, which ignore the interplay between long-term temporal context and short-term cues. Different from these works, our method considers the interplay by designing a new recurrent YoTube detector and static YoTube detector. The recurrent YoTube detector sequentially regresses candidate bounding boxes using Recurrent Neural Network learned long-term temporal contexts. The static YoTube detector produces bounding boxes using rich appearance cues in every single frame. To fully exploit the complementary appearance, motion, and temporal context, we train the recurrent and\u00a0\u2026", "total_citations": {"2017": 1, "2018": 17, "2019": 22, "2020": 6, "2021": 4, "2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:uDGL6kOW6j0C": {"external_link": "http://openaccess.thecvf.com/content_ECCV_2018/html/Chuhui_Xue_Accurate_Scene_Text_ECCV_2018_paper.html", "authors": ["Chuhui Xue", "Shijian Lu", "Fangneng Zhan"], "publication_date": "2018", "conference": "Proceedings of the European conference on computer vision (ECCV)", "pages": "355-372", "description": "This paper presents a scene text detection technique that exploits bootstrapping and text border semantics for accurate localization of texts in scenes. A novel bootstrapping technique is designed which samples multiple \u2018subsections\u2019 of a word or text line and accordingly relieves the constraint of limited training data effectively. At the same time, the repeated sampling of text \u2018subsections\u2019 improves the consistency of the predicted text feature maps which is critical in predicting a single complete instead of multiple broken boxes for long words or text lines. In addition, a semantics-aware text border detection technique is designed which produces four types of text border segments for each scene text. With semantics-aware text borders, scene texts can be localized more accurately by regressing text pixels around the ends of words or text lines instead of all text pixels which often leads to inaccurate localization while dealing with long words or text lines. Extensive experiments demonstrate the effectiveness of the proposed techniques, and superior performance is obtained over several public datasets, eg 80.1 f-score for the MSRA-TD500, 67.1 f-score for the ICDAR2017-RCTW, etc.", "total_citations": {"2018": 1, "2019": 24, "2020": 29, "2021": 46, "2022": 24, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:PVgj2kMGcgYC": {"external_link": "http://openaccess.thecvf.com/content_ECCV_2018/html/Fangneng_Zhan_Verisimilar_Image_Synthesis_ECCV_2018_paper.html", "authors": ["Fangneng Zhan", "Shijian Lu", "Chuhui Xue"], "publication_date": "2018", "conference": "Proceedings of the European Conference on Computer Vision (ECCV)", "pages": "249-266", "description": "The requirement of large amounts of annotated images has become one grand challenge while training deep neural network models for various visual detection and recognition tasks. This paper presents a novel image synthesis technique that aims to generate a large amount of annotated scene text images for training accurate and robust scene text detection and recognition models. The proposed technique consists of three innovative designs. First, it realizes\" semantic coherent\" synthesis by embedding texts at semantically sensible regions within the background image, where the semantic coherence is achieved by leveraging the semantic annotations of objects and image regions that have been created in the prior semantic segmentation research. Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region, which coincides with the fact that texts are often placed around homogeneous regions for better visibility in scenes. Third, it designs an adaptive text appearance model that determines the color and brightness of embedded texts by learning from the feature of real scene text images adaptively. The proposed technique has been evaluated over five public datasets and the experiments show its superior performance in training accurate and robust scene text detection and recognition models.", "total_citations": {"2019": 15, "2020": 16, "2021": 50, "2022": 81, "2023": 53}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:kJDgFkosVoMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8270144/", "authors": ["Dinh Nguyen Van", "Shijian Lu", "Xiang Bai", "Nizar Ouarti", "Mounir Mokhtari"], "publication_date": "2017/11/9", "conference": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)", "volume": "1", "pages": "1295-1300", "publisher": "IEEE", "description": "Automatic reading texts in scenes is an attracting increasing interest in recent years due to various context awareness applications. Leverage on the advantages of object proposal in generic object detection, we propose a max-pooling based scene text proposal technique aiming for automatic extraction of texts in scenes. Given a scene image, a max-pooling based grouping technique is designed to search for scene text proposals within a feature map which is computed from image edges. Searched proposals are then ranked by a scoring function that is defined based on the histogram of oriented gradient. The proposed technique has been evaluated on two publicly available scene text datasets, including the ICDAR2015 dataset and the Street View Text (SVT) dataset. Experiments show that the proposed technique obtains superior proposal performance as compared with state-of-the-arts, especially when a small\u00a0\u2026", "total_citations": {"2019": 1, "2020": 0, "2021": 0, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:43bX7VzcjpAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8270164/", "authors": ["Baoguang Shi", "Cong Yao", "Minghui Liao", "Mingkun Yang", "Pei Xu", "Linyan Cui", "Serge Belongie", "Shijian Lu", "Xiang Bai"], "publication_date": "2017/11/9", "conference": "2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)", "volume": "1", "pages": "1429-1434", "publisher": "IEEE", "description": "Chinese is the most widely used language in the world. Algorithms that read Chinese text in natural images facilitate applications of various kinds. Despite the large potential value, datasets and competitions in the past primarily focus on English, which bares very different characteristics than Chinese. This report introduces RCTW, a new competition that focuses on Chinese text reading. The competition features a large-scale dataset with over 12,000 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams. This report includes dataset description, task definitions, evaluation protocols, and results summaries and analysis. Through this competition, we call for more future research on the Chinese text reading problem.", "total_citations": {"2017": 1, "2018": 13, "2019": 34, "2020": 30, "2021": 48, "2022": 40, "2023": 31}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:tH6gc1N1XXoC": {"external_link": "http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_TORNADO_A_Spatio-Temporal_ICCV_2017_paper.html", "authors": ["Hongyuan Zhu", "Romain Vial", "Shijian Lu"], "publication_date": "2017/10/1", "conference": "Proceedings of the IEEE Conference on Computer Vision", "pages": "5813-5821", "description": "Given a video clip, action proposal aims to quickly generate a number of spatio-temporal tubes that enclose candidate human activities. Recently, the regression-based object detectors and long-term recurrent convolutional network (LRCN) have demonstrated superior performance in human action detection and recognition. However, the regression-based detectors performs inference without considering the temporal context among neighboring frames, and the LRCN using global visual percepts lacks the capability to capture local temporal dynamics. In this paper, we present a novel framework called TORNADO for human action proposal detection in un-trimmed video clips. Specifically, we propose a spatial-temporal convolutional network that combines the advantages of regression-based detector and LRCN by empowering Convolutional LSTM with regression capability. Our approach consists of a temporal convolutional regression network (T-CRN) and a spatial regression network (S-CRN) which are trained end-to-end on both RGB and OpticalFlow streams. They fuse appearance, motion and temporal contexts to regress the bounding boxes of candidate human actions simultaneously in 28 FPS. The action proposals are constructed by solving dynamic programming with peak trimming of the generated action boxes. Extensive experiments on the challenging UCF-101 and UCF-Sports datasets show that our method achieves superior performance as compared with the state-of-the-arts.", "total_citations": {"2017": 3, "2018": 5, "2019": 18, "2020": 18, "2021": 12, "2022": 8, "2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:gKiMpY-AVTkC": {"external_link": "http://openaccess.thecvf.com/content_iccv_2017/html/Tian_WeText_Scene_Text_ICCV_2017_paper.html", "authors": ["Shangxuan Tian", "Shijian Lu", "Chongshou Li"], "publication_date": "2017/10/1", "conference": "Proceedings of the IEEE Conference on Computer Vision", "description": "The requiring of large amounts of annotated training data has become a common constraint on various deep learning systems. In this paper, we propose a weakly supervised scene text detection method (WeText) that trains robust and accurate scene text detection models by learning from unannotated or weakly annotated data. With a\" light\" supervised model trained on a small fully annotated dataset, we explore semi-supervised and weakly supervised learning on a large unannotated dataset and a large weakly annotated dataset, respectively. For the unsupervised learning, the light supervised model is applied to the unannotated dataset to search for more character training samples, which are further combined with the small annotated dataset to retrain a superior character detection model. For the weakly supervised learning, the character searching is guided by high-level annotations of words/text lines that are widely available and also much easier to prepare. In addition, we design an unified scene character detector by adapting regression based deep networks, which greatly relieves the error accumulation issue that widely exists in most traditional approaches. Extensive experiments across different unannotated and weakly annotated datasets show that the scene text detection performance can be clearly boosted under both scenarios, where the weakly supervised learning can achieve the state-of-the-art performance by using only 229 fully annotated scene text images.", "total_citations": {"2018": 15, "2019": 19, "2020": 21, "2021": 17, "2022": 15, "2023": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:gVv57TyPmFsC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8051100/", "authors": ["Tao Chen", "Shijian Lu", "Jiayuan Fan"], "publication_date": "2017/9/26", "journal": "IEEE transactions on pattern analysis and machine intelligence", "volume": "40", "issue": "10", "pages": "2522-2528", "publisher": "IEEE", "description": "The marriage between the deep convolutional neural network (CNN) and region proposals has made breakthroughs for object detection in recent years. While the discriminative object features are learned via a deep CNN for classification, the large intra-class variation and deformation still limit the performance of the CNN based object detection. We propose a subcategory-aware CNN (S-CNN) to solve the object intra-class variation problem. In the proposed technique, the training samples are first grouped into multiple subcategories automatically through a novel instance sharing maximum margin clustering process. A multi-component Aggregated Channel Feature (ACF) detector is then trained to produce more latent training samples, where each ACF component corresponds to one clustered subcategory. The produced latent samples together with their subcategory labels are further fed into a CNN classifier to\u00a0\u2026", "total_citations": {"2018": 10, "2019": 9, "2020": 5, "2021": 11, "2022": 4, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:0CzhzZyukY4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8296319/", "authors": ["Jean-Baptiste Weibel", "Hui Li Tan", "Shijian Lu"], "publication_date": "2017/9/17", "conference": "2017 IEEE International Conference on Image Processing (ICIP)", "pages": "440-444", "publisher": "IEEE", "description": "Visual attention modelling is an important research topic with a wide range of applications in visual tracking, perceptual quality assessment, re-targeting, video summarization, etc. In this paper, we propose a visual attention model that captures both bottom-up spatial-temporal saliency and top-down objectness. Leveraging on co-occurrence histograms, the proposed model captures a number of low-level cues including contrast, gradient, as well as, magnitude and gradient of optical flow. Additionally, the proposed model incorporates mid-level objectness cue which helps to boost the modelling performance greatly. The proposed model obtained superior AUC-ROCs when evaluated over the ASCMN dataset and the UCF Sports Action dataset.", "total_citations": {"2018": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:5icHVeHT4IsC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8296639/", "authors": ["Romain Vial", "Hongyuan Zhu", "Yonghong Tian", "Shijian Lu"], "publication_date": "2017/9/17", "conference": "2017 IEEE International Conference on Image Processing (ICIP)", "pages": "2035-2039", "publisher": "IEEE", "description": "In this paper, we propose a new approach for searching action proposals in unconstrained videos. Our method first produces snippet action proposals by combining state-of-the-art YOLO detector (Static YOLO) and our regression based RNN detector (Recurrent YOLO). Then, these short action proposals are integrated to form final action proposals by solving two-pass dynamic programming which maximizes actioness score and temporal smoothness concurrently. Our experimental comparison with other state-of-the-arts on challenging UCF101 dataset shows that our method advances state-of-the-art proposal generation performance while maintaining low computational cost.", "total_citations": {"2017": 1, "2018": 2, "2019": 2, "2020": 0, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:lvd772isFD0C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8022877/", "authors": ["Jiayuan Fan", "Tao Chen", "Shijian Lu"], "publication_date": "2017/8/30", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "volume": "28", "issue": "11", "pages": "3163-3173", "publisher": "IEEE", "description": "This paper presents a new technique for hyperspectral image (HSI) classification by using superpixel guided deep-sparse-representation learning. The proposed technique constructs a hierarchical architecture by exploiting the sparse coding to learn the HSI representation. Specifically, a multiple-layer architecture using different superpixel maps is designed, where each superpixel map is generated by downsampling the superpixels gradually along with enlarged spatial regions for labeled samples. In each layer, sparse representation of pixels within every spatial region is computed to construct a histogram via the sum-pooling with l 1  normalization. Finally, the representations (features) learned from the multiple-layer network are aggregated and trained by a support vector machine classifier. The proposed technique has been evaluated over three public HSI data sets, including the Indian Pines image set, the\u00a0\u2026", "total_citations": {"2017": 1, "2018": 1, "2019": 4, "2020": 8, "2021": 4, "2022": 14, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:GFxP56DSvIMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7937848/", "authors": ["Tao Chen", "Shijian Lu"], "publication_date": "2017/6/1", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "volume": "55", "issue": "9", "pages": "5264-5273", "publisher": "IEEE", "description": "Oil spill inspection is critical to the marine and coastal ecosystems, and has been widely studied by various remote sensing technologies, such as synthetic aperture radar and hyperspectral. To improve the temporal resolution and the inspection flexibility, we propose a novel aerial image-based system that can find oil spills timely from images captured using onboard optical cameras installed in unmanned aerial vehicle or airplanes. In particular, a subcategory-aware feature selection (FS) and support vector machine (SVM) joint optimization technique is proposed to learn a discriminative model that can tell the existence of oil spills within an optical image of the marine surface. A set of color-based features is first extracted and concatenated together to characterize the oil spill incidence in an image, where a new color autocorrelogram is designed, which can better describe each color's spatial distribution in an image\u00a0\u2026", "total_citations": {"2017": 1, "2018": 3, "2019": 1, "2020": 0, "2021": 7, "2022": 3, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Ehil0879vHcC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/8296476/", "authors": ["Andrei Polzounov", "Artsiom Ablavatski", "Sergio Escalera", "Shijian Lu", "Jianfei Cai"], "publication_date": "2017/5/15", "conference": "International Conference on Image Processing", "description": "In recent years, text recognition has achieved remarkable success in recognizing scanned document text. However, word recognition in natural images is still an open problem, which generally requires time consuming post-processing steps. We present a novel architecture for individual word detection in scene images based on semantic segmentation. Our contributions are twofold: the concept of WordFence, which detects border areas surrounding each individual word and a novel pixelwise weighted softmax loss function which penalizes background and emphasizes small text regions. WordFence ensures that each word is detected individually, and the new loss function provides a strong training signal to both text and word border localization. The proposed technique avoids intensive post-processing, producing an end-to-end word detection system. We achieve superior localization recall on common\u00a0\u2026", "total_citations": {"2017": 2, "2018": 7, "2019": 5, "2020": 6, "2021": 9, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:sNmaIFBj_lkC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0895611116300799", "authors": ["Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan"], "publication_date": "2017/4/1", "journal": "Computerized Medical Imaging and Graphics", "volume": "57", "pages": "40-49", "publisher": "Pergamon", "description": "Autoimmune diseases (AD) are the abnormal response of the immune system of the body to healthy tissues. ADs have generally been on the increase. Efficient computer aided diagnosis of ADs through classification of the human epithelial type 2 (HEp-2) cells become beneficial. These methods make lower diagnosis costs, faster response and better diagnosis repeatability. In this paper, we present an automated HEp-2 cell image classification technique that exploits the sparse coding of the visual features together with the Bag of Words model (SBoW). In particular, SURF (Speeded Up Robust Features) and SIFT (Scale-invariant feature transform) features are specially integrated to work in a complementary fashion. This method helps greatly improve the cell classification accuracy. Additionally, a hierarchical max-pooling method is proposed to aggregate the local sparse codes in different layers to provide final\u00a0\u2026", "total_citations": {"2017": 7, "2018": 2, "2019": 4, "2020": 2, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:u-coK7KVo8oC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7926732/", "authors": ["Mounir Mokhtari Dinh Nguyen", "Shijian Lu", "Nizar Ouarti"], "publication_date": "2017/3/24", "conference": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)", "publisher": "IEEE", "description": "Text proposal has been gaining interest in recent years due to the great success of object proposal in categoriesindependent object localization. In this paper, we present a novel text-specific proposal technique that provides superior bounding boxes for accurate text localization in scenes. The proposed technique, which we call Text Edge Box (TEB), uses a binary edge map, a gradient map and an orientation map of an image as inputs. Connected components are first found within the binary edge map, which are scored by two proposed low-cue text features that are extracted in the gradient map and the orientation map, respectively. These scores present text probability of connected components and are aggregated in a text edge image. Scene texts proposals are finally generated by grouping the connected components and estimating their likelihood of being words. The proposed TEB has been evaluated on the\u00a0\u2026", "total_citations": {"2019": 1, "2020": 0, "2021": 1, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:8xutWZnSdmoC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7926696/", "authors": ["Artsiom Ablavatski", "Shijian Lu", "Jianfei Cai"], "publication_date": "2017/3/24", "conference": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)", "pages": "971-978", "publisher": "IEEE", "description": "We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) \u2014 an improved attention-based architecture for multiple object recognition. The proposed model is a fully differentiable unit that can be optimized end-to-end by using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was employed as visual attention mechanism which allows to learn the geometric transformation of objects within images. With the combination of the Spatial Transformer and the powerful recurrent architecture, the proposed EDRAM can localize and recognize objects simultaneously. EDRAM has been evaluated on two publicly available datasets including MNIST Cluttered (with 70K cluttered digits) and SVHN (with up to 250k real world images of house numbers). Experiments show that it obtains superior performance as compared with the state-of-the-art models.", "total_citations": {"2017": 1, "2018": 9, "2019": 9, "2020": 11, "2021": 4, "2022": 6}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:KbBQZpvPDL4C": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320316303314", "authors": ["Bolan Su", "Shijian Lu"], "publication_date": "2017/3/1", "journal": "Pattern Recognition", "volume": "63", "pages": "397-405", "publisher": "Pergamon", "description": "Recognition of texts in scenes is one of the most important tasks in many computer vision applications. Though different scene text recognition techniques have been developed, scene text recognition under a generic condition is still a very open and challenging research problem. One major factor that defers the advance in this research area is character touching, where many characters in scene images are heavily touched with each other and cannot be segmented for recognition. In this paper, we proposed a novel scene text recognition technique that performs word level recognition without character segmentation. Our proposed technique has three advantages. First it converts each word image into a sequential signal for the scene text recognition. Second, it adapts the recurrent neural network (RNN) with Long Short Term Memory (LSTM), the technique that has been widely used for handwriting recognition in\u00a0\u2026", "total_citations": {"2017": 7, "2018": 14, "2019": 26, "2020": 24, "2021": 31, "2022": 21, "2023": 15}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LhH-TYMQEocC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7811211/", "authors": ["Jiayuan Fan", "Tao Chen", "Shijian Lu"], "publication_date": "2017/1/9", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "volume": "55", "issue": "4", "pages": "2250-2261", "publisher": "IEEE", "description": "This paper proposes a novel unsupervised feature learning algorithm for land-use scene recognition on very high resolution remote sensing imagery. The proposed technique utilizes a multipath sparse coding architecture in order to capture multiple aspects of discriminative structures within complex remote sensing sceneries. Unlike the previous sparse coding and bag-of-visual-words-based techniques that rely on the handcrafted feature descriptors such as scale-invariant feature transform, the proposed technique extracts dense low-level features from the raw data, including the visual (RGB) data and near-infrared (NIR) data, using image patches of varying sizes at different layers. The proposed technique has been evaluated on three data sets, including the 21-category UC Merced landuse RGB data set with a 1-ft spatial resolution, the 9-category ground scene RGB-NIR data set, and the 10-category Singapore\u00a0\u2026", "total_citations": {"2017": 6, "2018": 10, "2019": 11, "2020": 8, "2021": 8, "2022": 7, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:r_AWSJRzSzQC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10004/100040W/Spectral-spatial-hyperspectral-image-classification-using-super-pixel-based-spatial/10.1117/12.2241033.short", "authors": ["Jiayuan Fan", "Hui Li Tan", "Maria Toomik", "Shijian Lu"], "publication_date": "2016/10/18", "conference": "Image and Signal Processing for Remote Sensing XXII", "volume": "10004", "pages": "315-321", "publisher": "SPIE", "description": "Spatial pyramid matching has demonstrated its power for image recognition task by pooling features from spatially increasingly fine sub-regions. Motivated by the concept of feature pooling at multiple pyramid levels, we propose a novel spectral-spatial hyperspectral image classification approach using superpixel-based spatial pyramid representation. This technique first generates multiple superpixel maps by decreasing the superpixel number gradually along with the increased spatial regions for labelled samples. By using every superpixel map, sparse representation of pixels within every spatial region is then computed through local max pooling. Finally, features learned from training samples are aggregated and trained by a support vector machine (SVM) classifier. The proposed spectral-spatial hyperspectral image classification technique has been evaluated on two public hyperspectral datasets, including the\u00a0\u2026", "total_citations": {"2019": 1, "2020": 3, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:yMeIxYmEMEAC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10004/100040V/M-estimation-for-robust-sparse-unmixing-of-hyperspectral-images/10.1117/12.2241237.short", "authors": ["Maria Toomik", "Shijian Lu", "James DB Nelson"], "publication_date": "2016/10/18", "conference": "Image and Signal Processing for Remote Sensing XXII", "volume": "10004", "pages": "301-314", "publisher": "SPIE", "description": "Hyperspectral unmixing methods often use a conventional least squares based lasso which assumes that the data follows the Gaussian distribution. The normality assumption is an approximation which is generally invalid for real imagery data. We consider a robust (non-Gaussian) approach to sparse spectral unmixing of remotely sensed imagery which reduces the sensitivity of the estimator to outliers and relaxes the linearity assumption. The method consists of several appropriate penalties. We propose to use an \u2113p norm with 0 < p < 1 in the sparse regression problem, which induces more sparsity in the results, but makes the problem non-convex. On the other hand, the problem, though non-convex, can be solved quite straightforwardly with an extensible algorithm based on iteratively reweighted least squares. To deal with the huge size of modern spectral libraries we introduce a library reduction step, similar to\u00a0\u2026", "total_citations": {"2018": 1, "2019": 0, "2020": 1, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:fFSKOagxvKUC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10004/100040N/Regions-of-interest-extraction-from-remote-sensing-imageries-using-visual/10.1117/12.2240749.short", "authors": ["Hui Li Tan", "Jiayuan Fan", "Maria Toomik", "Shijian Lu"], "publication_date": "2016/10/18", "conference": "Image and Signal Processing for Remote Sensing XXII", "volume": "10004", "pages": "229-235", "publisher": "SPIE", "description": "Processing and analysing large volume of remote sensing data is both labour intensive and time consuming. Therefore, there is a need to effectively and efficiently identify meaningful regions in these remote sensing data for timely resource management. In this paper, we propose a visual attention model for identifying regions-of-interest in remote sensing data. The proposed model incorporates both bottom-up spatial saliency and top-down objectness, by fusing a co-occurrence histogram saliency model with the BING objectness model. The co-occurrence histogram saliency model is constructed by first building a 2D co-occurrence histogram that captures co-occurrence and occurrence of image intensities, and then using the 2D co-occurrence histogram to model local and global saliency. On the other hand, the BING objectness model is constructed by resizing image intensities in variable-sized windows to 8x8\u00a0\u2026", "total_citations": {"2018": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:zLWjf1WUPmwC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0167865516000544", "authors": ["Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan"], "publication_date": "2016/10/15", "journal": "Pattern Recognition Letters", "volume": "82", "pages": "64-71", "publisher": "North-Holland", "description": "Autoimmune Diseases (AD) are among the top 10 leading causes of death in female children and women in all age groups up to 64 years. They are widely diagnosed by various antibody tests that typically apply the Indirect Immunofluorescence (IIF) to the Human Epithelial Type-2 (HEp-2) cells. Automated classification of Hep-2 cells has attracted much research interest in recent years, and many of these approaches employ patch-based models and the Bag of Words (BoW) scheme, but often face several typical constraints such as the need to process a huge number of overlapped image patches, tuning of various parameters and etc. We propose a superpixel based Hep-2 cell classification technique by calculating the sparse codes of image patches which are prepared in a more intelligent way. In particular, the superpixel approach guides the determination of the right image patches while aggregating the\u00a0\u2026", "total_citations": {"2016": 1, "2017": 2, "2018": 6, "2019": 10, "2020": 6, "2021": 2, "2022": 1, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Ri6SYOTghG4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7505596/", "authors": ["Tao Chen", "Shijian Lu"], "publication_date": "2016/7/7", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "volume": "27", "issue": "11", "pages": "2333-2343", "publisher": "IEEE", "description": "It is important for a moving observer to be able to identify his/her surrounding objects and determine whether these objects are moving or stationary, which is called object-level motion detection. Detecting object-level motion from moving cameras is a difficult problem to solve for collision-free navigation due to the dual motion introduced by the mixture of the camera motion and the object motion. This paper presents a novel technique that detects object-level motion from a freely moving camera using only two consecutive video frames. A context-aware motion descriptor (CMD) is designed based on the object\u2019s moving speed and moving direction relative to that of the moving camera. The CMD employs the contextual information, e.g., the optical flow of the image background surrounding the moving object of interest, which describes the object motion behavior better than other contexts such as the camera\u2019s GPS and\u00a0\u2026", "total_citations": {"2017": 4, "2018": 8, "2019": 14, "2020": 6, "2021": 6, "2022": 7, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:z_wVstp3MssC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7328325/", "authors": ["Tao Chen", "Shijian Lu"], "publication_date": "2016/6", "journal": "IEEE Transactions on Vehicular Technology", "volume": "65", "issue": "6", "pages": "4006 - 4015", "publisher": "IEEE", "description": "Real-time traffic sign detection and recognition has been receiving increasingly more attention in recent years due to the popularity of driver-assistance systems and autonomous vehicles. This paper proposes an accurate and efficient traffic sign detection technique by exploring AdaBoost and support vector regression (SVR) for discriminative detector learning. Different from the reported traffic sign detection techniques, a novel saliency estimation approach is first proposed, where a new saliency model is built based on the traffic sign-specific color, shape, and spatial information. By incorporating the saliency information, enhanced feature pyramids are built to learn an AdaBoost model that detects a set of traffic sign candidates from images. A novel iterative codeword selection algorithm is then designed to generate a discriminative codebook for the representation of sign candidates, as detected by the AdaBoost\u00a0\u2026", "total_citations": {"2016": 3, "2017": 15, "2018": 16, "2019": 22, "2020": 10, "2021": 16, "2022": 19, "2023": 8}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:_Ybze24A_UAC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320315002691", "authors": ["Shangxuan Tian", "Ujjwal Bhattacharya", "Shijian Lu", "Bolan Su", "Qingqing Wang", "Xiaohua Wei", "Yue Lu", "Chew Lim Tan"], "publication_date": "2016/3/1", "journal": "Pattern Recognition", "volume": "51", "pages": "125-134", "publisher": "Pergamon", "description": "Automatic machine reading of texts in scenes is largely restricted by the poor character recognition accuracy. In this paper, we extend the Histogram of Oriented Gradient (HOG) and propose two new feature descriptors: Co-occurrence HOG (Co-HOG) and Convolutional Co-HOG (ConvCo-HOG) for accurate recognition of scene texts of different languages. Compared with HOG which counts orientation frequency of each single pixel, the Co-HOG encodes more spatial contextual information by capturing the co-occurrence of orientation pairs of neighboring pixels. Additionally, ConvCo-HOG exhaustively extracts Co-HOG features from every possible image patches within a character image for more spatial information. The two features have been evaluated extensively on five scene character datasets of three different languages including three sets in English, one set in Chinese and one set in Bengali. Experiments\u00a0\u2026", "total_citations": {"2015": 1, "2016": 6, "2017": 13, "2018": 15, "2019": 22, "2020": 33, "2021": 25, "2022": 19, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:HeT0ZceujKMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7760431/", "authors": ["Sadaf Monajemi", "Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan", "Saeid Sanei", "Sim-Heng Ong"], "publication_date": "2016", "conference": "2016 24th European Signal Processing Conference (EUSIPCO)", "pages": "1163-1167", "publisher": "IEEE", "description": "Automatic classification of human epithelial type-2 (HEp-2) cells can improve the diagnostic process of autoimmune diseases (ADs) in terms of lower cost, faster response, and better repeatability. However, most of the proposed methods for classification of HEp-2 cells suffer from several constraints including tedious parameter tuning, massive memory requirement, and high computational costs. We propose an adaptive distributed dictionary learning (ADDL) method where the dictionary learning problem is reformulated as a distributed learning task. With the help of this approach, we develop an automatic and robust method that effectively handles the complexity of the problem in terms of memory and computational cost and also obtains superior classification accuracy.", "total_citations": {"2016": 1, "2017": 1, "2018": 5, "2019": 1, "2020": 2, "2021": 2, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:gsN89kCJA0AC": {"external_link": "http://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Discriminative_Multi-Modal_Feature_CVPR_2016_paper.html", "authors": ["Hongyuan Zhu", "Jean-Baptiste Weibel", "Shijian Lu"], "publication_date": "2016", "conference": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "pages": "2969-2976", "description": "RGBD scene recognition has attracted increasingly attention due to the rapid development of depth sensors and their wide application scenarios. While many research has been conducted, most work used hand-crafted features which are difficult to capture high-level semantic structures. Recently, the feature extracted from deep convolutional neural network has produced state-of-the-art results for various computer vision tasks, which inspire researchers to explore incorporating CNN learned features for RGBD scene understanding. On the other hand, most existing work combines rgb and depth features without adequately exploiting the consistency and complementary information between them. Inspired by some recent work on RGBD object recognition using multi-modal feature fusion, we introduce a novel discriminative multi-modal fusion framework for rgbd scene recognition for the first time which simultaneously considers the inter-and intra-modality correlation for all samples and meanwhile regularizing the learned features to be discriminative and compact. The results from the multimodal layer can be back-propagated to the lower CNN layers, hence the parameters of the CNN layers and multimodal layers are updated iteratively until convergence. Experiments on the recently proposed large scale SUN RGB-D datasets show that our method achieved the state-of-the-art without any image segmentation.", "total_citations": {"2016": 1, "2017": 18, "2018": 19, "2019": 27, "2020": 13, "2021": 20, "2022": 11, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:uWiczbcajpAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7475951/", "authors": ["Nadia M. Thalmann Hongyuan Zhu", "Jiangbo Lu", "Jianfei Cai", "Jianmin Zheng", "Shijian Lu"], "publication_date": "2016", "journal": "IEEE Transactions on Multimedia", "volume": "18", "issue": "8", "pages": "1516 - 1530", "description": "Localizing, identifying, and extracting humans with consistent appearance jointly from a personal photo stream is an important problem and has wide applications. The strong variations in foreground and background and irregularly occurring foreground humans make this realistic problem challenging. Inspired by advancements in object detection, scene understanding, and image cosegmentation, we explore explicit constraints to label and segment human objects rather than other nonhuman objects and \u201cstuff.\u201d We refer to such a problem as multiple human identification and cosegmentation (MHIC). To identify specific human subjects, we propose an efficient human instance detector by combining an extended color line model with a poselet-based human detector. Moreover, to capture high-level human shape information, a novel soft shape cue is proposed. It is initialized by the human detector, then further\u00a0\u2026", "total_citations": {"2017": 1, "2018": 3, "2019": 4, "2020": 3, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:nrtMV_XWKgEC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7368889/", "authors": ["Tao Chen", "Shijian Lu"], "publication_date": "2016", "publisher": "IEEE Transactions on Circuits and Systems for Video Technology", "description": "Vehicle detection and vehicle viewpoint estimation are both crucial for assistive and autonomous driving systems. In this paper, we propose a soft discriminative mixture of viewpoint (SDMoV) models for joint vehicle detection and vehicle viewpoint estimation. The proposed SDMoV model is learned in two steps. First, a discriminative viewpoint-specific component model, which aims to maximize vehicle viewpoint classification accuracy, is learned for each cluster of vehicle images with similar viewpoint. Second, a new soft margin objective function, which aims to maximize vehicle detection accuracy, is designed to retrain these component models into a mixture of viewpoint models. The proposed SDMoV model is capable of detecting vehicles and estimating their viewpoints simultaneously. Experiments on three state-of-the-art datasets show that the proposed SDMoV model achieves superior accuracy for both\u00a0\u2026", "total_citations": {"2016": 1, "2017": 5, "2018": 3, "2019": 4, "2020": 2, "2021": 3, "2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:epqYDVWIO7EC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7457846/", "authors": ["Jiayuan Fan", "Tao Chen", "Shijian Lu"], "publication_date": "2015/12/13", "conference": "2015 Visual Communications and Image Processing (VCIP)", "pages": "1-4", "publisher": "IEEE", "description": "Automatic vegetation coverage detection plays a key role for monitoring and management of land usage, environmental variation, and urban planning. This paper presents a novel vegetation coverage detection technique for very high resolution multi-spectral satellite imagery. The proposed technique consists of two stages including a supervised patch-level scoring stage and an unsupervised pixel-level classification stage. In the first stage, a support vector regression (SVR) technique is developed which scores each image patch and generates a coarse patch-level vegetation map. In the second stage, an unsupervised pixel-level vegetation classification technique is developed, which produces a more detailed vegetation map by re-scoring those uncertain pixels based on the computed SVR scores. Experiments on very high resolution multi-spectral satellite images show that the proposed technique outperforms\u00a0\u2026", "total_citations": {"2017": 1, "2018": 0, "2019": 1, "2020": 2, "2021": 1, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:tKAzc9rXhukC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9643/96430S/Multipath-sparse-coding-for-scene-classification-in-very-high-resolution/10.1117/12.2194494.short", "authors": ["Jiayuan Fan", "Hui Li Tan", "Shijian Lu"], "publication_date": "2015/10/15", "conference": "Image and Signal Processing for Remote Sensing XXI", "volume": "9643", "pages": "268-275", "publisher": "SPIE", "description": "With the rapid development of various satellite sensors, automatic and advanced scene classification technique is urgently needed to process a huge amount of satellite image data. Recently, a few of research works start to implant the sparse coding for feature learning in aerial scene classification. However, these previous research works use the single-layer sparse coding in their system and their performances are highly related with multiple low-level features, such as scale-invariant feature transform (SIFT) and saliency. Motivated by the importance of feature learning through multiple layers, we propose a new unsupervised feature learning approach for scene classification on very high resolution satellite imagery. The proposed unsupervised feature learning utilizes multipath sparse coding architecture in order to capture multiple aspects of discriminative structures within complex satellite scene images. In\u00a0\u2026", "total_citations": {"2016": 1, "2017": 2, "2018": 1, "2019": 0, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:7T2F9Uy0os0C": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9643/964318/L-shaped-corner-detector-for-rooftop-extraction-from-satellite-aerial/10.1117/12.2194279.short", "authors": ["Hui Li Tan", "Jiayuan Fan", "Shijian Lu"], "publication_date": "2015/10/15", "conference": "Image and Signal Processing for Remote Sensing XXI", "volume": "9643", "pages": "423-429", "publisher": "SPIE", "description": "Rooftop extraction from satellite/aerial imagery is an important geospatial problem with many practical applications. However, rooftop extraction remains a challenging problem due to the diverse characteristics and appearances of the buildings, as well as the quality of the satellite/aerial images. Many existing rooftop extraction methods use rooftop corners as a basic component. Nonetheless, existing rooftop corner detectors either suffer from high missed detection or introduce high false alarm. Based on the observation that rooftop corners are typically of L-shape, we propose an L-shaped corner detector for automatic rooftop extraction from high resolution satellite/aerial imagery. The proposed detector considers information in a spatial circle around each pixel to construct a feature map which captures the probability of L-shaped corner at every pixel. Our experimental results on a rooftop database of over 200\u00a0\u2026", "total_citations": {"2019": 1, "2020": 0, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:XD-gHx7UXLsC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7351264/", "authors": ["Tao Chen", "Shijian Lu"], "publication_date": "2015/9/27", "conference": "2015 IEEE International Conference on Image Processing (ICIP)", "pages": "2557-2561", "publisher": "IEEE", "description": "Automatic lane marking detection plays an important role in intelligent transportation systems. We present an effective lane marking detection technique that utilizes the context-aware information of lane marking on the urban roads. The proposed technique consists of two innovations. First, the context-aware color, texture and shape features which characterise both lane markings and their road context are designed to represent the lane markings on the road surface. Second, a hard negative mining technique is developed based on the Maximum Stable Extreme Region (MSER) detector and adaboost training. Experiments on a real world dataset demonstrate the superior performance of the proposed approach.", "total_citations": {"2016": 1, "2017": 1, "2018": 2, "2019": 0, "2020": 1, "2021": 1, "2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:PR6Y55bgFSsC": {"external_link": "https://arxiv.org/abs/1507.04512", "authors": ["Hongyuan Zhu", "Shijian Lu", "Jianfei Cai", "Quangqing Lee"], "publication_date": "2015/7/16", "conference": "BMVC 2015", "description": "Object proposal has become a popular paradigm to replace exhaustive sliding window search in current top-performing methods in PASCAL VOC and ImageNet. Recently, Hosang et al. conduct the first unified study of existing methods' in terms of various image-level degradations. On the other hand, the vital question \"what object-level characteristics really affect existing methods' performance?\" is not yet answered. Inspired by Hoiem et al.'s work in categorical object detection, this paper conducts the first meta-analysis of various object-level characteristics' impact on state-of-the-art object proposal methods. Specifically, we examine the effects of object size, aspect ratio, iconic view, color contrast, shape regularity and texture. We also analyse existing methods' localization accuracy and latency for various PASCAL VOC object classes. Our study reveals the limitations of existing methods in terms of non-iconic view, small object size, low color contrast, shape regularity etc. Based on our observations, lessons are also learned and shared with respect to the selection of existing object proposal technologies as well as the design of the future ones.", "total_citations": {"2016": 2, "2017": 3, "2018": 0, "2019": 4, "2020": 1, "2021": 1, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:VL0QpB8kHFEC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S1047320315000802", "authors": ["Tao Chen", "Shijian Lu", "Jiayuan Fan"], "publication_date": "2015/7/1", "journal": "Journal of Visual Communication and Image Representation", "volume": "30", "pages": "289-298", "publisher": "Academic Press", "description": "This paper presents an effective approach that incorporates contextual information into vocabulary tree learning for mobile landmark recognition. For most existing mobile landmark recognition works, the context information (GPS or direction) is mainly used to reduce the search space in a heuristic and insufficient manner. Some recent work uses the context information for codebook learning but only the GPS information is explored. We propose an effective mobile landmark recognition approach which exploits both context (direction and location) and content information for vocabulary tree learning and image recognition. The proposed approach has two major contributions: (i) it proposes an information gain-based codeword discrimination learning method to evaluate the discriminative capability of each direction-aware codeword, as generated by a context-aware vocabulary tree, and (ii) it develops a context-aware\u00a0\u2026", "total_citations": {"2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:5awf1xo2G04C": {"external_link": "https://link.springer.com/article/10.1007/s10032-015-0237-z", "authors": ["Shijian Lu", "Tao Chen", "Shangxuan Tian", "Joo-Hwee Lim", "Chew-Lim Tan"], "publication_date": "2015/6", "journal": "International Journal on Document Analysis and Recognition (IJDAR)", "volume": "18", "pages": "125-135", "publisher": "Springer Berlin Heidelberg", "description": "This paper presents a scene text extraction technique that automatically detects and segments texts from scene images. Three text-specific features are designed over image edges with which a set of candidate text boundaries is first detected. For each detected candidate text boundary, one or more candidate characters are then extracted by using a local threshold that is estimated based on the surrounding image pixels. The real characters and words are finally identified by a support vector regression model that is trained using bags-of-words representation. The proposed technique has been evaluated over the latest ICDAR-2013 Robust Reading Competition dataset. Experiments show that it obtains superior F-measures of 78.19\u00a0% and 75.24\u00a0% (on atom level), respectively, for the scene text detection and segmentation tasks.", "total_citations": {"2014": 1, "2015": 4, "2016": 18, "2017": 20, "2018": 24, "2019": 17, "2020": 10, "2021": 9, "2022": 7, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:dQ2og3OwTAUC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7163964/", "authors": ["Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan"], "publication_date": "2015/4/16", "conference": "2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)", "pages": "679-682", "publisher": "IEEE", "description": "This paper studies automated classification of Human Epithelial Type-2 (HEp-2) cell images which is essential in diagnosing the Autoimmune Diseases (AD). The prevalent approach for this problem makes use of the Bag of Words (BoW) model and sparse coding scheme on over complete dictionaries, where the dictionary dimension is usually much larger than feature dimension. In addition, this approach usually requires manual selection of the dictionary dimension which is often troublesome and dependent highly on specific applications and datasets. We proposed a non-parametric Bayesian model that is capable of determining the dictionary dimension automatically by exploiting the Indian Buffet Process (IBP). This proposed model has been evaluated on two public HEp-2 benchmarking datasets, i.e., ICPR2012 and ICIP2013 where the SIFT and SURF features of the cell image are extracted in a grid manner\u00a0\u2026", "total_citations": {"2015": 2, "2016": 5, "2017": 3, "2018": 1, "2019": 0, "2020": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ZfRJV9d4-WMC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9420/94200R/Detection-of-high-grade-atypia-nuclei-in-breast-cancer-imaging/10.1117/12.2081793.short", "authors": ["Henri No\u00ebl", "Ludovic Roux", "Shijian Lu", "Thomas Boudier"], "publication_date": "2015/3/17", "conference": "Medical Imaging 2015: Digital Pathology", "volume": "9420", "pages": "180-185", "publisher": "SPIE", "description": "Along with mitotic count, nuclear pleomorphism or nuclear atypia is an important criterion for the grading of breast cancer in histopathology. Though some works have been done in mitosis detection (ICPR 2012,1 MICCAI 2013,2 and ICPR 2014), not much work has been dedicated to automated nuclear atypia grading, especially the most difficult task of detection of grade 3 nuclei. We propose the use of Convolutional Neural Networks for the automated detection of cell nuclei, using images from the three grades of breast cancer for training. The images were obtained from ICPR contests. Additional manual annotation was performed to classify pixels into five classes: stroma, nuclei, lymphocytes, mitosis and fat. At total of 3,000 thumbnail images of 101 \u00d7 101 pixels were used for training. By dividing this training set in an 80/20 ratio we could obtain good training results (around 90%). We tested our CNN on images of\u00a0\u2026", "total_citations": {"2017": 1, "2018": 1, "2019": 0, "2020": 2, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:eq2jaN3J8jMC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S1047320315002035", "authors": ["Hongyuan Zhu", "Fanman Meng", "Jianfei Cai", "Shijian Lu"], "publication_date": "2015/2/3", "journal": "Journal of Visual Communications and Image Representation", "description": "Image segmentation refers to the process to divide an image into meaningful non-overlapping regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. While many segmentation algorithms exist, there are only a few sparse and outdated summarizations available. Thus, in this paper, we aim to provide a comprehensive review of the recent progress in the field. Covering 190 publications, we give an overview of broad segmentation topics including not only the classic unsupervised methods, but also the recent weakly-/semi-supervised methods and the fully-supervised methods. In addition, we review the existing influential datasets and evaluation metrics. We also suggest some design choices and research directions for future research in image segmentation.", "total_citations": {"2015": 3, "2016": 20, "2017": 31, "2018": 51, "2019": 44, "2020": 53, "2021": 36, "2022": 26, "2023": 17}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:NJ774b8OgUMC": {"external_link": null, "authors": ["Chew-Lim Tan Shangxuan Tian", "Yifeng Pan", "Chang Huang", "Shijian Lu", "Kai Yu"], "publication_date": "2015", "conference": "IEEE International Conference on Computer Vision (ICCV)", "total_citations": {"2014": 1, "2015": 1, "2016": 11, "2017": 52, "2018": 59, "2019": 51, "2020": 28, "2021": 29, "2022": 20, "2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Fu2w8maKXqMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7333942/", "authors": ["Dimosthenis Karatzas", "Lluis Gomez-Bigorda", "Anguelos Nicolaou", "Suman Ghosh", "Andrew Bagdanov", "Masakazu Iwamura", "Jiri Matas", "Lukas Neumann", "Vijay Ramaseshan Chandrasekhar", "Shijian Lu", "Faisal Shafait", "Seiichi Uchida", "Ernest Valveny"], "publication_date": "2015", "conference": "12th International Conference on Document Analysis and Recognition", "description": "Results of the ICDAR 2015 Robust Reading Competition are presented. A new Challenge 4 on Incidental Scene Text has been added to the Challenges on Born-Digital Images, Focused Scene Images and Video Text. Challenge 4 is run on a newly acquired dataset of 1,670 images evaluating Text Localisation, Word Recognition and End-to-End pipelines. In addition, the dataset for Challenge 3 on Video Text has been substantially updated with more video sequences and more accurate ground truth data. Finally, tasks assessing End-to-End system performance have been introduced to all Challenges. The competition took place in the first quarter of 2015, and received a total of 44 submissions. Only the tasks newly introduced in 2015 are reported on. The datasets, the ground truth specification and the evaluation protocols are presented together with the results and a brief summary of the participating methods.", "total_citations": {"2016": 48, "2017": 73, "2018": 96, "2019": 182, "2020": 209, "2021": 306, "2022": 283, "2023": 236}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:uLbwQdceFCQC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-319-24574-4_37", "authors": ["Ali Taalimi", "Shahab Ensafi", "Hairong Qi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan"], "publication_date": "2015", "conference": "International Conference on Medical Image Computing and Computer Assisted Intervention", "description": "Use of automatic classification for Indirect Immunofluorescence (IIF) images of HEp-2 cells is increasingly gaining interest in Antinuclear Autoantibodies (ANAs) detection. In order to improve the classification accuracy, we propose a multi-modal joint dictionary learning method, to obtain a discriminative and reconstructive dictionary while training a classifier simultaneously. Here, the term \u2018multi-modal\u2019 refers to features extracted using different algorithms from the same data set. To utilize information fusion between feature modalities the algorithm is designed so that sparse codes of all modalities of each sample share the same sparsity pattern. The contribution of this paper is two-fold. First, we propose a new framework for multi-modal fusion at the feature level. Second, we impose an additional constraint on consistency of sparse coefficients among different modalities of the same class. Extensive\u00a0\u2026", "total_citations": {"2015": 2, "2016": 3, "2017": 12, "2018": 2, "2019": 3, "2020": 2, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:JQOojiI6XY0C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7333778/", "authors": ["Shangxuan Tian", "Shijian Lu", "Bolan Su", "Chew Lim Tan"], "publication_date": "2015", "conference": "12th International Conference on Document Analysis and Recognition", "description": "Text segmentation provides important clues for the accurate identification of character locations and the analysis of character properties such as shape estimation and texture synthesis. In this paper, we propose a robust text segmentation method that employs Markov Random Field (MRF) and use graph cut algorithms to solve the energy minimization problem. To effectively select accurate seeds to boost the text segmentation performance, stroke feature transform is adopted to robustly identify text seeds and text edges. Background seeds are obtained near the text edges in order to well preserve the text boundaries. The energy functions are defined as an MRF consisting of data energy and smoothness energy which can be efficiently solved by graph cut algorithms. One distinctive property of the proposed technique is that it can identify more distinctive seeds so that only one cut is needed to well separate the text\u00a0\u2026", "total_citations": {"2015": 1, "2016": 2, "2017": 1, "2018": 3, "2019": 0, "2020": 1, "2021": 1, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:UHK10RUVsp4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7333789/", "authors": ["Bolan Su", "Xi Zhang", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2015", "conference": "12th International Conference on Document Analysis and Recognition", "description": "Recognition of handwritten text is a useful technique that can be applied in different applications, such as signature recognition, bank check recognition, etc. However, the off-line handwritten text recognition in an unconstrained situation is still a very challenging task due to the high complexity of text strokes and image background. This paper presents a novel segmented handwritten text recognition technique that ensembles recurrent neural network (RNN) classifiers. Two RNN models are first trained that take advantage of the widely used geometrical feature and the Histogram of Oriented Gradient (HOG) feature, respectively. Given a handwritten word image, the optimal recognition result is then obtained by integrating the two trained RNN models together with a lexicon. Experiments on public datasets show the superior performance of our proposed technique.", "total_citations": {"2015": 1, "2016": 3, "2017": 1, "2018": 1, "2019": 1, "2020": 2, "2021": 2, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:_B80troHkn4C": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-319-13168-9_30", "authors": ["Bolan Su", "Shijian Lu"], "publication_date": "2014/12/1", "book": "Pacific Rim Conference on Multimedia", "pages": "268-273", "publisher": "Springer International Publishing", "description": "In this paper, we proposed a robust parking lot marking detection technique that is one important component for intelligent transportation systems and assisted/autonomous driving. Our system learns features of parking lot markings from training data and matches these templates to detected features in the test video during runtime. In the proposed system, maximally stable extremal regions (MSER) are used to detect a set of parking lot marking candidates. Features are then extracted from the detected candidates and Support Vector Machine (SVM) is applied to classify the parking lot marking in an efficient manner. With the detected parking lot markings, a parking lot is estimated by fitting two adjacent parking lot markings. The proposed technique is tested on real world street-view videos captured with an in-car camera. The experimental results show that the proposed technique is robust and capable of\u00a0\u2026", "total_citations": {"2016": 1, "2017": 1, "2018": 0, "2019": 1, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:SdhP9T11ey4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/7025805/", "authors": ["Tao Chen", "Jiayuan Fan", "Shijian Lu"], "publication_date": "2014/10/27", "conference": "2014 IEEE International Conference on Image Processing (ICIP)", "pages": "3963-3967", "publisher": "IEEE", "description": "This paper presents a codebook learning based mobile landmark recognition technique based on context information that is acquired from mobile devices. Previous codebook learning methods are mainly developed on nonmobile platforms such as desktop PC, hence underutilize context features such as location and direction information as provided by the mobile devices. The proposed technique employs both the direction and location information to learn the codebook for mobile landmark recognition. A set of direction-aware leaf codewords are first generated by using direction data to decompose the leaf nodes of the original SVT. A visual word significance learning algorithm is then developed by considering location information to generate a compact codebook for image encoding. Experiments on the NTU50Landmark database show that the proposed method can achieve good recognition performance in\u00a0\u2026", "total_citations": {"2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hC7cP41nSMkC": {"external_link": "https://patents.google.com/patent/US8849727B2/en", "inventors": "Shijian Lu, Cuntai Guan, Haihong Zhang", "publication_date": "2014/9/30", "patent_office": "WO", "patent_number": "WO2009145725A1", "application_number": "PCT/SG2008/000192", "description": "A method or system for classifying brain signals in a BCI. The system comprises a model building unit for building a subject-independent model using labelled brain signals from a pool of subjects."}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:AXPGKjj_ei8C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6945173/", "authors": ["Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan"], "publication_date": "2014/8/26", "conference": "2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "pages": "6732-6735", "publisher": "IEEE", "description": "With the prevalence of brain-related diseases like Alzheimer in an increasing ageing population, Connectomics, the study of connections between neurons of the human brain, has emerged as a novel and challenging research topic. Accurate and fully automatic algorithms are needed to deal with the increasing amount of data from the brain images. This paper presents an automatic 3D neuron reconstruction technique where neurons within each slice image are first segmented and then linked across multiple slices within the publicly available Electron Microscopy dataset (SNEMI3D). First, random Forest classifier is adapted on top of superpixels for the neuron segmentation within each slice image. The maximum overlap between two consecutive images is then calculated for neuron linking, where the adjacency matrix of two different labeling of the segments is used to distinguish neuron merging and splitting\u00a0\u2026", "total_citations": {"2014": 1, "2015": 1, "2016": 0, "2017": 1, "2018": 1, "2019": 0, "2020": 0, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:BJbdYPG6LGMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6977284/", "authors": ["Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan"], "publication_date": "2014/8/24", "conference": "2014 22nd International Conference on Pattern Recognition", "pages": "3321-3326", "publisher": "IEEE", "description": "It has been estimated that autoimmune diseases are among the top ten leading causes of death among women in all age groups up to 65 years. However, the detection of it by indirect immunofluorescence (IIF) image analysis depends heavily on the experience of the physicians. An accurate and automatic Computer Aided Diagnosis (CAD) system will help greatly for the classification of the Human Epithelial type 2 (HEp-2) cell images with little human intervention. In this paper we present an automatic HEp-2 cell image classification technique that exploits different spatial scaled image representation and sparse coding of SIFT features. Additionally, spatial max pooling of sparse coding at different scales is used to boost the classification performance. The proposed method is tested on the ICPR 2012 contest dataset and experiments show that it clearly outperforms state-of-the-art techniques in cell and image level\u00a0\u2026", "total_citations": {"2014": 2, "2015": 5, "2016": 6, "2017": 2, "2018": 1, "2019": 0, "2020": 1, "2021": 1, "2022": 0, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Y5dfb0dijaUC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6973543/", "authors": ["Shahab Ensafi", "Shijian Lu", "Ashraf A Kassim", "Chew Lim Tan"], "publication_date": "2014/8/24", "conference": "2014 1st Workshop on Pattern Recognition Techniques for Indirect Immunofluorescence Images", "pages": "29-32", "publisher": "IEEE", "description": "In this work we present an automatic HEp-2 cell image classification technique that exploits different spatial scaled image representation and sparse coding of SIFT and SURF features. The proposed method is applied on the ICIP2013 dataset in the I3A workshop, which is held in ICPR 2014 conference. Experiments are designed to capture the accuracies on training set with cross validation method. Additionally, the prior information on positive and intensity levels of cells are used to boost the overall performance. Finally, different number of iterations on learning the dictionary is studied to find the optimum one.", "total_citations": {"2014": 1, "2015": 3, "2016": 12, "2017": 7, "2018": 8, "2019": 0, "2020": 2, "2021": 2, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:eMMeJKvmdy0C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6977271/", "authors": ["Bolan Su", "Thien Anh Dinh", "Abhinit Kumar Ambastha", "Tianxia Gong", "Tomi Silander", "Shijian Lu", "CC Tchoyoson Lim", "Boon Chuan Pang", "Cheng Kiang Lee", "Tze-Yun Leong", "Chew Lim Tan"], "publication_date": "2014/8/24", "conference": "2014 22nd International Conference on Pattern Recognition", "pages": "3245-3250", "publisher": "IEEE", "description": "Clinical features found in brain CT scan images are widely used in traumatic brain injury (TBI) as indicators for Glasgow Outcome Scale (GOS) prediction. However, due to the lack of automated methods to measure and quantify the CT scan image features, the computerized prediction of GOS in TBI has not been well studied. This paper introduces an automated GOS prediction system for traumatic brain CT images. Different from most existing systems that perform the prognosis based on pre-processed data, our system directly works on brain CT scan images based on the image features. Our system can also be extended to large dataset with easy adaptation. For each new image of a CT scan series, our proposed system first makes use of sparse representation model that predicts the GOS of each CT image slice using Gabor features. Logistic regression, which integrates the GOS of each CT scan slice with a pre\u00a0\u2026", "total_citations": {"2015": 1, "2016": 0, "2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:WA5NYHcadZ8C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6977217/", "authors": ["Bolan Su", "Shijian Lu", "Shangxuan Tian", "Joo Hwee Lim", "Chew Lim Tan"], "publication_date": "2014/8/24", "conference": "2014 22nd International Conference on Pattern Recognition", "pages": "2926-2931", "publisher": "IEEE", "description": "Recognition of characters in natural images is a challenging task due to the complex background, variations of text size and perspective distortion, etc. Traditional optical character recognition (OCR) engine cannot perform well on those unconstrained text images. A novel technique is proposed in this paper that makes use of convolutional cooccurrence histogram of oriented gradient (ConvCoHOG), which is more robust and discriminative than both the histogram of oriented gradient (HOG) and the co-occurrence histogram of oriented gradients (CoHOG). In the proposed technique, a more informative feature is constructed by exhaustively extracting features from every possible image patches within character images. Experiments on two public datasets including the ICDAr 2003 Robust Reading character dataset and the Street View Text (SVT) dataset, show that our proposed character recognition technique obtains\u00a0\u2026", "total_citations": {"2015": 5, "2016": 10, "2017": 5, "2018": 4, "2019": 7, "2020": 4, "2021": 1, "2022": 2, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:HE397vMXCloC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6977179/", "authors": ["Shangxuan Tian", "Shijian Lu", "Bolan Su", "Chew Lim Tan"], "publication_date": "2014/8/24", "conference": "2014 22nd International Conference on Pattern Recognition", "pages": "2703-2708", "publisher": "IEEE", "description": "The segmentation of scene text from the image background has shown great importance in scene text recognition. In this paper, we propose a multi-level MSER technology that identifies the best-quality text candidates from a set of stable regions that are extracted from different color channel images. In order to identify the best-quality text candidates, a segmentation score is defined which exploits four measures to evaluate the text probability of each stable region including: 1) Stroke width that measures the small stroke width variation of the text, 2) Boundary curvature that measures the smoothness of the stable region boundary, 3) Character confidence that measures the likelihood of a stable region being text based on a pre-trained support vector classifier, 4) Color constancy that measures the global color consistency of each selected text candidate. Finally, the MSERs with the best segmentation score from each\u00a0\u2026", "total_citations": {"2014": 1, "2015": 3, "2016": 3, "2017": 7, "2018": 6, "2019": 3, "2020": 4, "2021": 2, "2022": 0, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:5ugPr518TE4C": {"external_link": "https://patents.google.com/patent/US20140003723A1/en", "inventors": "Shijian Lu, Joo Hwee Lim", "publication_date": "2014/1/2", "patent_office": "SG", "patent_number": "SG10201510667SA", "application_number": "SG10201510667SA", "description": "A text detection device is provided. The text detection device may include: an image input circuit configured to receive an image; an edge property determination circuit configured to determine a plurality of edge properties for each of a plurality of scales of the image; and a text location determination circuit configured to determine a text location in the image based on the plurality of edge properties for the plurality of scales of the image.", "total_citations": {"2016": 3, "2017": 0, "2018": 1, "2019": 3, "2020": 1, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LjlpjdlvIbIC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-319-16865-4_3", "authors": ["Bolan Su", "Shijian Lu"], "publication_date": "2014", "book": "Computer Vision--ACCV 2014", "pages": "35-48", "publisher": "Springer International Publishing", "description": "Scene text recognition is a useful but very challenging task due to uncontrolled condition of text in natural scenes. This paper presents a novel approach to recognize text in scene images. In the proposed technique, a word image is first converted into a sequential column vectors based on Histogram of Oriented Gradient (HOG). The Recurrent Neural Network (RNN) is then adapted to classify the sequential feature vectors into the corresponding word. Compared with most of the existing methods that follow a bottom-up approach to form words by grouping the recognized characters, our proposed method is able to recognize the whole word images without character-level segmentation and recognition. Experiments on a number of publicly available datasets show that the proposed method outperforms the state-of-the-art techniques significantly. In addition, the recognition results on publicly available\u00a0\u2026", "total_citations": {"2015": 9, "2016": 20, "2017": 22, "2018": 29, "2019": 44, "2020": 35, "2021": 43, "2022": 35, "2023": 16}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:B3FOqHPlNUQC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6628603/", "authors": ["Bolan Su", "Shuangxuan Tian", "Shijian Lu", "Thien Anh Dinh", "Chew Lim Tan"], "publication_date": "2013/8/25", "conference": "2013 12th International Conference on Document Analysis and Recognition", "pages": "155-159", "publisher": "IEEE", "description": "Document Image Binarization is a technique to segment text out from the background region of a document image, which is a challenging task due to high intensity variations of the document foreground and background. Recently, a series of document image binarization contests (DIBCOs) had been held that have drawn great research interest in this area. Several document binarization techniques have been proposed and achieve great performance on the contest datasets. However, those proposed techniques may not perform well on all kinds of degraded document images because it is difficult to design a classification method that correctly models the non-uniform degraded document background and text foreground simultaneously. In this paper, we propose a self learning classification framework that combines binary outputs of different binarization methods. The proposed framework makes used of the sparse\u00a0\u2026", "total_citations": {"2014": 1, "2015": 3, "2016": 1, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:4fKUyHm3Qg0C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6628751/", "authors": ["Shangxuan Tian", "Shijian Lu", "Bolan Su", "Chew Lim Tan"], "publication_date": "2013/8/25", "conference": "2013 12th International Conference on Document Analysis and Recognition", "pages": "912-916", "publisher": "IEEE", "description": "Scene text recognition is a fundamental step in End-to-End applications where traditional optical character recognition (OCR) systems often fail to produce satisfactory results. This paper proposes a technique that uses co-occurrence histogram of oriented gradients (Co-HOG) to recognize the text in scenes. Compared with histogram of oriented gradients (HOG), Co-HOG is a more powerful tool that captures spatial distribution of neighboring orientation pairs instead of just a single gradient orientation. At the same time, it is more efficient compared with HOG and therefore more suitable for real-time applications. The proposed scene text recognition technique is evaluated on ICDAR2003 character dataset and Street View Text (SVT) dataset. Experiments show that the Co-HOG based technique clearly outperforms state-of-the-art techniques that use HOG, Scale Invariant Feature Transform (SIFT), and Maximally Stable\u00a0\u2026", "total_citations": {"2013": 1, "2014": 7, "2015": 12, "2016": 11, "2017": 10, "2018": 8, "2019": 3, "2020": 8, "2021": 7, "2022": 4, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:eJXPG6dFmWUC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6582415/", "authors": ["Shijian Lu", "Cheston Tan", "Joo-Hwee Lim"], "publication_date": "2013/8/16", "journal": "IEEE transactions on pattern analysis and machine intelligence", "volume": "36", "issue": "1", "pages": "195-201", "publisher": "IEEE", "description": "This paper presents a visual saliency modeling technique that is efficient and tolerant to the image scale variation. Different from existing approaches that rely on a large number of filters or complicated learning processes, the proposed technique computes saliency from image histograms. Several two-dimensional image co-occurrence histograms are used, which encode not only \"how many\" (occurrence) but also \"where and how\" (co-occurrence) image pixels are composed into a visual image, hence capturing the \"unusualness\" of an object or image region that is often perceived by either global \"uncommonness\" (i.e., low occurrence frequency) or local \"discontinuity\" with respect to the surrounding (i.e., low co-occurrence frequency). The proposed technique has a number of advantageous characteristics. It is fast and very easy to implement. At the same time, it involves minimal parameter tuning, requires no\u00a0\u2026", "total_citations": {"2014": 6, "2015": 11, "2016": 9, "2017": 8, "2018": 9, "2019": 9, "2020": 7, "2021": 6, "2022": 4, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:l7t_Zn2s7bgC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6490032/", "authors": ["Palaiahnakote Shivakumara", "Trung Quy Phan", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2013/3/28", "journal": "IEEE transactions on circuits and systems for video technology", "volume": "23", "issue": "10", "pages": "1729-1739", "publisher": "IEEE", "description": "Text detection in videos is challenging due to low resolution and complex background of videos. Besides, an arbitrary orientation of scene text lines in video makes the problem more complex and challenging. This paper presents a new method that extracts text lines of any orientations based on gradient vector flow (GVF) and neighbor component grouping. The GVF of edge pixels in the Sobel edge map of the input frame is explored to identify the dominant edge pixels which represent text components. The method extracts edge components corresponding to dominant pixels in the Sobel edge map, which we call text candidates (TC) of the text lines. We propose two grouping schemes. The first finds nearest neighbors based on geometrical properties of TC to group broken segments and neighboring characters which results in word patches. The end and junction points of skeleton of the word patches are\u00a0\u2026", "total_citations": {"2014": 11, "2015": 17, "2016": 6, "2017": 13, "2018": 5, "2019": 7, "2020": 1, "2021": 11, "2022": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:fPk4N6BV_jEC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6373726/", "authors": ["Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2012/12/3", "journal": "IEEE transactions on image processing", "volume": "22", "issue": "4", "pages": "1408-1417", "publisher": "IEEE", "description": "Segmentation of text from badly degraded document images is a very challenging task due to the high inter/intra-variation between the document background and the foreground text of different document images. In this paper, we propose a novel document image binarization technique that addresses these issues by using adaptive image contrast. The adaptive image contrast is a combination of the local image contrast and the local image gradient that is tolerant to text and background variation caused by different types of document degradations. In the proposed technique, an adaptive contrast map is first constructed for an input degraded document image. The contrast map is then binarized and combined with Canny's edge map to identify the text stroke edge pixels. The document text is further segmented by a local threshold that is estimated based on the intensities of detected text stroke edge pixels within a\u00a0\u2026", "total_citations": {"2011": 1, "2012": 2, "2013": 9, "2014": 34, "2015": 62, "2016": 52, "2017": 37, "2018": 43, "2019": 42, "2020": 23, "2021": 29, "2022": 19, "2023": 19}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ldfaerwXgEUC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6460806/", "authors": ["Bolan Su", "Shijian Lu", "Trung Quy Phan", "Chew Lim Tan"], "publication_date": "2012/11/11", "conference": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)", "pages": "3042-3045", "publisher": "IEEE", "description": "Images with text are frequently used on Internet for different purposes. Automatic recognition of text from web images plays an important role on extraction and retrieval of web information. However, the web images are usually in low resolution with artifacts and special effects, which makes word recognition a challenge task even after the text has been localized. In this paper, we propose a robust text recognition technique to efficiently convert the web images into text format. The proposed technique first makes use of the L0 norm smoothing to increase the edge contrast of the input web images. The images are then binarized on each color channel. A connected component analysis is followed to identify the possible character components. Finally the character candidates are recognized by the OCR engine after skew correction. Extensive experiments have been conducted on the latest ICDAR 2011 robust reading\u00a0\u2026", "total_citations": {"2012": 1, "2013": 1, "2014": 2, "2015": 2, "2016": 1, "2017": 0, "2018": 0, "2019": 0, "2020": 2, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:70eg2SAEIzsC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6460845/", "authors": ["Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2012/11/11", "conference": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)", "pages": "3200-3203", "publisher": "IEEE", "description": "Document image binarization is an important preprocessing technique for document image analysis that segments the text from the document image backgrounds. Many techniques have been proposed and successfully applied in different applications, such as document image retrieval. However, these techniques may perform poorly on degraded document images. In this paper, we propose a learning framework that makes use of the Markov Random Field to improve the performance of the existing document image binarization methods for those degraded document images. Extensive experiments on the recent Document Image Bina-rization Contest datasets demonstrate that significant improvements of the existing binarization methods when applying our proposed framework.", "total_citations": {"2012": 1, "2013": 1, "2014": 5, "2015": 7, "2016": 6, "2017": 3, "2018": 4, "2019": 4, "2020": 2, "2021": 2, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:vV6vV6tmYwMC": {"external_link": "https://patents.google.com/patent/US20120155726A1/en", "inventors": "Huiqi Li, Joo Hwee Lim, Jiang Jimmy Liu, Wing Kee Damon Wong, Ngan Meng Tan, Zhuo Zhang, Shijian Lu, Tien Yin Wong", "publication_date": "2012/6/21", "patent_office": "WO", "patent_number": "WO2011025451A1", "application_number": "PCT/SG2009/000297", "description": "A method for determining a grade of nuclear cataract in a test image. The method includes:(1a) defining a contour of a lens structure in the test image, the defined contour of the lens structure comprising a segment around a boundary of a nucleus of the lens structure;(1b) extracting features from the test image based on the defined contour of the lens structure in the test image; and (1c) determining the grade of nuclear cataract in the test image based on the extracted features and a grading model.", "total_citations": {"2018": 2, "2019": 2, "2020": 2, "2021": 0, "2022": 3, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:lSLTfruPkqcC": {"external_link": "https://patents.google.com/patent/US20120157820A1/en", "inventors": "Zhou Zhang, Jiang Jimmy Liu, Wing Kee Damon Wong, Joo Hwee Jim, Ngan Meng Tan, Huiqi Li, Shijian Lu, Tien Yin Wong", "publication_date": "2012/6/21", "patent_office": "WO", "patent_number": "WO2011025452A1", "application_number": "PCT/SG2009/000298", "description": "A method for detecting disc haemorrhages in a retinal fundus image. The method includes (a) identifying a ring-shaped region of interest in the retinal fundus image encompassing the optic disc boundary;(b) removing blood vessel regions in the identified region of interest;(c) detecting candidate disc haemorrhages from the removed blood vessels regions in the identified region of interest; and (d) screening the candidate disc haemorrhages. The detected disc haemorrhages may be used to aid in the detection of glaucoma.", "total_citations": {"2014": 1, "2015": 0, "2016": 0, "2017": 1, "2018": 0, "2019": 1, "2020": 0, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:isC4tDSrTZIC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6196211/", "authors": ["Palaiahnakote Shivakumara", "Rushi Padhuman Sreedhar", "Trung Quy Phan", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2012/5/7", "journal": "IEEE Transactions on Circuits and systems for Video Technology", "volume": "22", "issue": "8", "pages": "1227-1235", "publisher": "IEEE", "description": "Multioriented text detection in video frames is not as easy as detection of captions or graphics or overlaid texts, which usually appears in the horizontal direction and has high contrast compared to its background. Multioriented text generally refers to scene text that makes text detection more challenging and interesting due to unfavorable characteristics of scene text. Therefore, conventional text detection methods may not give good results for multioriented scene text detection. Hence, in this paper, we present a new enhancement method that includes the product of Laplacian and Sobel operations to enhance text pixels in videos. To classify true text pixels, we propose a Bayesian classifier without assuming a priori probability about the input frame but estimating it based on three probable matrices. Three different ways of clustering are performed on the output of the enhancement method to obtain the three probable\u00a0\u2026", "total_citations": {"2012": 2, "2013": 8, "2014": 14, "2015": 12, "2016": 21, "2017": 15, "2018": 14, "2019": 9, "2020": 12, "2021": 9, "2022": 12, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:GnPB-g6toBAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6195355/", "authors": ["Bolan Su", "Shijian Lu", "Umapada Pal", "Chew Lim Tan"], "publication_date": "2012/3/27", "conference": "2012 10th IAPR International Workshop on Document Analysis Systems", "pages": "160-164", "publisher": "IEEE", "description": "Musical staff line detection and removal techniques detect the staff positions in musical documents and segment musical score from musical documents by removing those staff lines. It is an important preprocessing step for ensuing the Optical Music Recognition tasks. This paper proposes an effective staff line detection and removal method that makes use of the global information of the musical document and models the staff line shape. It first estimates the staff height and space, and then models the shape of the staff line by examining the orientation of the staff pixels. At last the estimated model is used to find out the location of staff lines and hence to remove those detected staff lines. The proposed technique is simple, robust, and involves few parameters. It has been tested on the dataset of the recent staff removal competition held under the International Conference of Document Analysis and Recognition(ICDAR\u00a0\u2026", "total_citations": {"2013": 3, "2014": 6, "2015": 6, "2016": 7, "2017": 7, "2018": 7, "2019": 5, "2020": 4, "2021": 1, "2022": 3, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:iH-uZ7U-co4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6195331/", "authors": ["Danni Zhao", "Palaiahnakote Shivakumara", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2012/3/27", "conference": "2012 10th IAPR International Workshop on Document Analysis Systems", "pages": "38-42", "publisher": "IEEE", "description": "In this paper, we present new features based on Spatial-Gradient-Features (SGF) at block level for identifying six video scripts namely, Arabic, Chinese, English, Japanese, Korean and Tamil. This works helps in enhancing the capability of the current OCR on video text recognition by choosing an appropriate OCR engine when video contains multi-script frames. The input for script identification is the text blocks obtained by our text frame classification method. For each text block, we obtain horizontal and vertical gradient information to enhance the contrast of the text pixels. We divide the horizontal gradient block into two equal parts as upper and lower at the centroid in the horizontal direction. Histogram on the horizontal gradient values of the upper and the lower part is performed to select dominant text pixels. In the same way, the method selects dominant pixels from the right and the left parts obtained by dividing\u00a0\u2026", "total_citations": {"2011": 1, "2012": 0, "2013": 3, "2014": 4, "2015": 11, "2016": 5, "2017": 4, "2018": 1, "2019": 7, "2020": 2, "2021": 3, "2022": 4, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:mB3voiENLucC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/2245276.2245424", "authors": ["Bolan Su", "Shijian Lu", "Tan Chew Lim"], "publication_date": "2012/3/26", "book": "Proceedings of the 27th Annual ACM Symposium on Applied Computing", "pages": "767-770", "description": "Motion blur often decreases the quality of document image and makes the text information within the document images unreachable by optical character recognition (OCR) or by a person. This paper presents a blur correction technique that aims to correct motion blur within document images. Given a blurred document image, an alpha channel map is first constructed based on specific image characteristics that are associated with text documents. Several blur parameters including blur direction and blur extent are then estimated from the constructed alpha channel map. Finally the blurred document image is restored by using Richardson-Lucy deconvolution technique based on the estimated blur parameters. Experiments on a number of document images with motion blur show that the proposed technique improves the document visual quality as well as the OCR performance significantly.", "total_citations": {"2012": 1, "2013": 0, "2014": 0, "2015": 1, "2016": 1, "2017": 2, "2018": 0, "2019": 0, "2020": 1, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:3s1wT3WcHBgC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-642-33786-4_24", "authors": ["Shijian Lu", "Joo-Hwee Lim"], "publication_date": "2012", "conference": "Computer Vision\u2013ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VII 12", "pages": "321-332", "publisher": "Springer Berlin Heidelberg", "description": "We proposed a computational visual saliency modeling technique. The proposed technique makes use of a color co-occurrence histogram (CCH) that captures not only \u201chow many\u201d but also \u201cwhere and how\u201d image pixels are composed into a visually perceivable image. Hence the CCH encodes image saliency information that is usually perceived as the discontinuity between an image region or object and its surrounding. The proposed technique has a number of distinctive characteristics: It is fast, discriminative, tolerant to image scale variation, and involves minimal parameter tuning. Experiments over benchmarking datasets show that it predicts fixational eye tracking points accurately and a superior AUC of 71.25 is obtained.", "total_citations": {"2013": 6, "2014": 4, "2015": 12, "2016": 7, "2017": 4, "2018": 1, "2019": 0, "2020": 4, "2021": 2, "2022": 3, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:TFP_iSt0sucC": {"external_link": "https://escholarship.org/content/qt17q754gx/qt17q754gx.pdf", "authors": ["Hsueh-Cheng Wang", "Shijian Lu", "Joo-Hwee Lim", "Marc Pomplun"], "publication_date": "2012", "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society", "volume": "34", "issue": "34", "description": "Previous studies have found that viewers\u2019 attention is disproportionately attracted by texts, and one possible reason is that viewers have developed a \u201ctext detector\u201d in their visual system to bias their attention toward text features. To verify this hypothesis, we add a text detector module to a visual attention model and test if the inclusion increases the model\u2019s ability to predict eye fixation positions, particularly in scenes without any text. A model including text detector, saliency, and center bias is found to predict viewers\u2019 eye fixations better than the same model without text detector, even in textabsent images. Furthermore, adding the text detector\u2013which was designed for English texts\u2013improves the prediction of both English-and Chinese-speaking viewers\u2019 attention but with a stronger effect for English-speaking viewers. These results support the conclusion that, due to the viewers\u2019 everyday reading training, their attention in natural scenes is biased toward text features.", "total_citations": {"2012": 1, "2013": 0, "2014": 0, "2015": 0, "2016": 1, "2017": 3, "2018": 0, "2019": 0, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:BqipwSGYUEgC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/2072298.2072024", "authors": ["Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2011/11/28", "book": "Proceedings of the 19th ACM international conference on Multimedia", "pages": "1397-1400", "description": "Many digital images contain blurred regions which are caused by motion or defocus. Automatic detection and classification of blurred image regions are very important for different multimedia analyzing tasks. This paper presents a simple and effective automatic image blurred region detection and classification technique. In the proposed technique, blurred image regions are first detected by examining singular value information for each image pixels. The blur types (i.e. motion blur or defocus blur) are then determined based on certain alpha channel constraint that requires neither image deblurring nor blur kernel estimation. Extensive experiments have been conducted over a dataset that consists of 200 blurred image regions and 200 image regions with no blur that are extracted from 100 digital images. Experimental results show that the proposed technique detects and classifies the two types of image blurs\u00a0\u2026", "total_citations": {"2012": 2, "2013": 11, "2014": 16, "2015": 22, "2016": 26, "2017": 23, "2018": 24, "2019": 29, "2020": 30, "2021": 17, "2022": 25, "2023": 14}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:BzfGm06jWhQC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6065269/", "authors": ["Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2011/9/18", "conference": "2011 International Conference on Document Analysis and Recognition", "pages": "22-26", "publisher": "IEEE", "description": "Document image binarization has been studied for decades, and many practical binarization techniques have been proposed for different kinds of document images. However, many state-of-the-art methods are particularly suitable for the document images that suffer from certain specific type of image degradation or have certain specific type of image characteristics. In this paper, we propose a classification framework to combine different thresholding methods and produce better performance for document image binarization. Given the binarization results of some reported methods, the proposed framework divides the document image pixels into three sets, namely, foreground pixels, background pixels and uncertain pixels. A classifier is then applied to iteratively classify those uncertain pixels into foreground and background, based on the pre-selected froeground and background sets. Extensive experiments over\u00a0\u2026", "total_citations": {"2011": 1, "2012": 8, "2013": 16, "2014": 18, "2015": 16, "2016": 12, "2017": 5, "2018": 4, "2019": 12, "2020": 6, "2021": 12, "2022": 3, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ZeXyd9-uunAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6065493/", "authors": ["Deepak Rajendran", "Palaiahnakote Shivakumara", "Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2011/9/18", "conference": "2011 International Conference on Document Analysis and Recognition", "pages": "1165-1169", "publisher": "IEEE", "description": "This paper presents a new method based on Fourier and moments features to extract words and characters from a video text line in any direction for recognition. Unlike existing methods which output the entire text line to the ensuing recognition algorithm, the proposed method obtains each extracted character from the text line as input to the recognition algorithm because the background of a single character is relatively simple compared to the text line and words. Max-Min clustering criterion is introduced to obtain text cluster from the extracted Fourier and moments feature set. Union of the text cluster with Canny operation of the input video text line is proposed to obtain missing text candidates. Then a run length criterion is used for extraction of words. From the words, we propose a new idea for extracting characters from the text candidates of each word image based on the fact that the text height difference at the\u00a0\u2026", "total_citations": {"2012": 2, "2013": 2, "2014": 4, "2015": 1, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 1, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Wp0gIr-vW9MC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6065290/", "authors": ["Palaiahnakote Shivakumara", "Trung Quy Phan", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2011/9/18", "conference": "2011 International Conference on Document Analysis and Recognition", "pages": "131-135", "publisher": "IEEE", "description": "We present a new video character recognition method based on hierarchical classification. In the first step, we propose a method for character segmentation of the text line detected by the text detection method. The segmentation algorithm uses dynamic programming to find least-cost paths in the gray domain to identify the spaces between characters. For the segmented characters, we get a Canny edge image as input for the character recognition step. We introduce hierarchical classification based on voting criteria with structural features to classify 62 character classes into different smaller classes. We divide the perimeter of a character into 8 segments according to 8 directions at the centroid. Then the shape of each segment is studied to recognize the characters based on distances between the centroid and end points, and distances between the midpoint and end points. Our experiments on 1462 characters of\u00a0\u2026", "total_citations": {"2012": 2, "2013": 0, "2014": 1, "2015": 1, "2016": 1, "2017": 1, "2018": 0, "2019": 0, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:qxL8FJ1GzNcC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/6065508/", "authors": ["Trung Quy Phan", "Palaiahnakote Shivakumara", "Zhang Ding", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2011/9/18", "conference": "2011 International Conference on Document Analysis and Recognition", "pages": "1240-1244", "publisher": "IEEE", "description": "In this paper, we present a new method for video script identification which is essential before choosing an appropriate OCR engine for identifying text lines when a video frame contains more than one language. The input for script identification is the text lines obtained by our text detection method. We extract upper and lower extreme points for each connected component of Canny edges of text lines. The extracted points are connected to study the behavior of upper and lower lines. The direction of each 10-pixel segment of the lines is determined using PCA. The average angle of the segments of the upper and lower lines is computed to study the smoothness and cursiveness of the lines. In addition, to discriminate the scripts accurately, the method divides a text line into five equal zones horizontally to study the smoothness and cursiveness of the upper and lower lines of each zone. We evaluate the method by\u00a0\u2026", "total_citations": {"2011": 1, "2012": 1, "2013": 3, "2014": 5, "2015": 10, "2016": 7, "2017": 6, "2018": 2, "2019": 6, "2020": 0, "2021": 3, "2022": 3, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:QIV2ME_5wuYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5981395/", "authors": ["Shijian Lu"], "publication_date": "2011/8/11", "journal": "IEEE Transactions on medical imaging", "volume": "30", "issue": "12", "pages": "2126-2133", "publisher": "IEEE", "description": "Under the framework of computer-aided diagnosis, this paper presents an accurate and efficient optic disc (OD) detection and segmentation technique. A circular transformation is designed to capture both the circular shape of the OD and the image variation across the OD boundary simultaneously. For each retinal image pixel, it evaluates the image variation along multiple evenly-oriented radial line segments of specific length. The pixels with the maximum variation along all radial line segments are determined, which can be further exploited to locate both the OD center and the OD boundary accurately. Experiments show that OD detection accuracies of 99.75%, 97.5%, and 98.77% are obtained for the STARE dataset, the ARIA dataset, and the MESSIDOR dataset, respectively, and the OD center error lies around six pixels for the STARE dataset and the ARIA dataset which is much smaller than that of state-of-the\u00a0\u2026", "total_citations": {"2012": 3, "2013": 9, "2014": 22, "2015": 25, "2016": 27, "2017": 14, "2018": 26, "2019": 27, "2020": 15, "2021": 18, "2022": 15, "2023": 12}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:zYLM7Y9cAGgC": {"external_link": "https://link.springer.com/article/10.1007/s10032-010-0130-8", "authors": ["Shijian Lu", "Bolan Su", "Chew Lim Tan"], "publication_date": "2010/12", "journal": "International Journal on Document Analysis and Recognition (IJDAR)", "volume": "13", "pages": "303-314", "publisher": "Springer-Verlag", "description": "Document images often suffer from different types of degradation that renders the document image binarization a challenging task. This paper presents a document image binarization technique that segments the text from badly degraded document images accurately. The proposed technique is based on the observations that the text documents usually have a document background of the uniform color and texture and the document text within it has a different intensity level compared with the surrounding document background. Given a document image, the proposed technique first estimates a document background surface through an iterative polynomial smoothing procedure. Different types of document degradation are then compensated by using the estimated document background surface. The text stroke edge is further detected from the compensated document image by using L1-norm image\u00a0\u2026", "total_citations": {"2009": 2, "2010": 1, "2011": 8, "2012": 19, "2013": 30, "2014": 47, "2015": 44, "2016": 24, "2017": 20, "2018": 21, "2019": 27, "2020": 18, "2021": 15, "2022": 14, "2023": 14}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:4JMBOYKVnBMC": {"external_link": "https://link.springer.com/article/10.1007/s10044-009-0169-7", "authors": ["Shijian Lu", "Linlin Li", "Chew Lim Tan"], "publication_date": "2010/11", "journal": "Pattern Analysis and Applications", "volume": "13", "pages": "469-475", "publisher": "Springer-Verlag", "description": "Document scripts and document orientations are important information for the document digitalization. Prior work has been reported to identify document scripts and document orientations, whereas most reported methods are very sensitive to document skew and low image resolution. This paper reports a document script and document orientation identification method that addresses this issue by converting a document image into a pair of document vectors using the density and distribution of character strokes. Experiments over 3,024 document images of 12 scripts show that the proposed methods are accurate and tolerant to various types of document degradation.", "total_citations": {"2014": 1, "2015": 2, "2016": 0, "2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:YFjsv_pBGBYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5639660/", "authors": ["Shijian Lu"], "publication_date": "2010/10/16", "conference": "2010 3rd International Conference on Biomedical Engineering and Informatics", "volume": "1", "pages": "141-145", "publisher": "IEEE", "description": "This paper presents an automatic optic disc detection technique that locates the optic disc through retinal background surface estimation and retinal blood vessel analysis. In the proposed technique, a retinal background surface is first estimated through an iterative Savitzky-Golay smoothing procedure. Multiple optic disc candidates are then detected from the difference between the retinal image under study and the estimated retinal background surface. Finally, the real optic disc is identified through the incorporation of the directional retinal blood vessel information. Experiments over four public datasets show that the proposed technique is accurate (acc. at 97.94%) and tolerant to retinal lesion and imaging artifacts.", "total_citations": {"2013": 2, "2014": 0, "2015": 1, "2016": 2, "2017": 1, "2018": 1, "2019": 0, "2020": 0, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:8k81kl-MbHgC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5599852/", "authors": ["Shijian Lu", "Joo Hwee Lim"], "publication_date": "2010/10/14", "journal": "IEEE Transactions on Biomedical Engineering", "volume": "58", "issue": "1", "pages": "88-94", "publisher": "IEEE", "description": "Under the framework of computer-aided eye disease diagnosis, this paper presents an automatic optic disc (OD) detection technique. The proposed technique makes use of the unique circular brightness structure associated with the OD, i.e., the OD usually has a circular shape and is brighter than the surrounding pixels whose intensity becomes darker gradually with their distances from the OD center. A line operator is designed to capture such circular brightness structure, which evaluates the image brightness variation along multiple line segments of specific orientations that pass through each retinal image pixel. The orientation of the line segment with the minimum/maximum variation has specific pattern that can be used to locate the OD accurately. The proposed technique has been tested over four public datasets that include 130, 89, 40, and 81 images of healthy and pathological retinas, respectively\u00a0\u2026", "total_citations": {"2010": 1, "2011": 3, "2012": 14, "2013": 10, "2014": 18, "2015": 14, "2016": 17, "2017": 14, "2018": 19, "2019": 10, "2020": 10, "2021": 11, "2022": 9, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:blknAaTinKkC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5653473/", "authors": ["Shijian Lu", "Joo Hwee Lim"], "publication_date": "2010/9/26", "conference": "2010 IEEE International Conference on Image Processing", "pages": "833-836", "publisher": "IEEE", "description": "This paper presents an automatic optic disc (OD) detection technique. Given a retinal image, the proposed method first estimates a retinal background surface through an iterative Savitzky-Golay smoothing procedure. The OD is then detected through the global thresholding of the difference between the retinal image and the estimated background surface. Finally, an OD boundary is determined after a pair of morphological post-processing operations. The proposed technique has been tested over three public datasets that are composed of 130, 89, and 40 retinal images, respectively. Experiments show that an average OD detection accuracy of 96.91% is attained. In addition, 84.37% OD pixels are correctly located compared with the manually labeled ones.", "total_citations": {"2012": 1, "2013": 1, "2014": 3, "2015": 6, "2016": 2, "2017": 2, "2018": 2, "2019": 4, "2020": 2, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:j3f4tGmQtD8C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5649080/", "authors": ["Shijian Lu", "Joo Hwee Lim"], "publication_date": "2010/9/26", "conference": "2010 IEEE International Conference on Image Processing", "pages": "4073-4076", "publisher": "IEEE", "description": "This paper presents an automatic macula detection technique that makes use of the circular brightness profile of the macula: the macula is usually darker than the surrounding pixels whose intensities increase gradually with their distances from the macula center. A line operator is designed to capture the macula circular brightness profile, which evaluates the image brightness variation along multiple line segments of specific orientations that pass through each retinal image pixel. The orientation of the line segment with the minimum/ maximum variation has specific patterns that indicate the position of the macula efficiently. The proposed technique has been tested over DRIVE project's dataset and the STARE project's dataset. Experiments show that the accuracies reach up to 100% and 95.45%, respectively, based on 35 and 44 retinal images having discernible macula within the two public datasets.", "total_citations": {"2012": 2, "2013": 1, "2014": 5, "2015": 1, "2016": 2, "2017": 3, "2018": 1, "2019": 4, "2020": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:3fE2CSJIrl8C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5597185/", "authors": ["Su Bolan", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2010/8/23", "conference": "Pattern Recognition (ICPR), 2010 20th International Conference on", "pages": "3187-3190", "publisher": "IEEE", "description": "Document Image Binarization techniques have been studied for many years, and many practical binarization techniques have been developed and applied successfully on commercial document analysis systems. However, the current state-of-the-art methods, fail to produce good binarization results for many badly degraded document images. In this paper, we propose a self-training learning framework for document image binarization. Based on reported binarization methods, the proposed framework first divides document image pixels into three categories, namely, foreground pixels, background pixels and uncertain pixels. A classifier is then trained by learning from the document image pixels in the foreground and background categories. Finally, the uncertain pixels are classified using the learned pixel classifier. Extensive experiments have been conducted over the dataset that is used in the recent Document\u00a0\u2026", "total_citations": {"2011": 3, "2012": 7, "2013": 4, "2014": 4, "2015": 7, "2016": 6, "2017": 2, "2018": 3, "2019": 3, "2020": 3, "2021": 0, "2022": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:roLk4NBRz8UC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5497101/", "authors": ["Shijian Lu", "Carol Yim-lui Cheung", "Jiang Liu", "Joo Hwee Lim", "Christopher Kai-shun Leung", "Tien Yin Wong"], "publication_date": "2010/6/28", "journal": "IEEE Transactions on biomedical engineering", "volume": "57", "issue": "10", "pages": "2605-2608", "publisher": "IEEE", "description": "Under the framework of computer-aided diagnosis, optical coherence tomography (OCT) has become an established ocular imaging technique that can be used in glaucoma diagnosis by measuring the retinal nerve fiber layer thickness. This letter presents an automated retinal layer segmentation technique for OCT images. In the proposed technique, an OCT image is first cut into multiple vessel and nonvessel sections by the retinal blood vessels that are detected through an iterative polynomial smoothing procedure. The nonvessel sections are then filtered by a bilateral filter and a median filter that suppress the local image noise but keep the global image variation across the retinal layer boundary. Finally, the layer boundaries of the filtered nonvessel sections are detected, which are further classified to different retinal layers to determine the complete retinal layer boundaries. Experiments over OCT for four\u00a0\u2026", "total_citations": {"2010": 2, "2011": 4, "2012": 5, "2013": 8, "2014": 9, "2015": 9, "2016": 11, "2017": 11, "2018": 12, "2019": 16, "2020": 8, "2021": 10, "2022": 6, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:evX43VCCuoAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5515225/", "authors": ["Damon WK Wong", "Jiang Liu", "Joo Hwee Lim", "Ngan Meng Tan", "Zhuo Zhang", "Huiqi Li", "Shijian Lu", "Tien Yin Wong"], "publication_date": "2010/6/15", "conference": "2010 5th IEEE Conference on Industrial Electronics and Applications", "pages": "1690-1694", "publisher": "IEEE", "description": "Glaucoma is the second leading cause of blindness worldwide. The risk of glaucoma can be determined by calculating the cup to disc ratio in retinal fundus images. To accurately detect the optic cup, kinks or bends in small and medium vessels are important indicators of the cup boundary. In this paper, we present a method of detecting such vessels, through the extraction of patches and generation of hybrid features in a SVM-based model. The segmentation results show good potential for the further development of this method.", "total_citations": {"2014": 1, "2015": 3, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:9yKSN-GCB0IC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/1815330.1815351", "authors": ["Bolan Su", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2010/6/9", "book": "Proceedings of the 9th IAPR International Workshop on Document Analysis Systems", "pages": "159-166", "description": "This paper presents a new document image binarization technique that segments the text from badly degraded historical document images. The proposed technique makes use of the image contrast that is defined by the local image maximum and minimum. Compared with the image gradient, the image contrast evaluated by the local maximum and minimum has a nice property that it is more tolerant to the uneven illumination and other types of document degradation such as smear. Given a historical document image, the proposed technique first constructs a contrast image and then detects the high contrast image pixels which usually lie around the text stroke boundary. The document text is then segmented by using local thresholds that are estimated from the detected high contrast pixels within a local neighborhood window. The proposed technique has been tested over the dataset that is used in the recent\u00a0\u2026", "total_citations": {"2010": 3, "2011": 20, "2012": 21, "2013": 28, "2014": 60, "2015": 49, "2016": 36, "2017": 16, "2018": 23, "2019": 24, "2020": 19, "2021": 21, "2022": 16, "2023": 9}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:_Qo2XoVZTnwC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7623/76234J/Automatic-optic-disc-segmentation-based-on-image-brightness-and-contrast/10.1117/12.844654.short", "authors": ["Shijian Lu", "Jiang Liu", "Joo Hwee Lim", "Zhuo Zhang", "Ngan Meng Tan", "Wing Kee Wong", "Huiqi Li", "Tien Yin Wong"], "publication_date": "2010/3/12", "conference": "Medical Imaging 2010: Image Processing", "volume": "7623", "pages": "1404-1411", "publisher": "SPIE", "description": "Untreated glaucoma leads to permanent damage of the optic nerve and resultant visual field loss, which can progress to blindness. As glaucoma often produces additional pathological cupping of the optic disc (OD), cupdisc- ratio is one measure that is widely used for glaucoma diagnosis. This paper presents an OD localization method that automatically segments the OD and so can be applied for the cup-disc-ratio based glaucoma diagnosis. The proposed OD segmentation method is based on the observations that the OD is normally much brighter and at the same time have a smoother texture characteristics compared with other regions within retinal images. Given a retinal image we first capture the ODs smooth texture characteristic by a contrast image that is constructed based on the local maximum and minimum pixel lightness within a small neighborhood window. The centre of the OD can then be determined\u00a0\u2026", "total_citations": {"2014": 1, "2015": 1, "2016": 0, "2017": 1, "2018": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:k_IJM867U9cC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7624/762439/Enhancement-of-optic-cup-detection-through-an-improved-vessel-kink/10.1117/12.844126.short", "authors": ["Damon WK Wong", "Jiang Liu", "Ngan Meng Tan", "Zhuo Zhang", "Shijian Lu", "Joo Hwee Lim", "Huiqi Li", "Tien Yin Wong"], "publication_date": "2010/3/9", "conference": "Medical Imaging 2010: Computer-Aided Diagnosis", "volume": "7624", "pages": "921-928", "publisher": "SPIE", "description": "Glaucoma is a leading cause of blindness. The presence and extent of progression of glaucoma can be determined if the optic cup can be accurately segmented from retinal images. In this paper, we present a framework which improves the detection of the optic cup. First, a region of interest is obtained from the retinal fundus image, and a pallor-based preliminary cup contour estimate is determined. Patches are then extracted from the ROI along this contour. To improve the usability of the patches, adaptive methods are introduced to ensure the patches are within the optic disc and to minimize redundant information. The patches are then analyzed for vessels by an edge transform which generates pixel segments of likely vessel candidates. Wavelet, color and gradient information are used as input features for a SVM model to classify the candidates as vessel or non-vessel. Subsequently, a rigourous non-parametric\u00a0\u2026", "total_citations": {"2012": 1, "2013": 0, "2014": 0, "2015": 2, "2016": 0, "2017": 0, "2018": 1, "2019": 0, "2020": 0, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hFOr9nPyWt4C": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7624/76240G/Automatic-classification-of-pathological-myopia-in-retinal-fundus-images-using/10.1117/12.844122.short", "authors": ["Jiang Liu", "Damon WK Wong", "Ngan Meng Tan", "Zhuo Zhang", "Shijian Lu", "Joo Hwee Lim", "Huiqi Li", "Seang Mei Saw", "Louis Tong", "Tien Yin Wong"], "publication_date": "2010/3/9", "conference": "Medical Imaging 2010: Computer-Aided Diagnosis", "volume": "7624", "pages": "126-133", "publisher": "SPIE", "description": "Pathological myopia is the seventh leading cause of blindness. We introduce a framework based on PAMELA (PAthological Myopia dEtection through peripapilLary Atrophy) for the detection of pathological myopia from fundus images. The framework consists of a pre-processing stage which extracts a region of interest centered on the optic disc. Subsequently, three analysis modules focus on detecting specific visual indicators. The optic disc tilt ratio module gives a measure of the axial elongation of the eye through inference from the deformation of the optic disc. In the texturebased ROI assessment module, contextual knowledge is used to demarcate the ROI into four distinct, clinically-relevant zones in which information from an entropy transform of the ROI is analyzed and metrics generated. In particular, the preferential appearance of peripapillary atrophy (PPA) in the temporal zone compared to the nasal zone is\u00a0\u2026", "total_citations": {"2012": 2, "2013": 1, "2014": 1, "2015": 1, "2016": 0, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 0, "2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:9ZlFYXVOiuMC": {"external_link": "https://www.spiedigitallibrary.org/conference-proceedings-of-spie/7624/762438/Classification-of-left-and-right-eye-retinal-images/10.1117/12.844638.short", "authors": ["Ngan Meng Tan", "Jiang Liu", "Damon WK Wong", "Zhuo Zhang", "Shijian Lu", "Joo Hwee Lim", "Huiqi Li", "Tien Yin Wong"], "publication_date": "2010/3/9", "conference": "Medical Imaging 2010: Computer-Aided Diagnosis", "volume": "7624", "pages": "913-920", "publisher": "SPIE", "description": "Retinal image analysis is used by clinicians to diagnose and identify, if any, pathologies present in a patient's eye. The developments and applications of computer-aided diagnosis (CAD) systems in medical imaging have been rapidly increasing over the years. In this paper, we propose a system to classify left and right eye retinal images automatically. This paper describes our two-pronged approach to classify left and right retinal images by using the position of the central retinal vessel within the optic disc, and by the location of the macula with respect to the optic nerve head. We present a framework to automatically identify the locations of the key anatomical structures of the eye- macula, optic disc, central retinal vessels within the optic disc and the ISNT regions. A SVM model for left and right eye retinal image classification is trained based on the features from the detection and segmentation. An advantage of this\u00a0\u2026", "total_citations": {"2010": 1, "2011": 0, "2012": 8, "2013": 1, "2014": 0, "2015": 0, "2016": 0, "2017": 1, "2018": 1, "2019": 1, "2020": 2, "2021": 2, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:dTyEYWd-f8wC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5384075/", "authors": ["NM Tan", "DWK Wong", "J Liu", "WJ Ng", "Z Zhang", "JH Lim", "Z Tan", "Y Tang", "H Li", "S Lu", "TY Wong"], "publication_date": "2009/12/2", "conference": "2009 International Conference on Biomedical and Pharmaceutical Engineering", "pages": "1-5", "publisher": "IEEE", "description": "This paper proposes a method to detect the macula in the retinal fundus image automatically. The method makes use of the optic disc height obtained from the ARGALI to define the region of interest. Regions of dark spots are then detected by finding the coordinates with the lowest pixel intensity and determining the average pixel neighbourhood intensities. These regions are ranked to determine the region containing the macula. This algorithm was tested on 162 images, and an accuracy of 98.8% was achieved. The results are promising for further development and use of this method in AMD studies and physiology localization.", "total_citations": {"2010": 1, "2011": 1, "2012": 2, "2013": 3, "2014": 5, "2015": 1, "2016": 5, "2017": 2, "2018": 3, "2019": 1, "2020": 3, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:r0BpntZqJG4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5384072/", "authors": ["DWK Wong", "NM Tan", "J Liu", "Z Tan", "Y Tang", "Z Zhang", "JH Lim", "WJ Ng", "H Li", "S Lu", "TY Wong"], "publication_date": "2009/12/2", "conference": "2009 International Conference on Biomedical and Pharmaceutical Engineering", "pages": "1-5", "publisher": "IEEE", "description": "We propose a method for improving the accuracy of the optic cup detected from the ARGALI system. This method makes use of key points from the branching points of large vessels, the analysis of intensity variation and kinks from small vessels to obtain an enhanced optic cup. Measures used to assess the detection of the optic cup showed an 11% and 40% improvement in the mean average overlap and relative area difference respectively over the previous method. The difference in the CDR error was also shown to be reduced to less than 0.1CDR units. The improved optic cup detection is more consistent with the clinical ground truth, facilitating its use in ARGALI for mass screening for glaucoma for early detection to save sight.", "total_citations": {"2013": 1, "2014": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:cFHS6HbyZ2cC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5413965/", "authors": ["Jiang Liu", "Shijian Lu", "Joo-Hwee Lim", "Zhuo Zhang", "TN Meng", "Damon Wing Kee Wong", "Huiqi Li", "Tien Yin Wong"], "publication_date": "2009/11/7", "conference": "2009 16th IEEE International Conference on Image Processing (ICIP)", "pages": "3893-3896", "publisher": "IEEE", "description": "This paper presents a photometric restoration technique that automatically corrects shading within retinal images taken with a fundus camera. The proposed technique is based on the observation that the background of retinal images usually shows flat reflectance variations due to its high similarity in color and texture. It estimates shading through an iterative polynomial interpolation procedure that first estimates a shading image through a horizontal interpolation process and then improves the shading estimation by a vertical interpolation process. Once the shading image is estimated, a reflectance image can accordingly be determined based on the luminance of the retina image under study. Experiments on 161 retinal images of different qualities show promising results.", "total_citations": {"2010": 2, "2011": 1, "2012": 0, "2013": 0, "2014": 0, "2015": 1, "2016": 0, "2017": 0, "2018": 2, "2019": 0, "2020": 1, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hMod-77fHWUC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5305076/", "authors": ["Zhuo Zhang", "Jiang Liu", "Wing Kee Wong", "Ngan Meng Tan", "Joo Hwee Lim", "Shijian Lu", "Huiqi Li", "Ziyang Liang", "Tien Ying Wong"], "publication_date": "2009/10/17", "conference": "2009 2nd International Conference on Biomedical Engineering and Informatics", "pages": "1-4", "publisher": "IEEE", "description": "Glaucoma is the one of the two major causes of blindness, which can be diagnosed through measurement of neuro-retinal optic cup-to-disc ratio (CDR). Automatic calculation of optic cup boundary is challenging due to the interweavement of blood vessels with the surrounding tissues around the cup. A multimodality fusion approach for neuroretinal cup detection improves the accuracy of the boundary estimation. The algorithm's effectiveness is demonstrated on 71 manually segmented retina fundus images collected from Singapore Eye Research Institute. By comparing our automatic cup height measurement to ground truth, we found that our method accurately detected neuro-retinal cup height for 69 images, achieved 97.2% accuracy. The evaluation was based on a criterion that is more stringent than the clinically acceptable interor intra-observer variability. This further leads to a large clinical evaluation of the\u00a0\u2026", "total_citations": {"2010": 1, "2011": 0, "2012": 4, "2013": 1, "2014": 2, "2015": 3, "2016": 3, "2017": 1, "2018": 2, "2019": 1, "2020": 3, "2021": 3, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:RHpTSmoSYBkC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5304895/", "authors": ["Huiqi Li", "Joo Hwee Lim", "Jiang Liu", "Damon Wing Kee Wong", "Ngan Meng Tan", "Shijian Lu", "Zhuo Zhang", "Tien Yin Wong"], "publication_date": "2009/10/17", "conference": "2009 2nd International Conference on Biomedical Engineering and Informatics", "pages": "1-4", "publisher": "IEEE", "description": "Cataract is the leading cause of blindness worldwide. Two automatic grading systems are presented in this paper for nuclear cataract and cortical cataract diagnosis respectively. Model-based approach was applied to detect anatomical structure in slit-lamp images. Features were extracted based on the lens structure and severity of nuclear cataract was predicted using support vector machines (SVM) regression. For cortical cataract, the opacity was detected using region growing. The seeds were selected by local thresholding and edge detection in radial direction. Cortical cataract was graded based on the area of cortical opacity. Both of the systems were tested by clinical data and results show that the automatic systems can provide objective grading of cataracts.", "total_citations": {"2011": 1, "2012": 0, "2013": 1, "2014": 0, "2015": 2, "2016": 3, "2017": 2, "2018": 0, "2019": 0, "2020": 1, "2021": 3, "2022": 1, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:L8Ckcad2t8MC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5332913/", "authors": ["Zhuo Zhang", "Jiang Liu", "Neetu Sara Cherian", "Ying Sun", "Joo Hwee Lim", "Wing Kee Wong", "Ngan Meng Tan", "Shijian Lu", "Huiqi Li", "Tien Ying Wong"], "publication_date": "2009/9/3", "conference": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "pages": "1441-1444", "publisher": "IEEE", "description": "Glaucoma is the second leading cause of blindness. Glaucoma can be diagnosed through measurement of neuro-retinal optic cup-to-disc ratio (CDR). Automatic calculation of optic cup boundary is challenging due to the interweavement of blood vessels with the surrounding tissues around the cup. A Convex Hull based Neuro-Retinal Optic Cup Ellipse Optimization algorithm improves the accuracy of the boundary estimation. The algorithm's effectiveness is demonstrated on 70 clinical patient's data set collected from Singapore Eye Research Institute. The root mean squared error of the new algorithm is 43% better than the ARGALI system which is the state-of-the-art. This further leads to a large clinical evaluation of the algorithm involving 15 thousand patients from Australia and Singapore.", "total_citations": {"2011": 1, "2012": 2, "2013": 2, "2014": 7, "2015": 22, "2016": 0, "2017": 2, "2018": 4, "2019": 5, "2020": 2, "2021": 7, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:7PzlFSSx8tAC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5333517/", "authors": ["Ngan Meng Tan", "Jiang Liu", "Damon Wing Kee Wong", "Joo Hwee Lim", "Zhuo Zhang", "S Lu", "H Li", "Seang Mei Saw", "Louis Tong", "Tien Yin Wong"], "publication_date": "2009/9/3", "conference": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "pages": "3609-3612", "publisher": "IEEE", "description": "Pathological myopia, the seventh leading cause of legal blindness in United States, is a condition caused by pathological axial elongation and eyes that deviates from the normal distribution curve of axial length, resulting in impaired vision. Studies have shown that ocular risks associated with myopia should not be underestimated, and there is a public health need to prevent the onset or progression of myopia. Peripapillary atrophy (PPA) is one of the clinical indicators for pathological myopia. In this paper, we introduce a novel method, to detect pathological myopia via peripapaillary atrophy feature by means of variational level set. This method is a core algorithm of our system, PAMELA, an automated system for the detection of pathological myopia. The proposed method has been tested on 40 images from Singapore Cohort study Of the Risk factors for Myopia (SCORM), producing a 95% accuracy of correct\u00a0\u2026", "total_citations": {"2011": 1, "2012": 7, "2013": 2, "2014": 2, "2015": 1, "2016": 2, "2017": 1, "2018": 3, "2019": 0, "2020": 2, "2021": 4, "2022": 4, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:4DMP91E08xMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5332917/", "authors": ["Shijian Lu", "Jiang Liu", "Joo Hwee Lim", "Zhuo Zhang", "Tan Ngan Meng", "Wing Kee Wong", "Huiqi Li", "Tian Yin Wong"], "publication_date": "2009/9/3", "conference": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "pages": "1453-1456", "publisher": "IEEE", "description": "With the advances of computer technology, more and more computer-aided diagnosis (CAD) systems have been developed to provide the ldquosecond opinionrdquo. This paper reports an automatic fundus image classification technique that is designed to screen out the severely degraded fundus images that cannot be processed by traditional CAD systems. The proposed technique classifies fundus images based on the image range property. In particular, it first calculates a number of range images from a fundus image at different resolutions. A feature vector is then constructed based on the histogram of the calculated range images. Finally, fundus images can be classified by a linear discriminant classifier that is built by learning from a large number of normal and abnormal training fundus images. Experiments over 644 fundus images of different qualities show that the classification accuracy of the proposed\u00a0\u2026", "total_citations": {"2010": 1, "2011": 0, "2012": 1, "2013": 0, "2014": 2, "2015": 2, "2016": 1, "2017": 0, "2018": 1, "2019": 1, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:aqlVkmm33-oC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5334735/", "authors": ["Huiqi Li", "Joo Hwee Lim", "Jiang Liu", "Damon Wing Kee Wong", "Ngan Meng Tan", "Shijian Lu", "Zhuo Zhang", "Tien Yin Wong"], "publication_date": "2009/9/3", "conference": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "pages": "3693-3696", "publisher": "IEEE", "description": "An automatic diagnosis system of nuclear cataract is presented in this paper. Nuclear cataract is graded according to the severity of opacity using slit-lamp lens images. Anatomical structure in the lens image is detected using a modified active shape model (ASM). Based on the anatomical landmark, local features are extracted according to clinical grading protocol. Support vector machine (SVM) regression is employed to train a grading model for grade prediction. The system is tested using clinical images and clinical ground truth. More than five thousands slit-lamp images were tested. The success rate of feature extraction is 95% and the mean grading difference is 0.36. The automatic diagnosis system can help to improve the grading objectivity and save the workload of ophthalmologists.", "total_citations": {"2011": 1, "2012": 0, "2013": 0, "2014": 1, "2015": 0, "2016": 2, "2017": 1, "2018": 2, "2019": 4, "2020": 2, "2021": 8, "2022": 7, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:MXK_kJrjxJIC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/5332534/", "authors": ["DWK Wong", "J Liu", "JH Lim", "NM Tan", "Z Zhang", "S Lu", "H Li", "MH Teo", "KL Chan", "TY Wong"], "publication_date": "2009/9/3", "conference": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "pages": "5777-5780", "publisher": "IEEE", "description": "Glaucoma is a leading cause of permanent blindness. ARGALI, an automated system for glaucoma detection, employs several methods for segmenting the optic cup and disc from retinal images, combined using a fusion network, to determine the cup to disc ratio (CDR), an important clinical indicator of glaucoma. This paper discusses the use of SVM as an alternative fusion strategy in ARGALI, and evaluates its performance against the component methods and neural network (NN) fusion in the CDR calculation. The results show SVM and NN provide similar improvements over the component methods, but with SVM having a greater consistency over the NN, suggesting potential for SVM as a viable option in ARGALI.", "total_citations": {"2010": 1, "2011": 5, "2012": 6, "2013": 7, "2014": 7, "2015": 12, "2016": 6, "2017": 2, "2018": 4, "2019": 4, "2020": 3, "2021": 7, "2022": 3, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:qjMakFHDy7sC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4785192/", "authors": ["Shijian Lu", "Cuntai Guan", "Haihong Zhang"], "publication_date": "2009/2/18", "journal": "IEEE Transactions on Neural Systems and Rehabilitation Engineering", "volume": "17", "issue": "2", "pages": "135-145", "publisher": "IEEE", "description": "Conventional brain computer interfaces rely on a guided calibration procedure to address the problem of considerable variations in electroencephalography (EEG) across human subjects. This calibration, however, implies inconvenience to the end users. In this paper, we propose an online-adaptive-learning method to address this problem for P300-based brain computer interfaces. By automatically capturing subject-specific EEG characteristics during online operation, this method allows a new user to start operating a P300-based brain-computer interface without guided (supervised) calibration. The basic principle is to first learn a generic model termed  subject-independent model  offline from EEG of a pool of subjects to capture common P300 characteristics. For a new user, a new model termed  subject-specific model  is then adapted online based on EEG recorded from the new subject and the corresponding\u00a0\u2026", "total_citations": {"2009": 2, "2010": 8, "2011": 13, "2012": 8, "2013": 18, "2014": 19, "2015": 15, "2016": 10, "2017": 12, "2018": 12, "2019": 5, "2020": 16, "2021": 11, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:KlAtU1dfN6UC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4761452/", "authors": ["Shijian Lu", "Cuntai Guan", "Haihong Zhang"], "publication_date": "2008/12/8", "conference": "2008 19th International Conference on Pattern Recognition", "pages": "1-4", "publisher": "IEEE", "description": "This paper presents a subject-independent EEG (Electroencephalogram) classification technique and its application to a P300-based word speller. Due to EEG variations across subjects, a user calibration procedure is usually required to build a subject-specific classification model (SSCM). We remove the user calibration through the boosting of a committee of weak classifiers learned from EEG of a pool of subjects. In particular, we ensemble the weak classifiers based on their confidence that is evaluated according to the classification consistency. Experiments over ten subjects show that the proposed technique greatly outperforms the supervised classification models, hence making P300-based BCIs more convenient for practical uses.", "total_citations": {"2011": 2, "2012": 3, "2013": 0, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 1, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LkGwnXOMwfcC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4649233/", "authors": ["Shijian Lu", "Cuntai Guan", "Haihong Zhang"], "publication_date": "2008/8/20", "conference": "2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "pages": "638-641", "publisher": "IEEE", "description": "This paper presents an unsupervised subject modeling technique and its application to a P300-based word speller. Due to EEG variations across subjects, a special training procedure is required to learn a subject-specific classification model (SSCM). To deal with the inter-subject variation, we first study a subject independent classification model (SICM) that is learned from EEG of a pool of subjects. Next we further adapt the SICM by learning from a subset of the pooled EEG that is automatically selected based on its similarity to the EEG of a new subject. Experiments over ten healthy subjects show that the SICM learned from all pooled EEG outperforms the cross-subject models greatly. More importantly, the adapted SICM achieves virtually the same performance as the SSCM, hence removing the complicated and tedious training procedure.", "total_citations": {"2009": 1, "2010": 3, "2011": 4, "2012": 2, "2013": 0, "2014": 0, "2015": 0, "2016": 1, "2017": 0, "2018": 2, "2019": 2, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:_kc_bZDykSQC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4634141/", "authors": ["Shijian Lu", "Cuntai Guan", "Haihong Zhang"], "publication_date": "2008/6/1", "conference": "2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)", "pages": "2461-2465", "publisher": "IEEE", "description": "This paper proposes an approach to learn subject-independent P300 models for EEG-based brain-computer interfaces. The P300 models are first learned using a pool of existing subjects and Fisher linear discriminant, and then autonomously adapted to the unlabeled data of a new subject using an unsupervised machine learning technique. In data analysis, we apply this technique to a set of EEG data of 10 subjects performing word spelling in an oddball paradigm. The results are very positive: the adapted models with unlabeled data yield virtually the same classification accuracy as the conventional methods with labeled data. Therefore, it proves the feasibility of P300-based BCIs which can be applied directly to a new subject without training sessions.", "total_citations": {"2009": 1, "2010": 0, "2011": 0, "2012": 0, "2013": 0, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:UeHWp8X0CEIC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0031320307004669", "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2008/5/1", "journal": "Pattern Recognition", "volume": "41", "issue": "5", "pages": "1799-1809", "publisher": "Pergamon", "description": "This paper reports a document retrieval technique that retrieves machine-printed Latin-based document images through word shape coding. Adopting the idea of image annotation, a word shape coding scheme is proposed, which converts each word image into a word shape code by using a few shape features. The text contents of imaged documents are thus captured by a document vector constructed with the converted word shape code and word frequency information. Similarities between different document images are then gauged based on the constructed document vectors. We divide the retrieval process into two stages. Based on the observation that documents of the same language share a large number of high-frequency language-specific stop words, the first stage retrieves documents with the same underlying language as that of the query document. The second stage then re-ranks the documents\u00a0\u2026", "total_citations": {"2008": 5, "2009": 2, "2010": 6, "2011": 7, "2012": 3, "2013": 4, "2014": 3, "2015": 4, "2016": 4, "2017": 1, "2018": 1, "2019": 2, "2020": 1, "2021": 2, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:u-x6o8ySG0sC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4492785/", "authors": ["Shijian Lu", "Linlin Li", "Chew Lim Tan"], "publication_date": "2008/4/18", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "volume": "30", "issue": "11", "pages": "1913-1918", "publisher": "IEEE", "description": "This paper presents a document retrieval technique that is capable of searching document images without optical character recognition (OCR). The proposed technique retrieves document images by a new word shape coding scheme, which captures the document content through annotating each word image by a word shape code. In particular, we annotate word images by using a set of topological shape features including character ascenders/descenders, character holes, and character water reservoirs. With the annotated word shape codes, document images can be retrieved by either query keywords or a query document image. Experimental results show that the proposed document image retrieval technique is fast, efficient, and tolerant to various types of document degradation.", "total_citations": {"2007": 1, "2008": 0, "2009": 6, "2010": 10, "2011": 16, "2012": 20, "2013": 14, "2014": 19, "2015": 15, "2016": 7, "2017": 11, "2018": 5, "2019": 11, "2020": 2, "2021": 6, "2022": 1, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:NaGl4SEjCO4C": {"external_link": null, "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2008/1", "journal": "IEEE transactions on pattern analysis and machine intelligence", "volume": "30", "issue": "1", "pages": "14-24", "publisher": "IEEE", "total_citations": {"2008": 8, "2009": 6, "2010": 9, "2011": 4, "2012": 3, "2013": 5, "2014": 9, "2015": 10, "2016": 5, "2017": 9, "2018": 2, "2019": 4, "2020": 2, "2021": 6, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:YOwf2qJgpHMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4378731/", "authors": ["Shijian Lu", "Linlin Li", "Chew Lim Tan"], "publication_date": "2007/9/23", "conference": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)", "volume": "1", "pages": "352-356", "publisher": "IEEE", "description": "This paper presents a language identification technique that detects Latin-based languages of imaged documents without OCR. The proposed technique detects languages through the word shape coding, which converts each word image into a word shape code and accordingly transforms each document image into an electronic document vector. For each Latin-based language under study, a language template is first constructed through a corpus-based learning process. The underlying language of the query document is then determined based on the similarity between the query document vector and multiple constructed language templates. Compared with the reported methods, the proposed language identification technique is fast, accurate, and tolerant to text segmentation error caused by noise and various types of document degradation. Experimental results show some promising results.", "total_citations": {"2009": 1, "2010": 0, "2011": 0, "2012": 0, "2013": 0, "2014": 0, "2015": 1, "2016": 1, "2017": 1, "2018": 1, "2019": 0, "2020": 0, "2021": 0, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:0EnyYjriUFMC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4377064/", "authors": ["Shijian Lu", "C-L Tan"], "publication_date": "2007/9/23", "conference": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)", "volume": "2", "pages": "994-998", "publisher": "IEEE", "description": "This paper presents a keyword spotting technique that locates keywords within document images captured by a digital camera. In the proposed technique, the shape of word images in perspective view is captured by using three perspective invariants, namely, holes, water reservoirs, and character ascenders and descenders. Given a camera image of document, text line and word images are first segmented through the connected component analysis. The three perspective invariants are then detected through two rounds of scanning process, which transliterate each character image into a character shape code of dimension six and so convert each word image into a word shape code. Keywords within camera images of documents are finally located through a partial matching process. Experiments show some promising results.", "total_citations": {"2009": 3, "2010": 2, "2011": 3, "2012": 0, "2013": 0, "2014": 0, "2015": 1, "2016": 1, "2017": 1, "2018": 1, "2019": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:UebtZRa9Y70C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4378677/", "authors": ["Linlin Li", "Shijian Lu", "C-L Tan"], "publication_date": "2007/9/23", "conference": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)", "volume": "1", "pages": "68-72", "publisher": "IEEE", "description": "In order to capture the content of an imaged document but avoid the time-consuming full-scale OCR which is fragile to handle touching characters, a fast and segmentation- free keyword spotting method is proposed in this paper. The keyword spotting method is based on word shape coding technique. The proposed coding scheme has little ambiguity, and can be swiftly executed. It is a promising technique to boost better document image retrieval. The strength of the proposed method is demonstrated in a document filtering experiment. The experimental results show that document filtering based on the proposed method is more than 20 times faster than the one based on OCR, and has comparable filtering accuracy.", "total_citations": {"2006": 1, "2007": 0, "2008": 0, "2009": 5, "2010": 2, "2011": 2, "2012": 2, "2013": 2, "2014": 2, "2015": 0, "2016": 1, "2017": 3, "2018": 1, "2019": 2, "2020": 0, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Se3iqnhoufwC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4378711/", "authors": ["Shijian Lu", "C-L Tan"], "publication_date": "2007/9/23", "conference": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)", "volume": "1", "pages": "237-241", "publisher": "IEEE", "description": "This paper presents an identification technique that automatically detects the underlying script and orientation of scanned document images. In the proposed technique, document script and orientation are identified by using the stroke density and distribution, which convert each document image into a document vector. For each script at each orientation, a number of reference document vectors are first constructed. Script and orientation of the query document are then determined according to the similarity between the query document vector and multiple pre- constructed reference document vectors by using the K-nearest neighbor algorithm. Experiments show that the proposed technique is tolerant to the document skew and able to detect orientations of documents of different scripts.", "total_citations": {"2008": 1, "2009": 2, "2010": 3, "2011": 1, "2012": 2, "2013": 0, "2014": 3, "2015": 2, "2016": 3, "2017": 3, "2018": 4, "2019": 0, "2020": 2, "2021": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:YsMSGLbcyi4C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4377002/", "authors": ["Shijian Lu", "Jie Wang", "Chew Lim Tan"], "publication_date": "2007/9/23", "conference": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)", "volume": "2", "pages": "684-688", "publisher": "IEEE", "description": "This paper presents a document skew and orientation detection technique. The proposed technique estimates document skew and orientation based on the observation that text images normally hold a large amount of equidistant interline spacings and the number of character ascenders is statistically much larger than that of character descenders. Given a document image with arbitrary skew and orientation, white run histograms are first constructed through scanning documents in horizontal and vertical directions. Document skew is then estimated by using the white runs that exactly span the interline spacing. Lastly, document orientation is determined according to the numbers of character ascenders and descenders, which are detected by using the white runs that cross the interline spacing and lie over character ascenders and descenders. Experiments show that the proposed technique is fast, accurate, and\u00a0\u2026", "total_citations": {"2008": 1, "2009": 3, "2010": 4, "2011": 4, "2012": 2, "2013": 5, "2014": 5, "2015": 0, "2016": 1, "2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:2osOgNQ5qMEC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/4378723/", "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2007/9/23", "conference": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)", "volume": "1", "pages": "312-316", "publisher": "IEEE", "description": "This paper presents a document image binarization technique that segments text from badly illuminated document images. Based on the observations that text documents normally lie over a planar or smoothly curved surface and have a uniformly colored background, badly illuminated document images are binarized by using a smoothing polynomial surface, which estimates the shading variation and compensates the shading degradation based on the estimated shading variation. Badly illuminated document images are accordingly binarized through the global thresholding of the compensated document images. Compared with the reported methods, the proposed technique is tolerant to the variations in text size and document contrast. At the same time, it is much faster and able to produce a binary text image with little background noise.", "total_citations": {"2006": 1, "2007": 0, "2008": 1, "2009": 6, "2010": 5, "2011": 8, "2012": 10, "2013": 7, "2014": 4, "2015": 6, "2016": 6, "2017": 2, "2018": 3, "2019": 4, "2020": 2, "2021": 8, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:fbc8zXXH2BUC": {"external_link": "https://link.springer.com/chapter/10.1007/978-3-540-88188-9_19", "authors": ["Linlin Li", "Shijian Lu", "Chew Lim Tan"], "publication_date": "2007/9/20", "conference": "International Workshop on Graphics Recognition", "pages": "191-201", "publisher": "Springer, Berlin, Heidelberg", "description": "Patent document images maintained by the U.S. patent database have a specific format, in which figures and descriptions are separated into different pages. This makes it difficult for users to refer to a figure while reading the description or vice versa. The system introduced in this paper is to prepare these patent documents for a friendly browsing interface. The system is able to segment an imaged page with several figures into individual figures and extract caption and label information from the figure. After obtaining captions and labels, figures and the relevant description are linked together, and thus users could easily refer from a description to the figure or vice versa.", "total_citations": {"2009": 1, "2010": 0, "2011": 0, "2012": 1, "2013": 0, "2014": 0, "2015": 0, "2016": 0, "2017": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:WF5omc3nYNoC": {"external_link": "https://dl.acm.org/doi/abs/10.1145/1284420.1284426", "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2007/8/28", "book": "Proceedings of the 2007 ACM symposium on Document engineering", "pages": "3-8", "description": "This paper presents a document image thresholding technique that binarizes badly illuminated document images by the photometric correction. Based on the observation that illumination normally varies smoothly and document images often contain a uniformly colored background, the global shading variation is estimated by using a two-dimensional Savitzky-Golay filter that fits a least square polynomial surface to the luminance of a badly illuminated document image. With the knowledge of the global shading variation, shading degradation is then corrected through a compensation process that produces animage with roughly uniform illumination. Badly illuminated document images are accordingly binarized through the global thresholding of the compensated ones. Experiments show that the proposed thresholding technique is fast, robust, and efficient for the binarization of badly illuminated document images.", "total_citations": {"2008": 2, "2009": 5, "2010": 5, "2011": 2, "2012": 3, "2013": 3, "2014": 1, "2015": 2, "2016": 1, "2017": 1, "2018": 1, "2019": 0, "2020": 0, "2021": 1, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:IjCSPb-OGe4C": {"external_link": "https://dl.acm.org/doi/abs/10.1145/1180639.1180673", "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2006/10/23", "book": "Proceedings of the 14th ACM international conference on Multimedia", "pages": "113-116", "description": "This paper presents an automatic orientation detection and categorization technique that is capable of detecting the orientation of multilingual documents with arbitrary skew and categorizing document images according to the underlying languages. We carry out orientation detection and categorization through document vectorization, which encodes document orientation and language information and converts each document image into an electronic document vector through the exploitation of the density and distribution of vertical component runs. For each language of interest, a pair of vector templates is first constructed through a training process. Orientation and category of the query image are then determined based on distances between the query document vector and the constructed vector templates. Experiments over 492 testing document images show that the average orientation detection and\u00a0\u2026", "total_citations": {"2009": 5, "2010": 3, "2011": 5, "2012": 2, "2013": 0, "2014": 2, "2015": 2, "2016": 2, "2017": 1, "2018": 1, "2019": 1, "2020": 3, "2021": 1, "2022": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:hqOjcs7Dif8C": {"external_link": "https://ieeexplore.ieee.org/abstract/document/1699386/", "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2006/8/20", "conference": "18th International Conference on Pattern Recognition (ICPR'06)", "volume": "2", "pages": "1042-1045", "publisher": "IEEE", "description": "As camera resolution increases, high-speed non-contact text capture through a digital camera is opening up a new channel for document capture and understanding. Unfortunately, perspective and geometric distortions in camera image of documents make it hard to recognize the document content properly. In this paper, we propose a character recognition technique, which is capable of recognizing camera text lying over a planar or smoothly curved surface in perspective views. In our proposed method, a few perspective invariants including character ascender and descender, centroid intersection numbers, and water reservior are first detected. Camera texts are then recognized using a classification and regression tree (CART) structure. Experimental results show our method is fast and improves recognition performance greatly", "total_citations": {"2007": 1, "2008": 2, "2009": 2, "2010": 0, "2011": 2, "2012": 0, "2013": 2, "2014": 2, "2015": 4, "2016": 3, "2017": 2, "2018": 1, "2019": 0, "2020": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Y0pCki6q_DkC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/1699051/", "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2006/8/20", "conference": "18th International Conference on Pattern Recognition (ICPR'06)", "volume": "1", "pages": "971-974", "publisher": "IEEE", "description": "For document images captured by a digital camera, perspective and geometric distortions make it hard to recognize the document content properly. In this paper, we propose an integrated document restoration technique, which is capable of removing perspective and geometric distortions, and producing a flattened and fronto-parallel text image that is friendly to the generic OCR systems. The proposed document restoration is accomplished through grid modeling, which divides camera images into multiple quadrilateral grids using vertical text directions and the x lines and base lines. The global distortions are then removed through grid regularization that transforms the quadrilateral grids together with the pixel contents to the regular square grids. Experimental results show the proposed method is fast and easy for implementation", "total_citations": {"2007": 1, "2008": 2, "2009": 1, "2010": 2, "2011": 2, "2012": 4, "2013": 1, "2014": 1, "2015": 3, "2016": 5, "2017": 5, "2018": 5, "2019": 2, "2020": 5, "2021": 3, "2022": 7, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:W7OEmFMy1HYC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0262885606000904", "authors": ["Shijian Lu", "Ben M Chen", "Chi Chung Ko"], "publication_date": "2006/8/1", "journal": "Image and Vision Computing", "volume": "24", "issue": "8", "pages": "837-848", "publisher": "Elsevier", "description": "As camera resolution increases, high-speed non-contact text capture through a digital camera is opening up a new channel for text capture and understanding. Unfortunately, the captured document images are normally coupled with the perspective and geometric distortions that cannot be handled by the existing optical character recognition (OCR) systems. In this paper, we propose a new technique, which is capable of removing the perspective and geometric distortions, and reconstructing the fronto-parallel view of text with a single document image. Different from reported approaches in the literature, the restoration of the distorted camera documents is carried out through the image partition, which divides the documents into multiple small image patches where text can be approximated to lie on a planar surface. The global distortion is thus corrected through the local rectification of the partitioned image patches\u00a0\u2026", "total_citations": {"2007": 2, "2008": 0, "2009": 0, "2010": 2, "2011": 2, "2012": 1, "2013": 3, "2014": 0, "2015": 5, "2016": 1, "2017": 5, "2018": 1, "2019": 3, "2020": 0, "2021": 3, "2022": 4, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:Tyk-4Ss8FVUC": {"external_link": "https://link.springer.com/chapter/10.1007/11669487_21", "authors": ["Shijian Lu", "Chew Lim Tan", "Weihua Huang"], "publication_date": "2006", "conference": "Document Analysis Systems VII: 7th International Workshop, DAS 2006, Nelson, New Zealand, February 13-15, 2006. Proceedings 7", "pages": "232-242", "publisher": "Springer Berlin Heidelberg", "description": "This paper presents a language identification technique that differentiates Latin-based languages in degraded and distorted document images. Different from the reported methods that transform word images through a character shape coding process, our method directly captures word shapes with the local extremum points and the horizontal intersection numbers, which are both tolerant of noise, character segmentation errors, and slight skew distortions. For each language studied, a word shape template and a word frequency template are firstly constructed based on the proposed word shape coding scheme. Identification is then accomplished based on Bray Curtis or Hamming distance between the word shape code of query images and the constructed word shape and frequency templates. Experiments show the average identification rate upon eight Latin-based languages reaches over 99%. ...", "total_citations": {"2006": 1, "2007": 2, "2008": 3, "2009": 1, "2010": 7, "2011": 0, "2012": 1, "2013": 0, "2014": 0, "2015": 1, "2016": 0, "2017": 0, "2018": 0, "2019": 1, "2020": 1, "2021": 1, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:d1gkVwhDpl0C": {"external_link": "https://link.springer.com/chapter/10.1007/11669487_43", "authors": ["Shijian Lu", "Chew Lim Tan"], "publication_date": "2006", "conference": "Document Analysis Systems VII: 7th International Workshop, DAS 2006, Nelson, New Zealand, February 13-15, 2006. Proceedings 7", "pages": "484-495", "publisher": "Springer Berlin Heidelberg", "description": "This paper presents a document restoration technique that is able to flatten curled document images captured through a digital camera. The proposed method corrects camera images of documents through image partition, which divides distorted text lines into multiple small patches based on the identified vertical stroke boundary (VSB) and the fitted x-line and baseline of text lines. Target rectangles are then constructed through the exploitation of the characters enclosed within the partitioned image patches. With the constructed target rectangles and the partitioned image patches, global geometric distortion is finally removed through the local rectification of partitioned image patches one by one. Experimental results show that the proposed technique is fast, accurate, and easy for implementation.", "total_citations": {"2005": 1, "2006": 0, "2007": 11, "2008": 3, "2009": 8, "2010": 4, "2011": 5, "2012": 2, "2013": 3, "2014": 0, "2015": 3, "2016": 2, "2017": 4, "2018": 0, "2019": 3, "2020": 1, "2021": 0, "2022": 1, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:u5HHmVD_uO8C": {"external_link": "https://www.sciencedirect.com/science/article/pii/S0262885605000119", "authors": ["Shijian Lu", "Ben M Chen", "Chi Chung Ko"], "publication_date": "2005/5/1", "journal": "Image and Vision Computing", "volume": "23", "issue": "5", "pages": "541-553", "publisher": "Elsevier", "description": "In this paper, we deal with the problem of document image rectification from image captured by digital cameras. The improvement on the resolution of digital camera sensors has brought more and more applications for non-contact text capture. Unfortunately, perspective distortion in the resulting image makes it hard to properly identify the contents of the captured text using traditional optical character recognition (OCR) systems. We propose in this work a new technique, which is capable of removing perspective distortion and recovering the fronto-parallel view of text with a single image. Different from reported approaches in the literature, the image rectification is carried out using character stroke boundaries and tip points (SBTP), which are extracted from character strokes based on multiple fuzzy sets and morphological operators. The algorithm needs neither high-contrast document boundary (HDB) nor paragraph\u00a0\u2026", "total_citations": {"2004": 1, "2005": 1, "2006": 10, "2007": 7, "2008": 3, "2009": 11, "2010": 7, "2011": 12, "2012": 12, "2013": 8, "2014": 9, "2015": 7, "2016": 6, "2017": 7, "2018": 6, "2019": 8, "2020": 7, "2021": 7, "2022": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:ufrVoPGSRksC": {"external_link": "https://scholarbank.nus.edu.sg/handle/10635/78052", "authors": ["S Lu", "CL Tan"], "publication_date": "2005", "description": "As camera resolution increases, high-speed noncontact text capture through a digital camera is opening up a new channel for text capture and understanding. Unfortunately, the skew, perspective, and geometric distortions coupled within the captured images make it hard to recognize the document text using the generic OCR systems. In this paper, we propose a document restoration technique, which is capable of removing the three types of distortions, and reconstructing the fronto-parallel view of the document text using a single document image captured through a digital camera. Different from the reported techniques, the proposed restoration technique is carried out based on the vertical stroke boundary and the top line and base line of text lines. Experimental results show the proposed technique is fast, accurate, and robust.", "total_citations": {"2005": 1, "2006": 0, "2007": 5, "2008": 1, "2009": 1, "2010": 0, "2011": 2, "2012": 0, "2013": 1, "2014": 0, "2015": 0, "2016": 0, "2017": 0, "2018": 0, "2019": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:4TOpqqG69KYC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/1421713/", "authors": ["Shijian Lu", "Ben M Chen", "Chi Chung Ko"], "publication_date": "2004/10/24", "conference": "2004 International Conference on Image Processing, 2004. ICIP'04.", "volume": "5", "pages": "2877-2880", "publisher": "IEEE", "description": "In this paper, we deal with the problem of document image rectification from images captured by digital cameras. The improvement on the resolution of digital camera sensors has brought more and more applications for non-contact text capture. Unfortunately, perspective distortion coupled with resulting images makes it harder to properly identify the contents of captured texts using the traditional optical character recognition (OCR) system. We propose in this work a new technique, which is capable of removing distortion and recovering the fronto-parallel view of text with a single image. Different from reported approaches in the literature, the image rectification is carried out using character boundary and tip point, which are extracted from character strokes based on multiple fuzzy sets and morphological operators. The algorithm needs neither camera calibration nor high-contrast document boundary. Experimental\u00a0\u2026", "total_citations": {"2006": 1, "2007": 1, "2008": 0, "2009": 1, "2010": 0, "2011": 0, "2012": 1, "2013": 1, "2014": 1, "2015": 1, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:W5xh706n7nkC": {"external_link": "https://scholar.google.com/scholar?cluster=13948339306982241304&hl=en&oi=scholarr", "authors": ["S Lu", "C Ko", "B Chen", "C Cheng"], "publication_date": "2002/6/24", "journal": "The 2002 International Conference on Imaging Science, Systems, and Technology", "pages": "24-27", "publisher": "Las Vegas, USA", "description": "A web-based 3D virtual experimentation system has been successfully developed for teaching and learning purposes in the National University of Singapore (NUS). Rather than simulating the instrument display on the client as is often done in other virtual laboratories, the real-time video capture of the actual oscilloscope display is seamlessly integrated into the web-based 3D virtual environments. Our video subsystem can reflect the changes of oscilloscope display in real time through the implementation of its video capturing, transmitting, receiving and rendering modules when remote experimenters manipulate the real instruments through virtual instruments interface developed by 3D modeling. The exploitation of live video reduces the complexity of 3D modeling as well as promises a higher visual quality scene. The software and hardware structure of this video system is introduced. The customized API for the\u00a0\u2026", "total_citations": {"2005": 1, "2006": 1, "2007": 0, "2008": 0, "2009": 0, "2010": 1, "2011": 0, "2012": 0, "2013": 0, "2014": 0, "2015": 1, "2016": 0, "2017": 0, "2018": 0, "2019": 0, "2020": 0, "2021": 0, "2022": 0, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uYmK-A0AAAAJ&cstart=200&pagesize=100&sortby=pubdate&citation_for_view=uYmK-A0AAAAJ:LK8CI43ZvvMC": {"external_link": "https://arxiv.org/pdf/2310.01830", "authors": ["Zuhao Yang", "Fangneng Zhan", "Kunhao Liu", "Muyu Xu", "Shijian Lu"], "description": "The advancement of visual intelligence is intrinsically tethered to the availability of data. In parallel, generative Artificial Intelligence (AI) has unlocked the potential to create synthetic images that closely resemble real-world photographs, which prompts a compelling inquiry: how visual intelligence benefit from the advance of generative AI? This paper explores the innovative concept of harnessing these AI-generated images as a new data source, reshaping traditional model paradigms in visual intelligence. In contrast to real data, AI-generated data sources exhibit remarkable advantages, including unmatched abundance and scalability, the rapid generation of vast datasets, and the effortless simulation of edge cases. Built on the success of generative AI models, we examines the potential of their generated data in a range of applications, from training machine learning models to simulating scenarios for computational modelling, testing, and validation. We probe the technological foundations that support this groundbreaking use of generative AI, engaging in an in-depth discussion on the ethical, legal, and practical considerations that accompany this transformative paradigm shift. Through an exhaustive survey of current technologies and applications, this paper presents a comprehensive view of the synthetic era in visual intelligence. A project with this paper can be found at https://github. com/mwxely/AIGS."}}