{"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:8k81kl-MbHgC": {"external_link": "https://arxiv.org/abs/1603.06679", "authors": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao"], "publication_date": "2016/10", "conference": "EMNLP 2016: Conference on Empirical Methods in Natural Language Processing", "pages": "616-626", "publisher": "Association for Computational Linguistics", "description": "In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge.", "total_citations": {"2016": 1, "2017": 18, "2018": 31, "2019": 54, "2020": 87, "2021": 80, "2022": 81, "2023": 47}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:MXK_kJrjxJIC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/10974", "authors": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao"], "publication_date": "2016/12", "conference": "31st AAAI Conference on Artificial Intelligence (AAAI-17)", "publisher": "AAAI Press", "description": "The task of aspect and opinion terms co-extraction aims to explicitly extract aspect terms describing features of an entity and opinion terms expressing emotions from user-generated texts. To achieve this task, one effective approach is to exploit relations between aspect terms and opinion terms by parsing syntactic structure for each sentence. However, this approach requires expensive effort for parsing and highly depends on the quality of the parsing results. In this paper, we offer a novel deep learning model, named coupled multi-layer attentions. The proposed model provides an end-to-end solution and does not require any parsers or other linguistic resources for preprocessing. Specifically, the proposed model is a multi-layer attention network, where each layer consists of a couple of attentions with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms. They are learned interactively to dually propagate information between aspect terms and opinion terms. Through multiple layers, the model can further exploit indirect relations between terms for more precise information extraction. Experimental results on three benchmark datasets in SemEval Challenge 2014 and 2015 show that our model achieves state-of-the-art performances compared with several baselines.", "total_citations": {"2017": 6, "2018": 16, "2019": 52, "2020": 72, "2021": 89, "2022": 76, "2023": 61}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:kNdYIx-mwKoC": {"external_link": "https://aclanthology.org/P18-1202/", "authors": ["Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2018", "conference": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics", "volume": "1", "pages": "2171--2181", "description": "Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization. Supervised learning methods have proven to be effective for this task. However, in many domains, the lack of labeled data hinders the learning of a precise extraction model. In this case, unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any unlabeled target domain. In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations. We treat these relations as invariant \u201cpivot information\u201d across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree. In the end, we demonstrate state-of-the-art results on three benchmark datasets.", "total_citations": {"2018": 1, "2019": 5, "2020": 12, "2021": 16, "2022": 20, "2023": 17}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:Wp0gIr-vW9MC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/6460", "authors": ["Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2020/4/3", "journal": "Proceedings of the AAAI conference on artificial intelligence", "volume": "34", "issue": "05", "pages": "9225-9232", "description": "Information extraction (IE) aims to produce structured information from an input text, eg, Named Entity Recognition and Relation Extraction. Various attempts have been proposed for IE via feature engineering or deep learning. However, most of them fail to associate the complex relationships inherent in the task itself, which has proven to be especially crucial. For example, the relation between 2 entities is highly dependent on their entity types. These dependencies can be regarded as complex constraints that can be efficiently expressed as logical rules. To combine such logic reasoning capabilities with learning capabilities of deep neural networks, we propose to integrate logical knowledge in the form of first-order logic into a deep learning system, which can be trained jointly in an end-to-end manner. The integrated framework is able to enhance neural outputs with knowledge regularization via logic rules, and at the same time update the weights of logic rules to comply with the characteristics of the training data. We demonstrate the effectiveness and generalization of the proposed model on multiple IE tasks.", "total_citations": {"2020": 5, "2021": 14, "2022": 14, "2023": 10}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:YOwf2qJgpHMC": {"external_link": "https://proceedings.neurips.cc/paper_files/paper/2019/hash/f8e59f4b2fe7c5705bf878bbd494ccdf-Abstract.html", "authors": ["Shangyu Chen", "Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2019", "journal": "Advances in Neural Information Processing Systems", "volume": "32", "description": "Tremendous amount of parameters make deep neural networks impractical to be deployed for edge-device-based real-world applications due to the limit of computational power and storage space. Existing studies have made progress on learning quantized deep models to reduce model size and energy consumption, ie converting full-precision weights ('s) into discrete values ('s) in a supervised training manner. However, the training process for quantization is non-differentiable, which leads to either infinite or zero gradients () wrt . To address this problem, most training-based quantization methods use the gradient wrt () with clipping to approximate  by Straight-Through-Estimator (STE) or manually design their computation. However, these methods only heuristically make training-based quantization applicable, without further analysis on how the approximated gradients can assist training of a quantized network. In this paper, we propose to learn  by a neural network. Specifically, a meta network is trained using  and  as inputs, and outputs  for subsequent weight updates. The meta network is updated together with the original quantized network. Our proposed method alleviates the problem of non-differentiability, and can be trained in an end-to-end manner. Extensive experiments are conducted with CIFAR10/100 and ImageNet on various deep networks to demonstrate the advantage of our proposed method in terms of a faster convergence rate and better performance. Codes are released at:\\texttt {https://github. com/csyhhu/MetaQuant}", "total_citations": {"2019": 1, "2020": 7, "2021": 10, "2022": 11, "2023": 14}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:_kc_bZDykSQC": {"external_link": "https://ojs.aaai.org/index.php/AAAI/article/view/4206", "authors": ["Shangyu Chen", "Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2019/7/17", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "33", "issue": "01", "pages": "3329-3336", "description": "The advancement of deep models poses great challenges to real-world deployment because of the limited computational ability and storage space on edge devices. To solve this problem, existing works have made progress to prune or quantize deep models. However, most existing methods rely heavily on a supervised training process to achieve satisfactory performance, acquiring large amount of labeled training data, which may not be practical for real deployment. In this paper, we propose a novel layer-wise quantization method for deep neural networks, which only requires limited training data (1% of original dataset). Specifically, we formulate parameters quantization for each layer as a discrete optimization problem, and solve it using Alternative Direction Method of Multipliers (ADMM), which gives an efficient closed-form solution. We prove that the final performance drop after quantization is bounded by a linear combination of the reconstructed errors caused at each layer. Based on the proved theorem, we propose an algorithm to quantize a deep neural network layer by layer with an additional weights update step to minimize the final error. Extensive experiments on benchmark deep models are conducted to demonstrate the effectiveness of our proposed method using 1% of CIFAR10 and ImageNet datasets. Codes are available in: https://github. com/csyhhu/L-DNQ", "total_citations": {"2019": 1, "2020": 5, "2021": 12, "2022": 11, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:M3ejUd6NZC8C": {"external_link": "https://aaai.org/ojs/index.php/AAAI/article/view/4703", "authors": ["Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2019/7/17", "journal": "Proceedings of the AAAI Conference on Artificial Intelligence", "volume": "33", "issue": "01", "pages": "7192-7199", "description": "In fine-grained opinion mining, aspect and opinion terms extraction has become a fundamental task that provides key information for user-generated texts. Despite its importance, a lack of annotated resources in many domains impede the ability to train a precise model. Very few attempts have applied unsupervised domain adaptation methods to transfer fine-grained knowledge (in the word level) from some labeled source domain (s) to any unlabeled target domain. Existing methods depend on the construction of \u201cpivot\u201d knowledge, eg, common opinion terms or syntactic relations between aspect and opinion words. In this work, we propose an interactive memory network that consists of local and global memory units. The model could exploit both local and global memory interactions to capture intra-correlations among aspect words or opinion words themselves, as well as the interconnections between aspect and opinion words. The source space and the target space are aligned through these domaininvariant interactions by incorporating an auxiliary task and domain adversarial networks. The proposed model does not require any external resources and demonstrates promising results on 3 benchmark datasets.", "total_citations": {"2020": 4, "2021": 7, "2022": 6, "2023": 11}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:4TOpqqG69KYC": {"external_link": "https://direct.mit.edu/coli/article-abstract/45/4/705/93359", "authors": ["Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2020/1/1", "journal": "Computational Linguistics", "volume": "45", "issue": "4", "pages": "705-736", "publisher": "MIT Press", "description": "In fine-grained opinion mining, extracting aspect terms (a.k.a. opinion targets) and opinion terms (a.k.a. opinion expressions) from user-generated texts is the most fundamental task in order to generate structured opinion summarization. Existing studies have shown that the syntactic relations between aspect and opinion words play an important role for aspect and opinion terms extraction. However, most of the works either relied on predefined rules or separated relation mining with feature learning. Moreover, these works only focused on single-domain extraction, which failed to adapt well to other domains of interest where only unlabeled data are available. In real-world scenarios, annotated resources are extremely scarce for many domains, motivating knowledge transfer strategies from labeled source domain(s) to any unlabeled target domain. We observe that syntactic relations among target words to be\u00a0\u2026", "total_citations": {"2020": 3, "2021": 3, "2022": 10, "2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:qxL8FJ1GzNcC": {"external_link": "https://csyhhu.github.io/data/Co-Prune.pdf", "authors": ["Shangyu Chen", "Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2019/8/10", "conference": "IJCAI", "pages": "2102-2108", "description": "The advancement of deep models poses great challenges to real-world deployment because of the limited computational ability and storage space on edge devices. To solve this problem, existing works have made progress to compress deep models by pruning or quantization. However, most existing methods rely on a large amount of training data and a pre-trained model in the same domain. When only limited in-domain training data is available, these methods fail to perform well. This prompts the idea of transferring knowledge from a resource-rich source domain to a target domain with limited data to perform model compression. In this paper, we propose a method to perform cross-domain pruning by cooperatively training in both domains: taking advantage of data and a pre-trained model from the source domain to assist pruning in the target domain. Specifically, source and target pruned models are trained simultaneously and interactively, with source information transferred through the construction of a cooperative pruning mask. Our method significantly improves pruning quality in the target domain, and shed light to model compression in the cross-domain setting. Codes are available at https://github. com/csyhhu/Co-Prune.", "total_citations": {"2020": 4, "2021": 4, "2022": 8, "2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:ULOm3_A8WrAC": {"external_link": "https://www.ijcai.org/proceedings/2018/0622.pdf", "authors": ["Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2018/7/13", "conference": "IJCAI", "pages": "4475-4481", "description": "In fine-grained opinion mining, the task of aspect extraction involves the identification of explicit product features in customer reviews. This task has been widely studied in some major languages, eg, English, but was seldom addressed in other minor languages due to the lack of annotated corpus. To solve it, we develop a novel deep model to transfer knowledge from a source language with labeled training data to a target language without any annotations. Different from cross-lingual sentiment classification, aspect extraction across languages requires more fine-grained adaptation. To this end, we utilize transition-based mechanism that reads a word each time and forms a series of configurations that represent the status of the whole sentence. We represent each configuration as a continuous feature vector and align these representations from different languages into a shared space through an adversarial network. In addition, syntactic structures are also integrated into the deep model to achieve more syntactically-sensitive adaptations. The proposed method is end-to-end and achieves state-ofthe-art performance on English, French and Spanish restaurant review datasets.", "total_citations": {"2019": 5, "2020": 4, "2021": 4, "2022": 2, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:4DMP91E08xMC": {"external_link": "https://www.sciencedirect.com/science/article/pii/S000437021830599X", "authors": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier"], "publication_date": "2018/12/1", "journal": "Artificial Intelligence", "volume": "265", "pages": "1-17", "publisher": "Elsevier", "description": "Fine-grained opinion mining has attracted increasing attention recently because of its benefits for providing richer information compared with coarse-grained sentiment analysis. Under this problem, there are several existing works focusing on aspect (or opinion) terms extraction which utilize the syntactic relations among the words given by a dependency parser. These approaches, however, require additional information and highly depend on the quality of the parsing results. As a result, they may perform poorly on user-generated texts, such as product reviews, tweets, etc., whose syntactic structure is not precise. In this work, we offer an end-to-end deep learning model without any preprocessing. The model consists of a memory network that automatically learns the complicated interactions among aspect words and opinion words. Moreover, we extend the network with a multi-task manner to solve a finer-grained\u00a0\u2026", "total_citations": {"2020": 6, "2021": 4, "2022": 5, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:3fE2CSJIrl8C": {"external_link": "https://scholar.google.com/scholar?cluster=11777338089262226941&hl=en&oi=scholarr", "authors": ["Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier"], "publication_date": "2017/2", "journal": "arXiv preprint arXiv:1702.01776", "description": "In aspect-based sentiment analysis, most existing methods either focus on aspect/opinion terms extraction or aspect terms categorization. However, each task by itself only provides partial information to end users. To generate more detailed and structured opinion analysis, we propose a finer-grained problem, which we call category-specific aspect and opinion terms extraction. This problem involves the identification of aspect and opinion terms within each sentence, as well as the categorization of the identified terms. To this end, we propose an end-to-end multitask attention model, where each task corresponds to aspect/opinion terms extraction for a specific category. Our model benefits from exploring the commonalities and relationships among different tasks to address the data sparsity issue. We demonstrate its state-of-the-art performance on three benchmark datasets.", "total_citations": {"2017": 1, "2018": 2, "2019": 5, "2020": 1, "2021": 3, "2022": 1, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:9ZlFYXVOiuMC": {"external_link": "https://aclanthology.org/2020.emnlp-main.453/", "authors": ["Meixi Wu", "Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2020/11", "conference": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "pages": "5618-5628", "description": "Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process. Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT). We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework. The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent. Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.", "total_citations": {"2021": 1, "2022": 8, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:HDshCWvjkbEC": {"external_link": "https://arxiv.org/abs/2207.04564", "authors": ["Quanyu Long", "Tianze Luo", "Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2022", "journal": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "description": "In this work, we study Unsupervised Domain Adaptation (UDA) in a challenging self-supervised approach. One of the difficulties is how to learn task discrimination in the absence of target labels. Unlike previous literature which directly aligns cross-domain distributions or leverages reverse gradient, we propose Domain Confused Contrastive Learning (DCCL) to bridge the source and the target domains via domain puzzles, and retain discriminative representations after adaptation. Technically, DCCL searches for a most domain-challenging direction and exquisitely crafts domain confused augmentations as positive pairs, then it contrastively encourages the model to pull representations towards the other domain, thus learning more stable and effective domain invariances. We also investigate whether contrastive learning necessarily helps with UDA when performing other data augmentations. Extensive experiments demonstrate that DCCL significantly outperforms baselines.", "total_citations": {"2022": 3, "2023": 5}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:TQgYirikUcIC": {"external_link": "https://arxiv.org/abs/2210.03501", "authors": ["Hui Liu", "Wenya Wang", "Haoliang Li"], "publication_date": "2022", "journal": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing", "description": "Sarcasm is a linguistic phenomenon indicating a discrepancy between literal meanings and implied intentions. Due to its sophisticated nature, it is usually challenging to be detected from the text itself. As a result, multi-modal sarcasm detection has received more attention in both academia and industries. However, most existing techniques only modeled the atomic-level inconsistencies between the text input and its accompanying image, ignoring more complex compositions for both modalities. Moreover, they neglected the rich information contained in external knowledge, e.g., image captions. In this paper, we propose a novel hierarchical framework for sarcasm detection by exploring both the atomic-level congruity based on multi-head cross attention mechanism and the composition-level congruity based on graph neural networks, where a post with low congruity can be identified as sarcasm. In addition, we exploit the effect of various knowledge resources for sarcasm detection. Evaluation results on a public multi-modal sarcasm detection dataset based on Twitter demonstrate the superiority of our proposed model.", "total_citations": {"2023": 7}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:mVmsd5A6BfQC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9408409/", "authors": ["Tao Liang", "Wenya Wang", "Fengmao Lv"], "publication_date": "2021/4/19", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "volume": "33", "issue": "10", "pages": "5818-5829", "publisher": "IEEE", "description": "Fine-grained aspect term extraction is an essential subtask in aspect-based opinion analysis. It aims to identify the aspect terms (also known as opinion targets) of a product or service in each sentence. To learn a good aspect extraction model, an expensive annotation process is usually involved to acquire sufficient token-level labels for each domain, which is not realistic. To address this limitation, some previous works propose domain adaptation strategies to transfer knowledge from a sufficiently labeled source domain to unlabeled target domains. However, due to both the difficulty of fine-grained prediction problems and the large domain gap between different domains, the performance is still far from satisfactory. In this work, we conduct a pioneer study on leveraging sentence-level aspect category labels that can be usually available in commercial services, such as review sites or social media to promote token\u00a0\u2026", "total_citations": {"2020": 1, "2021": 3, "2022": 2, "2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:R3hNpaxXUhUC": {"external_link": "https://arxiv.org/abs/2209.01232", "authors": ["Wenya Wang", "Vivek Srikumar", "Hanna Hajishirzi", "Noah A Smith"], "publication_date": "2023", "conference": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "description": "In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models -- an elaboration generator and an answer predictor -- allowing each to influence the other. Using less than 0.5% of the parameters of GPT-3, our model outperforms alternatives with similar sizes and closes the gap on GPT-3 on four commonsense question answering benchmarks. Human evaluations show that the quality of the generated elaborations is high.", "total_citations": {"2022": 2, "2023": 3}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:hFOr9nPyWt4C": {"external_link": "https://direct.mit.edu/coli/article-abstract/47/4/775/106773", "authors": ["Wenya Wang", "Sinno Jialin Pan"], "publication_date": "2021/12/23", "journal": "Computational Linguistics", "volume": "47", "issue": "4", "pages": "775-812", "publisher": "MIT Press", "description": "Currently, deep learning models have been widely adopted and achieved promising results on various application domains. Despite their intriguing performance, most deep learning models function as black boxes, lacking explicit reasoning capabilities and explanations, which are usually essential for complex problems. Take joint inference in information extraction as an example. This task requires the identification of multiple structured knowledge from texts, which is inter-correlated, including entities, events, and the relationships between them. Various deep neural networks have been proposed to jointly perform entity extraction and relation prediction, which only propagate information implicitly via representation learning. However, they fail to encode the intensive correlations between entity types and relations to enforce their coexistence. On the other hand, some approaches adopt rules to explicitly\u00a0\u2026", "total_citations": {"2022": 3, "2023": 2}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:mB3voiENLucC": {"external_link": "https://aclanthology.org/2022.acl-long.343/", "authors": ["Wenya Wang", "Sinno Pan"], "publication_date": "2022/5", "conference": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "pages": "4999-5009", "description": "Multi-hop reading comprehension requires an ability to reason across multiple documents. On the one hand, deep learning approaches only implicitly encode query-related information into distributed embeddings which fail to uncover the discrete relational reasoning process to infer the correct answer. On the other hand, logic-based approaches provide interpretable rules to infer the target answer, but mostly work on structured data where entities and relations are well-defined. In this paper, we propose a deep-learning based inductive logic reasoning method that firstly extracts query-related (candidate-related) information, and then conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation. The reasoning process is accomplished via attentive memories with novel differentiable logic operators. To demonstrate the effectiveness of our model, we evaluate it on two reading comprehension datasets, namely WikiHop and MedHop.", "total_citations": {"2023": 4}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&citation_for_view=eOKISncAAAAJ:RHpTSmoSYBkC": {"external_link": "https://arxiv.org/abs/2305.03695", "authors": ["Jiacheng Liu", "Wenya Wang", "Dianzhuo Wang", "Noah A Smith", "Yejin Choi", "Hannaneh Hajishirzi"], "publication_date": "2023/5/5", "journal": "arXiv preprint arXiv:2305.03695", "description": "Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&cstart=20&pagesize=80&citation_for_view=eOKISncAAAAJ:_Qo2XoVZTnwC": {"external_link": "https://arxiv.org/abs/2305.05964", "authors": ["Hui Liu", "Wenya Wang", "Haoliang Li"], "publication_date": "2023", "journal": "Findings of the Association for Computational Linguistics: ACL 2023", "description": "Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems' reliability and practical deployment. Inspired by NeuralSymbolic AI which combines the learning ability of neural networks with the explainability of symbolic learning, we propose a novel logic-based neural model for multimodal misinformation detection which integrates interpretable logic clauses to express the reasoning process of the target task. To make learning effective, we parameterize symbolic logical elements using neural representations, which facilitate the automatic generation and evaluation of meaningful logic clauses. Additionally, to make our framework generalizable across diverse misinformation sources, we introduce five meta-predicates that can be instantiated with different correlations. Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the feasibility and versatility of our model.", "total_citations": {"2023": 1}}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&cstart=20&pagesize=80&citation_for_view=eOKISncAAAAJ:e5wmG9Sq2KIC": {"external_link": "https://ieeexplore.ieee.org/abstract/document/9932557/", "authors": ["Fengmao Lv", "Tao Liang", "Zhihui Fei", "Wenya Wang"], "publication_date": "2022/10/28", "journal": "IEEE Transactions on Neural Networks and Learning Systems", "publisher": "IEEE", "description": "Coupled aspect-opinion extraction aims to identify aspect-opinion pairs in the form of (aspect term, opinion term) or triplets in the form of (aspect term, opinion term, sentiment polarity) from user-generated texts. Compared to the traditional aspect-based sentiment prediction or extraction tasks, coupled aspect-opinion extraction needs to associate aspects with their corresponding opinions and organize opinion-related information into structured outputs. The existing works either divide this task into subproblems (i.e., term extraction and relation prediction) or utilize a unified tagging scheme. However, these methods only focus on atomic word-level interactions and ignore the intensive information propagation among different granularities (e.g., words and word pairs). To address this limitation, we propose a progressive multigranularity information propagation network that progressively explores three types of\u00a0\u2026"}, "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=eOKISncAAAAJ&cstart=20&pagesize=80&citation_for_view=eOKISncAAAAJ:aqlVkmm33-oC": {"external_link": "https://dr.ntu.edu.sg/handle/10356/75872", "authors": ["Wenya Wang"], "publication_date": "2018", "description": "Opinion mining or sentiment analysis provides a way to analyze the attitude or emotion of an opinion holder towards an entity or feature of any product. The emergence of opinion mining techniques comes along with the excessive volume of text data from social media made available to the public, such as product reviews, blog posts, forum discussions and Twitter website for social network.   Sentiment analysis has been actively studied among the Natural Language Processing (NLP) community since 2000. At this early stage, the main focus of sentiment analysis dealt with the overall sentiment polarity towards a given sentence or document, which we call coarse-grained sentiment analysis. For example, given a review sentence: \"The sound is clean and clear\", the desired output should be positive, without knowing the exact object the reviewer is talking about. For this task, various machine learning classifiers and deep learning models were developed to learn opinion-bearing features for the whole sentence or document to make predictions. However, a mere sentiment score is far from enough to provide all the necessary information for decision making. That's the reason why fine-grained opinion mining becomes more popular in a later stage.   Fine-grained opinion mining, or aspect-based sentiment analysis, mainly focuses on finding the opinion targets (aspects) from a given sentence and the emotions expressed towards them. This is more related to information extraction. Take the previous example review sentence, the model should be able to extract sound as the aspect and clean and clear as the opinion. The extracted information can be\u00a0\u2026"}}