[{"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:J_g5lzvAfSwC", "title": "Video processing from electro-optical sensors for object detection and tracking in a maritime environment: A survey", "authors": ["Dilip K Prasad", "Deepu Rajan", "Lily Rachmawati", "Eshan Rajabally", "Chai Quek"], "description": "We present a survey on maritime object detection and tracking approaches, which are essential for the development of a navigational system for autonomous ships. The electro-optical (EO) sensor considered here is a video camera that operates in the visible or the infrared spectra, which conventionally complements radar and sonar for situational awareness at sea and has demonstrated its effectiveness over the last few years. This paper provides a comprehensive overview of various approaches of video processing for object detection and tracking in the maritime environment. We follow an approach-based taxonomy wherein the advantages and limitations of each approach are compared. The object detection system consists of the following modules: horizon detection, static background subtraction, and foreground segmentation. Each of these has been studied extensively in maritime situations and has been\u00a0\u2026", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [5, 22, 29, 52, 68, 68, 58, 3]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:NaGl4SEjCO4C", "title": "Image colorization using similar images", "authors": ["Raj Kumar Gupta", "Alex Yong-Sang Chia", "Deepu Rajan", "Ee Sin Ng", "Huang Zhiyong"], "description": "We present a new example-based method to colorize a gray image. As input, the user needs only to supply a reference color image which is semantically similar to the target image. We extract features from these images at the resolution of superpixels, and exploit these features to guide the colorization process. Our use of a superpixel representation speeds up the colorization process. More importantly, it also empowers the colorizations to exhibit a much higher extent of spatial consistency in the colorization as compared to that using independent pixels. We adopt a fast cascade feature matching scheme to automatically find correspondences between superpixels of the reference and target images. Each correspondence is assigned a confidence based on the feature matching costs computed at different steps in the cascade, and high confidence correspondences are used to assign an initial set of chromatic\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 7, 10, 10, 27, 32, 41, 44, 48, 45, 32, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:EUQCXRtRnyEC", "title": "Improving image matting using comprehensive sampling sets", "authors": ["Ehsan Shahrian", "Deepu Rajan", "Brian Price", "Scott Cohen"], "description": "In this paper, we present a new image matting algorithm that achieves state-of-the-art performance on a benchmark dataset of images. This is achieved by solving two major problems encountered by current sampling based algorithms. The first is that the range in which the foreground and background are sampled is often limited to such an extent that the true foreground and background colors are not present. Here, we describe a method by which a more comprehensive and representative set of samples is collected so as not to miss out on the true samples. This is accomplished by expanding the sampling range for pixels farther from the foreground or background boundary and ensuring that samples from each color distribution are included. The second problem is the overlap in color distributions of foreground and background regions. This causes sampling based methods to fail to pick the correct samples for foreground and background. Our design of an objective function forces those foreground and background samples to be picked that are generated from well-separated distributions. Comparison on the dataset at and evaluation by www. alphamatting. com shows that the proposed method ranks first in terms of error measures used in the website.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 15, 18, 27, 25, 13, 31, 26, 30, 29, 25, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:iH-uZ7U-co4C", "title": "Random walks on graphs for salient object detection in images", "authors": ["Viswanath Gopalakrishnan", "Yiqun Hu", "Deepu Rajan"], "description": "We formulate the problem of salient object detection in images as an automatic labeling problem on the vertices of a weighted graph. The seed (labeled) nodes are first detected using Markov random walks performed on two different graphs that represent the image. While the global properties of the image are computed from the random walk on a complete graph, the local properties are computed from a sparse k-regular graph. The most salient node is selected as the one which is globally most isolated but falls on a locally compact object. A few background nodes and salient nodes are further identified based upon the random walk based hitting time to the most salient node. The salient nodes and the background nodes will constitute the labeled nodes. A new graph representation of the image that represents the saliency between nodes more accurately, the \u201cpop-out graph\u201d model, is computed further based upon\u00a0\u2026", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 3, 7, 21, 25, 28, 37, 31, 29, 12, 10, 7, 3, 4, 1]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:IjCSPb-OGe4C", "title": "Salient region detection by modeling distributions of color and orientation", "authors": ["Viswanath Gopalakrishnan", "Yiqun Hu", "Deepu Rajan"], "description": "We present a robust salient region detection framework based on the color and orientation distribution in images. The proposed framework consists of a color saliency framework and an orientation saliency framework. The color saliency framework detects salient regions based on the spatial distribution of the component colors in the image space and their remoteness in the color space. The dominant hues in the image are used to initialize an expectation-maximization (EM) algorithm to fit a Gaussian mixture model in the hue-saturation (H-S) space. The mixture of Gaussians framework in H-S space is used to compute the inter-cluster distance in the H-S domain as well as the relative spread among the corresponding colors in the spatial domain. Orientation saliency framework detects salient regions in images based on the global and local behavior of different orientations in the image. The oriented spectral\u00a0\u2026", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 13, 23, 17, 28, 25, 34, 27, 18, 16, 6, 8, 1, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:kNdYIx-mwKoC", "title": "Depth really Matters: Improving Visual Salient Region Detection with Depth.", "authors": ["Karthik Desingh", "K Madhava Krishna", "Deepu Rajan", "CV Jawahar"], "description": "Depth information has been shown to affect identification of visually salient regions in images. In this paper, we investigate the role of depth in saliency detection in the presence of (i) competing saliencies due to appearance,(ii) depth-induced blur and (iii) centre-bias. Having established through experiments that depth continues to be a significant contributor to saliency in the presence of these cues, we propose a 3D-saliency formulation that takes into account structural features of objects in an indoor setting to identify regions at salient depth levels. Computed 3D saliency is used in conjunction with 2D saliency models through non-linear regression using SVM to improve saliency maps. Experiments on benchmark datasets containing depth information show that the proposed fusion of 3D saliency with 2D saliency models results in an average improvement in ROC scores of about 9% over state-of-the-art 2D saliency models.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 11, 17, 28, 14, 15, 23, 25, 21, 23, 1]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:qUcmZB5y_30C", "title": "Salient region detection using weighted feature maps based on the human visual attention model", "authors": ["Yiqun Hu", "Xing Xie", "Wei-Ying Ma", "Liang-Tien Chia", "Deepu Rajan"], "description": "Detection of salient regions in images is useful for object based image retrieval and browsing applications. This task can be done using methods based on the human visual attention model [1], where feature maps corresponding to color, intensity and orientation capture the corresponding salient regions. In this paper, we propose a strategy for combining the salient regions from the individual feature maps based on a new Composite Saliency Indicator (CSI) which measures the contribution of each feature map to saliency. The method also carries out a dynamic weighting of individual feature maps. The experiment results indicate that this combination strategy reflects the salient regions in an image more accurately.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 3, 9, 3, 14, 14, 17, 13, 13, 18, 8, 15, 11, 6, 3, 3, 3, 2, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:ULOm3_A8WrAC", "title": "Simultaneous estimation of super-resolved scene and depth map from low resolution defocused observations", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "This paper presents a novel technique to simultaneously estimate the depth map and the focused image of a scene, both at a super-resolution, from its defocused observations. Super-resolution refers to the generation of high spatial resolution images from a sequence of low resolution images. Hitherto, the super-resolution technique has been restricted mostly to the intensity domain. In this paper, we extend the scope of super-resolution imaging to acquire depth estimates at high spatial resolution simultaneously. Given a sequence of low resolution, blurred, and noisy observations of a static scene, the problem is to generate a dense depth map at a resolution higher than one that can be generated from the observations as well as to estimate the true high resolution focused image. Both the depth and the image are modeled as separate Markov random fields (MRF) and a maximum a posteriori estimation method is\u00a0\u2026", "publication_year": 2003, "citations_by_year": {"year": [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 8, 14, 17, 12, 15, 6, 5, 8, 8, 11, 7, 8, 11, 6, 8, 1, 1, 3, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:pqnbT2bcN3wC", "title": "Multi-objective super resolution: concepts and examples", "authors": ["Deepu Rajan", "Subhasis Chaudhuri", "Manjunath V Joshi"], "description": "Described methods for simultaneously generating the super-resolved depth map and the image from LR observations. Structural information is embedded within the observations and, through the two formulations of DFD and SFS problems, we were able to generate the super-resolved images and the structures. The first method described here avoids correspondence and warping problems inherent in current SR techniques involving the motion cue in the LR observations and uses a more natural depth-related defocus as a natural cue in real aperture imaging. The second method, while again avoiding the correspondence problems, also demonstrates the usefulness of the generalized interpolation scheme leading to more flexibility in the final SR image, in the sense that the LR image can be viewed at SR with an arbitrary light source position. The quality of the super-resolved depth and intensity maps has been\u00a0\u2026", "publication_year": 2003, "citations_by_year": {"year": [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 3, 7, 17, 14, 17, 14, 5, 3, 9, 3, 5, 4, 3, 5, 3, 2, 1, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:9yKSN-GCB0IC", "title": "Random walks on graphs to model saliency in images", "authors": ["Viswanath Gopalakrishnan", "Yiqun Hu", "Deepu Rajan"], "description": "We formulate the problem of salient region detection in images as Markov random walks performed on images represented as graphs. While the global properties of the image are extracted from the random walk on a complete graph, the local properties are extracted from a k-regular graph. The most salient node is selected as the one which is globally most isolated but falls on a compact object. The equilibrium hitting times of the ergodic Markov chain holds the key for identifying the most salient node. The background nodes which are farthest from the most salient node are also identified based on the hitting times calculated from the random walk. Finally, a seeded salient region identification mechanism is developed to identify the salient parts of the image. The robustness of the proposed algorithm is objectively demonstrated with experiments carried out on a large image database annotated with \u201cground-truth\u00a0\u2026", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 10, 6, 13, 16, 17, 13, 9, 4, 5, 5, 4, 4, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:TFP_iSt0sucC", "title": "Weighted color and texture sample selection for image matting", "authors": ["Ehsan Shahrian", "Deepu Rajan"], "description": "Color information is leveraged by color sampling-based matting methods to find the best known samples for foreground and background color of unknown pixels. Such methods do not perform well if there is an overlap in the color distribution of foreground and background regions because color cannot distinguish between these regions and hence, the selected samples cannot reliably estimate the matte. Similarly, alpha propagation based matting methods may fail when the affinity among neighboring pixels is reduced by strong edges. In this paper, we overcome these two problems by considering texture as a feature that can complement color to improve matting. The contribution of texture and color is automatically estimated by analyzing the content of the image. An objective function containing color and texture components is optimized to choose the best foreground and background pair among a set of\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 8, 14, 18, 16, 13, 7, 11, 9, 6, 4, 3, 1]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:tOudhMTPpwUC", "title": "Generalized interpolation and its application in super-resolution imaging", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "In this paper, we present a generalized interpolation scheme for image expansion and generation of super-resolution images. This is done by decomposing the image into appropriate subspaces, carrying out interpolation in individual subspaces and subsequently transforming the interpolated values back to the image domain. Various optical and structural properties of the image, such as 3-D shape of an object, regional homogeneity, local variations in scene reflectivity, etc., can be better preserved during the interpolation process. The motivation for doing so has also been explained theoretically. The generalized interpolation scheme is also shown to be useful in perceptually based high-resolution representation of images where interpolation is done on individual groups as per the perceptual necessity. Further, this scheme is also applied to generation of high-resolution transparencies from low resolution\u00a0\u2026", "publication_year": 2001, "citations_by_year": {"year": [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 6, 3, 4, 6, 5, 4, 4, 5, 3, 8, 6, 9, 7, 6, 6, 6, 3, 6, 4, 4, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:5nxA0vEk-isC", "title": "Foreground motion detection by difference-based spatial temporal entropy image", "authors": ["Guo Jing", "Chng Eng Siong", "Deepu Rajan"], "description": "Human motion detection is a fundamental task for many computer vision tasks. The most popular method for motion detection is background subtraction where a background model needs to be maintained. In this paper an entropy based method for human motion detection is described which makes no use of background model. The difference image between consecutive images are calculated and at each particular pixel, and a spatio-temporal histogram is generated by accumulating pixels in difference image. This histogram is then normalized to calculate entropy and the magnitude of entropy is used to denote the significance of motion. Experiment results demonstrate that our method can detect motion object effectively and reliably.", "publication_year": 2004, "citations_by_year": {"year": [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 5, 9, 4, 2, 8, 11, 7, 7, 8, 6, 1, 9, 2, 9, 2, 1, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:_xSYboBqXhAC", "title": "Human activities recognition using depth images", "authors": ["Raj Gupta", "Alex Yong-Sang Chia", "Deepu Rajan"], "description": "We present a new method to classify human activities by leveraging on the cues available from depth images alone. Towards this end, we propose a descriptor which couples depth and spatial information of the segmented body to describe a human pose. Unique poses (i.e. codewords) are then identified by a spatial-based clustering step. Given a video sequence of depth images, we segment humans from the depth images and represent these segmented bodies as a sequence of codewords. We exploit unique poses of an activity and the temporal ordering of these poses to learn subsequences of codewords which are strongly discriminative for the activity. Each discriminative subsequence acts as a classifier and we learn a boosted ensemble of discriminative subsequences to assign a confidence score for the activity label of the test sequence. Unlike existing methods which demand accurate tracking of 3D joint\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 10, 9, 8, 11, 11, 11, 8, 8, 0, 2, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:ZeXyd9-uunAC", "title": "A split and merge based ellipse detector with self-correcting capability", "authors": ["Alex Yong-Sang Chia", "Susanto Rahardja", "Deepu Rajan", "Maylor Karhang Leung"], "description": "A novel ellipse detector based upon edge following is proposed in this paper. The detector models edge connectivity by line segments and exploits these line segments to construct a set of elliptical-arcs. Disconnected elliptical-arcs which describe the same ellipse are identified and grouped together by incrementally finding optimal pairings of elliptical-arcs. We extract hypothetical ellipses of an image by fitting an ellipse to the elliptical-arcs of each group. Finally, a feedback loop is developed to sieve out low confidence hypothetical ellipses and to regenerate a better set of hypothetical ellipses. In this aspect, the proposed algorithm performs self-correction and homes in on \u201cdifficult\u201d ellipses. Detailed evaluation on synthetic images shows that the algorithm outperforms existing methods substantially in terms of recall and precision scores under the scenarios of image cluttering, salt-and-pepper noise and partial\u00a0\u2026", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 5, 10, 5, 4, 6, 9, 10, 8, 6, 5, 6, 3, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:8k81kl-MbHgC", "title": "Robust subspace analysis for detecting visual attention regions in images", "authors": ["Yiqun Hu", "Deepu Rajan", "Liang-Tien Chia"], "description": "Detecting visually attentive regions of an image is a challenging but useful issue in many multimedia applications. In this paper, we describe a method to extract visual attentive regions in images using subspace estimation and analysis techniques. The image is represented in a 2D space using polar transformation of its features so that each region in the image lies in a 1D linear subspace. A new subspace estimation algorithm based on Generalized Principal Component Analysis (GPCA) is proposed. The robustness of subspace estimation is improved by using weighted least square approximation where weights are calculated from the distribution of K nearest neighbors to reduce the sensitivity of outliers. Then a new region attention measure is defined to calculate the visual attention of each region by considering both feature contrast and geometric properties of the regions. The method has been shown to be\u00a0\u2026", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 4, 7, 7, 11, 7, 5, 4, 2, 2, 3, 1, 3, 4, 4, 0, 2, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:SeFeTyx0c_EC", "title": "An MRF-based approach to generation of super-resolution images from blurred observations", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "This paper presents a new technique for generating a high resolution image from a blurred image sequence; this is also referred to as super-resolution restoration of images. The image sequence consists of decimated, blurred and noisy versions of the high resolution image. The high resolution image is modeled as a Markov random field (MRF) and a maximum a posteriori (MAP) estimation technique is used for super-resolution restoration. Unlike other super-resolution imaging methods, the proposed technique does not require sub-pixel registration of given observations. A simple gradient descent method is used to optimize the functional. The discontinuities in the intensity process can be preserved by introducing suitable line processes. Superiority of this technique to standard methods of image expansion like pixel replication and spline interpolation is illustrated.", "publication_year": 2002, "citations_by_year": {"year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 2, 5, 7, 5, 4, 6, 3, 5, 2, 8, 1, 2, 1, 4, 2, 1, 2, 2, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:LPZeul_q3PIC", "title": "Object detection in a maritime environment: Performance evaluation of background subtraction methods", "authors": ["Dilip K Prasad", "Chandrashekar Krishna Prasath", "Deepu Rajan", "Lily Rachmawati", "Eshan Rajabally", "Chai Quek"], "description": "This paper provides a benchmark of the performance of 23 classical and state-of-the-art background subtraction (BS) algorithms on visible range and near infrared range videos in the Singapore Maritime dataset. Importantly, our study indicates the limitations of the conventional performance evaluation criteria for maritime vision and proposes new performance evaluation criteria that is better suited to this problem. This paper provides insight into the specific challenges of BS in maritime vision. We identify four open challenges that plague BS methods in maritime scenario. These include spurious dynamics of water, wakes, ghost effect, and multiple detections. Poor recall and extremely poor precision of all the 23 methods, which have been otherwise successful for other challenging BS situations, allude to the need for new BS methods custom designed for maritime vision.", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 15, 7, 13, 19, 10, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:abG-DnoFyZgC", "title": "Regularized feature reconstruction for spatio-temporal saliency detection", "authors": ["Zhixiang Ren", "Shenghua Gao", "Liang-Tien Chia", "Deepu Rajan"], "description": "Multimedia applications such as image or video retrieval, copy detection, and so forth can benefit from saliency detection, which is essentially a method to identify areas in images and videos that capture the attention of the human visual system. In this paper, we propose a new spatio-temporal saliency detection framework on the basis of regularized feature reconstruction. Specifically, for video saliency detection, both the temporal and spatial saliency detection are considered. For temporal saliency, we model the movement of the target patch as a reconstruction process using the patches in neighboring frames. A Laplacian smoothing term is introduced to model the coherent motion trajectories. With psychological findings that abrupt stimulus could cause a rapid and involuntary deployment of attention, our temporal model combines the reconstruction error, regularizer, and local trajectory contrast to measure the\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 3, 7, 11, 11, 9, 6, 7, 6, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&citation_for_view=1bqFcwIAAAAJ:3s1wT3WcHBgC", "title": "Improved saliency detection based on superpixel clustering and saliency propagation", "authors": ["Zhixiang Ren", "Yiqun Hu", "Liang-Tien Chia", "Deepu Rajan"], "description": "Saliency detection is useful for high level applications such as adaptive compression, image retargeting, object recognition, etc. In this paper, we introduce an effective region-based solution for saliency detection. We first use the adaptive mean shift algorithm to extract superpixels from the input image, then apply Gaussian Mixture Model (GMM) to cluster superpixels based on their color similarity, and finally calculate the saliency value for each cluster using compactness metric together with modified PageRank propagation. This solution is able to represent the image in a perceptually meaningful way and is robust to over-segmentation. It highlights salient regions with full resolution, well-defined boundary. Experimental results show that both the adaptive mean shift and the modified PageRank algorithm contribute substantially to the saliency detection result. In addition, the ROC analysis demonstrates that our\u00a0\u2026", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 3, 10, 4, 15, 6, 2, 6, 4, 3, 1, 2, 1, 1]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:Tiz5es2fbqcC", "title": "Challenges in video based object detection in maritime scenario using computer vision", "authors": ["Dilip K Prasad", "C Krishna Prasath", "Deepu Rajan", "Lily Rachmawati", "Eshan Rajabaly", "Chai Quek"], "description": "This paper discusses the technical challenges in maritime image processing and machine vision problems for video streams generated by cameras. Even well documented problems of horizon detection and registration of frames in a video are very challenging in maritime scenarios. More advanced problems of background subtraction and object detection in video streams are very challenging. Challenges arising from the dynamic nature of the background, unavailability of static cues, presence of small objects at distant backgrounds, illumination effects, all contribute to the challenges as discussed here.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 2, 6, 8, 12, 10, 12, 6, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:dshw04ExmUIC", "title": "Motion detection with adaptive background and dynamic thresholds", "authors": ["Guo Jing", "Deepu Rajan", "Chng Eng Siong"], "description": "Human motion detection is a fundamental part for many computer vision tasks and various methods have been proposed. Background subtraction is a very popular method where the classification of pixels into motion pixels and background pixels is based on thresholding the difference image between a background image and a current image. The choice of threshold is crucial and many methods have been employed, from a preset threshold for the whole image to adaptive thresholds for each pixel. In this paper, a motion detection method by dynamically thresholding the difference image is proposed. We keep a static background image that is updated periodically, and compare the incoming frame with it to obtain the absolute difference image. If through a preliminary judgement that there are moving objects in the scene, Otsu's threshoding method is used to find the threshold to binarize the difference image\u00a0\u2026", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 5, 4, 3, 10, 14, 4, 3, 5, 0, 3, 1, 3, 2, 1, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:4JMBOYKVnBMC", "title": "A new motion histogram to index motion content in video segments", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "A new motion feature for video indexing is proposed in this paper. The motion content of the video at pixel level, is represented as a Pixel Change Ratio Map (PCRM). The PCRM enables us to capture the intensity of motion in a video sequence. It also indicates the spatial location and size of the moving object. The proposed motion feature is the motion histogram which is a non-uniformly quantized histogram of the PCRM. We demonstrate the usefulness of the motion histogram with three applications, viz., video retrieval, video clustering and video classification.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 5, 1, 7, 9, 6, 4, 5, 3, 1, 6, 3, 2, 1, 1, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:UeHWp8X0CEIC", "title": "Sparse coding for alpha matting", "authors": ["Jubin Johnson", "Ehsan Shahrian Varnousfaderani", "Hisham Cholakkal", "Deepu Rajan"], "description": "Existing color sampling-based alpha matting methods use the compositing equation to estimate alpha at a pixel from the pairs of foreground (F) and background (B) samples. The quality of the matte depends on the selected (F,B) pairs. In this paper, the matting problem is reinterpreted as a sparse coding of pixel features, wherein the sum of the codes gives the estimate of the alpha matte from a set of unpaired F and B samples. A non-parametric probabilistic segmentation provides a certainty measure on the pixel belonging to foreground or background, based on which a dictionary is formed for use in sparse coding. By removing the restriction to conform to (F,B) pairs, this method allows for better alpha estimation from multiple F and B samples. The same framework is extended to videos, where the requirement of temporal coherence is handled effectively. Here, the dictionary is formed by samples from multiple\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 13, 2, 11, 7, 8, 8, 4, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:u5HHmVD_uO8C", "title": "Adaptive hierarchical multi-class SVM classifier for texture-based image classification", "authors": ["Song Liu", "Haoran Yi", "L-T Chia", "Deepu Rajan"], "description": "In this paper, we present a new classification scheme based on support vector machines (SVM) and a new texture feature, called texture correlogram, for high-level image classification. Originally, SVM classifier is designed for solving only binary classification problem. In order to deal with multiple classes, we present a new method to dynamically build up a hierarchical structure from the training dataset. The texture correlogram is designed to capture spatial distribution information. Experimental results demonstrate that the proposed classification scheme and texture feature are effective for high-level image classification task and the proposed classification scheme is more efficient than the other schemes while achieving almost the same classification accuracy. Another advantage of the proposed scheme is that the underlying hierarchical structure of the SVM classification tree manifests the interclass relationships\u00a0\u2026", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 2, 2, 4, 3, 6, 5, 8, 3, 6, 7, 3, 2, 1, 1, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:fPk4N6BV_jEC", "title": "Weighted color and texture sample selection for image matting", "authors": ["Ehsan Shahrian Varnousfaderani", "Deepu Rajan"], "description": "Color sampling based matting methods find the best known samples for foreground and background colors of unknown pixels. Such methods do not perform well if there is an overlap in the color distribution of foreground and background regions because color cannot distinguish between these regions and hence, the selected samples cannot reliably estimate the matte. Furthermore, current sampling based matting methods choose samples that are located around the boundaries of foreground and background regions. In this paper, we overcome these two problems. First, we propose texture as a feature that can complement color to improve matting by discriminating between known regions with similar colors. The contribution of texture and color is automatically estimated by analyzing the content of the image. Second, we combine local sampling with a global sampling scheme that prevents true foreground or\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 6, 7, 10, 6, 9, 1, 5, 1, 4, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:JoZmwDi-zQgC", "title": "Speech emotion recognition with co-attention based multi-level acoustic information", "authors": ["Heqing Zou", "Yuke Si", "Chen Chen", "Deepu Rajan", "Eng Siong Chng"], "description": "Speech Emotion Recognition (SER) aims to help the machine to understand human\u2019s subjective emotion from only audio in-formation. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the pro-posed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub.", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [2, 47, 0]}, "topic": "Speech Emotion Recognition", "agg_topic": "Natural Language Processing", "final_topic": "Artificial Intelligenc"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:9ZlFYXVOiuMC", "title": "Multimodal semantic analysis and annotation for basketball video", "authors": ["Song Liu", "Min Xu", "Haoran Yi", "Liang-Tien Chia", "Deepu Rajan"], "description": "This paper presents a new multiple-modality method for extracting semantic information from basketball video. The visual, motion, and audio information are extracted from video to first generate some low-level video segmentation and classification. Domain knowledge is further exploited for detecting interesting events in the basketball video. For video, both visual and motion prediction information are utilized for shot and scene boundary detection algorithm; this will be followed by scene classification. For audio, audio keysounds are sets of specific audio sounds related to semantic events and a classification method based on hidden Markov model (HMM) is used for audio keysound identification. Subsequently, by analyzing the multimodal information, the positions of potential semantic events, such as \"foul\" and \"shot at the basket,\" are located with additional domain knowledge. Finally, a video annotation\u00a0\u2026", "publication_year": 2006, "citations_by_year": {"year": [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 2, 5, 5, 7, 4, 8, 3, 2, 1, 3, 2, 0, 1, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:M3NEmzRMIkIC", "title": "Adaptive local context suppression of multiple cues for salient visual attention detection", "authors": ["Yiqun Hu", "Deepu Rajan", "L-T Chia"], "description": "Visual attention is obtained through determination of contrasts of low level features or attention cues like intensity, color etc. We propose a new texture attention cue that is shown to be more effective for images where the salient object regions and background have similar visual characteristics. Current visual attention models do not consider local contextual information to highlight attention regions. We also propose a feature combination strategy by suppressing saliency based on context information that is effective in determining the true attention region. We compare our approach with other visual attention models using a novel average discrimination ratio measure.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 1, 1, 5, 2, 7, 8, 5, 2, 6, 2, 3, 1, 1, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:aqlVkmm33-oC", "title": "Data fusion techniques for super-resolution imaging", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "In this paper, we present two new techniques of using data fusion, based on the modality of the data generation process, to generate a super-resolved image from a sequence of low-resolution image intensity data. First, we develop a generalized interpolation scheme wherein an image is decomposed into appropriate subspaces, interpolation is carried out in individual subspaces and subsequently the interpolated values are transformed back to the image domain. Various structural properties of the image, such as 3D shape of an object, regional homogeneity, local variations in scene reflectivity, etc., can be better preserved during the interpolation process. The issue of preserving the structure in an image has received very little attention in the information fusion literature and we concentrate on this issue. In the second method, the data to be fused consist of a sequence of decimated, blurred and noisy versions of\u00a0\u2026", "publication_year": 2002, "citations_by_year": {"year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 3, 1, 3, 8, 4, 4, 3, 3, 4, 1, 2, 3, 2, 0, 2, 2, 1, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:cFHS6HbyZ2cC", "title": "Human action recognition using pose-based discriminant embedding", "authors": ["Behrouz Saghafi", "Deepu Rajan"], "description": "Manifold learning is an efficient approach for recognizing human actions. Most of the previous embedding methods are learned based on the distances between frames as data points. Thus they may be efficient in the frame recognition framework, but they will not guarantee to give optimum results when sequences are to be classified as in the case of action recognition in which temporal constraints convey important information. In the sequence recognition framework, sequences are compared based on the distances defined between sets of points. Among them Spatio-temporal Correlation Distance (SCD) is an efficient measure for comparing ordered sequences. In this paper we propose a novel embedding which is optimum in the sequence recognition framework based on SCD as the distance measure. Specifically, the proposed embedding minimizes the sum of the distances between intra-class sequences\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 7, 4, 11, 7, 3, 1, 5, 2, 2, 2, 2, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:hC7cP41nSMkC", "title": "Salient motion detection in compressed domain", "authors": ["Karthik Muthuswamy", "Deepu Rajan"], "description": "We propose a novel motion saliency framework using DCT coefficients and motion vectors as features from MPEG-2 videos. The DCT coefficients of the luma and chroma components are used to calculate the spatial saliency of a frame while the motion vectors are utilized to refine it. Spatio-temporal similarity maps are calculated separately in order to cater to tracking shots. Results show that the proposed motion saliency framework is computationally nearly an 8-fold improvement over the best performing state-of-the-art method while nearly equalling its performance in identifying salient regions.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 11, 5, 6, 7, 6, 3, 1, 1, 2, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:RHpTSmoSYBkC", "title": "Region-of-interest based image resolution adaptation for mpeg-21 digital item", "authors": ["Yiqun Hu", "Liang-Tien Chia", "Deepu Rajan"], "description": "The upcoming MPEG-21 standard proposes a general framework for augmented use of multimedia services in different network environments, for various users with various terminal devices. In the context of image adaptation, terminals with different screen size limitation require the multimedia adaptation engine to adapt image resources intelligently. Saliency map based visual attention analysis provides some intelligence for finding the attention area within the image. In this paper, we improved the standard MPEG-21 metadata driven adaptation engine by using enhanced saliency map based visual attention model which provides a mean to intelligently adapt JPEG2000 image resolution for different terminal devices with varying screen size according to human visual attention.", "publication_year": 2004, "citations_by_year": {"year": [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 2, 6, 7, 5, 2, 3, 2, 0, 0, 2, 2, 0, 1, 0, 2, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:u9iWguZQMMsC", "title": "Object recognition by discriminative combinations of line segments, ellipses, and appearance features", "authors": ["Alex Yong-Sang Chia", "Deepu Rajan", "Maylor Karhang Leung", "Susanto Rahardja"], "description": "We present a novel contour-based approach that recognizes object classes in real-world scenes using simple and generic shape primitives of line segments and ellipses. Compared to commonly used contour fragment features, these primitives support more efficient representation since their storage requirements are independent of object size. Additionally, these primitives are readily described by their geometrical properties and hence afford very efficient feature comparison. We pair these primitives as shape-tokens and learn discriminative combinations of shape-tokens. Here, we allow each combination to have a variable number of shape-tokens. This, coupled with the generic nature of primitives, enables a variety of class-specific shape structures to be learned. Building on the contour-based method, we propose a new hybrid recognition method that combines shape and appearance features. Each\u00a0\u2026", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 7, 6, 4, 7, 6, 3, 3, 0, 1, 0, 2, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:D03iK_w7-QYC", "title": "Coherent phrase model for efficient image near-duplicate retrieval", "authors": ["Yiqun Hu", "Xiangang Cheng", "Liang-Tien Chia", "Xing Xie", "Deepu Rajan", "Ah-Hwee Tan"], "description": "This paper presents an efficient and effective solution for retrieving image near-duplicate (IND) from image database. We introduce the coherent phrase model which incorporates the coherency of local regions to reduce the quantization error of the bag-of-words (BoW) model. In this model, local regions are characterized by  visual phrase  of multiple descriptors instead of visual word of single descriptor. We propose two types of visual phrase to encode the coherency in feature and spatial domain, respectively. The proposed model reduces the number of false matches by using this coherency and generates sparse representations of images. Compared to other method, the local coherencies among multiple descriptors of every region improve the performance and preserve the efficiency for IND retrieval. The proposed method is evaluated on several benchmark datasets for IND retrieval. Compared to the state-of-the\u00a0\u2026", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 3, 2, 3, 5, 3, 2, 1, 2, 8, 1, 2, 0, 2, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:3fE2CSJIrl8C", "title": "Backtracking ScSPM image classifier for weakly supervised top-down saliency", "authors": ["Hisham Cholakkal", "Jubin Johnson", "Deepu Rajan"], "description": "Top-down saliency models produce a probability map that peaks at target locations specified by a task/goal such as object detection. They are usually trained in a supervised setting involving annotations of objects. We propose a weakly supervised top-down saliency framework using only binary labels that indicate the presence/absence of an object in an image. First, the probabilistic contribution of each image patch to the confidence of an ScSPM-based classifier produces a Reverse-ScSPM (R-ScSPM) saliency map. Neighborhood information is then incorporated through a contextual saliency map which is estimated using logistic regression learnt on patches having high R-ScSPM saliency. Both the saliency maps are combined to obtain the final saliency map. We evaluate the performance of the proposed weakly supervised top-down saliency and achieves comparable performance with fully supervised approaches. Experiments are carried out on 5 challenging datasets across 3 different applications.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 5, 10, 7, 6, 6, 2, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:BqipwSGYUEgC", "title": "Sparse codes as Alpha Matte.", "authors": ["Jubin Johnson", "Deepu Rajan", "Hisham Cholakkal"], "description": "In this paper, image matting is cast as a sparse coding problem wherein the sparse codes directly give the estimate of the alpha matte. Hence, there is no need to use the matting equation that restricts the estimate of \u03b1 from a single pair of foreground (F) and background (B) samples. A probabilistic segmentation provides a confidence value on the pixel belonging to F or B, based on which a dictionary is formed for use in sparse coding. This allows the estimate of \u03b1 from more than just one pair of (F, B) samples. Experimental results on a benchmark dataset show the proposed method performs close to state-of-the-art methods.", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 7, 9, 9, 2, 6, 1, 1, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:AXPGKjj_ei8C", "title": "Deep residual pooling network for texture recognition", "authors": ["Shangbo Mao", "Deepu Rajan", "Liang Tien Chia"], "description": "Current deep learning-based texture recognition methods extract spatial orderless features from pre-trained deep learning models that are trained on large-scale image datasets. These methods either produce high dimensional features or have multiple steps like dictionary learning, feature encoding and dimension reduction. In this paper, we propose a novel end-to-end learning framework that not only overcomes these limitations, but also demonstrates faster learning. The proposed framework incorporates a residual pooling layer consisting of a residual encoding module and an aggregation module. The residual encoder preserves the spatial information for improved feature learning and the aggregation module generates orderless feature for classification through a simple averaging. The feature has the lowest dimension among previous deep texture recognition approaches, yet it achieves state-of-the-art\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [4, 20, 12, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:eflP2zaiRacC", "title": "Generation of super-resolution images from blurred observations using an MRF model", "authors": ["D Rajan", "S Chaudhuri"], "description": null, "publication_year": 2002, "citations_by_year": {"year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 4, 4, 2, 4, 1, 2, 6, 1, 2, 0, 4, 0, 0, 1, 3, 0, 0, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:p2g8aNsByqUC", "title": "MSCM-LiFe: multi-scale cross modal linear feature for horizon detection in maritime images", "authors": ["Dilip K Prasad", "Deepu Rajan", "C Krishna Prasath", "Lily Rachmawati", "Eshan Rajabally", "Chai Quek"], "description": "This paper proposes a new method for horizon detection called the multi-scale cross modal linear feature. This method integrates three different concepts related to the presence of horizon in maritime images to increase the accuracy of horizon detection. Specifically it uses the persistence of horizon in multi-scale median filtering, and its detection as a linear feature commonly detected by two different methods, namely the Hough transform of edgemap and the intensity gradient.We demonstrate the performance of the method over 13 videos comprising of more than 3000 frames and show that the proposed method detects horizon with small error in most of the cases, outperforming three state-of-the-art methods.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 2, 8, 3, 8, 4, 5, 3, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:OU6Ihb5iCvQC", "title": "MuSCoWERT: multi-scale consistence of weighted edge Radon transform for horizon detection in maritime images", "authors": ["Dilip K Prasad", "Deepu Rajan", "Lily Rachmawati", "Eshan Rajabally", "Chai Quek"], "description": "This paper addresses the problem of horizon detection, a fundamental process in numerous object detection algorithms, in a maritime environment. The maritime environment is characterized by the absence of fixed features, the presence of numerous linear features in dynamically changing objects and background and constantly varying illumination, rendering the typically simple problem of detecting the horizon a challenging one. We present a novel method called multi-scale consistence of weighted edge Radon transform, abbreviated as MuSCoWERT. It detects the long linear features consistent over multiple scales using multi-scale median filtering of the image followed by Radon transform on a weighted edge map and computing the histogram of the detected linear features. We show that MuSCoWERT has excellent performance, better than seven other contemporary methods, for 84 challenging maritime\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 2, 3, 5, 6, 6, 4, 3, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:738O_yMBCRsC", "title": "DCT domain watermarking scheme using Chinese Remainder Theorem for image authentication", "authors": ["Jagdish C Patra", "Jiliang E Phua", "Deepu Rajan"], "description": "In this paper, we propose a novel Chinese Remainder Theorem (CRT) based watermarking scheme that works in the Discrete Cosine Transform (DCT) domain. We first reviewed a Singular Value Decomposition (SVD) based watermarking scheme [11] followed by an existing CRT based watermarking scheme [7] that works in the spatial domain and their shortcomings are highlighted. The proposed CRT based scheme is more resistant to different types of attacks, particularly to JPEG compression; in addition, it improves the security feature of the watermarking scheme. Experimental results have shown that the proposed scheme makes the watermark perceptually invisible and has better robustness to common image manipulation techniques such as JPEG compression, brightening and sharpening effects compared to the spatial domain based CRT scheme; in addition, its computational complexity is much lower\u00a0\u2026", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 4, 1, 4, 1, 1, 3, 1, 4, 2, 2, 3, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:qjMakFHDy7sC", "title": "Hybrid shift map for video retargeting", "authors": ["Yiqun Hu", "Deepu Rajan"], "description": "We propose a new method for video retargeting, which can generate spatial-temporal consistent video. The new measure called spatial-temporal naturality preserves the motion in the source video without any motion analysis in contrast to other methods that need motion estimation. This advantage prevents the retargeted video from degenerating due to the propagation of the errors in motion analysis. It allows the proposed method to be applied on challenging videos with complex camera and object motion. To improve the efficiency of the retargeting process, we retarget video using a 3D shift map in low resolution and refine it using an incremental 2D shift map in higher resolution. This new hierarchical framework, denoted as hybrid shift map, can produce satisfactory retargeting results while significantly improving the computational efficiency.", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 4, 5, 2, 6, 2, 4, 0, 1, 2, 0, 0, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:roLk4NBRz8UC", "title": "Semantic video indexing and summarization using subtitles", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "How to build semantic index for multimedia data is an important and challenging problem for multimedia information systems. In this paper, we present a novel approach to build a semantic video index for digital videos by analyzing the subtitle files of DVD/DivX videos. The proposed approach for building semantic video index consists of 3 stages, viz., script extraction, script partition and script vector representation. First, the scripts are extracted from the subtitle files that are available in the DVD/DivX videos. Then, the extracted scripts are partitioned into segments. Finally, the partitioned script segments are converted into a tfidf vector based representation, which acts as the semantic index. The efficiency of the semantic index is demonstrated through video retrieval and summarization applications. Experimental results demonstrate that the proposed approach is very promising.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 3, 1, 2, 1, 2, 0, 4, 1, 4, 3, 2, 1, 0, 1, 3, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:CHSYGLWDkRkC", "title": "Temporally coherent and spatially accurate video matting", "authors": ["Ehsan Shahrian", "Brian Price", "Scott Cohen", "Deepu Rajan"], "description": "Image and video matting are still challenging problems in areas with low foreground\u2010background contrast. Video matting also has the challenge of ensuring temporally coherent mattes because the human visual system is highly sensitive to temporal jitter and flickering. On the other hand, video provides the opportunity to use information from other frames to improve the matte accuracy on a given frame. In this paper, we present a new video matting approach that improves the temporal coherence while maintaining high spatial accuracy in the computed mattes. We build sample sets of temporal and local samples that cover all the color distributions of the object and background over all previous frames. This helps guarantee spatial accuracy and temporal coherence by ensuring that proper samples are found even when distantly located in space or time. An explicit energy term encourages temporal consistency in the\u00a0\u2026", "publication_year": 2014, "citations_by_year": {"year": [2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 4, 4, 0, 5, 1, 1, 1, 5, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:l7t_Zn2s7bgC", "title": "Detection of visual attention regions in images using robust subspace analysis", "authors": ["Yiqun Hu", "Deepu Rajan", "Liang-Tien Chia"], "description": "In this paper, we describe a new framework to extract visual attention regions in images using robust subspace estimation and analysis techniques. We use simple features like hue and intensity endowed with scale adaptivity in order to represent smooth and textured areas in an image. A polar transformation maps homogeneity in the features into a linear subspace that also encodes spatial information of a region. A new subspace estimation algorithm based on the Generalized Principal Component Analysis (GPCA) is proposed to estimate multiple linear subspaces. Sensitivity to outliers is achieved by weighted least squares estimate of the subspaces in which weights calculated from the distribution of K nearest neighbors are assigned to data points. Iterative refinement of the weights is proposed to handle the issue of estimation bias when the number of data points in each subspace is very different. A new region\u00a0\u2026", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [4, 3, 2, 3, 3, 6, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:4fKUyHm3Qg0C", "title": "Video forgery detection and localization using normalized cross-correlation of moment features", "authors": ["M Mathai", "D Rajan", "Sabu Emmanuel"], "description": "Digital technology enabled tampering of digital videos much easier using sophisticated image/video editing software. As a result, the integrity of image/video content can no longer be taken for granted and a number of forensic related issues arise paving the way for many security concerns. So detection of video forgery has become a critical requirement to ensure integrity of video data. A video forgery detection and localization method based on statistical moment features and normalized cross correlation factor is proposed. The features from prediction-error array are calculated for each frame block (set of a certain number of continuous frames in the video). The normalized cross correlation of those features between duplicated frame blocks will be high as compared to other non-duplicated ones. By using calculated threshold, based on mean-squared error, the duplication is confirmed. The location of duplicated\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 4, 2, 5, 4, 4, 3, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:mVmsd5A6BfQC", "title": "Simultaneous estimation of super-resolved intensity and depth maps from low resolution defocused observations of a scene", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "This paper presents a novel technique to simultaneously estimate the depth map and the focused image of a scene, both at a super-resolution, from its defocused observations. Given a sequence of low resolution, blurred and noisy observations of a static scene, the problem is to generate a dense depth map at a resolution higher than one that can be generated from the observations as well as to estimate the true focused, super-resolved image. Both the depth and the intensity maps are modeled as separate Markov random fields (MRF) and a maximum a posteriori estimation method is used to recover the high resolution fields. Since there is no relative motion between the scene and the camera, as is the case with most of the super-resolution and structure recovery techniques, we do away with the correspondence problem.", "publication_year": 2001, "citations_by_year": {"year": [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 4, 4, 2, 0, 0, 2, 0, 3, 2, 2, 1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:SP6oXDckpogC", "title": "Generation of super-resolution images from blurred observations using Markov random fields", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "This paper presents a new technique for generating a high resolution image from a blurred image sequence; this is also referred to as super-resolution restoration of images. The image sequence consists of decimated, blurred and noisy versions of the high resolution image. The high resolution image is modeled as a Markov random field (MRF) and a maximum a posteriori (MAP) estimation technique is used. A simple gradient descent method is used to optimize the functional. Further, line fields are introduced in the cost function and optimization using Graduated Non-Convexity (GNC) is shown to yield improved results. Lastly, we present results of optimization using Simulated Annealing (SA).", "publication_year": 2001, "citations_by_year": {"year": [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 1, 2, 3, 3, 0, 1, 0, 0, 3, 1, 3, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:tkaPQYYpVKoC", "title": "An interpretable Neural Fuzzy Hammerstein-Wiener network for stock price prediction", "authors": ["Chen Xie", "Deepu Rajan", "Quek Chai"], "description": "An interpretable regression model is proposed in this paper for stock price prediction. Conventional offline neuro-fuzzy systems are only able to generate implications based on fuzzy rules induced during training, which requires the training data to be able to adequately represent all system behaviors. However, the distributions of test and training data could be significantly different, e.g., due to drastic data shifts. We address this problem through a novel approach that integrates a neuro-fuzzy system with the Hammerstein-Wiener model forming an indivisible five-layer network, where the implication of the neuro-fuzzy system is realized by the linear dynamic computation of the Hammerstein-Wiener model. The input and output nonlinearities of the Hammerstein-Wiener model are replaced by the nonlinear fuzzification and defuzzification processes of the fuzzy system so that the fuzzy linguistic rules, induced from the\u00a0\u2026", "publication_year": 2021, "citations_by_year": {"year": [2021, 2022, 2023, "unknown"], "num_citations": [0, 9, 14, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:b0M2c_1WBrUC", "title": "Backtracking spatial pyramid pooling-based image classifier for weakly supervised top\u2013down salient object detection", "authors": ["Hisham Cholakkal", "Jubin Johnson", "Deepu Rajan"], "description": "Top-down (TD) saliency models produce a probability map that peaks at target locations specified by a task or goal such as object detection. They are usually trained in a fully supervised (FS) setting involving pixel-level annotations of objects. We propose a weakly supervised TD saliency framework using only binary labels that indicate the presence or absence of an object in an image. First, the probabilistic contribution of each image region to the confidence of a convolutional neural network-based image classifier is computed through a backtracking strategy to produce TD saliency. From a set of saliency maps of an image produced by fast bottom-up (BU) saliency approaches, we select the best saliency map suitable for the TD task. The selected BU saliency map is combined with the TD saliency map. Features having high combined saliency are used to train a linear SVM classifier to estimate feature saliency\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 7, 3, 7, 3, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:e5wmG9Sq2KIC", "title": "Top-down saliency with locality-constrained contextual sparse coding.", "authors": ["Hisham Cholakkal", "Deepu Rajan", "Jubin Johnson"], "description": "We propose a sparse coding based framework for top-down salient object detection in which three locality constraints are integrated. First is the spatial or contextual locality constraint in which features from adjacent regions have similar code, second is the feature-domain locality constraint in which similar features have similar code, and third is the category-domain locality constraint in which features are coded using similar atoms from each partition of the dictionary, where each partition corresponds to an object category. This faster coding strategy produces better saliency maps compared to conventional sparse coding. Proposed codes are max-pooled over a spatial neighborhood for saliency estimation. In spite of its simplicity, the proposed top-down saliency achieves state-of-the-art results at patch-level on two challenging datasets-Graz-02 and PASCAL VOC-07. A novel Gaussian-weighted interpolation further improves pixel-level saliency map derived from the patch-level map.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 5, 3, 7, 3, 2, 0, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:r0BpntZqJG4C", "title": "Object recognition by discriminative combinations of line segments and ellipses", "authors": ["Alex Yong-Sang Chia", "Susanto Rahardja", "Deepu Rajan", "Maylor Karhang Leung"], "description": "We present a contour based approach to object recognition in real-world images. Contours are represented by generic shape primitives of line segments and ellipses. These primitives offer substantial flexibility to model complex shapes. We pair connected primitives as shape tokens, and learn category specific combinations of shape tokens. We do not restrict combinations to have a fixed number of tokens, but allow each combination to flexibly evolve to best represent a category. This, coupled with the generic nature of primitives, enables a variety of discriminative shape structures of a category to be learned. We compare our approach with related methods and state-of-the-art contour based approaches on two demanding datasets across 17 categories. Highly competitive results are obtained. In particular, on the challenging Weizmann horse dataset, we attain improved image classification and object detection\u00a0\u2026", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 4, 1, 3, 2, 5, 2, 1, 1, 0, 0, 0, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:NMxIlDl6LWMC", "title": "Structural descriptors for category level object detection", "authors": ["Alex Yong-Sang Chia", "Susanto Rahardja", "Deepu Rajan", "Maylor KH Leung"], "description": "We propose a new class of descriptors which exhibits the ability to yield meaningful structural descriptions of objects. These descriptors are constructed from two types of image primitives: quadrangles and ellipses. The primitives are extracted from an image based on human cognitive psychology and model local parts of objects. Experiments reveal that these primitives densely cover objects in images. In this regard, structural information of an object can be comprehensively described by these primitives. It is found that a combination of simple spatial relationships between primitives plus a small set of geometrical attributes provide rich and accurate local structural descriptions of objects. Category level object detection of four-legged animals, bicycles, and cars images is demonstrated under scaling, moderate viewpoint variations, and background clutter. Promising results are achieved.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 2, 4, 4, 0, 1, 1, 0, 0, 1, 0, 2, 2, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:2osOgNQ5qMEC", "title": "Affective content detection in sitcom using subtitle and audio", "authors": ["Min Xu", "Liang-Tien Chia", "Haoran Yi", "Deepu Rajan"], "description": "From a personalized media point of view, many users favor a flexible tool to quickly browse the affective content in a video. Such affective content may cause audiences' strong reactions or special emotional experiences, such as anger, sadness, fear, joy and love. This paper attempts to extract affective content for digital videos by analyzing the subtitle files of DVD/DivX videos and utilize audio event to assist affective content detection. Firstly, videos are segmented by dialogue script partition. Compared to traditional video shot, video segmented by scripts is not affected by camera changes and shooting angles and easy to include video segments with compact content. Secondly, emotion-related vocabularies in video script are detected to locate affective video content. Using script to directly access video content avoids complex video analysis. Thirdly, audio event detection is utilized to assist affective content\u00a0\u2026", "publication_year": 2006, "citations_by_year": {"year": [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 3, 2, 5, 1, 1, 0, 4, 0, 0, 1, 0, 1, 1, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:mvPsJ3kp5DgC", "title": "Free-head appearance-based eye gaze estimation on mobile devices", "authors": ["Liu Jigang", "Bu Sung Lee Francis", "Deepu Rajan"], "description": "Eye gaze tracking plays an important role in human-computer interaction applications. In recent years, many research have been performed to explore gaze estimation methods to handle free-head movement, most of which focused on gaze direction estimation. Gaze point estimation on the screen is another important application. In this paper, we proposed a two-step training network, called GazeEstimator, to improve the estimation accuracy of gaze location on mobile devices. The first step is to train an eye landmarks localization network on 300W-LP dataset [1], and the second step is to train a gaze estimation network on GazeCapture dataset [2]. Some processing operations are performed between the two networks for data cleaning. The first network is able to localize eye precisely on the image, while the gaze estimation network use only eye images and eye grids as inputs, and it is robust to facial expressions\u00a0\u2026", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 7, 5, 5, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:VOx2b1Wkg3QC", "title": "Are object detection assessment criteria ready for maritime computer vision?", "authors": ["Dilip K Prasad", "Huixu Dong", "Deepu Rajan", "Chai Quek"], "description": "Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime\u00a0\u2026", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 2, 10, 6, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:0EnyYjriUFMC", "title": "Embedding visual words into concept space for action and scene recognition", "authors": ["Behrouz Saghafi", "Elahe Farahzadeh", "Deepu Rajan", "Andrzej Sluzek"], "description": "In this paper we propose a novel approach to introducing semantic relations into the bag-of-words framework. We use the latent semantic models, such as LSA and pLSA, in order to define semantically-rich features and embed the visual features into a semantic space. The semantic features used in LSA technique are derived from the low-rank approximation of word-document occurrence matrix by SVD. Similarly, by using the pLSA approach, the topic-specific distributions of words can be considered dimensions of a concept space. In the proposed space, the distances between words represent the semantic distances which are used for constructing a discriminative and semantically meaningful vocabulary. We have tested our approach on the KTH action database and on the Fifteen Scene database and have achieved very promising results on both.", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 5, 3, 5, 4, 0, 1, 1, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:XiSMed-E-HIC", "title": "Efficient image retrieval using MPEG-7 descriptors", "authors": ["Wang Surong", "Chia Liang-Tien", "Deepu Rajan"], "description": "In this paper, a new method to calculate the similarity among images using dominant color descriptor is discussed. Using earth mover's distance (EMD), better retrieval results can be obtained compared with those obtained from the original MPEG-7 reference software (XM) [Text of ISO/IEC 15938-6/FDIS Information Technology-Multimedia content description interface-Part 6: Reference Software]. To further improve the retrieval accuracy, texture information from edge histogram descriptor is added. In order to reduce the retrieval time, two different methods which can prune the images far from the query image are discussed. One is the lower bound of EMD, while the other is the M-tree index based on EMD distance. Experiments show that the lower bound is easier to implement and more efficient than the M-tree.", "publication_year": 2003, "citations_by_year": {"year": [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 2, 4, 2, 0, 0, 0, 2, 2, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:dhFuZR0502QC", "title": "Spatiotemporal saliency detection via sparse representation", "authors": ["Zhixiang Ren", "Shenghua Gao", "Deepu Rajan", "Liang-Tien Chia", "Yun Huang"], "description": "Multimedia applications like retrieval, copy detection etc. can gain from saliency detection, which is essentially a method to identify areas in images and videos that capture the attention of the human visual system. In this paper, we propose a new spatiotemporal saliency framework for videos based on sparse representation. For temporal saliency, we model the movement of the target patch as a reconstruction process, and the overlapping patches in neighboring frames are used to reconstruct the target patch. The learned coefficients encode the positions of the matched patches, which are able to represent the motion trajectory of the target patch. We also introduce a smoothing term into our sparse coding framework to learn coherent motion trajectories. Based on the psychological findings that abrupt stimulus could cause a rapid and involuntary deployment of attention, our temporal model combines the\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 2, 2, 1, 3, 3, 1, 3, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:hFOr9nPyWt4C", "title": "A unified approach to detection of shot boundaries and subshots in compressed video", "authors": ["Yi Haoran", "Deepu Rajan", "Chia Liang Tien"], "description": "This paper describes a method to partition a video sequence into shots and subshots. By subshots, we mean one or a combination of the three camera motions of pan, tilt and zoom. The proposed technique detects both hard cuts and gradual transitions in MPEG compressed video using a single technique. We also present a motion estimation algorithm to compute the dominant motion represented by an affine model. The motion information is used to refine the location of dissolves as well as to subdivide the shot into subshots, thus providing a characterization of camera motion. We consider the dissimilarity between the I-, P- and B-frames with respect to the type of macroblocks used for encoding. Unlike previous algorithms reported, our method requires minimal decompression of the video sequence and uses very loose thresholds. The algorithm is evaluated on several types of video sequences to demonstrate its\u00a0\u2026", "publication_year": 2003, "citations_by_year": {"year": [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 5, 1, 6, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:dfsIfKJdRG4C", "title": "Double low rank matrix recovery for saliency fusion", "authors": ["Junxia Li", "Lei Luo", "Fanlong Zhang", "Jian Yang", "Deepu Rajan"], "description": "In this paper, we address the problem of fusing various saliency detection methods such that the fusion result outperforms each of the individual methods. We observe that the saliency regions shown in different saliency maps are with high probability covering parts of the salient object. With image regions being represented by the saliency values of multiple saliency maps, the object regions have strong correlation and thus lie in a low-dimensional subspace. Meanwhile, most of the background regions tend to have lower saliency values in various saliency maps. They are also strongly correlated and lie in a low-dimensional subspace that is independent of the object subspace. Therefore, an image can be represented as the combination of two low rank matrices. To obtain a unified low rank matrix that represents the salient object, this paper presents a double low rank matrix recovery model for saliency fusion. The\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 3, 4, 1, 3, 2, 2, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:u-x6o8ySG0sC", "title": "Particle filter framework for salient object detection in videos", "authors": ["Karthik Muthuswamy", "Deepu Rajan"], "description": "Salient object detection in videos is challenging because of the competing motion in the background, resulting from camera tracking an object of interest, or motion of objects in the foreground. The authors present a fast method to detect salient video objects using particle filters, which are guided by spatio\u2010temporal saliency maps and colour feature with the ability to quickly recover from false detections. The proposed method for generating spatial and motion saliency maps is based on comparing local features with dominant features present in the frame. A region is marked salient if there is a large difference between local and dominant features. For spatial saliency, hue and saturation features are used, while for motion saliency, optical flow vectors are used as features. Experimental results on standard datasets for video segmentation and for saliency detection show superior performance over state\u2010of\u2010the\u2010art\u00a0\u2026", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 7, 2, 2, 3, 0, 1, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:u_35RYKgDlwC", "title": "A split and merge based ellipse detector", "authors": ["Alex YS Chia", "Deepu Rajan", "Maylor KH Leung", "Susanto Rahardja"], "description": "We present an ellipse detector that continually pools lower level information of the edge pixels together to achieve robust detection of the ellipses present in the image. In addition, the parameters of the detected ellipses are continually refined using a close loop system driven by Gestalt psychology. We highlight that we do not rely on the geometrical properties of the ellipses to detect the ellipses. In this aspect, our algorithm is well suited to detect partially occluded ellipses in the image. Experiments on real and synthetic images demonstrate the robustness of our algorithm in which both complete and incomplete ellipses can be detected. In particular, experimental results show that the mean detection accuracy of our algorithm surpasses 92% even with around 90% outliers in the images. This detection performance is superior to that achieved by the robust regression, least squares and the hough transform based\u00a0\u2026", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 5, 1, 2, 3, 0, 1, 1, 0, 1, 0, 0, 2, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:maZDTaKrznsC", "title": "A motion based scene tree for browsing and retrieval of compressed videos", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "This paper describes a fully automatic content-based approach for browsing and retrieval of MPEG-2 compressed video. The first step of the approach is the detection of shot boundaries based on motion vectors available from the compressed video stream. The next step involves the construction of a scene tree from the shots obtained earlier. The scene tree is shown to capture some semantic information as well as to provide a construct for hierarchical browsing of compressed videos. Finally, we build a new model for video similarity based on global as well as local motion associated with each node in the scene tree. To this end, we propose new approaches to camera motion and object motion estimation. The experimental results demonstrate that the integration of the above techniques results in an efficient framework for browsing and searching large video databases.", "publication_year": 2004, "citations_by_year": {"year": [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 3, 0, 4, 2, 2, 1, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:UebtZRa9Y70C", "title": "A classifier-guided approach for top-down salient object detection", "authors": ["Hisham Cholakkal", "Jubin Johnson", "Deepu Rajan"], "description": "We propose a framework for top-down salient object detection that incorporates a tightly coupled image classification module. The classifier is trained on novel category-aware sparse codes computed on object dictionaries used for saliency modeling. A misclassification indicates that the corresponding saliency model is inaccurate. Hence, the classifier selects images for which the saliency models need to be updated. The category-aware sparse coding produces better image classification accuracy as compared to conventional sparse coding with a reduced computational complexity. A saliency-weighted max-pooling is proposed to improve image classification, which is further used to refine the saliency maps. Experimental results on Graz-02 and PASCAL VOC-07 datasets demonstrate the effectiveness of salient object detection. Although the role of the classifier is to support salient object detection, we evaluate its\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 1, 3, 3, 2, 5, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:Mojj43d5GZwC", "title": "An image similarity descriptor for classification tasks", "authors": ["Liangliang Wang", "Deepu Rajan"], "description": "We develop an image similarity descriptor for an image pair, based on deep features. The development consists of two parts - selecting the deep layer whose features are to be included in the descriptor, and a representation of the similarity between the images in the pair. The selection of the deep layer follows a sparse representation of the feature maps followed by multi-output support vector regression. The similarity representation is based on a novel correlation between the histograms of the feature maps of the two images. Experiments to demonstrate the effectiveness of the proposed descriptor are carried out on four applications that can be cast as classification tasks.", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 5, 7, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:B3FOqHPlNUQC", "title": "Salient object detection through over-segmentation", "authors": ["Xuejie Zhang", "Zhixiang Ren", "Deepu Rajan", "Yiqun Hu"], "description": "In this paper we present a salient object detection model from an over-segmented image. The input image is initially segmented by the mean-shift segmentation algorithm and then over-segmented by a quad mesh to even smaller segments. Such segmented regions overcome the disadvantage of using patches or single pixels to compute saliency. Segments that are similar and spread over the image receive low saliency and a segment which is distinct in the whole image or in a local region receives high saliency. We express this as a color compactness measure which is used to derive saliency level directly. Our method is shown to outperform six existing methods in the literature using a saliency detection database containing images with human-labeled object contour ground truth. The proposed saliency model has been shown to be useful for an image retargeting application.", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 5, 1, 4, 0, 1, 1, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:WbkHhVStYXYC", "title": "Scene classification using multiple features in a two-stage probabilistic classification framework", "authors": ["Zhan-Li Sun", "Deepu Rajan", "Liang-Tien Chia"], "description": "Information in the frequency domain is useful in image classification. For natural scene classification, Oliva and Torralba proposed a global feature by sampling the power spectrum of the filtered image. In this paper, we present a hybrid global feature for scene classification. To capture the textural characteristics of the image in the frequency domain, we propose two feature extraction strategies based on gray-level co-occurrence matrices. Both contain statistics of the co-occurrence matrix, but the first one is of a much higher dimension than the second. We demonstrate that the proposed feature is a helpful supplement to the energy feature in terms of increased classification accuracy for real scene images. In order to combine these two kinds of features and further improve the classification accuracy, a posterior probability based two-stage classification method is proposed in which a linear combination of the\u00a0\u2026", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 2, 4, 4, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:sSrBHYA8nusC", "title": "CNN-based gender classification in near-infrared periocular images", "authors": ["Anirudh Manyala", "Hisham Cholakkal", "Vijay Anand", "Vivek Kanhangad", "Deepu Rajan"], "description": "Periocular region has emerged as a key biometric trait with potential applications in the forensics domain. In this paper, we explore two convolutional neural network (CNN)-based approaches for gender classification using near-infrared images of the periocular region. In the first stage, our approaches automatically detect and extract left and right periocular regions. The first approach utilizes a domain-specific pre-trained CNN to extract deep features from the periocular images. A trained support vector machine\u00a0(SVM) then utilizes these features to predict the gender information. The second approach employs an end-to-end classifier obtained by fine-tuning a pre-trained CNN on the periocular images. Performance evaluations have been carried out on three databases, which includes an in-house and two public databases. Local binary pattern and histogram of oriented gradient-based methods have been\u00a0\u2026", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [3, 2, 1, 3, 4, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:tS2w5q8j5-wC", "title": "A linear dynamical system framework for salient motion detection", "authors": ["Viswanath Gopalakrishnan", "Deepu Rajan", "Yiqun Hu"], "description": "Detection of salient motion in a video involves determining which motion is attended to by the human visual system in the presence of background motion that consists of complex visuals that are constantly changing. Salient motion is marked by its predictability compared to the more complex unpredictable motion of the background such as fluttering of leaves, ripples in water, dispersion of smoke, and others. We introduce a novel approach to detect salient motion based on the concept of \u201cobservability\u201d from the output pixels, when the video sequence is represented as a linear dynamical system. The group of output pixels with maximum saliency is further used to model the holistic dynamics of the salient region. The pixel saliency map is bolstered by two region-based saliency maps, which are computed based on the similarity of dynamics of the different spatiotemporal patches in the video with the salient region\u00a0\u2026", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 3, 1, 2, 2, 2, 1, 1, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:nb7KW1ujOQ8C", "title": "Sustained observability for salient motion detection", "authors": ["Viswanath Gopalakrishnan", "Yiqun Hu", "Deepu Rajan"], "description": "Detection of the motion of foreground objects on the backdrop of constantly changing and complex visuals has always been challenging. The motion of foreground objects, which is termed as salient motion, is marked by its predictability compared to the more complex unpredictable motion of the backgrounds like fluttering of leaves, ripples in water, smoke filled environments etc. We introduce a novel approach to detect this salient motion based on the control theory concept of \u2019observability\u2019 from the outputs, when the video sequence is represented as a linear dynamical system. The resulting algorithm is tested on a set of challenging sequences and compared to the state-of-the-art methods to showcase its superior performance on grounds of its computational efficiency and detection capability of the salient motion.", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 4, 3, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:4TOpqqG69KYC", "title": "Global motion compensated key frame extraction from compressed videos", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "A key frame extraction approach, based on change detection of DC images extracted from compressed video, is proposed in this paper. We define a simple pixel change map that captures additional information in a frame with respect to its adjacent frames. Since global motion contributes to pixel changes, falsely indicating the presence of key frames, it is compensated by adaptively filtering the pixel change map using a modified version of the least mean square (LMS) algorithm. The prediction errors thus obtained are used to subsequently select the key frames. The key frames are selected so that the cumulative prediction error is partitioned into equal amounts in each segment. The entire procedure is computationally simple and flexible. Experimental results illustrate the good performance of the proposed algorithm.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 4, 3, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:_kc_bZDykSQC", "title": "Salient motion detection through state controllability", "authors": ["Karthik Muthuswamy", "Deepu Rajan"], "description": "Salient motion detection is a challenging task especially when the motion is obscured by dynamic background motion. Salient motion is characterized by its consistency while the non-salient background motion typically consists of dynamic motion such as fog, waves, fire etc. In this paper, we present a novel framework for identifying salient motion by modelling the video sequence as a linear dynamic system and using controllability of states to estimate salient motion. The proposed saliency detection algorithm is tested on a challenging benchmark video dataset and the performance is compared with other state-of-the-art algorithms. The results of the comparison indicate that the proposed algorithm demonstrates superior performance when compared to other state-of-the-art methods and with higher computational efficiency.", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 2, 5, 0, 2, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:yD5IFk8b50cC", "title": "An authentication watermarking scheme with transaction tracking enabled", "authors": ["S Emmanuel", "AP Vinod", "D Rajan", "CK Heng"], "description": "Business intelligence and customer satisfaction would be two key factors that would be used by the digital media commerce companies. For business intelligence purpose annotations for transaction tracking are required and the customers want that the digital media be authentic. In this paper, we present a novel watermarking scheme that supports authentication and transaction tracking functionalities. The watermarking scheme makes use of finite state machine principles. The proposed scheme is blind and asymmetric as it does not require the original image for watermark extraction and the watermark extraction key is different than its embedding key. The algorithm is implemented and tested for its visual quality, compression overhead, execution time overhead and payload capacity. It is found that the algorithm has high visual quality, high payload capacity, low compression overhead and low execution time\u00a0\u2026", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 1, 0, 1, 3, 1, 2, 1, 2, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:_B80troHkn4C", "title": "A generalized interpolation scheme for image scaling and super-resolution", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": null, "publication_year": 1999, "citations_by_year": {"year": [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 1, 3, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:q3oQSFYPqjQC", "title": "CamType: assistive text entry using gaze with an off-the-shelf webcam", "authors": ["Yi Liu", "Bu-Sung Lee", "Deepu Rajan", "Andrzej Sluzek", "Martin J McKeown"], "description": "As modern assistive technology advances, eye-based text entry systems have been developed to help a subset of physically challenged people to improve their communication ability. However, speed of text entry in early eye-typing system tends to be relatively slow due to dwell time. Recently, dwell-free methods have been proposed which outperform the dwell-based systems in terms of speed and resilience, but the extra eye-tracking device is still an indispensable equipment. In this article, we propose a prototype of eye-typing system using an off-the-shelf webcam without the extra eye tracker, in which the appearance-based method is proposed to estimate people\u2019s gaze coordinates on the screen based on the frontal face images captured by the webcam. We also investigate some critical issues of the appearance-based method, which helps to improve the estimation accuracy and reduce computing\u00a0\u2026", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 7, 1, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:Tyk-4Ss8FVUC", "title": "A learning-based approach for automatic image and video colorization", "authors": ["Raj Kumar Gupta", "Alex Yong-Sang Chia", "Deepu Rajan", "Huang Zhiyong"], "description": "In this paper, we present a color transfer algorithm to colorize a broad range of gray images without any user intervention. The algorithm uses a machine learning-based approach to automatically colorize grayscale images. The algorithm uses the superpixel representation of the reference color images to learn the relationship between different image features and their corresponding color values. We use this learned information to predict the color value of each grayscale image superpixel. As compared to processing individual image pixels, our use of superpixels helps us to achieve a much higher degree of spatial consistency as well as speeds up the colorization process. The predicted color values of the gray-scale image superpixels are used to provide a 'micro-scribble' at the centroid of the superpixels. These color scribbles are refined by using a voting based approach. To generate the final colorization result, we use an optimization-based approach to smoothly spread the color scribble across all pixels within a superpixel. Experimental results on a broad range of images and the comparison with existing state-of-the-art colorization methods demonstrate the greater effectiveness of the proposed algorithm.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 1, 2, 1, 1, 1, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:4OULZ7Gr8RgC", "title": "Efficient 2D viewpoint combination for human action recognition", "authors": ["Behrouz Saghafi", "Deepu Rajan", "Wanqing Li"], "description": "The ability to recognize human actions using a single viewpoint is affected by phenomena such as self-occlusions or occlusions by other objects. Incorporating multiple cameras can help overcome these issues. However, the question remains how to efficiently use information from all viewpoints to increase performance. Researchers have reconstructed a 3D model from multiple views to reduce dependency on viewpoint, but this 3D approach is often computationally expensive. Moreover, the quality of each view influences the overall model and the reconstruction is limited to volumes where the views overlap. In this paper, we propose a novel method to efficiently combine 2D data from different viewpoints. Spatio-temporal features are extracted from each viewpoint and then used in a bag-of-words framework to form histograms. Two different sizes of codebook are exploited. The similarity between the obtained\u00a0\u2026", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 2, 2, 4, 1, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:bFI3QPDXJZMC", "title": "Attention-from-motion: A factorization approach for detecting attention objects in motion", "authors": ["Yiqun Hu", "Deepu Rajan", "Liang-Tien Chia"], "description": "This paper introduces the notion of attention-from-motion in which the objective is to identify, from an image sequence, only those object in motions that capture visual attention (VA). Following the important concept in film production, viz, the tracking shot, we define the attention object in motion (AOM) as those that are tracked by the camera. Three components are proposed to form an attention-from-motion framework: (i) a new factorization form of the measurement matrix to describe dynamic geometry of moving object observed by moving camera; (ii) determination of single AOM based on the analysis of certain structure on the motion matrix; (iii) an iterative framework for detecting multiple AOMs. The proposed analysis of structure from factorization enables the detection of AOMs even when only partial data is available due to occlusion and over-segmentation. Without recovering the motion of either object or camera\u00a0\u2026", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 4, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:_Qo2XoVZTnwC", "title": "Event on demand with MPEG-21 video adaptation system", "authors": ["Min Xu", "Jiaming Li", "Liang-Tien Chia", "Yiqun Hu", "Bu-Sung Lee", "Deepu Rajan", "Jesse S Jin"], "description": "In this paper, we present an event-on-demand (EoD)video adaptation system. The proposed system supports users in deciding their events of interest and considers network conditions to adapt video source by event selection and frame dropping.Firstly, events are detected by audio/video analysis and annotated by the description schemes (DSs)provided by MPEG-7 Multimedia Description Schemes (MDSs). And then, to achieve a generic adaptation solution, the adaptation is developed following MPEG-21 Digital Item Adaptation (DIA)framework. We look at early release of the MPEG-21 Reference Software on XML generation and develop our own system for EoD video adaptation in three steps:1) the event information is parsed from MPEG-7 annotation XML file together with bitstream to generate generic Bitstream Syntax Description (gBSD). 2) Users' preference, Network Characteristic and Adaptation QoS (AQoS\u00a0\u2026", "publication_year": 2006, "citations_by_year": {"year": [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 2, 0, 3, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:35N4QoGY0k4C", "title": "A motion-based scene tree for compressed video content management", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "This paper describes a fully automatic content-based approach for browsing and retrieval of MPEG-2 compressed video. The first step of the approach is the detection of shot boundaries based on motion vectors available from the compressed video stream. The next step involves the construction of a scene tree from the shots obtained earlier. The scene tree is shown to capture some semantic information as well as provide a construct for hierarchical browsing of compressed videos. Finally, we build a new model for video similarity based on global as well as local motion associated with each node in the scene tree. To this end, we propose new approaches to camera motion and object motion estimation. The experimental results demonstrate that the integration of the above techniques results in an efficient framework for browsing and searching large video databases.", "publication_year": 2006, "citations_by_year": {"year": [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 0, 3, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:eQOLeE2rZwMC", "title": "Video saliency detection with robust temporal alignment and local-global spatial contrast", "authors": ["Zhixiang Ren", "Liang-Tien Chia", "Deepu Rajan"], "description": "Video saliency detection, the task to detect attractive content in a video, has broad applications in multimedia understanding and retrieval. In this paper, we propose a new framework for spatiotemporal saliency detection. To better estimate the salient motion in temporal domain, we take advantage of robust alignment by sparse and low-rank decomposition to jointly estimate the salient foreground motion and the camera motion. Consecutive frames are transformed and aligned, and then decomposed to a low-rank matrix representing the background and a sparse matrix indicating the objects with salient motion. In the spatial domain, we address several problems of local center-surround contrast based model, and demonstrate how to utilize global information and prior knowledge to improve spatial saliency detection. Individual component evaluation demonstrates the effectiveness of our temporal and spatial methods\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:ldfaerwXgEUC", "title": "Dynamic programming-based reverse frame selection for VBR video delivery under constrained resources", "authors": ["Dayong Tao", "Jianfei Cai", "Haoran Yi", "Deepu Rajan", "Liang-Tien Chia", "King Ngi Ngan"], "description": "In this paper, we investigate optimal frame-selection algorithms based on dynamic programming for delivering stored variable bit rate (VBR) video under both bandwidth and buffer size constraints. Our objective is to find a feasible set of frames that can maximize the video's accumulated motion values without violating any constraint. It is well known that dynamic programming has high complexity. In this research, we propose to eliminate nonoptimal intermediate frame states, which can effectively reduce the complexity of dynamic programming. Moreover, we propose a reverse frame selection (RFS) algorithm, where the selection starts from the last frame and ends at the first frame. Compared with the conventional dynamic programming-based forward frame selection, the RFS is able to find all of the optimal results for different preloads in one round. We further extend the RFS scheme to solve the problem of frame\u00a0\u2026", "publication_year": 2006, "citations_by_year": {"year": [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:7PzlFSSx8tAC", "title": "An efficient video classification system based on HMM in compressed domain", "authors": ["Yi Haoran", "Deepu Rajan", "Chia Liang-Tien"], "description": "In this paper, we present our method for effective and efficient classification of different types of video that make use of the low level visual features, color and motion features. The color features, similar to the MPEG-7 scalable color descriptors, are the compressed color histograms extracted from the DC images of the video sequences. The motion features, similar to MPEG-7 motion activity descriptors, are extracted from the motion vectors. Both features are extracted from the compressed domain and give a good characterization of the video in both the spatial and temporal dimension. We then classify different types of video using hidden Markov model. First, we train the hidden Markov model for each type of video. Then the trained HMMs are used to classify incoming videos using maximum likelihood classification.", "publication_year": 2003, "citations_by_year": {"year": [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 0, 5, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:P5F9QuxV20EC", "title": "A data-driven approach to understanding skill in photographic composition", "authors": ["Todd S Sachs", "Ramakrishna Kakarala", "Shannon L Castleman", "Deepu Rajan"], "description": "Photography requires not only equipment but also skill to reliably produce aesthetically-pleasing results. It can be argued that, for photography, skill is apparent even without sophisticated equipment. However, no scientific tests have been carried out to confirm that supposition. For that matter, there has been little scientific study on whether skill is apparent, whether it can be discerned by judges in blind tests. We report results of an experiment in which 33 subjects were asked to use identical cameras to photograph each of 7 pre-determined scenes, including a portrait, landscapes, and several man-made objects. Each photograph was then rated in a double-blind manner by 8 judges. Of those judges, 3 are professional photographic experts, and 5 are imaging researchers. The results show that expert judges are able to discern photographic skill to a statistically significant level, but that the enthusiasts, who\u00a0\u2026", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 0, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:GnPB-g6toBAC", "title": "Automatic generation of MPEG-7 compliant XML document for motion trajectory descriptor in sports video", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "The MPEG-7 standard is a step towards standardizing the description of multimedia content so that quick and efficient identification of relevant content can be facilitated, together with efficient management of information. The description definition language (DDL) is a schema language to represent valid MPEG-7 descriptors and description schemes. MPEG-7 instances are XML documents that conform to a particular MPEG-7 schema, as expressed in the DDL and that describe audiovisual content. In this paper, we pick one of the visual descriptors related to motion in a video sequence, viz., motion trajectory. It describes the displacements of objects in time, where an object is defined as a spatiotemporal region or set of spatiotemporal regions. We present a method of automatically extracting trajectories from video sequences and generating an XML document that conforms to the MPEG-7 schema. We use\u00a0\u2026", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 1, 1, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:L8Ckcad2t8MC", "title": "Automatic extraction of motion trajectories in compressed sports videos", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "This paper presents an algorithm for automatically extracting significant motion trajectories in sports videos. Our approach consists of four stages: global motion estimation, motion blob detection, trajectory evolution and trajectory refinement. Global motion is estimated from the motion vectors in the compressed video using an iterative algorithm with robust outlier rejection. A statistical hypothesis test is carried out within the Block Rejection Map(BRM), which is the by-product of the global motion estimation, for the detection of motion blobs. Trajectory evolution is the process in which the motion blobs are either appended to an existing trajectory or are considered to be the beginning of a new trajectory based on its distance to an adaptive trajectory description. Finally, the extracted motion trajectories are refined using a Kalman filter. Experimental results on both indoor and outdoor sports videos demonstrate the\u00a0\u2026", "publication_year": 2004, "citations_by_year": {"year": [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 1, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:d1gkVwhDpl0C", "title": "Maritime situational awareness using adaptive multi-sensor management under hazy conditions", "authors": ["Dilip K Prasad", "C Krishna Prasath", "Deepu Rajan", "Lily Rachmawati", "Eshan Rajabally", "Chai Quek"], "description": "This paper presents a multi-sensor architecture with an adaptive multi-sensor management system suitable for control and navigation of autonomous maritime vessels in hazy and poor-visibility conditions. This architecture resides in the autonomous maritime vessels. It augments the data from on-board imaging sensors and weather sensors with the AIS data and weather data from sensors on other vessels and the on-shore vessel traffic surveillance system. The combined data is analyzed using computational intelligence and data analytics to determine suitable course of action while utilizing historically learnt knowledge and performing live learning from the current situation. Such framework is expected to be useful in diverse weather conditions and shall be a useful architecture to provide autonomy to maritime vessels.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 2, 2, 1, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:KlAtU1dfN6UC", "title": "Background subtraction via coherent trajectory decomposition", "authors": ["Zhixiang Ren", "Liang-Tien Chia", "Deepu Rajan", "Shenghua Gao"], "description": "Background subtraction, the task to detect moving objects in a scene, is an important step in video analysis. In this paper, we propose an efficient background subtraction method based on coherent trajectory decomposition. We assume that the trajectories from background lie in a low-rank subspace, and foreground trajectories are sparse outliers in this background subspace. Meanwhile, the Markov Random Field (MRF) is used to encode the spatial coherency and trajectory consistency. With the low-rank decomposition and the MRF, our method can better handle videos with moving camera and obtain coherent foreground. Experimental results on a video dataset show our method achieves very competitive performance.", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:1sJd4Hv_s6UC", "title": "Click4BuildingID@ NTU: Click for Building Identification with GPS-enabled Camera Cell Phone", "authors": ["Chai Kiat Yeo", "Liang-Tien Chia", "Tat-Jen Cham", "Deepu Rajan"], "description": "A working prototype of a building identification service which can be used on any camera cell phones equipped with GPS capability has been developed. Users can simply snap photos of architectures and send them, together with the corresponding GPS coordinates, via MMS to a remote server. The server will match the photos with the stored, GPS-tagged images using a combination of scale saliency algorithm for feature matching and earth movers distance measure for scene matching. The estimated location and other information are then sent back to the users via MMS. This prototype will have better accuracy than systems which rely solely on photo recognition given the exploitation of GPS information. Moreover, it is computationally lighter since the recognition engine only needs to compare stored images which lie within the GPS coordinates error range. It is relatively inexpensive as no special phones or\u00a0\u2026", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:HDshCWvjkbEC", "title": "Salient object extraction combining visual attention and edge information", "authors": ["Y Hu", "X Xie", "WY Ma", "D Rajan", "LT Chia"], "description": "Object extraction is very useful in many object related image processing applications such as object recognition, object-based image retrieval. In this technical report, we introduce a new scheme for salient object extraction combing visual attention and edge information. The new scheme tides over the defect of visual attention model. On one hand, by combining edge information, the salient region detection can automatically grow or shrink to fit the potential object to improve viewer experience for image adaptation. On the other hand, visual attention cue helps to cluster the salient object edges which can be used to analysis object shape/contour for object-oriented processing. Visual attention provides a mechanism to locate the visual salient part of the image, which reduces the computation complexity of global image-level processing. But due to the lack of object edge information, the visual saliency map of the\u00a0\u2026", "publication_year": 2004, "citations_by_year": {"year": [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:JV2RwH3_ST0C", "title": "Automatic generation of MPEG-7 compliant XML document for motion trajectory descriptor in sports video", "authors": ["Yi Haoran", "Deepu Rajan", "Chia Liang-Tien"], "description": "The MPEG-7 standard is a step towards standardizing the description of multimedia content so that quick and efficient identification of relevant content can be facilitated, together with efficient management of information. The description definition language (DDL) is a schema language to represent valid MPEG-7 descriptors and description schemes. MPEG-7 instances are XML documents that conform to a particular MPEG-7 schema, as expressed in the DDL and that describe audiovisual content. In this paper, we pick one of the visual descriptors related to motion in a video sequence, viz., motion trajectory. It describes the displacements of objects in time, where an object is defined as a spatiotemporal region or set of spatiotemporal regions. We present a method of automatically extracting trajectories from video sequences and generating an XML document that conforms to the MPEG-7 schema. We use sports\u00a0\u2026", "publication_year": 2003, "citations_by_year": {"year": [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:olpn-zPbct0C", "title": "Weakly supervised top-down salient object detection", "authors": ["Hisham Cholakkal", "Jubin Johnson", "Deepu Rajan"], "description": "Top-down saliency models produce a probability map that peaks at target locations specified by a task/goal such as object detection. They are usually trained in a fully supervised setting involving pixel-level annotations of objects. We propose a weakly supervised top-down saliency framework using only binary labels that indicate the presence/absence of an object in an image. First, the probabilistic contribution of each image region to the confidence of a CNN-based image classifier is computed through a backtracking strategy to produce top-down saliency. From a set of saliency maps of an image produced by fast bottom-up saliency approaches, we select the best saliency map suitable for the top-down task. The selected bottom-up saliency map is combined with the top-down saliency map. Features having high combined saliency are used to train a linear SVM classifier to estimate feature saliency. This is integrated with combined saliency and further refined through a multi-scale superpixel-averaging of saliency map. We evaluate the performance of the proposed weakly supervised top-down saliency against fully supervised approaches and achieve state-of-the-art performance. Experiments are carried out on seven challenging datasets and quantitative results are compared with 36 closely related approaches across 4 different applications.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 1, 2, 0, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:f2IySw72cVMC", "title": "Sparse likelihood saliency detection", "authors": ["Minh Chau Hoang", "Deepu Rajan"], "description": "This paper addresses the problem of detection salient regions in images by exploiting the redundancy in image patches. We assume that redundant patches are more likely to be sparsely represented by other patches in the image while salient patches are not. Such sparse likelihood can be measured via L1-minimization by finding the sparse representation of an image patch based on a dictionary constructed using all other patches from the input image. We show that this approach leads to a robust saliency algorithm and the evaluation based on a database of 1000 images demonstrates that our algorithm achieves significant improvement over existing methods.", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:YOwf2qJgpHMC", "title": "Appearance-based action recognition in the tensor framework", "authors": ["Behrouz Saghafi Khadem", "Deepu Rajan"], "description": "There are multiple contributory factors taking place in an action video, e.g., person, clothing, illumination, etc. When these factors change together, conventional 1-mode analysis like PCA in action space encounters difficulties. The N-mode analysis overcomes this problem. In this paper, we propose a novel framework for recognition of actions using silhouettes based on N-mode SVD. We use the silhouette ensembles to form a 3 rd  order tensor comprising three modes: pixels, actions and people. Using N-mode SVD, we find the bases as well as the coefficients for the action space. For a query sequence, the resulting action-mode coefficients are compared with the learned coefficients to find the action class. Through experiments on a common database, we compare the proposed method with 1-mode PCA in appearance-base recognition of human actions and show that our method outperforms 1-mode analysis.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:2P1L_qKh6hAC", "title": "Improved keypoint matching method for near-duplicate keyframe retrieval", "authors": ["Ehsan Younessian", "Deepu Rajan", "Eng Siong Chng"], "description": "We propose a Near-Duplicate Keyframe (NDK) retrieval method that can handle extreme zooming and significant object motion. The first stage consists of eliminating false keypoint matches using symmetric property and a ratio of nearest and second-nearest neighbor distances. Then, a pattern coherency score is assigned to each pair of keyframes. These two features are combined through linear discriminant analysis (LDA) and the separating boundary is trained using SVM. Experiments are carried out for NDK retrieval on the Columbia and NTU datasets. The promising results confirm the effectiveness of our keypoint matching algorithm and show distinguishing power of our proposed features and feature weighting role in NDK retrieval.", "publication_year": 2009, "citations_by_year": {"year": [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:UxriW0iASnsC", "title": "Attention region selection with information from professional digital camera", "authors": ["Song Liu", "Liang-Tien Chia", "Deepu Rajan"], "description": "The attentive region extraction is a challenging issue for semantic interpretation of image and video content. The successful attentive region extraction greatly facilitates image classification, adaptation, compression and retrieval. Different from the traditional visual attention detection models, we propose a new attentive region extraction method based on out-of-focus blurring (OFB) technique used by professional photographers. Firstly, we combine metadata in Exchangeable Image File Format (EXIF) with visual features to quickly select professional photographs from image database. After that, an algorithm is implemented to automatically extract the attentive region from these photographs. This algorithm measures the saliency for individual pixels based on edge distribution of the images. The experimental results on OFB images have proved that our approach is able to overcome the contrast map selection problem\u00a0\u2026", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:hqOjcs7Dif8C", "title": "Semantic analysis of basketball video using motion information", "authors": ["Song Liu", "Haoran Yi", "Liang-Tien Chia", "Deepu Rajan", "Syin Chan"], "description": "This paper presents a new method for extracting semantic information from basketball video. Our approach consists of three stages: shot and scene boundary detection, scene classification and semantic video analysis for event detection. The scene boundary detection algorithm is based on both visual and motion prediction information. After the shot and scene boundary detection, a set of visual and motion features are extracted from scene or shot. The motion features, describing the total motion, camera motion and object motion within the scene respectively, are computed from the motion vector of the compressed video using an iterative algorithm with robust outlier rejection. Finally, the extracted features are used to differentiate offensive/defensive activities in the scenes. By analyzing the offensive/defensive activities, the positions of potential semantic events, such as foul and goal, are located\u00a0\u2026", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=20&pagesize=80&citation_for_view=1bqFcwIAAAAJ:TQgYirikUcIC", "title": "Generalized interpolation for super-resolution", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "In this chapter, we present a generalized interpolation scheme for image expansion and generation of super-resolution images. The underlying idea is to decompose the image into appropriate subspaces, to interpolate each of the subspacs individually and finally, to transform the interpolated values back to the image domain. This method is shown to presere various optical and structural properties of the image, such as 3-D shape of an object, regional homogeneity, local variations in scene reflectivity, etc. The motivation for doing so has also been explained theoretically. The generalized interpolation scheme is also shown to be useful in perceptually based high resolution representation of images where interpolation is done on individual groups as per the perceptual necessity. Further, this scheme is also applied to generation of highresolution transparencies from low resolution transparencies.", "publication_year": 2002, "citations_by_year": {"year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:WA5NYHcadZ8C", "title": "A deep hybrid fuzzy neural hammerstein-wiener network for stock price prediction", "authors": ["Xie Chen", "Deepu Rajan", "Chai Quek"], "description": "A deep hybrid fuzzy neural Hammerstein-Wiener model (FNHW), is proposed in this paper. The implication and inference of a neuro-fuzzy is based on the fuzzy rulebase that has been formed during traning. It requires the training data to be able to adequately represent entire system behaviors. However, the test data may vary with distribution shift in time series domain. Furthers, the training data may be derived from steady-state while the test data which is in the form of dynamically changing represented by drastic data shift under certain scenario such as financial crisis. A hybrid approach is proposed to employ neuro-fuzzy system to make prediction on steady-state data in parallel with the Hammerstein-Wiener model to predict the dynamically changing behavior. This is implemented by MLP as the control unit to decide the scale of contribution that each system is made to the final prediction. By doing this, the\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 2, 2, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:R3hNpaxXUhUC", "title": "Multi-modal fusion for associated news story retrieval", "authors": ["Ehsan Younessian", "Deepu Rajan"], "description": "In this paper, we investigate multi-modal approaches to retrieve associated news stories sharing the same main topic. In the visual domain, we employ near duplicate keyframe/scene detection method using local signatures to identify stories with mutual visual cues. Further, to improve the effectiveness of visual representation, we develop a semantic signature that contains pre-defined semantic visual concepts in a news story. We propose a visual concept weighting scheme to combine local and semantic signature similarities to obtain the enhanced visual content similarity. In the textual domain, we utilize Automatic Speech Recognition (ASR) and refined Optical Character Recognition (OCR) transcripts and determine the enhanced textual similarity using the proposed semantic similarity measure. To fuse textual and visual modalities, we investigate different early and late fusion approaches. In the proposed\u00a0\u2026", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 1, 1, 0, 1, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:QIV2ME_5wuYC", "title": "Scale adaptive visual attention detection by subspace analysis", "authors": ["Yiqun Hu", "Deepu Rajan", "Liang-Tien Chia"], "description": "We describe a method to extract visual attention regions in images by robust subspace analysis from simple feature like intensity endowed with scale adaptivity in order to represent textured areas in an image. The scale adaptive descriptor is mapped onto clusters in linear spaces. A new subspace estimation algorithm based on the Generalized Principal Component Analysis (GPCA) is proposed to estimate multiple linear subspaces. The visual attention of each region is calculated using a new region attention measure that considers feature contrast and spatial geometric properties. Compared with existing visual attention detection methods, the proposed method directly measures global visual attention at the region level as opposed to pixel level.", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:70eg2SAEIzsC", "title": "An event-driven sports video adaptation for the MPEG-21 DIA framework", "authors": ["Min Xu", "Jiaming Li", "Yiqun Hu", "Liang-Tien Chia", "Bu-Sung Lee", "Deepu Rajan", "Jianfei Cai"], "description": "We present an event-driven video adaptation system in this paper. Events are detected by audio/video analysis and annotated by the description schemes (DSs) provided by MPEG-7 multimedia description schemes (MDSs). And then, adaptation take account of users' preference of events and network characteristic to adapt video by event selection and frame dropping as following three steps: 1) the event information is parsed from MPEG-7 annotation XML file together with bitstream to generate generic bitstream syntax description (gBSD), 2) users' preference, network characteristic and adaptation QoS (AQoS) are considered for making adaptation decision, 3) adaptation engine automatically parses adaptation decisions and gBSD to achieve adaptation. Different from most existing adaptation work, the system adapts video by interesting events according to users' preference. To achieve a generic adaptation\u00a0\u2026", "publication_year": 2006, "citations_by_year": {"year": [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:Y0pCki6q_DkC", "title": "Locality and context\u2010aware top\u2010down saliency", "authors": ["Junxia Li", "Deepu Rajan", "Jian Yang"], "description": "In this study, the authors propose a novel framework for top\u2010down (TD) saliency detection, which is well suited to locate category\u2010specific objects in natural images. Saliency value is defined as the probability of a target based on its visual feature. They introduce an effective coding strategy called locality constrained contextual coding (LCCC) that enforces locality and contextual constraints. Furthermore, a contextual pooling operation is presented to take advantages of feature contextual information. Benefiting from LCCC and contextual pooling, the obtained feature representation has high discriminative power, which makes the authors' saliency detection method achieving competitive results with existing saliency detection algorithms. They also include bottom\u2010up cues into their framework to supplement the proposed TD saliency algorithm. Experimental results on three datasets (Graz\u201002, Weizmann Horse and\u00a0\u2026", "publication_year": 2018, "citations_by_year": {"year": [2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [2, 1, 0, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:zYLM7Y9cAGgC", "title": "Dynamic distortion maps for image retargeting", "authors": ["Xuejie Zhang", "Yiqun Hu", "Deepu Rajan"], "description": "We present an image retargeting method that incorporates image content distortion into a mesh optimization process through the generation of dynamic distortion maps. The warping process is driven by the distortion produced by the warping process itself. We retarget the image through an iterative mesh optimization process to minimize the visual distortion. An adaptive distortion map is iteratively constructed to describe the visual distortion between the original image and the retargeted image. The mesh mapping from the source image to the retargeted image is optimized through an energy minimization process. The objective of the optimization is to allow the distortion produced by the retargeting process to be distributed to smooth and highly textured regions which cause less visual distortion while preserving the geometrical structure of the mesh at regions that may cause distortion after retargeting. Experimental\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:K3LRdlH-MEoC", "title": "Unsupervised feature selection for salient object detection", "authors": ["Viswanath Gopalakrishnan", "Yiqun Hu", "Deepu Rajan"], "description": "Feature selection plays a crucial role in deciding the salient regions of an image as in any other pattern recognition problem. However the problem of identifying the relevant features that plays a fundamental role in saliency of an image has not received much attention so far. We introduce an unsupervised feature selection method to improve the accuracy of salient object detection. The noisy irrelevant features in the image are identified by maximizing the mixing rate of a Markov process running on a linear combination of various graphs, each representing a feature. The global optimum of this convex problem is achieved by maximizing the second smallest eigen value of the graph Laplacian via semi-definite programming. The enhanced image graph model, after the removal of irrelevant features, is shown to improve the salient object detection performance on a large image data base with annotated \u2018ground\u00a0\u2026", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:RYcK_YlVTxYC", "title": "Image classification: Are rule-based systems effective when classes are fixed and known?", "authors": ["Palaiahnakote Shivakumara", "Deepu Rajan", "Suresh A Sadananthan"], "description": "In this paper, we investigate if rule-based systems are useful for image classification problems when the number of classes is fixed. The rules are derived from simple edge features such as width and straightness. A class representative is calculated for each class according to the average percentage of edges that satisfy the rule for a particular class. This percentage for an unknown image is compared to the class representative to assign a label to it. The proposed system does not require extensive feature extraction and classification techniques. It is shown that the rule based system outperforms some of the reported results on scene classification.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:RGFaLdJalmkC", "title": "Advances in Multimedia Modeling: 13th International Multimedia Modeling Conference, MMM 2007, Singapore, January 9-12, 2007, Proceedings, Part II", "authors": ["Tat-Jen Cham", "Jianfei Cai", "Chitra Dorai", "Deepu Rajan", "Tat-Seng Chua", "Liang-Tien Chia"], "description": "The two volume set LNCS 4351 and LNCS 4352 constitutes the refereed proceedings of the 13th International Multimedia Modeling Conference, MMM 2007, held in Singapore in January 2007. Based on rigorous reviewing, the program committee selected 123 carefully revised full papers of the main technical sessions and 33 revised full papers of four special sessions from a total of 392 submissions for presentation in two volumes.", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:MXK_kJrjxJIC", "title": "ARIRS: association rule based image retrieval system", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "A new image retrieval system based on association rule (ARIRS) is described in this paper. Association rules have been used in application such as market basket analysis to capture relationships presented among items in large data sets. It is shown that association rules are able to find the frequent item pairs. In the image retrieval system, user\u2019s interaction is a very important part. But it is not easy for the user to specify their query. Relevance feedback is a commonly used technique for the image retrieval system to interactively interpret the user\u2019s desire. Here, we present a new scheme for mining the relevance feedback using association rules. The proposed scheme automatically establish the semantic association among images by mining the previous users\u2019 browsing and relevance feedback. Then images are retrieved based on their semantic association. Experimental results show promising performance of the proposed scheme.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:ye4kPcJQO24C", "title": "An embedded deep fuzzy association model for learning and explanation", "authors": ["Chen Xie", "Deepu Rajan", "Dilip K Prasad", "Chai Quek"], "description": "This paper explores the complementary benefits of embedding a deep learning model as a fully data-driven fuzzy implication operator of a five-layer neuro-fuzzy system for learning and explanations for the predictions of both steady-state and dynamically changing data. In traditional Mandani-type neuro-fuzzy systems, the entailment performed by the implication is realized using the fuzzy implication operator based on the fuzzy rules formed in the rule base during encoding and recall. Given the presence of a group of test data that are significantly different from the training data, the realization of entailments through the use of the implication operator based on the fuzzy rules formed in traditional neuro-fuzzy systems may not be adequate. This paper attempts to adopt a more direct approach by embedding a deep learning model in the neuro-fuzzy system to serve as a fuzzy implication operator, thereby allowing the\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 3, 0]}, "topic": "Deep Learning", "agg_topic": "Adversarial Attacks on Neural Network", "final_topic": "Artificial Intelligenc"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:9vf0nzSNQJEC", "title": "Smart interpretable model (SIM) enabling subject matter experts in rule generation", "authors": ["Hotman Christianto", "Gary Kee Khoon Lee", "Zhou Weigui Jair", "Henry Kasim", "Deepu Rajan"], "description": "Current Artificial Intelligence (AI) technologies are widely regarded as black boxes, whose internal structures are not inherently transparent, even though they provide powerful prediction capabilities. Having a transparent model that enables users to understand its inner workings allows them to appreciate the learning and inference process, leading to trust and higher confidence in the model. While methods that help with interpretability have been created, most of them require the user to have a certain level of AI knowledge and do not allow a user to fine-tune them based on prior knowledge. In this paper, we present a smart interpretable model (SIM) framework that requires little to no AI knowledge and can be used to create a set of fuzzy IF-THEN rules along with its corresponding membership functions at ease. The framework also allows users to incorporate prior knowledge during various steps in the framework\u00a0\u2026", "publication_year": 2022, "citations_by_year": {"year": [2022, 2023, "unknown"], "num_citations": [0, 3, 0]}, "topic": "Interpretability and Explainable AI", "agg_topic": "Explainable AI", "final_topic": "Artificial Intelligenc"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:HE397vMXCloC", "title": "Saliency-based bit plane detection for network applications", "authors": ["Maryam Asadzadeh Kaljahi", "Palaiahnakote Shivakumara", "Saqib Hakak", "Mohd Yamani Idna Idris", "Mohammad Hossein Anisi", "Deepu Rajan"], "description": "Transmitting image data without losing significant information is challenging for any network application especially when large color images are transmitted through TCP communication protocol. This is due to network limitations such as buffer overflow, underflow and network traffic flow etc. This paper presents a new method for image size reduction such that the network can transmit data without much loss of information, and hence, quality. The proposed method obtains bit planes for the color input images, which results in eight binary planes. Unlike the existing bit plane based image size reduction methods, which assume that the most significant plane or some other planes contain useful information, the proposed method finds the plane that contains dominant information automatically. For each plane, the proposed method explores the saliency that finds dominant information based on Markov Chain Process\u00a0\u2026", "publication_year": 2020, "citations_by_year": {"year": [2020, 2021, 2022, 2023, "unknown"], "num_citations": [1, 2, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:bEWYMUwI8FkC", "title": "Feasibility analysis of eye typing with a standard webcam", "authors": ["Yi Liu", "Bu Sung Lee", "Andrzej Sluzek", "Deepu Rajan", "Martin Mckeown"], "description": "With the development of assistive technology, eye typing has become an alternative form of text entry for physically challenged people with severe motor disabilities. However, additional eye-tracking devices need to be used to track eye movements which is inconvenient in some cases. In this paper, we propose an appearance-based method to estimate the person\u2019s gaze point using a webcam, and also investigate some practical issues of the method. The experimental results demonstrate the feasibility of eye typing using the proposed method.", "publication_year": 2016, "citations_by_year": {"year": [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 1, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:rO6llkc54NcC", "title": "Using texture to complement color in image matting", "authors": ["Ehsan Shahrian", "Deepu Rajan"], "description": "Current image matting methods based on color sampling use color to distinguish between foreground and background pixels. However, they fail when the corresponding color distributions overlap. Other methods that define correlation between neighboring pixels based on color aim to propagate the opacity parameter \u03b1 from known pixels to unknown pixels. However, strong edges of textured regions may block the propagation of \u03b1. In this paper, a new matting strategy is proposed that delivers an accurate matte by considering texture as a feature that can complement color even if the foreground and background color distributions overlap and the image is a complex one with highly textured regions. The texture feature is extracted in such a way as to increase distinction between foreground and background regions. An objective function containing color and texture components is optimized to find the best\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:k_IJM867U9cC", "title": "Multi-modal solution for unconstrained news story retrieval", "authors": ["Ehsan Younessian", "Deepu Rajan"], "description": "We propose a multi-modal approach to retrieve associated news stories sharing the same main topic. In the textual domain, we utilize Automatic Speech Recognition (ASR) and refined Optical Character Recognition (OCR) transcripts while in the visual domain we employ a Near Duplicate Keyframe detection method to identify stories with common visual clues. In addition, we adopt another visual representation namely semantic signature, indicating pre-defined semantic concepts included in the news story, to improve the discriminativness of visual modality. We propose a query-class weighting scheme to integrate the retrieval outcomes gained from visual modalities. Experimental results show the distinguishing power of the enhanced representation in individual modalities and the superiority of our fusion approach performance compared to existing strategies.", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:W7OEmFMy1HYC", "title": "Image retargeting in compressed domain", "authors": ["OV Ramana Murthy", "Karthik Muthuswamy", "Deepu Rajan", "Chia Liang Tien"], "description": "A simple algorithm for image retargeting in the compressed domain is proposed. Most existing retargeting algorithms work directly in the spatial domain of the raw image. Here, we work on the DCT coefficients of a JPEG-compressed image to generate a gradient map that serves as an importance map to help identify those parts in the image that need to be retained during the retargeting process. Each 8\u00d78 block of DCT coefficients is scaled based on the least importance value. Retargeting can be done both in the horizontal and vertical directions with the same framework. We also illustrate image enlargement using the same method. Experimental results show that the proposed algorithm produces less distortion in the retargeted image compared to some other algorithms reported recently.", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:ufrVoPGSRksC", "title": "Category-level detection based on object structures", "authors": ["Alex YS Chia", "Deepu Rajan", "Maylor KH Leung", "Susanto Rahardja"], "description": "We present a new class of descriptors which exhibit the ability to yield meaningful structural description of the object. These descriptors are constructed by harnessing the geometrical relationships and spatial configurations between two types of image primitives: Quadrangles and ellipses. Specifically, we extract the line segments from the line edge map of the image and exploit the spatial qualities of the line segments and the salient colors of the image to construct the quadrangles. The ellipses are extracted with a close loop system that is driven by Gestalt Psychology. Experimental results show very good performance for category-level object detection in which the objects in each category exhibit variations in form, scale and viewpoint.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:ns9cj8rnVeAC", "title": "Differential pixel-based low-power and high-speed implementation of DCT for on-board satellite image processing", "authors": ["A Prasad Vinod", "Deepu Rajan", "Ankita Singla"], "description": "Low-power and high-speed discrete cosine transform (DCT) implementation of the images captured by the satellites presents a hardware design problem. The cost of the DCT implementation is dominated by the complexity of the multiplication of input data (image) with the DCT matrix. The techniques for minimising the complexity of multiplication by employing a differential pixel method are presented. In the proposed method 8\u00d78 blocks of input image matrix are considered, the difference between the adjacent pixels is calculated and those differential pixels are used in DCT transformation. Synthesis results on 0.18\u2005\u00b5m CMOS technology show that the proposed method gives an average of 13.2% reduction in power consumption and 10.9% improvement in speed over the conventional method. The proposed method can also be combined with the common subexpression elimination method for further reduction.", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:uWQEDVKXjbEC", "title": "Super-resolution imaging using blur as a cue", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "In this chapter, we present a parametric method for generating a super-resolution image from a sequence consisting of blurred and noisy observations. The high resolution image is modeled as a Markov random field (MRF) and a maximum a posteriori (MAP) estimation technique is used for super-resolution restoration. Unlike other super-resolution imaging methods, the proposed technique does not require sub-pixel registration of given observations. A simple gradient descent method is used to optimize the cost. The discontinuities in the intensity process can be preserved by introducing suitable line processes. Superiority of this technique to standard methods of image interpolation is illustrated. The motivation for using blur as a cue is also explained.", "publication_year": 2002, "citations_by_year": {"year": [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:1yQoGdGgb4wC", "title": "Controllable Video Generation with Text-based Instructions", "authors": ["Ali K\u00f6ksal", "Kenan E Ak", "Ying Sun", "Deepu Rajan", "Joo Hwee Lim"], "description": "Most of the existing studies on controllable video generation either transfer disentangled motion to an appearance without detailed control over motion or generate videos of simple actions such as the movement of arbitrary objects conditioned on a control signal from users. In this study, we introduce Controllable Video Generation with text-based Instructions (CVGI) framework that allows text-based control over action performed on a video. CVGI generates videos where hands interact with objects to perform the desired action by generating hand motions with detailed control through text-based instruction from users. By incorporating the motion estimation layer, we divide the task into two sub-tasks: (1) control signal estimation and (2) action generation. In control signal estimation, an encoder models actions as a set of simple motions by estimating low-level control signals for text-based instructions with given initial\u00a0\u2026", "publication_year": 2023, "citations_by_year": {"year": [2023, "unknown"], "num_citations": [2, 0]}, "topic": "Computer Vision", "agg_topic": "Artificial Intelligenc", "final_topic": "Federated Learning"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:wbdj-CoPYUoC", "title": "Using eye gaze to play mind card-game using neural network", "authors": ["Qingyao Hu", "Seanglidet Yean", "Jigang Liu", "Bu Sung Lee", "Deepu Rajan", "Romphet Phattharaphon", "SEANGLID002 QHU003"], "description": "Since the invention of smartphones, touch has always been the primary interface for humans to interact with smartphones and tablets. As technology advances, researchers have been exploring other forms of interfaces such as voice control, virtual reality, handwriting digitizers, etc. One of the least explored and potential human-computer interfaces is eye gaze tracking which is the method of measuring the point of gaze. Studies of eye movement and eye tracking has been attracting interests from researchers and has progressed from eye-attached tracking to optical tracking and currently machine learning based tracking.In this paper, we proposed two ResNet-based convolutional neural network (CNN) models for card prediction which is a classification problem. Eye gazes are categorized into 32 areas whereby each eye gaze corresponds to an area on the mobile device\u2019s display. The experimental results show that we can achieve accuracy as high as 81%.", "publication_year": 2019, "citations_by_year": {"year": [2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 2, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:_FxGoFyzp5QC", "title": "L1-regularized reconstruction error as alpha matte", "authors": ["Jubin Johnson", "Hisham Cholakkal", "Deepu Rajan"], "description": "Sampling-based alpha matting methods have traditionally followed the compositing equation to estimate the alpha value at a pixel from a pair of foreground (F) and background (B) samples. The (F,B) pair that produces the least reconstruction error is selected, followed by alpha estimation. The significance of that residual error has been left unexamined. In this letter, we propose a video matting algorithm that uses L1-regularized reconstruction error of F and B samples as a measure of the alpha matte. A multiframe nonlocal means framework using coherency sensitive hashing is utilized to ensure temporal coherency in the video mattes. Qualitative and quantitative evaluations on a dataset exclusively for video matting demonstrate the effectiveness of the proposed matting algorithm.", "publication_year": 2017, "citations_by_year": {"year": [2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 1, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:pyW8ca7W8N0C", "title": "Temporal trimap propagation using motion-assisted shape blending", "authors": ["Jubin Johnson", "Deepu Rajan", "Hisham Cholakkal"], "description": "In digital matting, the availability of an accurate trimap is essential for pulling an accurate matte. This requirement becomes all the more critical in video matting where temporal coherence of the trimap is an added requirement. However, the task of manually drawing a trimap for every frame is not feasible. This paper proposes an adaptive trimap propagation framework that alleviates this task by automatically generating trimaps between two key-frames. 2-D shape blending is coupled with motion flow using a novel strategy based on angle criteria to generate accurate in-between foreground contours for the entire video sequence. A robust trimap generation step ensures temporal coherence between the contours. Quantitative and qualitative comparisons on complex video sequences demonstrates the effectiveness of the proposed method.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:g5m5HwL7SMYC", "title": "New edge characteristics for scene and object classification", "authors": ["Palaiahnakote Shivakumara", "Deepu Rajan", "Suresh Anand Sadananthan"], "description": "In this paper, we show that simple edge characteristics in images, when judiciously combined, can result in improved scene and object classification. Unlike existing methods that require a large number of training samples and complex learning schemes, our method discovers simple edge properties. We introduce three sets of edge properties, namely, centroid, compactness and aspect ratio of edges in the image. The combinations of these edge properties are used to discriminate among images in each class. A class representative is calculated for each class according to the average percentage of edges that satisfy the property of a particular class. This percentage for an unknown image is compared to the class representative to assign a label to it. It is shown that this simple edge properties-based method outperforms some of the state-of-the-art results on scene and object classification on standard databases.", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:O3NaXMp0MMsC", "title": "Scene signatures for unconstrained news video stories", "authors": ["Ehsan Younessian", "Deepu Rajan"], "description": "We propose a novel video signature called scene signature which is defined as a collection of SIFT descriptors. A scene signature represents the visual cues from a video scene in a compact and comprehensive manner. We detect Near Duplicate Keyframe clusters within a news story and then for each of them we generate an initial scene signature including most informative mutual and distinctive visual cues. Compared to conventional keypoint-trajectory-based signatures, we take the co-occurrence of SIFT keypoints into account. Moreover, we utilize keypoints describing novel visual clues in the scene. Next, through three steps of refinements on the initial scene signature we shorten the semantic gap to obtain more compact and semantically meaningful scene signatures. The experimental results confirm the efficiency, robustness and uniqueness of our proposed scene signature compared to other global\u00a0\u2026", "publication_year": 2012, "citations_by_year": {"year": [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:-f6ydRqryjwC", "title": "Multi-view Clustering of Visual Words Using Canonical Correlation Analysis for Human Action Recognition", "authors": ["Behrouz Saghafi", "Deepu Rajan"], "description": "In this paper we propose a novel approach for introducing semantic relations into the bag-of-words framework for recognizing human actions. We represent visual words in two different views: the original features and the document co-occurrence representation. The latter view conveys semantic relations but is large, sparse and noisy. We use canonical correlation analysis between the two views to find a subspace in which the words are more semantically distributed. We apply k-means clustering in the computed space to find semantically meaningful clusters and use them as the semantic visual vocabulary. Incorporating the semantic visual vocabulary the features are quantized to form more discriminative histograms. Eventually the histograms are classified using an SVM classifier. We have tested our approach on KTH action dataset and achieved promising results.", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:YFjsv_pBGBYC", "title": "Understanding Photographic Composition through Data-driven Approaches.", "authors": ["Dansheng Mao", "Ramakrishna Kakarala", "Deepu Rajan", "Shannon Lee Castleman"], "description": "Many elements contribute to a photograph\u2019s aesthetic value, include context, emotion, color, lightness, and composition. Of those elements, composition, which is how the arrangement of subjects, background, and features work together, is both highly challenging, and yet amenable, for understanding with computer vision techniques. Choosing famous monochromic photographs for which the composition is the dominant aesthetic contributor, we have developed data-driven approaches to understand composition. We obtain two novel results. The first shows relationships between the composition styles of master photographers based on their works, as obtained by analyzing extracted SIFT features. The second result, which relies on data obtained from eye-tracking equipment on both expert photographers and novices, shows that there are significant differences between them in what is salient in a photograph\u2019s composition.", "publication_year": 2010, "citations_by_year": {"year": [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:BrmTIyaxlBUC", "title": "Optimum delay order selection for linear equalization problems", "authors": ["Chng Eng Siong", "Sheng Chen", "Deepu Rajan"], "description": "This paper shows that the BER performance using linear equalizer for channel equalization problem is significantly dependent on delay order. To obtain optimum performance, the equalizer output should be derived from the equalizer with delay order having the best BER performance. An efficient method to evaluate the upper bound BER performance of a linear equalizer to find the optimum delay is proposed. The method is novel as the evaluation is performed using only the channel statistics and the equalizer\u2019s weights.", "publication_year": 2003, "citations_by_year": {"year": [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:blknAaTinKkC", "title": "A perceptually organized method for image interpolation", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "Perception of an image depends on its visual representation. In this paper we present a perceptually organized scheme for image expansion or scattered data interpolation. This is done by decomposing the image (or data) into appropriate perceptual groups, carrying out the interpolation in individual groups as per the perceptual necessity and subsequently transforming the interpolated values back to the image domain. Various perceptual properties of the image, such as the 3D shape of an object, textural homogeneity, local variations in scene reflectivity, visual transparency can be better preserved during the interpolation process.", "publication_year": 2000, "citations_by_year": {"year": [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:08ZZubdj9fEC", "title": "Local feature embedding for supervised image classification", "authors": ["Junxia Li", "Deepu Rajan", "Jian Yang"], "description": "Local feature embedding considers two constraints: intra-image spatial and inter-image feature affinity in the embedding process. However, it does not work well for the image classification task when the images are with intra-class variation, background clutter, etc. In this paper, we enhance the manifold structure by adding the class label of images into the embedding process. Since class labels are used in the training, our method can be considered as supervised. Four constituents are included in our model: feature consistency, spatial consistency, intra-class compactness and inter-class separability. With the defined Hausdorff distance between two images, different classifiers are exploited for classification. Extensive experiments on seven datasets demonstrate the effectiveness of our proposed image classification model.", "publication_year": 2015, "citations_by_year": {"year": [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:a0OBvERweLwC", "title": "Salient region detection by jointly modeling distinctness and redundancy of image content", "authors": ["Yiqun Hu", "Zhixiang Ren", "Deepu Rajan", "Liang-Tien Chia"], "description": "Salient region detection in images is a challenging task, despite its usefulness in many applications. By modeling an image as a collection of clusters, we design a unified clustering framework for salient region detection in this paper. In contrast to existing methods, this framework not only models content distinctness from the intrinsic properties of clusters, but also models content redundancy from the removed content during the retargeting process. The cluster saliency is initialized from both distinctness and redundancy and then propagated among different clusters by applying a clustering assumption between clusters and their saliency. The novel saliency propagation improves the robustness to clustering parameters as well as retargeting errors. The power of the proposed method is carefully verified on a standard dataset of 5000 real images with rectangle annotations as well as a subset with accurate\u00a0\u2026", "publication_year": 2011, "citations_by_year": {"year": [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:KxtntwgDAa4C", "title": "Object class recognition using quadrangles", "authors": ["Alex YS Chia", "Maylor KH Leung", "Deepu Rajan", "Susanto Rahardja"], "description": "We present a new class of human psychology inspired descriptors that exhibits the ability to yield meaningful structural descriptions of an object. Our framework involves (1) detecting salient pairings of lines segments which are extracted from the line edge map of an image and (2) exploiting these pairs of line segments to construct the structural descriptors. Specifically, we integrate the spatial qualities of the line segments with the perceptually salient colors of the image to jointly identify the salient pairings of the line segments. We term such pairings of line segments as the quadrangles. By harnessing the spatial configurations and the geometrical relationships between the quadrangles, we design descriptors which characterize the local structures of an object. Promising recognition results of the four-legged animals are presented.", "publication_year": 2008, "citations_by_year": {"year": [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:zA6iFVUQeVQC", "title": "A ZGPCA Algorithm for Subspace Estimation", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "We propose a new algorithm called the ZGPCA algorithm for subspace estimation based on the GPCA (Generalized Principal Component Analysis) algorithm. It is formulated within an FIR filter framework so that the norm vectors of the subspaces correspond to filter coefficients. It is shown that such an approach leads to a more accurate and computationally efficient method compared to the GPCA algorithm. We extend the ZGPCA algorithm to make it recursive so that subspaces with possibly different dimensions can be obtained. We also propose a new distance measure that can be used for k-means clustering of sample points within a subspace. Experimental results on synthetic data and applications on face clustering and sports video clustering show good performance of the proposed algorithm.", "publication_year": 2007, "citations_by_year": {"year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:lSLTfruPkqcC", "title": "Region-based image retrieval with scale and orientation invariant features", "authors": ["Surong Wang", "Liang-Tien Chia", "Deepu Rajan"], "description": "In this paper, we address the problem of image retrieval when the query is in the form of scaled and rotated regions of images in the database. The solution lies in identifying points that are invariant to scaling and rotation and determining a robust distance measure that returns images that contain the query regions. We use the Harris-Laplacian detector to detect the interest points which are then matched with similar points in the image database using a novel fuzzy distance measure. Images with closely matching interest points are further refined using a cross-correlation measure that results in the final set of retrieval images. Experimental results show the effectiveness of the proposal image retrieval strategy.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:j3f4tGmQtD8C", "title": "JPEG2000 image adaptation for MPEG-21 digital items", "authors": ["Yiqun Hu", "Liang-Tien Chia", "Deepu Rajan"], "description": "MPEG-21 user cases bring out a scenario of Universal Multimedia Access which is becoming the reality: people use different devices such as desktop PC, personal digital assistant as well as smartphone to access multimedia information. Viewing images on mobile devices is more and more popular than before. However, due to the screen size limitation, the experience of viewing large image on small screen devices is awkward. In this paper, an enhanced JPEG2000 image adaptation system is proposed for MPEG-21 digital item adaptation. The image is adapted considering both visual attentive region(s) of image and terminal screen size. Through the subjective testing, the system has been approved to be a solution of efficiently displaying large images in different devices.", "publication_year": 2005, "citations_by_year": {"year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:mB3voiENLucC", "title": "Motion Histogram: A New Motion Feature to Index Motion Content in Video Segment.", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "In this paper, we introduce a new feature, motion histogram, for motion based video indexing. First, the motion content of the video clip is summarized in the Pixel Change Ratio Map (PCRM). Then, the histogram feature with nonuniform quantization is extracted from the PCRM to index the digital video. The nonuniform quantization levels are found by a greed algorithm. Finally, the motion histogram features are used for video clips retrieval and classification. Experimental results demonstrate the effectiveness of the feature to index the motion content of the video segment.", "publication_year": 2004, "citations_by_year": {"year": [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, "unknown"], "num_citations": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:dTyEYWd-f8wC", "title": "2015 IEEE International Conference on Image Processing", "authors": ["Ioana ILEA", "Lionel BOMBRUN", "Christian GERMAIN", "Romulus TEREBES", "Monica BORDA", "Mohamad Mazen HITTAWE", "Satya MUDDAMSETTY", "Desire SIDIBE", "Fabrice MERIAUDEAU", "Junxia LI", "Deepu RAJAN", "Jian YANG", "Yuanhao ZHAI", "David NEUHOFF", "Tao CHEN", "Shijian LU", "Amir M RAHIMI", "Lakshmanan NATARAJ", "Bs MANJUNATH", "Zhipeng YE", "Peng LIU", "Xianglong TANG", "Wei ZHAO", "David MARS", "Hanwei WU", "Haopeng LI", "Markus FLIERL", "Chen YE"], "description": "Program Page 1 Program 2015 IEEE International Conference on Image Processing ARS-O1: \nClassification I STATISTICAL HYPOTHESIS TEST FOR ROBUST CLASSIFICATION ON THE \nSPACE OF COVARIANCE MATRICES page :271 Ioana ILEA, Universit\u00e9 de Bordeaux, \nLaboratoire IMS, Groupe Signal et Image; Technical University of Cluj-Na Lionel BOMBRUN, \nUniversit\u00e9 de Bordeaux, Laboratoire IMS, Groupe Signal et Image Christian GERMAIN, \nUniversit\u00e9 de Bordeaux, Laboratoire IMS, Groupe Signal et Image Romulus TEREBES, Technical \nUniversity of Cluj-Napoca Monica BORDA, Technical University of Cluj-Napoca MULTIPLE \nFEATURES EXTRACTION FOR TIMBER DEFECTS DETECTION AND CLASSIFICATION \nUSING SVM page :427 Mohamad Mazen HITTAWE, Le2i Satya MUDDAMSETTY, Le2i Desire \nSIDIBE, University of Burgundy,LE2I Fabrice MERIAUDEAU, University of Burgundy,LE2I \u2026", "publication_year": null, "citations_by_year": {"year": ["unknown"], "num_citations": [1]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:qxL8FJ1GzNcC", "title": "IMAGE RETRIEVAL USING DOMINANT COLOR DESCRIPTOR", "authors": ["Wang Surong", "Chia Liang-Tien", "Deepu Rajan"], "description": "In this paper, a new method used to calculate the similarity of Dominant Color Descriptor is discussed. Using Earth Mover\u2019s Distance (EMD), better retrieval results can be obtained compared with those obtained from the original MPEG-7 reference software (XM)[1]. In order to save the retrieval time, two different methods which can prune the images far from the query image are discussed. One is the lower bound of EMD, while the other is the M-tree index based on EMD distance. Experiments show that the lower bound is easier to implement and more efficient than the M-tree.", "publication_year": null, "citations_by_year": {"year": ["unknown"], "num_citations": [1]}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:dQ2og3OwTAUC", "title": "A Unified Framework for Guiding Generative AI with Wireless Perception in Resource Constrained Mobile Edge Networks", "authors": ["Jiacheng Wang", "Hongyang Du", "Dusit Niyato", "Jiawen Kang", "Zehui Xiong", "Deepu Rajan", "Shiwen Mao"], "description": "With the significant advancements in artificial intelligence (AI) technologies and powerful computational capabilities, generative AI (GAI) has become a pivotal digital content generation technique for offering superior digital services. However, directing GAI towards desired outputs still suffer the inherent instability of the AI model. In this paper, we design a novel framework that utilizes wireless perception to guide GAI (WiPe-GAI) for providing digital content generation service, i.e., AI-generated content (AIGC), in resource-constrained mobile edge networks. Specifically, we first propose a new sequential multi-scale perception (SMSP) algorithm to predict user skeleton based on the channel state information (CSI) extracted from wireless signals. This prediction then guides GAI to provide users with AIGC, such as virtual character generation. To ensure the efficient operation of the proposed framework in resource constrained networks, we further design a pricing-based incentive mechanism and introduce a diffusion model based approach to generate an optimal pricing strategy for the service provisioning. The strategy maximizes the user's utility while enhancing the participation of the virtual service provider (VSP) in AIGC provision. The experimental results demonstrate the effectiveness of the designed framework in terms of skeleton prediction and optimal pricing strategy generation comparing with other existing solutions.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}, "topic": "Artificial Intelligenc", "agg_topic": "Chemistry and Artificial Intelligenc", "final_topic": "Artificial Intelligenc"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:hkOj_22Ku90C", "title": "Towards Balanced Active Learning for Multimodal Classification", "authors": ["Meng Shen", "Yizheng Huang", "Jianxiong Yin", "Heqing Zou", "Deepu Rajan", "Simon See"], "description": "Training multimodal networks requires a vast amount of data due to their larger parameter space compared to unimodal networks. Active learning is a widely used technique for reducing data annotation costs by selecting only those samples that could contribute to improving model performance. However, current active learning strategies are mostly designed for unimodal tasks, and when applied to multimodal data, they often result in biased sample selection from the dominant modality. This unfairness hinders balanced multimodal learning, which is crucial for achieving optimal performance. To address this issue, we propose three guidelines for designing a more balanced multimodal active learning strategy. Following these guidelines, a novel approach is proposed to achieve more fair data selection by modulating the gradient embedding with the dominance degree among modalities. Our studies demonstrate that the proposed method achieves more balanced multimodal learning by avoiding greedy sample selection from the dominant modality. Our approach outperforms existing active learning strategies on a variety of multimodal classification tasks. Overall, our work highlights the importance of balancing sample selection in multimodal active learning and provides a practical solution for achieving more balanced active learning for multimodal classification.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}, "topic": "Machine Learning", "agg_topic": "Machine Learning", "final_topic": "Artificial Intelligenc"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:PR6Y55bgFSsC", "title": "UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning", "authors": ["Heqing Zou", "Meng Shen", "Chen Chen", "Yuchen Hu", "Deepu Rajan", "Eng Siong Chng"], "description": "Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. Specifically, we first capture task-related unimodal representations and the unimodal predictions from the introduced unimodal predicting task. Then the unimodal representations are aligned with the more effective one by the designed multimodal contrastive method under the supervision of the unimodal predictions. Experimental results with fused features on two image-text classification benchmarks UPMC-Food-101 and N24News show that our proposed Unimodality-Supervised MultiModal Contrastive UniS-MMC learning method outperforms current state-of-the-art multimodal methods. The detailed ablation study and analysis further demonstrate the advantage of our proposed method.", "publication_year": 2023, "citations_by_year": {"year": [], "num_citations": []}, "topic": "Multimodal Learning", "agg_topic": "Artificial Intelligence and Machine Learning", "final_topic": "Artificial Intelligenc"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:VL0QpB8kHFEC", "title": "UniS-MMC: Learning Unimodality-supervised Multimodal Contrastive Representations", "authors": ["Heqing Zou", "Meng Shen", "Chen Chen", "Yuchen Hu", "Deepu Rajan", "Ensiong Chng"], "description": "Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for final decisions.   However, just like a human's final decision can be confused by specific erroneous information from the environment, current multimodal learning methods also suffer from uncertain unimodal prediction when learning multimodal representations. In this work, we propose to contrastively explore reliable representations and increase the agreement among the unimodal representations that alone make potentially correct predictions. Specifically, we first capture task-related representations by directly sharing representations between unimodal and multimodal learning tasks. With the unimodal representations and predictions from the multitask-based framework, we then propose a novel multimodal contrastive learning method to align the representations towards the relatively more reliable modality under the weak supervision of the unimodal predictions. Experimental results on two image-text benchmarks UPMC-Food-101 and N24News, and two medical benchmarks ROSMAP and BRCA, show that our proposed Unimodality-supervised Multimodal Contrastive (UniS-MMC) learning method outperforms current state-of-the-art multimodal learning methods. The detailed ablation studies further demonstrate the advantage of our proposed method.", "publication_year": 2022, "citations_by_year": {"year": [], "num_citations": []}, "topic": "Multimodal Learning", "agg_topic": "Artificial Intelligence and Machine Learning", "final_topic": "Artificial Intelligenc"}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:t6usbXjVLHcC", "title": "Comparative Convolutional Neural Network for Younger Face Identification", "authors": ["Liangliang Wang", "Deepu Rajan"], "description": "We consider the problem of determining whether a pair of face images can be distinguishable in terms of age and if so, which is the younger of the two. We also determine the degree of distinguishability in which age differences are categorized into large, medium, small and tiny. We propose a comparative convolutional neural network combining two parallel deep architectures. Based on the two deep learnt face features, we introduce a comparative layer to represent their mutual relationships, followed by a concatenatation implementation. Softmax is adopted to complete the classification task. To demonstrate our approach, we construct a very large dataset consisting of over 1.7 million face image pairs with young/old labels.", "publication_year": 2019, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:ZHo1McVdvXMC", "title": "Salient object detection in tracking shots", "authors": ["Karthik Muthuswamy", "Deepu Rajan"], "description": "Tracking shots have posed a significant challenge for salient region detection due to the presence of highly competing background motion. In this paper, we propose a computationally efficient technique to detect salient objects in a tracking shot. We first separate the tracked foreground pixels from the background by accounting for the variability of the pixels in a set of frames. The focus of the tracked foreground pixels is utilized as a measure of saliency of objects in the scene. We evaluate the performance of this method by comparing the salient region detection with ground truth data of the location of the salient object that are manually generated. The results of the evaluation show that the proposed method is able to achieve superior salient object detection performance with very low computational load.", "publication_year": 2016, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:YsMSGLbcyi4C", "title": "Computer-aided evaluation of cataract surgery; a metric comparison of continuous circular capsulorhexis by trainee and specialist surgeons.", "authors": ["Augustinus Laude", "Praseedha Krishnan Aniyath", "Kiam Tian Seow", "Jian Wah Kwok", "Han Bor Fam", "Wee Jin Heng", "Deepu Rajan"], "description": "PurposeThere is a correlation between the centration and quality of the continuous circular capsulorhexis (CCC) and the subsequent refractive outcomes in cataract surgery. We developed a novel software evaluation tool based on video processing to assess the execution of CCC by comparing trainee and specialist surgeons from a teaching hospital. The software incorporates a novel performance metric that quantifies their performance.MethodsWe first detected the limbus of the eye in each video frame using Hough circle detection. Next, the capsulorhexis forceps is detected based on its linearity and specularity. Then a visual tool-tracking function is invoked based on an image similarity measure which is illumination invariant and computationally inexpensive. The number of capsular grasps is then found from a functional plot of distance between the pair of forceps tips. Other parameters computed include\u00a0\u2026", "publication_year": 2015, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:HoB7MX3m0LUC", "title": "Modeling Visual Saliency in Images and Videos", "authors": ["Yiqun Hu", "Viswanath Gopalakrishnan", "Deepu Rajan"], "description": "Visual saliency, which distinguishes \u201cinteresting\u201d visual content from others, plays an important role in multimedia and computer vision applications. This chapter starts with a brief overview of visual saliency as well as the literature of some popular models to detect salient regions. We describe two methods to model visual saliency\u2013one in images and the other in videos. Specifically, we introduce a graph-based method to model salient region in images in a bottom-up manner. For videos, we introduce a factorization based method to model attention object in motion, which utilizes the top-down knowledge of cameraman for model saliency. Finally, future directions for visual saliency modeling and additional reading materials are highlighted to familiarize readers with the research on visual saliency modeling for multimedia applications.", "publication_year": 2013, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:Se3iqnhoufwC", "title": "Content-Based Keyframe Clustering Using Near Duplicate Keyframe Identification", "authors": ["Ehsan Younessian", "Deepu Rajan"], "description": "In this paper, the authors propose an effective content-based clustering method for keyframes of news video stories using the Near Duplicate Keyframe (NDK) identification concept. Initially, the authors investigate the near-duplicate relationship, as a content-based visual similarity across keyframes, through the Near-Duplicate Keyframe (NDK) identification algorithm presented. The authors assign a near-duplicate score to each pair of keyframes within the story. Using an efficient keypoint matching technique followed by matching pattern analysis, this NDK identification algorithm can handle extreme zooming and significant object motion. In the second step, the weighted adjacency matrix is determined for each story based on assigned near duplicate score. The authors then use the spectral clustering scheme to remove outlier keyframes and partition remainders. Two sets of experiments are carried out to evaluate\u00a0\u2026", "publication_year": 2013, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:ZuybSZzF8UAC", "title": "Shape-simplifying image abstraction", "authors": ["Deepu Rajan"], "description": "The report documents a final year project at the School of Computer Engineering. In this project, a thorough study of algorithms for image abstraction is conducted to lay a foundation for the design and development of the Mean Curvature Flow (MCF) based framework. The framework demonstrates the constrained MCF technique, in an iterative and incremental manner, to simplify the image content. This image simplification technique \u2013 which combines the constrained MCF and Shock filtering processing (to effectively produce a stylistic abstraction of an image) \u2013 is presented in the research paper \u2018Shape- Simplifying Image Abstraction\u2019 [1] by Henry Kang and Seungyong Lee. Image abstraction refers to the task of simplifying scene information in the image while retaining or emphasizing meaningful features to convey. A lot of research work have been conducted to study and invent image abstraction techniques such as image segmentation, curve fitting, mean curvature flow etc. The technique chosen to study in this first part of the project is an integral method which uses constrained MCF for simplifying image content (shape and color) and shock filtering to protect important structures in the image (shape boundary, edge). The study is conducted in two steps. Firstly, the combined MCF and shock filtering algorithm is investigated thoroughly to compare with other image abstraction methods, and identify the limitations. Secondly, the proposed framework for the improved MCF based method in the paper [1] is studied and analyzed carefully for design and development in the second part of the project. At the end of this part, the MATLAB program is\u00a0\u2026", "publication_year": 2011, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:hMod-77fHWUC", "title": "Low power DCT implementation using differential pixels for on-board satellite image processing", "authors": ["Ankita Singla", "AP Vinod", "Deepu Rajan", "Edmund MK Lai"], "description": "Low power and high speed Discrete Cosine Transform (DCT) implementation of the images captured by the satellites presents a hardware design problem. The cost of the DCT implementation is dominated by the complexity of the multiplication of input data (image) with the DCT matrix. This paper presents the techniques for minimizing the complexity of multiplication by employing Differential Pixel Image (DPI). In our method we take 8\u00d78 blocks of input image matrix and calculate the difference between the adjacent pixels and use those differential pixels in DCT transformation. While conventional low complexity DCT implementation methods focus on optimizing the DCT matrix, our method achieves complexity reduction by using the difference of intensities of adjacent pixels of the image. Design examples show that an average of 12% reduction in power consumption and delay is achieved over the conventional\u00a0\u2026", "publication_year": 2007, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:PELIpwtuRlgC", "title": "Proceedings of the 13th international conference on Multimedia Modeling-Volume Part I", "authors": ["Tat-Jen Cham", "Jianfei Cai", "Chitra Dorai", "Deepu Rajan", "Tat-Seng Chua"], "description": "Proceedings of the 13th international conference on Multimedia Modeling - Volume Part I | \nGuide Proceedings ACM Digital Library home ACM home Google, Inc. (search) Advanced \nSearch Browse About Sign in Register Advanced Search Journals Magazines Proceedings \nBooks SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced \nSearch Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsMMM'07 \nABSTRACT No abstract available. Comments Login options Check if you have access through \nyour login credentials or your institution to get full access on this article. Sign in Full Access \nGet this Publication Information Contributors Published in Guide Proceedings cover image \nMMM'07: Proceedings of the 13th international conference on Multimedia Modeling - Volume \nPart I January 2007 796 pages ISBN:3540694218 Editors: Tat-Jen Cham , Jianfei Cai , \u2026", "publication_year": 2007, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:Wp0gIr-vW9MC", "title": "Video Indexing Using Motion Correlograms", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "In this report, we present a new motion feature, motion correlograms, to index the motion content in videos. First, the motion content of the video is summarized as a Pixel Change Ratio Map (PCRM). The PCRM captures the intensity and duration of the motion in a video, which are the two most important factors according to human\u2019s perception of the motion content. The PCRM also indicates the spatial location and size of the moving object (s). Then after a non-uniform quantization of the PCRM, the motion correlograms are extracted as the index to motion content. Experimental results on video retrieval and video classification demonstrate that the proposed motion correlogram outperforms not only the traditional motion vector based feature, but also the our previously proposed PCRM based motion histogram features.", "publication_year": 2004, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:4DMP91E08xMC", "title": "Optimal delay order selection for linear equalization problems", "authors": ["ES Chng", "Sheng Chen", "Deepu Rajan"], "description": "This paper shows that the BER performance using linear equalizer for channel equalization problem is significantly dependent on delay order. To obtain optimum performance, the equalizer output should be derived from the equalizer with delay order having the best BER performance. An efficient method to evaluate the upper bound BER performance of a linear equalizer to find the optimum delay is proposed. The method is novel as the evaluation is performed using only the channel statistics and the equalizer\u2019s weights.", "publication_year": 2003, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:vRqMK49ujn8C", "title": "Abu-Mostafa, Y. 277 Asharif, MR 51", "authors": ["S Auephanwiriyakul", "S Bengio", "L Bruzzone", "S Chaudhuri", "S Cohen", "O Colot", "R Cossu", "Y Dai", "M De Santo", "RPW Duin", "A Farina", "J F\u00fcurnkranz", "PD Gader", "JB Gao", "L Gautier", "R Haenni", "K Hanasaki", "CJ Harris", "KC Ho", "YK Ho", "R HoseinNezhad", "N Intrator", "H Kasdan", "JM Keller", "J Kittler", "LI Kuncheva", "JT Kwok", "BF La Scala", "Z Liu", "C Marcel", "S Marcel", "J Mari\u00e9ethoz", "FC Morabito", "B Moshiri", "M Pavel", "G Percannella", "D Rajan", "GL Rogova", "F Roli", "F Sadjadi", "C Sansone", "SB Serpico", "CA Shipp", "J Sill", "G Simone", "M Skurichina", "XB Song", "RJ Stanley", "PC Stomper", "P Svensson", "A Taleb-Ahmed", "K Tsukada", "P Vannoorenberghe", "M Vento", "G Vernazza", "Y Wang", "N Xiong"], "description": null, "publication_year": 2002, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:5ugPr518TE4C", "title": "listed alphabetically: Ang, Cheng Leong Arnold, David Asami, Tohru", "authors": ["Peter Astheimer", "Norman Badler", "Chandrajit Bajaj", "Xu Baowen", "Rana Barua", "George Bebis", "Uli Bockholt", "Guido Brunnett", "Wentong Cai", "Kai Yun Tony Chan", "Patra Jagdish Chandra", "Narendra S Chaudhari", "Lihui Chen", "William Chu", "Wayne Cochran", "Amitabha Das", "Manoranjan Dash", "Wlodzislaw Duch", "Sabu Emmanuel", "Jos\u00e9 Encarna\u00e7\u00e3o", "Yukio Fukui", "Sadaoki Furui", "Paul Gagnon", "Marina Gavrilova", "Hoe Lian Dion Goh", "Hans Hagen", "Norihiro Hagita", "Naoki Hashimoto", "Yulan He", "Keikichi Hirose", "Tung Shuen Anthony Ho", "Nick Holliman", "Yang Hongji", "Tanaka Hozumi", "Roger Hubbold", "Andres Iglesias", "Hitoshi Iida", "Masato Ishizaki", "Takayuki Itoh", "Ray Jarvis", "Lu Jianjiang", "Gyoba Jiro", "Mohan S Kankanhalli", "Hirokazu Kato", "Tatsuya Kawahara", "Myoung-Hee Kim", "Tadahiro Kitahashi", "Sirai Kiyoaki", "Stanislav Klimenko", "Tetsunori Kobayashi", "Pe Hin Hinny Kong", "Tosiyasu L Kunii", "Gilbert Arthur Lee", "Yong Tsui Lee", "Fang Li", "Ling Li", "Yingjiu Li", "Annie Luciani", "Herve Luga", "Jianhua Ma", "Nadia Magnenat-Thalmann", "Mitsunori Makino", "Shozo Makino", "Michal Masa", "Thom McLean", "Takeshi Miura", "Kazunori Miyata", "Sinya Miyazaki", "Wolfgang M\u00fceller-Wittig", "Karol Myszkowski", "Suganthan Ponnuthurai Nagaratnam", "Masayuki Nakajima", "Sukumar Nandi", "Anton Nijholt", "Shogo Nishida", "Kenji Ohmori", "Kian Seng Patrick Ong", "Michel Pasquier", "William Russell Pensyl", "Edmond Prakash", "Ekaterina Prasolova-F\u00f8rland", "Jochen Quick", "Deepu Rajan"], "description": "The conference offers a note of thanks and lists its reviewers.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:1qzjygNMrQYC", "title": "Analytical Exploration of Energy Savings for Parked Vehicles to Enhance VANET Connectivity.......................", "authors": ["G Sun", "M Yu", "D Liao", "V Chang", "KK Santhosh", "DP Dogra", "PP Roy", "L Lu", "H Huang", "DK Prasad", "CK Prasath", "D Rajan", "L Rachmawati", "E Rajabally", "C Quek"], "description": "Presents the table of contents for this issue of the publication.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:D_sINldO8mEC", "title": "A new motion histogram to index motion content", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "10 A new motion feature for video indexing is proposed in this paper. The motion content of the video at pixel level, is 11 represented as a Pixel Change Ratio Map (PCRM). The PCRM enables us to capture the intensity of motion in a video 12 sequence. It also indicates the spatial location and size of the moving object. The proposed motion feature is the motion 13 histogram which is a non-uniformly quantized histogram of the PCRM. We demonstrate the usefulness of the motion 14 histogram with three applications, viz., video retrieval, video clustering and video classification. 15\u00a9 2004 Elsevier BV All rights reserved.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:xtRiw3GOFMkC", "title": "ADAPTIVE LOCAL CONTEXT SUPPRESSION OF MULTIPLE CUES FOR SALIENT VISUAL ATTENTION DETECTION (FriPmPO1)", "authors": ["Yiqun Hu", "Deepu Rajan"], "description": "Visual attention is obtained through determination of contrasts of low level features or attention cues like intensity, color etc. We propose a new texture attention cue that is shown to be more effective for images where the salient object regions and background have similar visual characteristics. Current visual attention models do not consider local contextual information to highlight attention regions. We also propose a feature combination strategy by suppressing saliency based on context information that is effective in determining the true attention region. For an objective evaluation of our results, we also propose a new measure called the Average Discrimination Ratio.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:NhqRSupF_l8C", "title": "OPTIMIZATION-BASED MULTIPLE MPEG-7 DESCRIPTORS FOR IMAGE RETRIEVAL", "authors": ["Surong Wang", "Deepu Rajan", "Liang-Tien Chia"], "description": "In content-based image retrieval, combination of multiple features within a single model has been investigated as a promising technique to increase the retrieval efficiency. MPEG-7 standards provide standard descriptions for multimedia content. In this paper, a descriptor-weighting scheme for combining multiple MPEG-7 visual descriptors is discussed. The descriptors and relationships among them will be combined with optimal weights. We derive the weighting parameters of different descriptors based on optimized technique. An optimal model is build to find a set of optimal weights for a set of descriptors. Explicit solutions can be derived by Lagrange multipliers, which are optimal and easy to calculate. The calculation procedure is fully automatic and no manually work is needed. Experiments show that better retrieval results can be achieved than using single descriptor. It is also better than simple average\u00a0\u2026", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:vV6vV6tmYwMC", "title": "simultaneous Estimation of super-resolved Depth and Image from Low Resolution Defocused Observations", "authors": ["Deepu Rajan", "Subhasis Chaudhuri"], "description": "This paper presents a novel technique to simultaneously estimate the depth and the focused image of a scene both at a super-resolution, from its defocused observations. Super-resolution refers to the generation of high resolution images from a sequence of low resolution images. Hitherto, the super-resolution technique has been restricted only to the intensity domain. In this paper, we extend the scope of super-resolution to acquire depth estimates at high resolution simultaneously. Given a sequence of low resolution, blurred and noisy observations of a static scene, the problem is to generate a dense depth map at a resolution higher than one that can be generated from the observations as well as to estimate the true focused image. Both the depth as well as the image are modeled as separate Markov Random Fields and a maximum a posteriori method is used to derive a cost function which is then optimized using simulated annealing (SA).", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:isC4tDSrTZIC", "title": "Integration of Synthetic and Natural Audio/Video Registration Based on Scene Recognition and Natural Features Tracking Techniques for Wide-Area Augmented Reality", "authors": ["A Mademlis", "P Daras", "D Tzovaras", "MG Strintzis", "Y Hu", "X Cheng", "LT Chia", "X Xie", "D Rajan", "AH Tan", "D Ren", "YTH Li", "SHG Chan", "H Hu", "M Zhou", "Z Li"], "description": "Presents the table of contents for this issue of the periodical.", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:IWHjjKOFINEC", "title": "ADAPTIVE HIERARCHICAL MULTI\u2212 CLASS SVM CLASSIFIER FOR TEXTURE\u2212 BASED IMAGE CLASSIFICATION (FriAmOR5)", "authors": ["Song Liu", "Haoran Yi", "Deepu Rajan"], "description": "In this paper, we present a new classification scheme based on Support Vector Machines (SVM) and a new texture feature, called texture correlogram, for high\u2212 level image classification. Originally, SVM classifier is designed for solving only binary classification problem. In order to deal with multiple classes, we present a new method to dynamically build up a hierarchical structure from the training dataset. The texture correlogram is designed to capture spatial distribution information. Experimental results demonstrate that the proposed classification scheme and texture feature are effective for high\u2212 level image classification task and the proposed classification scheme is more efficient than the other schemes while achieving almost the same classification accuracy. Another advantage of the proposed scheme is that the underlying hierarchical structure of the SVM classification tree manifests the interclass\u00a0\u2026", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:M3ejUd6NZC8C", "title": "ROBUST VIDEO MOTION ANALYSIS IN FREQUENCY DOMAIN", "authors": ["Haoran Yi", "Deepu Rajan", "Liang-Tien Chia"], "description": "In this paper, we present a new approach for video motion analysis in frequency domain. First, the 3D video signal (xyt) is transformed into the frequency domain by 3D Fourier transform. After applying 3D Fourier transform on the video signal, we derive that the non-zeros coefficients should lie on one plane (coplanarity) under the condition that the video motion is translational. We call this plane video motion plane. Furthermore, the normal of the video motion plane gives the velocity of the motion. Therefore, the video motion estimation problem turns out to be a plane fitting problem in the 3D frequency domain with the nonzero Fourier coefficients. Robust M-estimator is used to fit the plane and give the estimated velocity. Experimental results on both synthetic and real video sequences illustrate robustness against noise and good performance of the proposed approach. keywords", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:Zph67rFs4hoC", "title": "145 Mining user hidden semantics from image content for image retrieval", "authors": ["Xiangjun Shen", "Shiguang Ju", "Siu-Yeung Cho", "Feng Li", "Jong-Ho Kim", "Byung-Gyu Kim", "Ernesto Bribiesca", "Yiqun Hu", "Deepu Rajan", "Liang-Tien Chia"], "description": null, "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:LkGwnXOMwfcC", "title": "EMPLOYING DIFFERENCE IMAGE IN AFFINE TRACKING", "authors": ["Guo Jing", "Deepu Rajan", "Chng Eng Siong"], "description": "EMPLOYING DIFFERENCE IMAGE IN AFFINE TRACKING Guo Jing, Deepu Rajan and Chng \nEng Siong School of Computer Engineering Nanyang Technological University Singapore, \n639798 {guoj0005, asdrajan, aseschng}@ntu.edu.sg ABSTRACT Human tracking is a well \ndeveloped research area in computer vision and various algorithms have been brought up. \nSome researchers see tracking as a iteratively optimiza- tion process in each frame to \ufb01nd the \noptimum transfor- mation parameter that minimize the distance between the transformed \nregion and the object model. The most of- ten used transformation are translations and af\ufb01ne \nwarps. In this paper, using af\ufb01ne tracking as example, we com- bine difference image between \nconsecutive image frames into the optimization process, ie the difference image pro- vides \nseveral starting points of gradient decent to search for the object, rather than only using one \u2026", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}, {"link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=1bqFcwIAAAAJ&cstart=100&pagesize=100&citation_for_view=1bqFcwIAAAAJ:WF5omc3nYNoC", "title": "Image Attention", "authors": ["Yiqun Hu", "Deepu Rajan", "Liang-Tien Chia"], "description": "Image Attention Page 1 1 Robust Subspace Analysis for Detecting Visual Attention Regions in \nImages Yiqun Hu, Deepu Rajan and Liang-Tien Chia Center of Multimedia and Network \nTechnology School of Computer Engineering Nanyang Tehnological University Image \nAttention \u220e Visual Attention \u2751 A mechanism of HVS to limit processing to important information \n\u220e Application \u2751 Image Browsing & Retrieval \u2751 Adaptive Content Delivery \u2751 Object Recognition \n\u2751 Mobile Robotics \u2751 Gaming Page 2 2 Different Perspectives \u220e Spatial-based Image Attention \n\u2751 Attention is deployed in different spatial locations \u2751 Before grouping, not information of \nobject \u220e Object-based Image Attention \u2751 Attention is deployed on the level of object; \u2751 Focus \nare shift between different object Space-based Attention Method \u220e Biological-based model \n\u2751 Based on the attention framework of Freman (Freman85) \u2751 \u201cCenter-surround\u201d contrast, \u2026", "publication_year": null, "citations_by_year": {"year": [], "num_citations": []}, "topic": null, "agg_topic": null, "final_topic": null}]