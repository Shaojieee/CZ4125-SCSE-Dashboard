{"2019": [{"title": "Deepstellar: Model-based quantitative analysis of stateful deep learning systems", "description": "Deep Learning (DL) has achieved tremendous success in many cutting-edge applications. However, the state-of-the-art DL systems still suffer from quality issues. While some recent progress has been made on the analysis of feed-forward DL systems, little study has been done on the Recurrent Neural Network (RNN)-based stateful DL systems, which are widely used in audio, natural languages and video processing, etc. In this paper, we initiate the very first step towards the quantitative analysis of RNN-based DL systems. We model RNN as an abstract state transition system to characterize its internal behaviors. Based on the abstract model, we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of RNNs. We further propose two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation. We evaluate\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:E8ajGqO0XoUC"}, {"title": "Vultron: catching vulnerable smart contracts once and for all", "description": "Despite the high stakes involved, smart contracts are often developed in an undisciplined way thus far. The existence of vulnerabilities compromises the security and reliability of smart contracts, and endangers the trust of participants in their ongoing businesses. Existing vulnerability detection techniques are often designed case-by-case, making them difficult to generalize. In this paper, we design general principles for detecting vulnerable smart contracts. Our key insight is that almost all the existing transaction-related vulnerabilities are due to the mismatch between the actual transferred amount and the amount reflected on the contract's internal bookkeeping. Based on this, we propose a precise and generally applicable technique, VULTRON, which can detect irregular transactions due to various types of adversarial exploits. We also report preliminary results applying our technique to real-world case studies.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:_IsBomjs8bsC"}, {"title": "A quantitative analysis framework for recurrent neural network", "description": "Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:GHsHDPAyICYC"}, {"title": "Coverage-guided fuzzing for feedforward neural networks", "description": "Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:feST4K8J0scC"}], "2014": [{"title": "Symbolic Optimization with SMT Solvers", "description": "The rise in efficiency of Satisfiability Modulo Theories (SMT) solvers has created numerous uses for them in software verification, program synthesis, functional programming, refinement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments (e.g., a witness for a bug) or proving unsatisfiability/validity(e.g., proving that a subtyping relation holds). We are often interested in finding not just an arbitrary satisfying assignment, but one that optimizes (minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions that maximize energy usage (performance bugs), or synthesizing short programs that do not make expensive API calls. Unfortunately, none of the available SMT solvers offer such optimization capabilities. In this paper, we present SYMBA, an efficient SMT-based optimization algorithm for objective functions in the theory of linear\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:60iIaj97TE0C"}, {"title": "Model checking approach to automated planning", "description": "Model checking provides a way to automatically explore the state space of a finite state system based on desired properties, whereas planning is to produce a sequence of actions that leads from the initial state to the target goal states. Previous research in this field proposed a number of approaches for connecting model checking with planning problem solving. In this paper, we investigate the feasibility of using an established model checking framework, Process Analysis Toolkit (PAT), as a planning solution provider for upper layer applications. To achieve this, we first carry out a number of experiments on different model checking tools in order to compare their performance and capabilities on planning problem solving. Our experimental results suggest that solving planning problems using model checkers is not only possible but also practical. We then propose a formal semantic mapping from the standard\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:QaSi33NTfwYC"}, {"title": "Management of time requirements in component-based systems", "description": "In component-based systems, a number of existing software components are combined in order to achieve business goals. Some of such goals may include system-level (global) timing requirements (GTR). It is essential to refine GTR into a set of component-level (local) timing requirements (LTRs) so that if a set of candidate components collectively meets them, then the corresponding GTR is also satisfied. Existing techniques for computing LTRs produce monolithic representations, that have dependencies over multiple components. Such representations do not allow for effective component selection and repair. In this paper, we propose an approach for building under-approximated LTRs (uLTR) consisting of independent constraints over components. We then show how uLTR can be used to improve the design, monitoring and repair of component-based systems under time requirements. We also report on\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:htyGaKyDgHMC"}], "2012": [{"title": "UFO: A framework for abstraction-and interpolation-based software verification", "description": "In this paper, we present Ufo, a framework and a tool for verifying (and finding bugs in) sequential C programs. The framework is built on top of the LLVM compiler infrastructure and is targeted at researchers designing and experimenting with verification algorithms. It allows definition of different abstract post operators, refinement strategies and exploration strategies. We have built three instantiations of the framework: a predicate abstraction-based version, an interpolation-based version, and a combined version which uses a novel and powerful combination of interpolation-based and predicate abstraction-based algorithms.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:u-x6o8ySG0sC"}, {"title": "Translating PDDL into csp#-the PAT approach", "description": "Model checking provides a way to automatically verify hardware and software systems, whereas the goal of planning is to produce a sequence of actions that leads from the initial state to the desired goal state. Recently research indicates that there is a strong connection between model checking and planning problem solving. In this paper, we investigate the feasibility of using a newly developed model checking framework, Process Analysis Toolkit (PAT), to serve as a planning solution provider for upper layer applications. We first carried out a number of experiments on different planning tools in order to compare their performance and capabilities. Our experimental results showed that the performance of the PAT model checker is comparable to that of state-of-art planners for certain categories of problems. We further propose a set of translation rules for mapping from a commonly used planning notation - PDDL\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:xyvS_IvSCKsC"}, {"title": "Planning as model checking tasks", "description": "Model checking provides a way to automatically verify hardware and software systems, whereas the goal of planning is to produce a sequence of actions that leads from the initial state to the desired goal states. Recently research indicates that there is a strong connection between model checking and planning problem solving. In this paper, we investigate the feasibility of using different model checking tools and techniques for solving classic planning problems. To achieve this, we carried out a number of experiments on different planning domains in order to compare the performance and capabilities of various tools. Our experimental results indicate that the performance of some model checkers is comparable to that of state-of-theart planners for certain categories of problems. In particular, a new planning module with specifically designed searching algorithm is implemented on top of the established model\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:0sTkTiv_uMkC"}], "2021": [{"title": "A survey of smart contract formal specification and verification", "description": "A smart contract is a computer program that allows users to automate their actions on the blockchain platform. Given the significance of smart contracts in supporting important activities across industry sectors including supply chain, finance, legal, and medical services, there is a strong demand for verification and validation techniques. Yet, the vast majority of smart contracts lack any kind of formal specification, which is essential for establishing their correctness. In this survey, we investigate formal models and specifications of smart contracts presented in the literature and present a systematic overview to understand the common trends. We also discuss the current approaches used in verifying such property specifications and identify gaps with the hope to recognize promising directions for future work.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:Ul_CLA4dPeMC"}, {"title": "Formal analysis of composable DeFi protocols", "description": "Decentralized finance (DeFi) has become one of the most successful applications of blockchain and smart contracts. The DeFi ecosystem enables a wide range of crypto-financial activities, while the underlying smart contracts often contain bugs, with many vulnerabilities arising from the unforeseen consequences of composing DeFi protocols together. In this paper, we propose a formal process-algebraic technique that models DeFi protocols in a compositional manner to allow for efficient property verification. We also conduct a case study to demonstrate the proposed approach in analyzing the composition of two interacting DeFi protocols, namely, Curve and Compound. Finally, we discuss how the proposed modeling and verification approach can be used to analyze financial and security properties of interest.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:XeErXHja3Z8C"}, {"title": "Diffbase: A differential factbase for effective software evolution management", "description": "Numerous tools and techniques have been developed to extract and analyze information from software development artifacts. Yet, there is a lack of effective method to process, store, and exchange information among different analyses. In this paper, we propose differential factbase, a uniform exchangeable representation supporting efficient querying and manipulation, based on the existing concept of program facts. We consider program changes as first-class objects, which establish links between intra-version facts of single program snapshots and provide insights on how certain artifacts evolve over time via inter-version facts. We implement a series of differential fact extractors supporting different programming languages and platforms, and demonstrate with usage scenarios the benefits of adopting differential facts in supporting software evolution management.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:NnTm98qLMbgC"}, {"title": "Eqbench: A dataset of equivalent and non-equivalent program pairs", "description": "Equivalence checking techniques help establish whether two versions of a program exhibit the same behavior. The majority of popular techniques for formally proving/refuting equivalence are evaluated on small and simplistic benchmarks, omitting \"difficult\" programming constructs, such as non-linear arithmetic, loops, floating-point arithmetic, and string and array manipulation. This hinders efficient evaluation of these techniques and the ability to establish their practical applicability in real scenarios. This paper addresses this gap by contributing EqBench - the largest and most comprehensive benchmark for equivalence checking analysis, which contains 147 equivalent and 125 non-equivalent cases, in both C and Java languages. We believe EqBench can facilitate a more realistic evaluation of equivalence checking techniques, assessing their individual strength and weaknesses. EqBench is publicly available at\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:rzmi0EmCOGEC"}, {"title": "EVOME: A software evolution management engine based on differential factbase", "description": "Managing large and fast-evolving software systems can be a challenging task. Numerous solutions have been developed to assist in this process, enhancing software quality and reducing development costs. These techniques\u2014e.g., regression test selection and change impact analysis\u2014are often built as standalone tools, unable to share or reuse information among them. In this paper, we introduce a software evolution management engine, EvoMe, to streamline and simplify the development of such tools, allowing them to be easily prototyped using an intuitive query language and quickly deployed for different types of projects. EvoMe is based on differential factbase, a uniform exchangeable representation of evolving software artifacts, and can be accessed directly through a Web interface. We demonstrate the usage and key features of EvoMe on real open-source software projects. The demonstration video can\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:o9ULDYDKYbIC"}], "2020": [{"title": "Typestate-guided fuzzer for discovering use-after-free vulnerabilities", "description": "Existing coverage-based fuzzers usually use the individual control flow graph (CFG) edge coverage to guide the fuzzing process, which has shown great potential in finding vulnerabilities. However, CFG edge coverage is not effective in discovering vulnerabilities such as use-after-free (UaF). This is because, to trigger UaF vulnerabilities, one needs not only to cover individual edges, but also to traverse some (long) sequence of edges in a particular order, which is challenging for existing fuzzers. To this end, we propose to model UaF vulnerabilities as typestate properties, and develop a typestate-guided fuzzer, named UAFL, for discovering vulnerabilities violating typestate properties. Given a typestate property, we first perform a static typestate analysis to find operation sequences potentially violating the property. Our fuzzing process is then guided by the operation sequences in order to progressively generate test\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:KqnX2w3egDsC"}, {"title": "Audee: Automated testing for deep learning frameworks", "description": "Deep learning (DL) has been applied widely, and the quality of DL system becomes crucial, especially for safety-critical applications. Existing work mainly focuses on the quality analysis of DL models, but lacks attention to the underlying frameworks on which all DL models depend. In this work, we propose Audee, a novel approach for testing DL frameworks and localizing bugs. Audee adopts a search-based approach and implements three different mutation strategies to generate diverse test cases by exploring combinations of model structures, parameters, weights and inputs. Audee is able to detect three types of bugs: logical bugs, crashes and Not-a-Number (NaN) errors. In particular, for logical bugs, Audee adopts a cross-reference check to detect behavioural inconsistencies across multiple frameworks (e.g., TensorFlow and PyTorch), which may indicate potential bugs in their implementations. For NaN errors\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:8gBurD7jEYQC"}, {"title": "Oracle-supported dynamic exploit generation for smart contracts", "description": "Despite the high stakes involved in smart contracts, they are often developed in an undisciplined manner, leaving the security and reliability of blockchain transactions at risk. In this article, we introduce ContraMaster\u2014an oracle-supported dynamic exploit generation framework for smart contracts. Existing approaches mutate only single transactions; ContraMaster exceeds these by mutating the transaction sequences. ContraMaster uses data-flow, control-flow, and the dynamic contract state to guide its mutations. It then monitors the executions of target contract programs, and validates the results against a general-purpose semantic test oracle to discover vulnerabilities. Being a dynamic technique, it guarantees that each discovered vulnerability is a violation of the test oracle and is able to generate the attack script to exploit this vulnerability. In contrast to rule-based approaches, ContraMaster has not shown any\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:L7vk9XBBNxgC"}, {"title": "ARDiff: scaling program equivalence checking via iterative abstraction and refinement of common code", "description": "Equivalence checking techniques help establish whether two versions of a program exhibit the same behavior. The majority of popular techniques for formally proving/refuting equivalence relies on symbolic execution \u2013 a static analysis approach that reasons about program behaviors in terms of symbolic input variables. Yet, symbolic execution is difficult to scale in practice due to complex programming constructs, such as loops and non-linear arithmetic.   This paper proposes an approach, named ARDiff, for improving the scalability of symbolic-execution-based equivalence checking techniques when comparing syntactically-similar versions of a program, e.g., for verifying the correctness of code upgrades and refactoring. Our approach relies on a set of novel heuristics to determine which parts of the versions\u2019 common code can be effectively pruned during the analysis, reducing the analysis complexity without\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:5VjbC5aozO0C"}, {"title": "ModCon: A model-based testing platform for smart contracts", "description": "Unlike those on public permissionless blockchains, smart contracts on enterprise permissioned blockchains are not limited by resource constraints, and therefore often larger and more complex. Current testing and analysis tools lack support for such contracts, which demonstrate stateful behaviors and require special treatment in quality assurance. In this paper, we present a model-based testing platform, called ModCon, relying on user-specified models to define test oracles, guide test generation, and measure test adequacy. ModCon is Web-based and supports both permissionless and permissioned blockchain platforms. We demonstrate the usage and key features of ModCon on real enterprise smart contract applications.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:TeJ9juy8vcMC"}, {"title": "Marble: Model-based robustness analysis of stateful deep learning systems", "description": "State-of-the-art deep learning (DL) systems are vulnerable to adversarial examples, which hinders their potential adoption in safety-and security-critical scenarios. While some recent progress has been made in analyzing the robustness of feed-forward neural networks, the robustness analysis for stateful DL systems, such as recurrent neural networks (RNNs), still remains largely uncharted. In this paper, we propose Marble, a model-based approach for quantitative robustness analysis of real-world RNN-based DL systems. Marble builds a probabilistic model to compactly characterize the robustness of RNNs through abstraction. Furthermore, we propose an iterative refinement algorithm to derive a precise abstraction, which enables accurate quantification of the robustness measurement. We evaluate the effectiveness of Marble on both LSTM and GRU models trained separately with three popular natural language\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:x2hKVfJWtf0C"}, {"title": "Towards automated verification of smart contract fairness", "description": "Smart contracts are computer programs allowing users to define and execute transactions automatically on top of the blockchain platform. Many of such smart contracts can be viewed as games. A game-like contract accepts inputs from multiple participants, and upon ending, automatically derives an outcome while distributing assets according to some predefined rules. Without clear understanding of the game rules, participants may suffer from fraudulent advertisements and financial losses. In this paper, we present a framework to perform (semi-)automated verification of smart contract fairness, whose results can be used to refute false claims with concrete examples or certify contract implementations with respect to desired fairness properties. We implement FairCon, which is able to check fairness properties including truthfulness, efficiency, optimality, and collusion-freeness for Ethereum smart contracts. We\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:BCdnXsLIVDwC"}, {"title": "Precfix: Large-Scale Patch Recommendation by Mining Defect-Patch Pairs", "description": "Patch recommendation is the process of identifying errors in software systems and suggesting suitable fixes for them. Patch recommendation can significantly improve developer productivity by reducing both the debugging and repairing time. Existing techniques usually rely on complete test suites and detailed debugging reports, which are often absent in practical industrial settings. In this paper, we propose Precfix, a pragmatic approach targeting large-scale industrial codebase and making recommendations based on previously observed debugging activities. Precfix collects defect-patch pairs from development histories, performs clustering, and extracts generic reusable patching patterns as recommendations. We conducted experimental study on an industrial codebase with 10K projects involving diverse defect patterns. We managed to extract 3K templates of defect-patch pairs, which have been successfully\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:U5uP8zs9lfgC"}, {"title": "GenSlice: Generalized semantic history slicing", "description": "Semantic history slicing addresses the problem of identifying changes related to a particular high-level functionality from the software change histories. Existing solutions are either imprecise, resulting in larger-than-necessary history slices, or inefficient, taking a long time to execute. In this paper, we develop a generalized history slicing framework, named GenSlice, which overcomes the aforementioned limitations. GenSlice abstracts existing history slicing techniques and change history management operations (such as splitting commits into fine-grained changes) as history transformation operators, making it possible to apply them sequentially in various orders. We study and prove properties of various orders of operators and devise a systematic approach for efficiently producing history slices that are optimal for practical purposes. We report on an empirical evaluation of our framework, demonstrating its\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:dFKc6_kCK1wC"}], "2015": [{"title": "Semantic Slicing of Software Version Histories", "description": "Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, \u201cinheriting\u201d additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:8p8iYwVyaVcC"}, {"title": "Angelic Verification: Precise Verification Modulo Unknowns", "description": "Verification of open programs can be challenging in the presence of an unconstrained environment. Verifying properties that depend on the environment yields a large class of uninteresting false alarms. Using a verifier on a program thus requires extensive initial investment in modeling the environment of the program. We propose a technique called angelic verification for verification of open programs, where we constrain a verifier to report warnings only when no acceptable environment specification exists to prove the assertion. Our framework is parametric in a vocabulary and a set of angelic assertions that allows a user to configure the tool. We describe a few instantiations of the framework and an evaluation on a set of real-world benchmarks to show that our technique is competitive with industrial-strength tools even without models of the environment.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:wkm4DBaukwsC"}], "2013": [{"title": "UFO: Verification with Interpolants and Abstract Interpretation: (Competition Contribution)", "description": "Verification Approach The algorithms underlying Ufo are described in [1-3]. The Ufo tool is described in more detail in [4].  Ufo marries the power and efficiency of numerical Abstract Interpretation (AI) domains [6] with the generalizing ability of interpolation-based software verification in an abstraction refinement loop.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:3lUAU8Oskd0C"}], "2018": [{"title": "Client-specific equivalence checking", "description": "Software is often built by integrating components created by different teams or even different organizations. With little understanding of changes in dependent components, it is challenging to maintain correctness and robustness of the entire system. In this paper, we investigate the effect of component changes on the behavior of their clients. We observe that changes in a component are often irrelevant to a particular client and thus can be adopted without any delays or negative effects. Following this observation, we formulate the notion of client-specific equivalence checking (CSE) and develop an automated technique optimized for checking such equivalence. We evaluate our technique on a set of benchmarks, including those from the existing literature on equivalence checking, and show its applicability and effectiveness.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:ehoypfNsBj8C"}, {"title": "CSlicerCloud: a web-based semantic history slicing framework", "description": "Traditional commit-based sequential organization of software version histories is insufficient for many development tasks which require high-level, semantic understanding of program functionality, such as porting features or cutting new releases. Semantic history slicing is a technique which uses well-organized unit tests as identifiers for corresponding software functionalities and extracts a set of commits that correspond to a specific high-level functionality. In this paper, we present CSlicerCloud, a Web-based semantic history slicing service tailored for Java projects hosted on GitHub. It is accessible through Web browsers and powered in the backend by a collection of history slicing techniques underneath. We evaluated CSlicerCloud on a dataset containing developer-annotated change histories collected from 10 open source software projects. A video demonstration which showcases the main features of CSlicerC\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:vjZqxyZ7hS4C"}], "2016": [{"title": "Precise semantic history slicing through dynamic delta refinement", "description": "Semantic history slicing solves the problem of extracting changes related to a particular high-level functionality from the software version histories. State-of-the-art techniques combine static program analysis and dynamic execution tracing to infer an over-approximated set of changes that can preserve the functional behaviors captured by a test suite. However, due to the conservative nature of such techniques, the sliced histories may contain irrelevant changes. In this paper, we propose a divide-and-conquer-style partitioning approach enhanced by dynamic delta refinement to produce minimal semantic history slices. We utilize deltas in dynamic invariants generated from successive test executions to learn significance of changes with respect to the target functionality. Empirical results indicate that these measurements accurately rank changes according to their relevance to the desired test behaviors and thus\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:u_mOZUIutIEC"}, {"title": "Using metric temporal logic to specify scheduling problems", "description": "We introduce Scheduling MTL (SMTL) an extension of Metric Temporal Logic that supports the specification of complex scheduling problems with repeated and conditional occurrences of activities, and rich temporal relationships among them. We define the syntax and semantics of SMTL, and explore natural restrictions of the language to gain tractability. We also provide an algorithm for finding a schedule to a problem specified as an SMTL formula, and establish a novel equivalence between a fragment of MTL and simple temporal networks, a widely-used formalism in AI temporal planning.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:XtJa11BXPS4C"}], "2017": [{"title": "A Dataset for Dynamic Discovery of Semantic Changes in Version Controlled Software Histories", "description": "Over the last few years, researchers proposed several semantic history slicing approaches that identify the set of semantically-related commits implementing a particular software functionality. However, there is no comprehensive benchmark for evaluating these approaches, making it difficult to assess their capabilities. This paper presents a dataset of 81 semantic change data collected from 8 real-world projects. The dataset is created for benchmarking semantic history slicing techniques. We provide details on the data collection process and the storage format. We also discuss usage and possible extensions of the dataset.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:VjBpw8Hezy4C"}, {"title": "FHistorian: Locating features in version histories", "description": "Feature location techniques aim to locate software artifacts that implement a specific program functionality, a.k.a. a feature. In this paper, we build upon the previous work of semantic history slicing to locate features in software version histories. We leverage the information embedded in version histories for identifying changes implementing features and discovering relationships between features. The identified feature changes are fully functional and guaranteed to preserve the desired behaviors. The resulting feature relationship graph is precise and can be used to assist in understanding of the underlying connections between the features. We evaluate the technique on a number of real-world case studies and compare our results with developer-specified feature annotations. We conclude that, when available, historical information of software changes can lead to precise identification of features in existing\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:J4E9jCG1tHUC"}, {"title": "Managing Software Evolution through Semantic History Slicing", "description": "Software change histories are results of incremental updates made by developers. As a side-effect of the software development process, version history is a surprisingly useful source of information for understanding, maintaining and reusing software. However, traditional commit-based sequential organization of version histories lacks semantic structure and thus are insufficient for many development tasks that require high-level, semantic understanding of program functionality, such as locating feature implementations and porting hot fixes. In this work, we propose to use well-organized unit tests as identifiers for corresponding software functionalities. We then present a family of automated techniques which analyze the semantics of historical changes and assist developers in many everyday practical settings. For validation, we evaluate our approaches on a benchmark of developer-annotated version history\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:hEXC_dOfxuUC"}], "2022": [{"title": "Finding permission bugs in smart contracts with role mining", "description": "Smart contracts deployed on permissionless blockchains, such as Ethereum, are accessible to any user in a trustless environment. Therefore, most smart contract applications implement access control policies to protect their valuable assets from unauthorized accesses. A difficulty in validating the conformance to such policies, i.e., whether the contract implementation adheres to the expected behaviors, is the lack of policy specifications. In this paper, we mine past transactions of a contract to recover a likely access control model, which can then be checked against various information flow policies and identify potential bugs related to user permissions. We implement our role mining and security policy validation in tool SPCon. The experimental evaluation on labeled smart contract role mining benchmark demonstrates that SPCon effectively mines more accurate user roles compared to the state-of-the-art role mining\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:TiIbgCYny7sC", "topics": "Access Control, Role Mining, Smart Contracts"}, {"title": "Towards understanding third-party library dependency in C/C++ ecosystem", "description": "Third-party libraries (TPLs) are frequently reused in software to reduce development cost and the time to market. However, external library dependencies may introduce vulnerabilities into host applications. The issue of library dependency has received considerable critical attention. Many package managers, such as Maven, Pip, and NPM, are proposed to manage TPLs. Moreover, a significant amount of effort has been put into studying dependencies in language ecosystems like Java, Python, and JavaScript except C/C++. Due to the lack of a unified package manager for C/C++, existing research has only few understanding of TPL dependencies in the C/C++ ecosystem, especially at large scale.  Towards understanding TPL dependencies in the C/C++ ecosystem, we collect existing TPL databases, package management tools, and dependency detection tools, summarize the dependency patterns of C/C\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:RJNGbXJAtMsC", "topics": "Dependency Analysis, Software Reuse, Package Management"}, {"title": "InvCon: A Dynamic Invariant Detector for Ethereum Smart Contracts", "description": "Smart contracts are self-executing computer programs deployed on blockchain to enable trustworthy exchange of value without the need of a central authority. With the absence of documentation and specifications, routine tasks such as program understanding, maintenance, verification, and validation, remain challenging for smart contracts. In this paper, we propose a dynamic invariant detection tool, InvCon, for Ethereum smart contracts to mitigate this issue. The detected invariants can be used to not only support the reverse engineering of contract specifications, but also enable standard-compliance checking for contract implementations. InvCon provides a Web-based interface and a demonstration video of it is available at: https://youtu.be/Y1QBHjDSMYk.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:s1ouQE5r0WUC", "topics": "Program Understanding, Verification, Validation"}, {"title": "SolSEE: A Source-Level Symbolic Execution Engine for Solidity", "description": "Most of the existing smart contract symbolic execution tools perform analysis on bytecode, which loses high-level semantic information presented in source code. This makes interactive analysis tasks\u2014such as visualization and debugging\u2014extremely challenging, and significantly limits the tool usability. In this paper, we present SolSEE, a source-level symbolic execution engine for Solidity smart contracts. We describe the design of SolSEE, highlight its key features, and demonstrate its usages through a Web-based user interface. SolSEE demonstrates advantages over other existing source-level analysis tools in the advanced Solidity language features it supports and analysis flexibility. A demonstration video is available at: https://sites.google.com/view/solsee/.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:pxXbYLTb8EgC", "topics": "Symbolic Execution, Smart Contracts, Solidity"}, {"title": "GALOIS: boosting deep reinforcement learning via generalizable logic synthesis", "description": "Despite achieving superior performance in human-level control problems, unlike humans, deep reinforcement learning (DRL) lacks high-order intelligence (eg, logic deduction and reuse), thus it behaves ineffectively than humans regarding learning and generalization in complex problems. Previous works attempt to directly synthesize a white-box logic program as the DRL policy, manifesting logic-driven behaviors. However, most synthesis methods are built on imperative or declarative programming, and each has a distinct limitation, respectively. The former ignores the cause-effect logic during synthesis, resulting in low generalizability across tasks. The latter is strictly proof-based, thus failing to synthesize programs with complex hierarchical logic. In this paper, we combine the above two paradigms together and propose a novel Generalizable Logic Synthesis (GALOIS) framework to synthesize hierarchical and strict cause-effect logic programs. GALOIS leverages the program sketch and defines a new sketch-based hybrid program language for guiding the synthesis. Based on that, GALOIS proposes a sketch-based program synthesis method to automatically generate white-box programs with generalizable and interpretable cause-effect logic. Extensive evaluations on various decision-making tasks with complex logic demonstrate the superiority of GALOIS over mainstream baselines regarding the asymptotic performance, generalizability, and great knowledge reusability across different environments.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:Ei5r6KrKXVQC", "topics": "Logic Synthesis, Deep Reinforcement Learning"}, {"title": "Large-scale analysis of non-termination bugs in real-world OSS projects", "description": "Termination is a crucial program property. Non-termination bugs can be subtle to detect and may remain hidden for long before they take effect. Many real-world programs still suffer from vast consequences (e.g., no response) caused by non-termination bugs. As a classic problem, termination proving has been studied for many years. Many termination checking tools and techniques have been developed and demonstrated effectiveness on existing well-established benchmarks. However, the capability of these tools in finding practical non-termination bugs has yet to be tested on real-world projects. To fill in this gap, in this paper, we conducted the first large-scale empirical study of non-termination bugs in real-world OSS projects. Specifically, we first devoted substantial manual efforts in collecting and analyzing 445 non-termination bugs from 3,142 GitHub commits and provided a systematic classifi-cation of the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:1paMEeroeoQC", "topics": "Program Analysis, Bug Detection, Empirical Study"}, {"title": "Demystifying performance regressions in string solvers", "description": "Over the past few years, SMT string solvers have found their applications in an increasing number of domains, such as program analyses in mobile and Web applications, which require the ability to reason about string values. A series of research has been carried out to find quality issues of string solvers in terms of its correctness and performance. Yet, none of them has considered the performance regressions happening across multiple versions of a string solver. To fill this gap, in this paper, we focus on solver performance regressions (SPRs), i.e., unintended slowdowns introduced during the evolution of string solvers. To this end, we develop  SPRFinder to not only generate test cases demonstrating SPRs, but also localize the probable causes of them, in terms of commits. We evaluated the effectiveness of  SPRFinder on three state-of-the-art string solvers, i.e., Z3Seq, Z3Str3, and CVC4. The results demonstrate\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:yCjxvIMm6_oC", "topics": "Program Analysis, Software Testing, Performance Optimization"}, {"title": "Property-Based Automated Repair of DeFi Protocols", "description": "Programming errors enable security attacks on smart contracts, which are used to manage large sums of financial assets. Automated program repair (APR) techniques aim to reduce developers\u2019 burden of manually fixing bugs by automatically generating patches for a given issue. Existing APR tools for smart contracts focus on mitigating typical smart contract vulnerabilities rather than violations of functional specification. However, in decentralized financial (DeFi) smart contracts, the inconsistency between intended behavior and implementation translates into the deviation from the underlying financial model, resulting in monetary losses for the application and its users. In this work, we propose DeFinery\u2014a technique for automated repair of a smart contract that does not satisfy a user-defined correctness property. To explore a larger set of diverse patches while providing formal correctness guarantees w.r.t. the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:WM2K3OHRCGMC", "topics": "Automated Program Repair, Smart Contracts, Decentralized Finance"}, {"title": "Semantic History Slicing", "description": "Feature location techniques aim to locate software artifacts that implement a specific program functionality, a.k.a. a feature. In this chapter, we introduce a technique called semantic history slicing, to locate features in software version histories. The key insight is that the information embedded in version histories can be used to establish a connection between features and changes implementing the features. We present the formal definitions as well as the general principles of semantic history slicing and provide details on a specific history slicing algorithm based on change dependency analysis. The identified feature implementing changes are fully functional and guaranteed to preserve the desired behaviors.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:IvSMUa3B7yYC", "topics": "Information Retrieval, Software Engineering"}, {"title": "Identifying Solidity Smart Contract API Documentation Errors", "description": "Smart contracts are gaining popularity as a means to support transparent, traceable, and self-executing decentralized applications, which enable the exchange of value in a trustless environment. Developers of smart contracts rely on various libraries, such as OpenZeppelin for Solidity contracts, to improve application quality and reduce development costs. The API documentations of these libraries are important sources of information for developers who are unfamiliar with the APIs. Yet, maintaining high-quality documentations is non-trivial, and errors in documentations may place barriers for developers to learn the correct usages of APIs. In this paper, we propose a technique, DocCon, to detect inconsistencies between documentations and the corresponding code for Solidity smart contract libraries. Our fact-based approach allows inconsistencies of different severity levels to be queried, from a database\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:w7CBUyPWg-0C", "topics": "Information Retrieval, Software Engineering"}], "2023": [{"title": "Client-Specific Upgrade Compatibility Checking via Knowledge-Guided Discovery", "description": "Modern software systems are complex, and they heavily rely on external libraries developed by different teams and organizations. Such systems suffer from higher instability due to incompatibility issues caused by library upgrades. In this article, we address the problem by investigating the impact of a library upgrade on the behaviors of its clients. We developed CompCheck, an automated upgrade compatibility checking framework that generates incompatibility-revealing tests based on previous examples. CompCheck first establishes an offline knowledge base of incompatibility issues by mining from open source projects and their upgrades. It then discovers incompatibilities for a specific client project, by searching for similar library usages in the knowledge base and generating tests to reveal the problems. We evaluated CompCheck on 202 call sites of 37 open source projects and the results show that\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:gFrPXmx1TSsC"}, {"title": "Smart Contract Parallel Execution with Fine-Grained State Accesses", "description": "As various optimizations being proposed recently, the performance of blockchains is no longer limited by the consensus protocols, successfully scaling to thousands of transactions per second. To further improve blockchains' throughput, exploiting the parallelism in smart contract executions becomes a clear solution to resolve the new performance bottleneck. The existing techniques perform concurrency control on smart contract transactions based on pre-determined read/write sets, which can hardly be calculated precisely. As a result, many parallelization opportunities are missed in order to maintain the correctness of transaction executions. In this paper, we propose a novel execution scheduling framework, DMVCC, to further increase the parallelism in smart contract executions, via more fine-grained control on state accesses. DMVCC improves over existing techniques with two key features: (1) write versioning\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:-38epGy1wY0C"}, {"title": "Responsibility in Context: On Applicability of Slicing in Semantic Regression Analysis", "description": "Numerous program slicing approaches aim to help developers troubleshoot regression failures - one of the most time-consuming development tasks. The main idea behind these approaches is to identify a subset of interdependent program statements relevant to the failure, minimizing the amount of code developers need to inspect. Accuracy and reduction rate achieved by slicing are the key considerations toward their applicability in practice: inspecting only the statements in a slice should be faster and more efficient than inspecting the code in full. In this paper, we report on our experiment applying one of the most recent and accurate slicing approaches, dual slicing, to the task of troubleshooting regression failures. As subjects, we use projects from the popular Defects4J benchmark and a systematically-collected set of eight large, open-source client-library project pairs with at least one library upgrade failure\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:Q3-QASNKTMEC"}], "2019": [{"title": "Deepstellar: Model-based quantitative analysis of stateful deep learning systems", "description": "Deep Learning (DL) has achieved tremendous success in many cutting-edge applications. However, the state-of-the-art DL systems still suffer from quality issues. While some recent progress has been made on the analysis of feed-forward DL systems, little study has been done on the Recurrent Neural Network (RNN)-based stateful DL systems, which are widely used in audio, natural languages and video processing, etc. In this paper, we initiate the very first step towards the quantitative analysis of RNN-based DL systems. We model RNN as an abstract state transition system to characterize its internal behaviors. Based on the abstract model, we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of RNNs. We further propose two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation. We evaluate\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:E8ajGqO0XoUC"}, {"title": "Vultron: catching vulnerable smart contracts once and for all", "description": "Despite the high stakes involved, smart contracts are often developed in an undisciplined way thus far. The existence of vulnerabilities compromises the security and reliability of smart contracts, and endangers the trust of participants in their ongoing businesses. Existing vulnerability detection techniques are often designed case-by-case, making them difficult to generalize. In this paper, we design general principles for detecting vulnerable smart contracts. Our key insight is that almost all the existing transaction-related vulnerabilities are due to the mismatch between the actual transferred amount and the amount reflected on the contract's internal bookkeeping. Based on this, we propose a precise and generally applicable technique, VULTRON, which can detect irregular transactions due to various types of adversarial exploits. We also report preliminary results applying our technique to real-world case studies.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:_IsBomjs8bsC"}, {"title": "A quantitative analysis framework for recurrent neural network", "description": "Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:GHsHDPAyICYC"}, {"title": "Coverage-guided fuzzing for feedforward neural networks", "description": "Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:feST4K8J0scC"}], "2014": [{"title": "Symbolic Optimization with SMT Solvers", "description": "The rise in efficiency of Satisfiability Modulo Theories (SMT) solvers has created numerous uses for them in software verification, program synthesis, functional programming, refinement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments (e.g., a witness for a bug) or proving unsatisfiability/validity(e.g., proving that a subtyping relation holds). We are often interested in finding not just an arbitrary satisfying assignment, but one that optimizes (minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions that maximize energy usage (performance bugs), or synthesizing short programs that do not make expensive API calls. Unfortunately, none of the available SMT solvers offer such optimization capabilities. In this paper, we present SYMBA, an efficient SMT-based optimization algorithm for objective functions in the theory of linear\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:60iIaj97TE0C"}, {"title": "Model checking approach to automated planning", "description": "Model checking provides a way to automatically explore the state space of a finite state system based on desired properties, whereas planning is to produce a sequence of actions that leads from the initial state to the target goal states. Previous research in this field proposed a number of approaches for connecting model checking with planning problem solving. In this paper, we investigate the feasibility of using an established model checking framework, Process Analysis Toolkit (PAT), as a planning solution provider for upper layer applications. To achieve this, we first carry out a number of experiments on different model checking tools in order to compare their performance and capabilities on planning problem solving. Our experimental results suggest that solving planning problems using model checkers is not only possible but also practical. We then propose a formal semantic mapping from the standard\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:QaSi33NTfwYC"}, {"title": "Management of time requirements in component-based systems", "description": "In component-based systems, a number of existing software components are combined in order to achieve business goals. Some of such goals may include system-level (global) timing requirements (GTR). It is essential to refine GTR into a set of component-level (local) timing requirements (LTRs) so that if a set of candidate components collectively meets them, then the corresponding GTR is also satisfied. Existing techniques for computing LTRs produce monolithic representations, that have dependencies over multiple components. Such representations do not allow for effective component selection and repair. In this paper, we propose an approach for building under-approximated LTRs (uLTR) consisting of independent constraints over components. We then show how uLTR can be used to improve the design, monitoring and repair of component-based systems under time requirements. We also report on\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:htyGaKyDgHMC"}], "2012": [{"title": "UFO: A framework for abstraction-and interpolation-based software verification", "description": "In this paper, we present Ufo, a framework and a tool for verifying (and finding bugs in) sequential C programs. The framework is built on top of the LLVM compiler infrastructure and is targeted at researchers designing and experimenting with verification algorithms. It allows definition of different abstract post operators, refinement strategies and exploration strategies. We have built three instantiations of the framework: a predicate abstraction-based version, an interpolation-based version, and a combined version which uses a novel and powerful combination of interpolation-based and predicate abstraction-based algorithms.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:u-x6o8ySG0sC"}, {"title": "Translating PDDL into csp#-the PAT approach", "description": "Model checking provides a way to automatically verify hardware and software systems, whereas the goal of planning is to produce a sequence of actions that leads from the initial state to the desired goal state. Recently research indicates that there is a strong connection between model checking and planning problem solving. In this paper, we investigate the feasibility of using a newly developed model checking framework, Process Analysis Toolkit (PAT), to serve as a planning solution provider for upper layer applications. We first carried out a number of experiments on different planning tools in order to compare their performance and capabilities. Our experimental results showed that the performance of the PAT model checker is comparable to that of state-of-art planners for certain categories of problems. We further propose a set of translation rules for mapping from a commonly used planning notation - PDDL\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:xyvS_IvSCKsC"}, {"title": "Planning as model checking tasks", "description": "Model checking provides a way to automatically verify hardware and software systems, whereas the goal of planning is to produce a sequence of actions that leads from the initial state to the desired goal states. Recently research indicates that there is a strong connection between model checking and planning problem solving. In this paper, we investigate the feasibility of using different model checking tools and techniques for solving classic planning problems. To achieve this, we carried out a number of experiments on different planning domains in order to compare the performance and capabilities of various tools. Our experimental results indicate that the performance of some model checkers is comparable to that of state-of-theart planners for certain categories of problems. In particular, a new planning module with specifically designed searching algorithm is implemented on top of the established model\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:0sTkTiv_uMkC"}], "2021": [{"title": "A survey of smart contract formal specification and verification", "description": "A smart contract is a computer program that allows users to automate their actions on the blockchain platform. Given the significance of smart contracts in supporting important activities across industry sectors including supply chain, finance, legal, and medical services, there is a strong demand for verification and validation techniques. Yet, the vast majority of smart contracts lack any kind of formal specification, which is essential for establishing their correctness. In this survey, we investigate formal models and specifications of smart contracts presented in the literature and present a systematic overview to understand the common trends. We also discuss the current approaches used in verifying such property specifications and identify gaps with the hope to recognize promising directions for future work.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:Ul_CLA4dPeMC"}, {"title": "Formal analysis of composable DeFi protocols", "description": "Decentralized finance (DeFi) has become one of the most successful applications of blockchain and smart contracts. The DeFi ecosystem enables a wide range of crypto-financial activities, while the underlying smart contracts often contain bugs, with many vulnerabilities arising from the unforeseen consequences of composing DeFi protocols together. In this paper, we propose a formal process-algebraic technique that models DeFi protocols in a compositional manner to allow for efficient property verification. We also conduct a case study to demonstrate the proposed approach in analyzing the composition of two interacting DeFi protocols, namely, Curve and Compound. Finally, we discuss how the proposed modeling and verification approach can be used to analyze financial and security properties of interest.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:XeErXHja3Z8C"}, {"title": "Diffbase: A differential factbase for effective software evolution management", "description": "Numerous tools and techniques have been developed to extract and analyze information from software development artifacts. Yet, there is a lack of effective method to process, store, and exchange information among different analyses. In this paper, we propose differential factbase, a uniform exchangeable representation supporting efficient querying and manipulation, based on the existing concept of program facts. We consider program changes as first-class objects, which establish links between intra-version facts of single program snapshots and provide insights on how certain artifacts evolve over time via inter-version facts. We implement a series of differential fact extractors supporting different programming languages and platforms, and demonstrate with usage scenarios the benefits of adopting differential facts in supporting software evolution management.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:NnTm98qLMbgC"}, {"title": "Eqbench: A dataset of equivalent and non-equivalent program pairs", "description": "Equivalence checking techniques help establish whether two versions of a program exhibit the same behavior. The majority of popular techniques for formally proving/refuting equivalence are evaluated on small and simplistic benchmarks, omitting \"difficult\" programming constructs, such as non-linear arithmetic, loops, floating-point arithmetic, and string and array manipulation. This hinders efficient evaluation of these techniques and the ability to establish their practical applicability in real scenarios. This paper addresses this gap by contributing EqBench - the largest and most comprehensive benchmark for equivalence checking analysis, which contains 147 equivalent and 125 non-equivalent cases, in both C and Java languages. We believe EqBench can facilitate a more realistic evaluation of equivalence checking techniques, assessing their individual strength and weaknesses. EqBench is publicly available at\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:rzmi0EmCOGEC"}, {"title": "EVOME: A software evolution management engine based on differential factbase", "description": "Managing large and fast-evolving software systems can be a challenging task. Numerous solutions have been developed to assist in this process, enhancing software quality and reducing development costs. These techniques\u2014e.g., regression test selection and change impact analysis\u2014are often built as standalone tools, unable to share or reuse information among them. In this paper, we introduce a software evolution management engine, EvoMe, to streamline and simplify the development of such tools, allowing them to be easily prototyped using an intuitive query language and quickly deployed for different types of projects. EvoMe is based on differential factbase, a uniform exchangeable representation of evolving software artifacts, and can be accessed directly through a Web interface. We demonstrate the usage and key features of EvoMe on real open-source software projects. The demonstration video can\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:o9ULDYDKYbIC"}], "2020": [{"title": "Typestate-guided fuzzer for discovering use-after-free vulnerabilities", "description": "Existing coverage-based fuzzers usually use the individual control flow graph (CFG) edge coverage to guide the fuzzing process, which has shown great potential in finding vulnerabilities. However, CFG edge coverage is not effective in discovering vulnerabilities such as use-after-free (UaF). This is because, to trigger UaF vulnerabilities, one needs not only to cover individual edges, but also to traverse some (long) sequence of edges in a particular order, which is challenging for existing fuzzers. To this end, we propose to model UaF vulnerabilities as typestate properties, and develop a typestate-guided fuzzer, named UAFL, for discovering vulnerabilities violating typestate properties. Given a typestate property, we first perform a static typestate analysis to find operation sequences potentially violating the property. Our fuzzing process is then guided by the operation sequences in order to progressively generate test\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:KqnX2w3egDsC"}, {"title": "Audee: Automated testing for deep learning frameworks", "description": "Deep learning (DL) has been applied widely, and the quality of DL system becomes crucial, especially for safety-critical applications. Existing work mainly focuses on the quality analysis of DL models, but lacks attention to the underlying frameworks on which all DL models depend. In this work, we propose Audee, a novel approach for testing DL frameworks and localizing bugs. Audee adopts a search-based approach and implements three different mutation strategies to generate diverse test cases by exploring combinations of model structures, parameters, weights and inputs. Audee is able to detect three types of bugs: logical bugs, crashes and Not-a-Number (NaN) errors. In particular, for logical bugs, Audee adopts a cross-reference check to detect behavioural inconsistencies across multiple frameworks (e.g., TensorFlow and PyTorch), which may indicate potential bugs in their implementations. For NaN errors\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:8gBurD7jEYQC"}, {"title": "Oracle-supported dynamic exploit generation for smart contracts", "description": "Despite the high stakes involved in smart contracts, they are often developed in an undisciplined manner, leaving the security and reliability of blockchain transactions at risk. In this article, we introduce ContraMaster\u2014an oracle-supported dynamic exploit generation framework for smart contracts. Existing approaches mutate only single transactions; ContraMaster exceeds these by mutating the transaction sequences. ContraMaster uses data-flow, control-flow, and the dynamic contract state to guide its mutations. It then monitors the executions of target contract programs, and validates the results against a general-purpose semantic test oracle to discover vulnerabilities. Being a dynamic technique, it guarantees that each discovered vulnerability is a violation of the test oracle and is able to generate the attack script to exploit this vulnerability. In contrast to rule-based approaches, ContraMaster has not shown any\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:L7vk9XBBNxgC"}, {"title": "ARDiff: scaling program equivalence checking via iterative abstraction and refinement of common code", "description": "Equivalence checking techniques help establish whether two versions of a program exhibit the same behavior. The majority of popular techniques for formally proving/refuting equivalence relies on symbolic execution \u2013 a static analysis approach that reasons about program behaviors in terms of symbolic input variables. Yet, symbolic execution is difficult to scale in practice due to complex programming constructs, such as loops and non-linear arithmetic.   This paper proposes an approach, named ARDiff, for improving the scalability of symbolic-execution-based equivalence checking techniques when comparing syntactically-similar versions of a program, e.g., for verifying the correctness of code upgrades and refactoring. Our approach relies on a set of novel heuristics to determine which parts of the versions\u2019 common code can be effectively pruned during the analysis, reducing the analysis complexity without\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:5VjbC5aozO0C"}, {"title": "ModCon: A model-based testing platform for smart contracts", "description": "Unlike those on public permissionless blockchains, smart contracts on enterprise permissioned blockchains are not limited by resource constraints, and therefore often larger and more complex. Current testing and analysis tools lack support for such contracts, which demonstrate stateful behaviors and require special treatment in quality assurance. In this paper, we present a model-based testing platform, called ModCon, relying on user-specified models to define test oracles, guide test generation, and measure test adequacy. ModCon is Web-based and supports both permissionless and permissioned blockchain platforms. We demonstrate the usage and key features of ModCon on real enterprise smart contract applications.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:TeJ9juy8vcMC"}, {"title": "Marble: Model-based robustness analysis of stateful deep learning systems", "description": "State-of-the-art deep learning (DL) systems are vulnerable to adversarial examples, which hinders their potential adoption in safety-and security-critical scenarios. While some recent progress has been made in analyzing the robustness of feed-forward neural networks, the robustness analysis for stateful DL systems, such as recurrent neural networks (RNNs), still remains largely uncharted. In this paper, we propose Marble, a model-based approach for quantitative robustness analysis of real-world RNN-based DL systems. Marble builds a probabilistic model to compactly characterize the robustness of RNNs through abstraction. Furthermore, we propose an iterative refinement algorithm to derive a precise abstraction, which enables accurate quantification of the robustness measurement. We evaluate the effectiveness of Marble on both LSTM and GRU models trained separately with three popular natural language\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:x2hKVfJWtf0C"}, {"title": "Towards automated verification of smart contract fairness", "description": "Smart contracts are computer programs allowing users to define and execute transactions automatically on top of the blockchain platform. Many of such smart contracts can be viewed as games. A game-like contract accepts inputs from multiple participants, and upon ending, automatically derives an outcome while distributing assets according to some predefined rules. Without clear understanding of the game rules, participants may suffer from fraudulent advertisements and financial losses. In this paper, we present a framework to perform (semi-)automated verification of smart contract fairness, whose results can be used to refute false claims with concrete examples or certify contract implementations with respect to desired fairness properties. We implement FairCon, which is able to check fairness properties including truthfulness, efficiency, optimality, and collusion-freeness for Ethereum smart contracts. We\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:BCdnXsLIVDwC"}, {"title": "Precfix: Large-Scale Patch Recommendation by Mining Defect-Patch Pairs", "description": "Patch recommendation is the process of identifying errors in software systems and suggesting suitable fixes for them. Patch recommendation can significantly improve developer productivity by reducing both the debugging and repairing time. Existing techniques usually rely on complete test suites and detailed debugging reports, which are often absent in practical industrial settings. In this paper, we propose Precfix, a pragmatic approach targeting large-scale industrial codebase and making recommendations based on previously observed debugging activities. Precfix collects defect-patch pairs from development histories, performs clustering, and extracts generic reusable patching patterns as recommendations. We conducted experimental study on an industrial codebase with 10K projects involving diverse defect patterns. We managed to extract 3K templates of defect-patch pairs, which have been successfully\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:U5uP8zs9lfgC"}, {"title": "GenSlice: Generalized semantic history slicing", "description": "Semantic history slicing addresses the problem of identifying changes related to a particular high-level functionality from the software change histories. Existing solutions are either imprecise, resulting in larger-than-necessary history slices, or inefficient, taking a long time to execute. In this paper, we develop a generalized history slicing framework, named GenSlice, which overcomes the aforementioned limitations. GenSlice abstracts existing history slicing techniques and change history management operations (such as splitting commits into fine-grained changes) as history transformation operators, making it possible to apply them sequentially in various orders. We study and prove properties of various orders of operators and devise a systematic approach for efficiently producing history slices that are optimal for practical purposes. We report on an empirical evaluation of our framework, demonstrating its\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:dFKc6_kCK1wC"}], "2015": [{"title": "Semantic Slicing of Software Version Histories", "description": "Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, \u201cinheriting\u201d additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:8p8iYwVyaVcC"}, {"title": "Angelic Verification: Precise Verification Modulo Unknowns", "description": "Verification of open programs can be challenging in the presence of an unconstrained environment. Verifying properties that depend on the environment yields a large class of uninteresting false alarms. Using a verifier on a program thus requires extensive initial investment in modeling the environment of the program. We propose a technique called angelic verification for verification of open programs, where we constrain a verifier to report warnings only when no acceptable environment specification exists to prove the assertion. Our framework is parametric in a vocabulary and a set of angelic assertions that allows a user to configure the tool. We describe a few instantiations of the framework and an evaluation on a set of real-world benchmarks to show that our technique is competitive with industrial-strength tools even without models of the environment.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:wkm4DBaukwsC"}], "2013": [{"title": "UFO: Verification with Interpolants and Abstract Interpretation: (Competition Contribution)", "description": "Verification Approach The algorithms underlying Ufo are described in [1-3]. The Ufo tool is described in more detail in [4].  Ufo marries the power and efficiency of numerical Abstract Interpretation (AI) domains [6] with the generalizing ability of interpolation-based software verification in an abstraction refinement loop.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:3lUAU8Oskd0C"}], "2018": [{"title": "Client-specific equivalence checking", "description": "Software is often built by integrating components created by different teams or even different organizations. With little understanding of changes in dependent components, it is challenging to maintain correctness and robustness of the entire system. In this paper, we investigate the effect of component changes on the behavior of their clients. We observe that changes in a component are often irrelevant to a particular client and thus can be adopted without any delays or negative effects. Following this observation, we formulate the notion of client-specific equivalence checking (CSE) and develop an automated technique optimized for checking such equivalence. We evaluate our technique on a set of benchmarks, including those from the existing literature on equivalence checking, and show its applicability and effectiveness.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:ehoypfNsBj8C"}, {"title": "CSlicerCloud: a web-based semantic history slicing framework", "description": "Traditional commit-based sequential organization of software version histories is insufficient for many development tasks which require high-level, semantic understanding of program functionality, such as porting features or cutting new releases. Semantic history slicing is a technique which uses well-organized unit tests as identifiers for corresponding software functionalities and extracts a set of commits that correspond to a specific high-level functionality. In this paper, we present CSlicerCloud, a Web-based semantic history slicing service tailored for Java projects hosted on GitHub. It is accessible through Web browsers and powered in the backend by a collection of history slicing techniques underneath. We evaluated CSlicerCloud on a dataset containing developer-annotated change histories collected from 10 open source software projects. A video demonstration which showcases the main features of CSlicerC\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:vjZqxyZ7hS4C"}], "2016": [{"title": "Precise semantic history slicing through dynamic delta refinement", "description": "Semantic history slicing solves the problem of extracting changes related to a particular high-level functionality from the software version histories. State-of-the-art techniques combine static program analysis and dynamic execution tracing to infer an over-approximated set of changes that can preserve the functional behaviors captured by a test suite. However, due to the conservative nature of such techniques, the sliced histories may contain irrelevant changes. In this paper, we propose a divide-and-conquer-style partitioning approach enhanced by dynamic delta refinement to produce minimal semantic history slices. We utilize deltas in dynamic invariants generated from successive test executions to learn significance of changes with respect to the target functionality. Empirical results indicate that these measurements accurately rank changes according to their relevance to the desired test behaviors and thus\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:u_mOZUIutIEC"}, {"title": "Using metric temporal logic to specify scheduling problems", "description": "We introduce Scheduling MTL (SMTL) an extension of Metric Temporal Logic that supports the specification of complex scheduling problems with repeated and conditional occurrences of activities, and rich temporal relationships among them. We define the syntax and semantics of SMTL, and explore natural restrictions of the language to gain tractability. We also provide an algorithm for finding a schedule to a problem specified as an SMTL formula, and establish a novel equivalence between a fragment of MTL and simple temporal networks, a widely-used formalism in AI temporal planning.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:XtJa11BXPS4C"}], "2017": [{"title": "A Dataset for Dynamic Discovery of Semantic Changes in Version Controlled Software Histories", "description": "Over the last few years, researchers proposed several semantic history slicing approaches that identify the set of semantically-related commits implementing a particular software functionality. However, there is no comprehensive benchmark for evaluating these approaches, making it difficult to assess their capabilities. This paper presents a dataset of 81 semantic change data collected from 8 real-world projects. The dataset is created for benchmarking semantic history slicing techniques. We provide details on the data collection process and the storage format. We also discuss usage and possible extensions of the dataset.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&citation_for_view=B61QIUIAAAAJ:VjBpw8Hezy4C"}, {"title": "FHistorian: Locating features in version histories", "description": "Feature location techniques aim to locate software artifacts that implement a specific program functionality, a.k.a. a feature. In this paper, we build upon the previous work of semantic history slicing to locate features in software version histories. We leverage the information embedded in version histories for identifying changes implementing features and discovering relationships between features. The identified feature changes are fully functional and guaranteed to preserve the desired behaviors. The resulting feature relationship graph is precise and can be used to assist in understanding of the underlying connections between the features. We evaluate the technique on a number of real-world case studies and compare our results with developer-specified feature annotations. We conclude that, when available, historical information of software changes can lead to precise identification of features in existing\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:J4E9jCG1tHUC"}, {"title": "Managing Software Evolution through Semantic History Slicing", "description": "Software change histories are results of incremental updates made by developers. As a side-effect of the software development process, version history is a surprisingly useful source of information for understanding, maintaining and reusing software. However, traditional commit-based sequential organization of version histories lacks semantic structure and thus are insufficient for many development tasks that require high-level, semantic understanding of program functionality, such as locating feature implementations and porting hot fixes. In this work, we propose to use well-organized unit tests as identifiers for corresponding software functionalities. We then present a family of automated techniques which analyze the semantics of historical changes and assist developers in many everyday practical settings. For validation, we evaluate our approaches on a benchmark of developer-annotated version history\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:hEXC_dOfxuUC"}], "2022": [{"title": "Finding permission bugs in smart contracts with role mining", "description": "Smart contracts deployed on permissionless blockchains, such as Ethereum, are accessible to any user in a trustless environment. Therefore, most smart contract applications implement access control policies to protect their valuable assets from unauthorized accesses. A difficulty in validating the conformance to such policies, i.e., whether the contract implementation adheres to the expected behaviors, is the lack of policy specifications. In this paper, we mine past transactions of a contract to recover a likely access control model, which can then be checked against various information flow policies and identify potential bugs related to user permissions. We implement our role mining and security policy validation in tool SPCon. The experimental evaluation on labeled smart contract role mining benchmark demonstrates that SPCon effectively mines more accurate user roles compared to the state-of-the-art role mining\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:TiIbgCYny7sC", "topics": "Access Control, Role Mining, Smart Contracts"}, {"title": "Towards understanding third-party library dependency in C/C++ ecosystem", "description": "Third-party libraries (TPLs) are frequently reused in software to reduce development cost and the time to market. However, external library dependencies may introduce vulnerabilities into host applications. The issue of library dependency has received considerable critical attention. Many package managers, such as Maven, Pip, and NPM, are proposed to manage TPLs. Moreover, a significant amount of effort has been put into studying dependencies in language ecosystems like Java, Python, and JavaScript except C/C++. Due to the lack of a unified package manager for C/C++, existing research has only few understanding of TPL dependencies in the C/C++ ecosystem, especially at large scale.  Towards understanding TPL dependencies in the C/C++ ecosystem, we collect existing TPL databases, package management tools, and dependency detection tools, summarize the dependency patterns of C/C\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:RJNGbXJAtMsC", "topics": "Dependency Analysis, Software Reuse, Package Management"}, {"title": "InvCon: A Dynamic Invariant Detector for Ethereum Smart Contracts", "description": "Smart contracts are self-executing computer programs deployed on blockchain to enable trustworthy exchange of value without the need of a central authority. With the absence of documentation and specifications, routine tasks such as program understanding, maintenance, verification, and validation, remain challenging for smart contracts. In this paper, we propose a dynamic invariant detection tool, InvCon, for Ethereum smart contracts to mitigate this issue. The detected invariants can be used to not only support the reverse engineering of contract specifications, but also enable standard-compliance checking for contract implementations. InvCon provides a Web-based interface and a demonstration video of it is available at: https://youtu.be/Y1QBHjDSMYk.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:s1ouQE5r0WUC", "topics": "Program Understanding, Verification, Validation"}, {"title": "SolSEE: A Source-Level Symbolic Execution Engine for Solidity", "description": "Most of the existing smart contract symbolic execution tools perform analysis on bytecode, which loses high-level semantic information presented in source code. This makes interactive analysis tasks\u2014such as visualization and debugging\u2014extremely challenging, and significantly limits the tool usability. In this paper, we present SolSEE, a source-level symbolic execution engine for Solidity smart contracts. We describe the design of SolSEE, highlight its key features, and demonstrate its usages through a Web-based user interface. SolSEE demonstrates advantages over other existing source-level analysis tools in the advanced Solidity language features it supports and analysis flexibility. A demonstration video is available at: https://sites.google.com/view/solsee/.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:pxXbYLTb8EgC", "topics": "Symbolic Execution, Smart Contracts, Solidity"}, {"title": "GALOIS: boosting deep reinforcement learning via generalizable logic synthesis", "description": "Despite achieving superior performance in human-level control problems, unlike humans, deep reinforcement learning (DRL) lacks high-order intelligence (eg, logic deduction and reuse), thus it behaves ineffectively than humans regarding learning and generalization in complex problems. Previous works attempt to directly synthesize a white-box logic program as the DRL policy, manifesting logic-driven behaviors. However, most synthesis methods are built on imperative or declarative programming, and each has a distinct limitation, respectively. The former ignores the cause-effect logic during synthesis, resulting in low generalizability across tasks. The latter is strictly proof-based, thus failing to synthesize programs with complex hierarchical logic. In this paper, we combine the above two paradigms together and propose a novel Generalizable Logic Synthesis (GALOIS) framework to synthesize hierarchical and strict cause-effect logic programs. GALOIS leverages the program sketch and defines a new sketch-based hybrid program language for guiding the synthesis. Based on that, GALOIS proposes a sketch-based program synthesis method to automatically generate white-box programs with generalizable and interpretable cause-effect logic. Extensive evaluations on various decision-making tasks with complex logic demonstrate the superiority of GALOIS over mainstream baselines regarding the asymptotic performance, generalizability, and great knowledge reusability across different environments.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:Ei5r6KrKXVQC", "topics": "Logic Synthesis, Deep Reinforcement Learning"}, {"title": "Large-scale analysis of non-termination bugs in real-world OSS projects", "description": "Termination is a crucial program property. Non-termination bugs can be subtle to detect and may remain hidden for long before they take effect. Many real-world programs still suffer from vast consequences (e.g., no response) caused by non-termination bugs. As a classic problem, termination proving has been studied for many years. Many termination checking tools and techniques have been developed and demonstrated effectiveness on existing well-established benchmarks. However, the capability of these tools in finding practical non-termination bugs has yet to be tested on real-world projects. To fill in this gap, in this paper, we conducted the first large-scale empirical study of non-termination bugs in real-world OSS projects. Specifically, we first devoted substantial manual efforts in collecting and analyzing 445 non-termination bugs from 3,142 GitHub commits and provided a systematic classifi-cation of the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:1paMEeroeoQC", "topics": "Program Analysis, Bug Detection, Empirical Study"}, {"title": "Demystifying performance regressions in string solvers", "description": "Over the past few years, SMT string solvers have found their applications in an increasing number of domains, such as program analyses in mobile and Web applications, which require the ability to reason about string values. A series of research has been carried out to find quality issues of string solvers in terms of its correctness and performance. Yet, none of them has considered the performance regressions happening across multiple versions of a string solver. To fill this gap, in this paper, we focus on solver performance regressions (SPRs), i.e., unintended slowdowns introduced during the evolution of string solvers. To this end, we develop  SPRFinder to not only generate test cases demonstrating SPRs, but also localize the probable causes of them, in terms of commits. We evaluated the effectiveness of  SPRFinder on three state-of-the-art string solvers, i.e., Z3Seq, Z3Str3, and CVC4. The results demonstrate\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:yCjxvIMm6_oC", "topics": "Program Analysis, Software Testing, Performance Optimization"}, {"title": "Property-Based Automated Repair of DeFi Protocols", "description": "Programming errors enable security attacks on smart contracts, which are used to manage large sums of financial assets. Automated program repair (APR) techniques aim to reduce developers\u2019 burden of manually fixing bugs by automatically generating patches for a given issue. Existing APR tools for smart contracts focus on mitigating typical smart contract vulnerabilities rather than violations of functional specification. However, in decentralized financial (DeFi) smart contracts, the inconsistency between intended behavior and implementation translates into the deviation from the underlying financial model, resulting in monetary losses for the application and its users. In this work, we propose DeFinery\u2014a technique for automated repair of a smart contract that does not satisfy a user-defined correctness property. To explore a larger set of diverse patches while providing formal correctness guarantees w.r.t. the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:WM2K3OHRCGMC", "topics": "Automated Program Repair, Smart Contracts, Decentralized Finance"}, {"title": "Semantic History Slicing", "description": "Feature location techniques aim to locate software artifacts that implement a specific program functionality, a.k.a. a feature. In this chapter, we introduce a technique called semantic history slicing, to locate features in software version histories. The key insight is that the information embedded in version histories can be used to establish a connection between features and changes implementing the features. We present the formal definitions as well as the general principles of semantic history slicing and provide details on a specific history slicing algorithm based on change dependency analysis. The identified feature implementing changes are fully functional and guaranteed to preserve the desired behaviors.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:IvSMUa3B7yYC", "topics": "Information Retrieval, Software Engineering"}, {"title": "Identifying Solidity Smart Contract API Documentation Errors", "description": "Smart contracts are gaining popularity as a means to support transparent, traceable, and self-executing decentralized applications, which enable the exchange of value in a trustless environment. Developers of smart contracts rely on various libraries, such as OpenZeppelin for Solidity contracts, to improve application quality and reduce development costs. The API documentations of these libraries are important sources of information for developers who are unfamiliar with the APIs. Yet, maintaining high-quality documentations is non-trivial, and errors in documentations may place barriers for developers to learn the correct usages of APIs. In this paper, we propose a technique, DocCon, to detect inconsistencies between documentations and the corresponding code for Solidity smart contract libraries. Our fact-based approach allows inconsistencies of different severity levels to be queried, from a database\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:w7CBUyPWg-0C", "topics": "Information Retrieval, Software Engineering"}], "2023": [{"title": "Client-Specific Upgrade Compatibility Checking via Knowledge-Guided Discovery", "description": "Modern software systems are complex, and they heavily rely on external libraries developed by different teams and organizations. Such systems suffer from higher instability due to incompatibility issues caused by library upgrades. In this article, we address the problem by investigating the impact of a library upgrade on the behaviors of its clients. We developed CompCheck, an automated upgrade compatibility checking framework that generates incompatibility-revealing tests based on previous examples. CompCheck first establishes an offline knowledge base of incompatibility issues by mining from open source projects and their upgrades. It then discovers incompatibilities for a specific client project, by searching for similar library usages in the knowledge base and generating tests to reveal the problems. We evaluated CompCheck on 202 call sites of 37 open source projects and the results show that\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:gFrPXmx1TSsC", "topics": "Information Retrieval, Software Engineering"}, {"title": "Smart Contract Parallel Execution with Fine-Grained State Accesses", "description": "As various optimizations being proposed recently, the performance of blockchains is no longer limited by the consensus protocols, successfully scaling to thousands of transactions per second. To further improve blockchains' throughput, exploiting the parallelism in smart contract executions becomes a clear solution to resolve the new performance bottleneck. The existing techniques perform concurrency control on smart contract transactions based on pre-determined read/write sets, which can hardly be calculated precisely. As a result, many parallelization opportunities are missed in order to maintain the correctness of transaction executions. In this paper, we propose a novel execution scheduling framework, DMVCC, to further increase the parallelism in smart contract executions, via more fine-grained control on state accesses. DMVCC improves over existing techniques with two key features: (1) write versioning\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:-38epGy1wY0C", "topics": "Smart Contract Execution, Parallelism, Concurrency Control"}, {"title": "Responsibility in Context: On Applicability of Slicing in Semantic Regression Analysis", "description": "Numerous program slicing approaches aim to help developers troubleshoot regression failures - one of the most time-consuming development tasks. The main idea behind these approaches is to identify a subset of interdependent program statements relevant to the failure, minimizing the amount of code developers need to inspect. Accuracy and reduction rate achieved by slicing are the key considerations toward their applicability in practice: inspecting only the statements in a slice should be faster and more efficient than inspecting the code in full. In this paper, we report on our experiment applying one of the most recent and accurate slicing approaches, dual slicing, to the task of troubleshooting regression failures. As subjects, we use projects from the popular Defects4J benchmark and a systematically-collected set of eight large, open-source client-library project pairs with at least one library upgrade failure\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=B61QIUIAAAAJ&cstart=20&pagesize=80&citation_for_view=B61QIUIAAAAJ:Q3-QASNKTMEC", "topics": "Program Slicing, Regression Analysis"}]}