{"2017": [{"title": "The mondrian data engine", "description": "The increasing demand for extracting value out of ever-growing data poses an ongoing challenge to system designers, a task only made trickier by the end of Dennard scaling. As the performance density of traditional CPU-centric architectures stagnates, advancing compute capabilities necessitates novel architectural approaches. Near-memory processing (NMP) architectures are reemerging as promising candidates to improve computing efficiency through tight coupling of logic and memory. NMP architectures are especially fitting for data analytics, as they provide immense bandwidth to memory-resident data and dramatically reduce data movement, the main source of energy consumption. Modern data analytics operators are optimized for CPU execution and hence rely on large caches and employ random memory accesses. In the context of NMP, such random accesses result in wasteful DRAM row buffer activations that account for a significant fraction of the total memory access energy. In addition, utilizing NMP\u2019s ample bandwidth with fine-grained random accesses requires complex hardware that cannot be accommodated under NMP\u2019s tight area and power constraints. Our thesis is that efficient NMP calls for an algorithm-hardware co-design that favors algorithms with sequential accesses to enable simple hardware that accesses memory in streams. We introduce an instance of such a co-designed NMP architecture for data analytics, the Mondrian Data Engine. Compared to a CPU-centric and a baseline NMP system, the Mondrian Data Engine improves the performance of basic data analytics operators by up to 49\u00d7 and 5\u00d7, and efficiency\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:2osOgNQ5qMEC"}], "2021": [{"title": "Benchmarking, analysis, and optimization of serverless function snapshots", "description": "Serverless computing has seen rapid adoption due to its high scalability and flexible, pay-as-you-go billing model. In serverless, developers structure their services as a collection of functions, sporadically invoked by various events like clicks. High inter-arrival time variability of function invocations motivates the providers to start new function instances upon each invocation, leading to significant cold-start delays that degrade user experience. To reduce cold-start latency, the industry has turned to snapshotting, whereby an image of a fully-booted function is stored on disk, enabling a faster invocation compared to booting a function from scratch.   This work introduces vHive, an open-source framework for serverless experimentation with the goal of enabling researchers to study and innovate across the entire serverless stack. Using vHive, we characterize a state-of-the-art snapshot-based serverless infrastructure\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:_FxGoFyzp5QC"}, {"title": "Analyzing tail latency in serverless clouds with stellar", "description": "Serverless computing has seen rapid adoption because of its instant scalability, flexible billing model, and economies of scale. In serverless, developers structure their applications as a collection of functions invoked by various events like clicks, and cloud providers take responsibility for cloud infrastructure management. As with other cloud services, serverless deployments require responsiveness and performance predictability manifested through low average and tail latencies. While the average end-to-end latency has been extensively studied in prior works, existing papers lack a detailed characterization of the effects of tail latency in real-world serverless scenarios and their root causes. In response, we introduce STeLLAR, an open-source serverless benchmarking framework, which enables an accurate performance characterization of serverless deployments. STeLLAR is provider-agnostic and highly\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:LkGwnXOMwfcC"}, {"title": "PTEMagnet: Fine-grained physical memory reservation for faster page walks in public clouds", "description": "The last few years have seen a rapid adoption of cloud computing for data-intensive tasks. In the cloud environment, it is common for applications to run under virtualization and to share a virtual machine with other applications (e.g., in a virtual private cloud setup). In this setting, our work identifies a new address translation bottleneck caused by memory fragmentation stemming from the interaction of virtualization, colocation, and the Linux memory allocator. The fragmentation results in the effective cache footprint of the host PT being larger than that of the guest PT. The bloated footprint of the host PT leads to frequent cache misses during nested page walks, increasing page walk latency.   In response to these observations, we propose PTEMagnet, a new software-only approach for reducing address translation latency in a public cloud. PTEMagnet prevents memory fragmentation through a fine-grained reservation\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:ufrVoPGSRksC"}], "2019": [{"title": "Prefetched address translation", "description": "With explosive growth in dataset sizes and increasing machine memory capacities, per-application memory footprints are commonly reaching into hundreds of GBs. Such huge datasets pressure the TLB, resulting in frequent misses that must be resolved through a page walk -- a long-latency pointer chase through multiple levels of the in-memory radix tree-based page table. Anticipating further growth in dataset sizes and their adverse affect on TLB hit rates, this work seeks to accelerate page walks while fully preserving existing virtual memory abstractions and mechanisms -- a must for software compatibility and generality. Our idea is to enable direct indexing into a given level of the page table, thus eliding the need to first fetch pointers from the preceding levels. A key contribution of our work is in showing that this can be done by simply ordering the pages containing the page table in physical memory to match the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:YsMSGLbcyi4C"}, {"title": "Mitigating load imbalance in distributed data serving with rack-scale memory pooling", "description": "To provide low-latency and high-throughput guarantees, most large key-value stores keep the data in the memory of many servers. Despite the natural parallelism across lookups, the load imbalance, introduced by heavy skew in the popularity distribution of keys, limits performance. To avoid violating tail latency service-level objectives, systems tend to keep server utilization low and organize the data in micro-shards, which provides units of migration and replication for the purpose of load balancing. These techniques reduce the skew but incur additional monitoring, data replication, and consistency maintenance overheads. In this work, we introduce RackOut, a memory pooling technique that leverages the one-sided remote read primitive of emerging rack-scale systems to mitigate load imbalance while respecting service-level objectives. In RackOut, the data are aggregated at rack-scale granularity, with all of the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:W7OEmFMy1HYC"}], "2016": [{"title": "SABRes: Atomic object reads for in-memory rack-scale computing", "description": "Modern in-memory services rely on large distributed object stores to achieve the high scalability essential to ser-vice thousands of requests concurrently. The independent and unpredictable nature of incoming requests results in random accesses to the object store, triggering frequent remote memory accesses. State-of-the-art distributed memory frameworks leverage the one-sided operations offered by RDMA technology to mitigate the traditionally high cost of remote memory access. Unfortunately, the limited semantics of RDMA one-sided operations bound remote memory access atomicity to a single cache block; therefore, atomic remote object access relies on software mechanisms. Emerging highly integrated rack-scale systems that reduce the latency of one-sided operations to a small multiple of DRAM latency expose the overhead of these software mechanisms as a major latency contributor. This technology\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:u5HHmVD_uO8C"}], "2018": [{"title": "Design guidelines for high-performance SCM hierarchies", "description": "With emerging storage-class memory (SCM) nearing commercialization, there is evidence that it will deliver the much-anticipated high density and access latencies within only a few factors of DRAM. Nevertheless, the latency-sensitive nature of memory-resident services makes seamless integration of SCM in servers questionable. In this paper, we ask the question of how best to introduce SCM for such servers to improve overall performance/cost over existing DRAM-only architectures. We first show that even with the most optimistic latency projections for SCM, the higher memory access latency results in prohibitive performance degradation. However, we find that deployment of a modestly sized high-bandwidth 3D stacked DRAM cache makes the performance of an SCM-mostly memory system competitive. The high degree of spatial locality that memory-resident services exhibit not only simplifies the DRAM cache\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:zYLM7Y9cAGgC"}, {"title": "Virtual address translation via learned page table indexes", "description": "Address translation is an established performance bottleneck [4] in workloads operating on large datasets due to frequent TLB misses and subsequent page table walks that often require multiple memory accesses to resolve. Inspired by recent research at Google on Learned Index Structures [14], we propose to accelerate address translation by introducing a new translation mechanism based on learned models using neural networks. We argue that existing software-based learned models are unable to outperform the traditional address translation mechanisms due to their high inference time, pointing toward the need for hardware-accelerated learned models. With a challenging goal to microarchitect a hardware-friendly learned page table index, we discuss a number of machine learning and systems trade-offs, and suggest future directions.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:Tyk-4Ss8FVUC"}, {"title": "Algorithm/architecture co-design for near-memory processing", "description": "With mainstream technologies to couple logic tightly with memory on the horizon, near-memory processing has re-emerged as a promising approach to improving performance and energy for data-centric computing. DRAM, however, is primarily designed for density and low cost, with a rigid internal organization that favors coarse-grain streaming rather than byte-level random access. This paper makes the case that treating DRAM as a block-oriented streaming device yields significant efficiency and performance benefits, which motivate for algorithm/architecture co-design to favor streaming access patterns, even at the price of a higher order algorithmic complexity. We present the Mondrian Data Engine that drastically improves the runtime and energy efficiency of basic in-memory analytic operators, despite doing more work as compared to traditional CPU-optimized algorithms, which heavily rely on random\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:IjCSPb-OGe4C"}], "2022": [{"title": "Lukewarm serverless functions: characterization and optimization", "description": "Serverless computing has emerged as a widely-used paradigm for running services in the cloud. In serverless, developers organize their applications as a set of functions, which are invoked on-demand in response to events, such as an HTTP request. To avoid long start-up delays of launching a new function instance, cloud providers tend to keep recently-triggered instances idle (or warm) for some time after the most recent invocation in anticipation of future invocations. Thus, at any given moment on a server, there may be thousands of warm instances of various functions whose executions are interleaved in time based on incoming invocations. This paper observes that (1) there is a high degree of interleaving among warm instances on a given server; (2) the individual warm functions are invoked relatively infrequently, often at the granularity of seconds or minutes; and (3) many function invocations complete within\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:Se3iqnhoufwC", "topics": "Serverless Computing, Cloud Computing, Function as a Service"}, {"title": "Data-centric serverless cloud architecture", "description": "Serverless has become a new dominant cloud architecture thanks to its high scalability and flexible, pay-as-you-go billing model. In serverless, developers compose their cloud services as a set of functions while providers take responsibility for scaling each function\u2019s resources according to traffic changes. Hence, the provider needs to timely spawn, or tear down, function instances (i.e., HTTP servers with user-provider handles), which cannot hold state across function invocations. Performance of a modern serverless cloud is bound by data movement. Serverless architecture separates compute resources and data management to allow function instances to run on any node in a cloud datacenter. This flexibility comes at the cost of the necessity to move function initialization state across the entire datacenter when spawning new instances on demand. Furthermore, to facilitate scaling, cloud providers restrict the serverless programming model to stateless functions (which cannot hold or share state across different functions), which lack efficient support for cross-function communication.  This thesis consists of four following research contributions that pave the way for a data-centric serverless cloud architecture. First, we introduce STeLLAR, an opensource serverless benchmarking framework, which enables an accurate performance characterization of serverless deployments. Using STeLLAR, we study three leading serverless clouds and identify that all of them follow the same conceptual architecture that comprises three essential subsystems, namely the worker fleet, the scheduler, and the storage. Our analysis quantifies the aspect of the data\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:UebtZRa9Y70C", "topics": "Distributed Systems, Cloud Computing, Serverless Computing"}], "2020": [{"title": "Bankrupt Covert Channel: Turning Network Predictability into Vulnerability", "description": "Recent years have seen a surge in the number of data leaks despite aggressive information-containment measures deployed by cloud providers. When attackers acquire sensitive data in a secure cloud environment, covert communication channels are a key tool to exfiltrate the data to the outside world. While the bulk of prior work focused on covert channels within a single CPU, they require the spy (transmitter) and the receiver to share the CPU, which might be difficult to achieve in a cloud environment with hundreds or thousands of machines.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:WF5omc3nYNoC"}], "2023": [{"title": "Enabling In-Vitro Serverless Systems Research", "description": "Serverless is an increasingly popular cloud computing paradigm that has stimulated new systems research opportunities. However, developing and evaluating serverless systems in a research setting (ie,\u201cin-vitro\u201d, without access to a largescale production cluster and real workloads) is challenging yet vital for innovation. Recently, several serverless providers have released production traces consisting of large sets of functions with their invocation inter-arrival time, execution time, and memory footprint distributions. However, executing the workload synthesized from these traces requires a massive cluster, making experiments expensive and timeconsuming.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:MXK_kJrjxJIC"}, {"title": "Expedited Data Transfers for Serverless Clouds", "description": "Serverless computing has emerged as a popular cloud deployment paradigm. In serverless, the developers implement their application as a set of chained functions that form a workflow in which functions invoke each other. The cloud providers are responsible for automatically scaling the number of instances for each function on demand and forwarding the requests in a workflow to the appropriate function instance. Problematically, today's serverless clouds lack efficient support for cross-function data transfers in a workflow, preventing the efficient execution of data-intensive serverless applications. In production clouds, functions transmit intermediate, i.e., ephemeral, data to other functions either as part of invocation HTTP requests (i.e., inline) or via third-party services, such as AWS S3 storage or AWS ElastiCache in-memory cache. The former approach is restricted to small transfer sizes, while the latter supports arbitrary transfers but suffers from performance and cost overheads. This work introduces Expedited Data Transfers (XDT), an API-preserving high-performance data communication method for serverless that enables direct function-to-function transfers. With XDT, a trusted component of the sender function buffers the payload in its memory and sends a secure reference to the receiver, which is picked by the load balancer and autoscaler based on the current load. Using the reference, the receiver instance pulls the transmitted data directly from the sender's memory. XDT is natively compatible with existing autoscaling infrastructure, preserves function invocation semantics, is secure, and avoids the cost and performance overheads of\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:3fE2CSJIrl8C"}], "2017": [{"title": "The mondrian data engine", "description": "The increasing demand for extracting value out of ever-growing data poses an ongoing challenge to system designers, a task only made trickier by the end of Dennard scaling. As the performance density of traditional CPU-centric architectures stagnates, advancing compute capabilities necessitates novel architectural approaches. Near-memory processing (NMP) architectures are reemerging as promising candidates to improve computing efficiency through tight coupling of logic and memory. NMP architectures are especially fitting for data analytics, as they provide immense bandwidth to memory-resident data and dramatically reduce data movement, the main source of energy consumption. Modern data analytics operators are optimized for CPU execution and hence rely on large caches and employ random memory accesses. In the context of NMP, such random accesses result in wasteful DRAM row buffer activations that account for a significant fraction of the total memory access energy. In addition, utilizing NMP\u2019s ample bandwidth with fine-grained random accesses requires complex hardware that cannot be accommodated under NMP\u2019s tight area and power constraints. Our thesis is that efficient NMP calls for an algorithm-hardware co-design that favors algorithms with sequential accesses to enable simple hardware that accesses memory in streams. We introduce an instance of such a co-designed NMP architecture for data analytics, the Mondrian Data Engine. Compared to a CPU-centric and a baseline NMP system, the Mondrian Data Engine improves the performance of basic data analytics operators by up to 49\u00d7 and 5\u00d7, and efficiency\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:2osOgNQ5qMEC"}], "2021": [{"title": "Benchmarking, analysis, and optimization of serverless function snapshots", "description": "Serverless computing has seen rapid adoption due to its high scalability and flexible, pay-as-you-go billing model. In serverless, developers structure their services as a collection of functions, sporadically invoked by various events like clicks. High inter-arrival time variability of function invocations motivates the providers to start new function instances upon each invocation, leading to significant cold-start delays that degrade user experience. To reduce cold-start latency, the industry has turned to snapshotting, whereby an image of a fully-booted function is stored on disk, enabling a faster invocation compared to booting a function from scratch.   This work introduces vHive, an open-source framework for serverless experimentation with the goal of enabling researchers to study and innovate across the entire serverless stack. Using vHive, we characterize a state-of-the-art snapshot-based serverless infrastructure\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:_FxGoFyzp5QC"}, {"title": "Analyzing tail latency in serverless clouds with stellar", "description": "Serverless computing has seen rapid adoption because of its instant scalability, flexible billing model, and economies of scale. In serverless, developers structure their applications as a collection of functions invoked by various events like clicks, and cloud providers take responsibility for cloud infrastructure management. As with other cloud services, serverless deployments require responsiveness and performance predictability manifested through low average and tail latencies. While the average end-to-end latency has been extensively studied in prior works, existing papers lack a detailed characterization of the effects of tail latency in real-world serverless scenarios and their root causes. In response, we introduce STeLLAR, an open-source serverless benchmarking framework, which enables an accurate performance characterization of serverless deployments. STeLLAR is provider-agnostic and highly\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:LkGwnXOMwfcC"}, {"title": "PTEMagnet: Fine-grained physical memory reservation for faster page walks in public clouds", "description": "The last few years have seen a rapid adoption of cloud computing for data-intensive tasks. In the cloud environment, it is common for applications to run under virtualization and to share a virtual machine with other applications (e.g., in a virtual private cloud setup). In this setting, our work identifies a new address translation bottleneck caused by memory fragmentation stemming from the interaction of virtualization, colocation, and the Linux memory allocator. The fragmentation results in the effective cache footprint of the host PT being larger than that of the guest PT. The bloated footprint of the host PT leads to frequent cache misses during nested page walks, increasing page walk latency.   In response to these observations, we propose PTEMagnet, a new software-only approach for reducing address translation latency in a public cloud. PTEMagnet prevents memory fragmentation through a fine-grained reservation\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:ufrVoPGSRksC"}], "2019": [{"title": "Prefetched address translation", "description": "With explosive growth in dataset sizes and increasing machine memory capacities, per-application memory footprints are commonly reaching into hundreds of GBs. Such huge datasets pressure the TLB, resulting in frequent misses that must be resolved through a page walk -- a long-latency pointer chase through multiple levels of the in-memory radix tree-based page table. Anticipating further growth in dataset sizes and their adverse affect on TLB hit rates, this work seeks to accelerate page walks while fully preserving existing virtual memory abstractions and mechanisms -- a must for software compatibility and generality. Our idea is to enable direct indexing into a given level of the page table, thus eliding the need to first fetch pointers from the preceding levels. A key contribution of our work is in showing that this can be done by simply ordering the pages containing the page table in physical memory to match the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:YsMSGLbcyi4C"}, {"title": "Mitigating load imbalance in distributed data serving with rack-scale memory pooling", "description": "To provide low-latency and high-throughput guarantees, most large key-value stores keep the data in the memory of many servers. Despite the natural parallelism across lookups, the load imbalance, introduced by heavy skew in the popularity distribution of keys, limits performance. To avoid violating tail latency service-level objectives, systems tend to keep server utilization low and organize the data in micro-shards, which provides units of migration and replication for the purpose of load balancing. These techniques reduce the skew but incur additional monitoring, data replication, and consistency maintenance overheads. In this work, we introduce RackOut, a memory pooling technique that leverages the one-sided remote read primitive of emerging rack-scale systems to mitigate load imbalance while respecting service-level objectives. In RackOut, the data are aggregated at rack-scale granularity, with all of the\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:W7OEmFMy1HYC"}], "2016": [{"title": "SABRes: Atomic object reads for in-memory rack-scale computing", "description": "Modern in-memory services rely on large distributed object stores to achieve the high scalability essential to ser-vice thousands of requests concurrently. The independent and unpredictable nature of incoming requests results in random accesses to the object store, triggering frequent remote memory accesses. State-of-the-art distributed memory frameworks leverage the one-sided operations offered by RDMA technology to mitigate the traditionally high cost of remote memory access. Unfortunately, the limited semantics of RDMA one-sided operations bound remote memory access atomicity to a single cache block; therefore, atomic remote object access relies on software mechanisms. Emerging highly integrated rack-scale systems that reduce the latency of one-sided operations to a small multiple of DRAM latency expose the overhead of these software mechanisms as a major latency contributor. This technology\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:u5HHmVD_uO8C"}], "2018": [{"title": "Design guidelines for high-performance SCM hierarchies", "description": "With emerging storage-class memory (SCM) nearing commercialization, there is evidence that it will deliver the much-anticipated high density and access latencies within only a few factors of DRAM. Nevertheless, the latency-sensitive nature of memory-resident services makes seamless integration of SCM in servers questionable. In this paper, we ask the question of how best to introduce SCM for such servers to improve overall performance/cost over existing DRAM-only architectures. We first show that even with the most optimistic latency projections for SCM, the higher memory access latency results in prohibitive performance degradation. However, we find that deployment of a modestly sized high-bandwidth 3D stacked DRAM cache makes the performance of an SCM-mostly memory system competitive. The high degree of spatial locality that memory-resident services exhibit not only simplifies the DRAM cache\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:zYLM7Y9cAGgC"}, {"title": "Virtual address translation via learned page table indexes", "description": "Address translation is an established performance bottleneck [4] in workloads operating on large datasets due to frequent TLB misses and subsequent page table walks that often require multiple memory accesses to resolve. Inspired by recent research at Google on Learned Index Structures [14], we propose to accelerate address translation by introducing a new translation mechanism based on learned models using neural networks. We argue that existing software-based learned models are unable to outperform the traditional address translation mechanisms due to their high inference time, pointing toward the need for hardware-accelerated learned models. With a challenging goal to microarchitect a hardware-friendly learned page table index, we discuss a number of machine learning and systems trade-offs, and suggest future directions.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:Tyk-4Ss8FVUC"}, {"title": "Algorithm/architecture co-design for near-memory processing", "description": "With mainstream technologies to couple logic tightly with memory on the horizon, near-memory processing has re-emerged as a promising approach to improving performance and energy for data-centric computing. DRAM, however, is primarily designed for density and low cost, with a rigid internal organization that favors coarse-grain streaming rather than byte-level random access. This paper makes the case that treating DRAM as a block-oriented streaming device yields significant efficiency and performance benefits, which motivate for algorithm/architecture co-design to favor streaming access patterns, even at the price of a higher order algorithmic complexity. We present the Mondrian Data Engine that drastically improves the runtime and energy efficiency of basic in-memory analytic operators, despite doing more work as compared to traditional CPU-optimized algorithms, which heavily rely on random\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:IjCSPb-OGe4C"}], "2022": [{"title": "Lukewarm serverless functions: characterization and optimization", "description": "Serverless computing has emerged as a widely-used paradigm for running services in the cloud. In serverless, developers organize their applications as a set of functions, which are invoked on-demand in response to events, such as an HTTP request. To avoid long start-up delays of launching a new function instance, cloud providers tend to keep recently-triggered instances idle (or warm) for some time after the most recent invocation in anticipation of future invocations. Thus, at any given moment on a server, there may be thousands of warm instances of various functions whose executions are interleaved in time based on incoming invocations. This paper observes that (1) there is a high degree of interleaving among warm instances on a given server; (2) the individual warm functions are invoked relatively infrequently, often at the granularity of seconds or minutes; and (3) many function invocations complete within\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:Se3iqnhoufwC", "topics": "Serverless Computing, Cloud Computing, Function as a Service"}, {"title": "Data-centric serverless cloud architecture", "description": "Serverless has become a new dominant cloud architecture thanks to its high scalability and flexible, pay-as-you-go billing model. In serverless, developers compose their cloud services as a set of functions while providers take responsibility for scaling each function\u2019s resources according to traffic changes. Hence, the provider needs to timely spawn, or tear down, function instances (i.e., HTTP servers with user-provider handles), which cannot hold state across function invocations. Performance of a modern serverless cloud is bound by data movement. Serverless architecture separates compute resources and data management to allow function instances to run on any node in a cloud datacenter. This flexibility comes at the cost of the necessity to move function initialization state across the entire datacenter when spawning new instances on demand. Furthermore, to facilitate scaling, cloud providers restrict the serverless programming model to stateless functions (which cannot hold or share state across different functions), which lack efficient support for cross-function communication.  This thesis consists of four following research contributions that pave the way for a data-centric serverless cloud architecture. First, we introduce STeLLAR, an opensource serverless benchmarking framework, which enables an accurate performance characterization of serverless deployments. Using STeLLAR, we study three leading serverless clouds and identify that all of them follow the same conceptual architecture that comprises three essential subsystems, namely the worker fleet, the scheduler, and the storage. Our analysis quantifies the aspect of the data\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:UebtZRa9Y70C", "topics": "Distributed Systems, Cloud Computing, Serverless Computing"}], "2020": [{"title": "Bankrupt Covert Channel: Turning Network Predictability into Vulnerability", "description": "Recent years have seen a surge in the number of data leaks despite aggressive information-containment measures deployed by cloud providers. When attackers acquire sensitive data in a secure cloud environment, covert communication channels are a key tool to exfiltrate the data to the outside world. While the bulk of prior work focused on covert channels within a single CPU, they require the spy (transmitter) and the receiver to share the CPU, which might be difficult to achieve in a cloud environment with hundreds or thousands of machines.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:WF5omc3nYNoC"}], "2023": [{"title": "Enabling In-Vitro Serverless Systems Research", "description": "Serverless is an increasingly popular cloud computing paradigm that has stimulated new systems research opportunities. However, developing and evaluating serverless systems in a research setting (ie,\u201cin-vitro\u201d, without access to a largescale production cluster and real workloads) is challenging yet vital for innovation. Recently, several serverless providers have released production traces consisting of large sets of functions with their invocation inter-arrival time, execution time, and memory footprint distributions. However, executing the workload synthesized from these traces requires a massive cluster, making experiments expensive and timeconsuming.", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:MXK_kJrjxJIC", "topics": "Cloud Computing, Systems Research, Serverless Computing"}, {"title": "Expedited Data Transfers for Serverless Clouds", "description": "Serverless computing has emerged as a popular cloud deployment paradigm. In serverless, the developers implement their application as a set of chained functions that form a workflow in which functions invoke each other. The cloud providers are responsible for automatically scaling the number of instances for each function on demand and forwarding the requests in a workflow to the appropriate function instance. Problematically, today's serverless clouds lack efficient support for cross-function data transfers in a workflow, preventing the efficient execution of data-intensive serverless applications. In production clouds, functions transmit intermediate, i.e., ephemeral, data to other functions either as part of invocation HTTP requests (i.e., inline) or via third-party services, such as AWS S3 storage or AWS ElastiCache in-memory cache. The former approach is restricted to small transfer sizes, while the latter supports arbitrary transfers but suffers from performance and cost overheads. This work introduces Expedited Data Transfers (XDT), an API-preserving high-performance data communication method for serverless that enables direct function-to-function transfers. With XDT, a trusted component of the sender function buffers the payload in its memory and sends a secure reference to the receiver, which is picked by the load balancer and autoscaler based on the current load. Using the reference, the receiver instance pulls the transmitted data directly from the sender's memory. XDT is natively compatible with existing autoscaling infrastructure, preserves function invocation semantics, is secure, and avoids the cost and performance overheads of\u00a0\u2026", "link": "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=cZ9jA9oAAAAJ&citation_for_view=cZ9jA9oAAAAJ:3fE2CSJIrl8C", "topics": "Distributed Systems, Cloud Computing, Data Communication"}]}