{"full_name": "Chng Eng Siong", "designation": "Associate Professor, School of Computer Science and Engineering", "email": "aseschng@ntu.edu.sg", "image_path": "./profile_img/chng_eng_siong.jpg", "biography": "Dr Chng Eng Siong is currently an Associate Professor in the School of Computer Science and Engineering (SCSE), Nanyang Technological University (NTU), Singapore. Prior to joining NTU in 2003, he has worked in several research centers/companies, namely: Knowles Electronics (USA), Lernout and Hauspie (Belgium), Institute of Infocomm Research (I2R, Singapore), and RIKEN (Japan). He received both PhD and BEng (Hons) from Edinburgh University, U.K. in 1996 and 1991 respectively. His area of focus is in speech research, machine learning and signal processing. To date, he has been Principal Investigator of research grants awarded by AI-SG, NTU-Rolls Royce, Mindef, MOE and AStar with a total funding amount of over S$9 million under the \u201cSpeech and Language Technology Program (SLTP)\u201d at SCSE. He has graduated more than 15 PhD students and 8 Masters Engineering students. His publications include 2 edited books and over 100 journal/conference papers. He has served as the publication chair for 5 international conferences (Human Agent Interaction 2016, INTERSPEECH 2014, APSIPA-2010, APSIPA-2011, ISCSLP-2006) and local organizing committee in ASRU 2019.", "grants": "Artificial Intelligence-based advisory system for blood sugar management in elderly diabeticsDevelopment of Medical AI Speech-to-Text Engine for Implementation in the Emergency Department and Acute WardISSACMulti-Channel Far-field Speaker Diarization for Meeting Rooms with Overlap Speaker DetectionProject ANPASSENSASEAF FY22 Dr Luong Hieu Thi (Voice Anti-Spoofing, Fake Speech Detection)SingaKids Pic2Speak: Multilingual AI Tutor \u2013 Uplifting Singapore\u2019s Bilingual EdgeSingle-Channel Far-field Speaker Diarization for Interview Rooms with Far-field Voice Activity DetectionStrategic Centre For Research In Privacy-Preserving Technologies & Systems (SCRIPTS)Voice command recognition under noisy conditions", "google_scholar": "https://scholar.google.com/citations?user=FJodrCcAAAAJ", "orcid": "https://orcid.org/0000-0001-6257-7399", "github": null, "scopus": "https://www.scopus.com/authid/detail.uri?authorId=24474196000", "web_of_science": null, "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp00098", "other_websites": ["https://personal.ntu.edu.sg/aseschng/default.html"], "interests": ["Speech and Language processing", "Digital Signal Processing", "Pattern Recognition"], "bachelor_degree": null, "masters": null, "phd": "Edinburgh University, U.K.", "collaboration_network": {"target": ["Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Haizhou Li", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Xiong Xiao", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Zhizheng Wu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Chenglin Xu", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Lei Xie", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Xiaohai Tian", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Sheng Chen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Tomi Kinnunen", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Kong Aik Lee", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Changsheng Xu", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Jinjun Wang", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Tong Rong", "Wei Rao", "Wei Rao", "Wei Rao", "Wei Rao", "Wei Rao", "Wei Rao", "Wei Rao", "Wei Rao", "Wei Rao", "Wei Rao", "Douglas L. Jones", "Douglas L. Jones", "Douglas L. Jones", "Douglas L. Jones", "Douglas L. Jones", "Douglas L. Jones", "Douglas L. Jones", "Douglas L. Jones", "Bernard Mulgrew", "Bernard Mulgrew", "Bernard Mulgrew", "Bernard Mulgrew", "Bernard Mulgrew", "Bernard Mulgrew", "Bernard Mulgrew", "Bernard Mulgrew", "Bernard Mulgrew", "dau-cheng lyu", "dau-cheng lyu", "dau-cheng lyu", "dau-cheng lyu", "dau-cheng lyu", "dau-cheng lyu", "dau-cheng lyu", "dau-cheng lyu", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Longbiao Wang", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Nguyen Trung Hieu", "Tien-Ping Tan", "Tien-Ping Tan", "Tien-Ping Tan", "Tien-Ping Tan", "Tien-Ping Tan", "Tien-Ping Tan", "Tien-Ping Tan", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Cheung-Chi Leung", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Hao Huang", "Jonathan Dennis", "Jonathan Dennis", "Jonathan Dennis", "Jonathan Dennis", "Jonathan Dennis", "Jonathan Dennis", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Van Tung Pham", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Deepu Rajan", "Nancy F. Chen", "Nancy F. Chen", "Nancy F. Chen", "Nancy F. Chen", "Nancy F. Chen", "Nancy F. Chen", "Nancy F. Chen", "Nancy F. Chen", "Nancy F. Chen", "Tanja Schultz", "Tanja Schultz", "Tanja Schultz", "Tanja Schultz", "Tanja Schultz", "Thi-Nga Ho", "Thi-Nga Ho", "Thi-Nga Ho", "Thi-Nga Ho", "Thi-Nga Ho", "Thi-Nga Ho", "Thi-Nga Ho", "Thi-Nga Ho", "Thi-Nga Ho", "Siu Wa Lee", "Siu Wa Lee", "Siu Wa Lee", "Siu Wa Lee", "Jinyu Li", "Jinyu Li", "Jinyu Li", "Jinyu Li", "Jinyu Li", "Jinyu Li", "Professor Eliathamby Ambikairajah", "Professor Eliathamby Ambikairajah", "Professor Eliathamby Ambikairajah", "Professor Eliathamby Ambikairajah", "Chin-Hui Lee", "Chin-Hui Lee", "Chin-Hui Lee", "Chin-Hui Lee", "Thang Vu", "Thang Vu", "Omid Dehzangi", "Omid Dehzangi", "Omid Dehzangi", "Omid Dehzangi", "Omid Dehzangi", "Omid Dehzangi", "Omid Dehzangi", "Omid Dehzangi", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Van Hai DO (\u0110\u1ed7 V\u0103n H\u1ea3i)", "Rafael E. Banchs", "Rafael E. Banchs", "Rafael E. Banchs", "Rafael E. Banchs", "Rafael E. Banchs", "Yerbolat Khassanov", "Yerbolat Khassanov", "Yerbolat Khassanov", "Yerbolat Khassanov", "Yerbolat Khassanov", "Guan Cuntai", "Pham Chau Khoa", "Pham Chau Khoa", "David Siu-Yeung Cho", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao", "Shengkui Zhao"], "target_id": ["z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "z8_x7C8AAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "HHjJ0doAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "K6zhweAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Ww8tQKAAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "Qddov9wAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "6gc45QcAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "ZikGt_kAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "e3SPjpoAAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "SZegiA4AAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "hI9NRDkAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "Dk7JgNcAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "PbeihqwAAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "peTocw8AAAAJ", "gX8LBfgAAAAJ", "gX8LBfgAAAAJ", "gX8LBfgAAAAJ", "gX8LBfgAAAAJ", "gX8LBfgAAAAJ", "gX8LBfgAAAAJ", "gX8LBfgAAAAJ", "gX8LBfgAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "FpQuaoAAAAAJ", "WTEa2rUAAAAJ", "WTEa2rUAAAAJ", "WTEa2rUAAAAJ", "WTEa2rUAAAAJ", "WTEa2rUAAAAJ", "WTEa2rUAAAAJ", "WTEa2rUAAAAJ", "WTEa2rUAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "1Z9_5ZgAAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "RA5lzN8AAAAJ", "YoLTWS0AAAAJ", "YoLTWS0AAAAJ", "YoLTWS0AAAAJ", "YoLTWS0AAAAJ", "YoLTWS0AAAAJ", "YoLTWS0AAAAJ", "YoLTWS0AAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "yu6lqekAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gCxEFxIAAAAJ", "gu1cgVgAAAAJ", "gu1cgVgAAAAJ", "gu1cgVgAAAAJ", "gu1cgVgAAAAJ", "gu1cgVgAAAAJ", "gu1cgVgAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "8o42XvkAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "1bqFcwIAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "K3Z9UiAAAAAJ", "CupDmmcAAAAJ", "CupDmmcAAAAJ", "CupDmmcAAAAJ", "CupDmmcAAAAJ", "CupDmmcAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "dwJiKYIAAAAJ", "gaFdCm8AAAAJ", "gaFdCm8AAAAJ", "gaFdCm8AAAAJ", "gaFdCm8AAAAJ", "grUvupMAAAAJ", "grUvupMAAAAJ", "grUvupMAAAAJ", "grUvupMAAAAJ", "grUvupMAAAAJ", "grUvupMAAAAJ", "o1icFYgAAAAJ", "o1icFYgAAAAJ", "o1icFYgAAAAJ", "o1icFYgAAAAJ", "k4k05hcAAAAJ", "k4k05hcAAAAJ", "k4k05hcAAAAJ", "k4k05hcAAAAJ", "Nxbi5YgAAAAJ", "Nxbi5YgAAAAJ", "bJ7nBvwAAAAJ", "bJ7nBvwAAAAJ", "bJ7nBvwAAAAJ", "bJ7nBvwAAAAJ", "bJ7nBvwAAAAJ", "bJ7nBvwAAAAJ", "bJ7nBvwAAAAJ", "bJ7nBvwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "KvTREtwAAAAJ", "V5ou5AMAAAAJ", "V5ou5AMAAAAJ", "V5ou5AMAAAAJ", "V5ou5AMAAAAJ", "V5ou5AMAAAAJ", "NT47rxAAAAAJ", "NT47rxAAAAAJ", "NT47rxAAAAAJ", "NT47rxAAAAAJ", "NT47rxAAAAAJ", "sg4vxPoAAAAJ", "SVUh_TEAAAAJ", "SVUh_TEAAAAJ", "01O4Vd0AAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ", "kmhaqeEAAAAJ"], "type": ["Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "NTU", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "location": ["National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "Northwestern Polytechnical University", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Southampton", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "University of Eastern Finland", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Singapore Institute of Technology", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "University of Illinois", "University of Illinois", "University of Illinois", "University of Illinois", "University of Illinois", "University of Illinois", "University of Illinois", "University of Illinois", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Tianjin University", "Tianjin University", "Tianjin University", "Tianjin University", "Tianjin University", "Tianjin University", "Tianjin University", "Tianjin University", "Tianjin University", "Tianjin University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Universiti Sains Malaysia", "Universiti Sains Malaysia", "Universiti Sains Malaysia", "Universiti Sains Malaysia", "Universiti Sains Malaysia", "Universiti Sains Malaysia", "Universiti Sains Malaysia", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "University Bremen", "University Bremen", "University Bremen", "University Bremen", "University Bremen", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "Microsoft", "University of New South Wales", "University of New South Wales", "University of New South Wales", "University of New South Wales", "Georgia Institute of Technology", "Georgia Institute of Technology", "Georgia Institute of Technology", "Georgia Institute of Technology", "University of Stuttgart", "University of Stuttgart", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Nazarbayev University", "Nazarbayev University", "Nazarbayev University", "Nazarbayev University", "Nazarbayev University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "year": [2009, 2012, 2015, "unknown", 2008, 2006, 2017, 2014, 2013, 2013, 2009, 2012, 2015, 2014, 2013, 2006, 2009, 2016, 2020, 2015, 2012, 2012, 2008, 2012, 2010, 2016, 2009, 2016, 2012, 2012, 2016, 2013, 2018, 2014, 2016, 2014, 2016, 2012, 2023, 2020, 2014, 2009, 2015, 2015, 2008, 2012, 2011, 2015, 2006, 2018, 2016, 2016, 2016, 2021, 2009, 2015, 2020, 2011, 2017, 2016, 2010, 2010, 2013, 2006, 2017, 2014, 2017, 2022, 2008, 2016, 2015, 2018, 2016, 2017, 2006, 2010, 2013, 2017, 2008, 2006, 2014, 2010, 2015, 2014, 2011, 2009, 2015, 2016, 2010, 2010, 2017, 2007, 2014, 2014, 2015, 2015, 2013, 2019, 2015, 2011, 2016, 2009, 2018, 2010, 2008, 2020, 2016, "unknown", 2010, 2010, 2016, 2012, 2011, 2008, 2014, 2010, 2021, 2007, 2021, 2014, 2010, 2014, 2014, 2019, 2007, 2011, 2017, 2013, 2010, 2009, 2009, 2013, 2009, 2019, 2014, 2012, 2010, 2012, 2010, 2014, 2014, 2015, 2015, 2015, 2007, 2016, 2008, 2017, 2008, 2012, 2012, 2016, 2015, 2016, 2016, 2013, 2015, 2016, 2016, 2018, 2018, 2014, 2016, 2007, 2017, 2017, 2016, 2013, 2014, 2017, 2013, 2014, 2014, 2016, 2007, 2015, 2015, 2013, 2015, 2015, 2011, 2012, 2012, 2015, 2009, 2013, 2013, 2017, 2016, 2015, 2016, 2016, 2009, 2009, 2016, 2016, 2014, 2016, 2015, 2014, 2015, 2014, 2017, 2012, 2011, 2015, 2016, 2010, 2013, 2014, 2008, 2012, 2017, 2015, 2011, 2015, 2015, 2016, 2018, 2016, 2012, 2014, 2012, 2010, 2014, 2013, 2017, 2013, 2015, 2015, 2012, 2016, 2012, 2013, 2014, 2013, 2013, 2014, 2010, 2020, 2022, 2016, 2017, 2021, 2021, 2020, 2019, 2017, 2016, 2021, 2020, 2014, 2018, 2019, 2019, 2014, 2016, 2018, 2017, "unknown", 2016, 2018, 2018, 2010, 2014, 2012, 2015, 2016, 2016, 2017, 2016, 2018, 2015, 2020, 2014, 2015, 2014, 2017, 2010, 2015, 2022, 2017, 2017, 2016, 2016, 2017, 2019, 2015, 2013, 2018, 2015, 2017, 2016, 2017, 2017, 2015, 2014, 2005, 1996, 1996, 2004, 1995, 1994, 1996, 1998, 2005, 2003, 1994, 1995, 1995, 2012, 2012, 2013, 2006, 2012, "unknown", 2006, 2010, 2013, 2006, 2016, 2013, 2006, 2009, 2012, 2023, 2013, 2006, 2010, 2023, 2009, 2016, 2006, 2016, 2016, 2006, 2006, 2005, 2005, 2004, 2007, 2004, 2004, 2006, 2008, 2006, 2005, 2006, 2005, 2004, 2007, 2004, 2004, 2006, 2008, 2006, 2009, 2006, 2010, 2007, 2006, 2010, 2008, 2009, 2006, 2008, 2006, 2011, 2009, 2022, 2020, 2018, 2018, 2016, 2016, 2019, 2018, 2019, 2017, 2016, 2017, 2015, 2015, 2016, 2016, 2014, 2016, 2015, 1996, 1994, 1995, 1996, 1998, 1994, 1994, 1995, 1995, 2013, 2012, 2012, 2010, 2010, 2015, 2013, 2013, 2022, 2016, 2022, 2016, 2023, 2016, 2020, 2021, 2015, 2021, "unknown", 2009, 2023, 2008, 2009, 2023, 2023, 2023, 2008, 2014, 2010, 2007, 2010, 2012, 2010, 2010, 2015, 2009, 2013, 2020, 2021, 2014, 2020, 2021, 2016, 2015, 2016, 2020, 2015, "unknown", 2021, 2022, 2022, 2021, 2021, "unknown", 2022, 2016, 2023, 2021, 2016, 2012, 2014, 2013, 2014, 2013, 2013, 2015, 2015, 2021, 2021, 2021, 2014, 2014, 2016, 2018, 2016, 2020, 2016, 2020, 2014, 2016, 2017, 2017, 2021, 2005, 2022, 2004, 2023, 2009, 2003, 2015, "unknown", 2014, 2016, 2018, 2016, 2014, 2017, 2017, 2012, "unknown", 2010, 2009, 2012, 2018, 2019, 2022, 2016, 2023, 2017, 2023, 2018, 2022, 2016, 2014, 2015, 2017, 2011, 2009, 2012, 2011, 2009, 2014, 2010, 2012, 2016, 2009, 2015, 2016, 2015, 2009, 2012, 2012, 2008, 2010, 2012, 2010, 2010, 2011, 2009, 2008, 2012, 2012, 2015, 2016, 2014, 2015, 2015, 2014, 2016, 2015, 2016, 2012, 2015, 2013, 2014, 2015, 2021, 2017, 2021, 2020, 2021, 2004, 2012, 2011, 2007, "unknown", 2009, 2023, 2017, 2015, 2015, 2023, 2023, 2016, 2016, 2010, 2014, 2016, 2015], "title": ["Cluster criterion functions in spectral subspace and their application in speaker clustering", "Collection and annotation of malay conversational speech corpus", "A learning-based approach to direction of arrival estimation in noisy and reverberant environments", "ICASSP 2022 Organizing Committee", "T-test distance and clustering criterion for speaker diarization", "Integrating acoustic, prosodic and phonotactic features for spoken language identification", "On time-frequency mask estimation for MVDR beamforming with application in robust speech recognition", "Semi-supervised training for bottle-neck feature based DNN-HMM hybrid systems", "Language diarization for code-switch conversational speech", "Constrained adaptation of histogram equalization for robust speech recognition", "Target-aware language models for spoken language recognition", "Context dependant phone mapping for cross-lingual acoustic modeling", "Decoupling word-pair distance and co-occurrence information for effective long history context language modeling", "A discriminatively trained Hough Transform for frame-level phoneme recognition", "Temporal filter design by minimum KL divergence criterion for robust speech recognition", "Temporal discrete cosine transform: Towards longer term temporal features for speaker verification", "A study on hidden Markov model's generalization capability for speech recognition", "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation", "Time-domain neural network approach for speech bandwidth extension", "A density peak clustering approach to unsupervised acoustic subword units discovery", "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition", "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case", "Effect of Feature Smoothing for Robust Speech Recognition", "Lasso environment model combination for robust speech recognition", "Modeling broadcast news prosody using conditional random fields for story segmentation", "Content-aware local variability vector for speaker verification with short utterance", "A target-oriented phonotactic front-end for spoken language recognition", "Spoofing speech detection using temporal convolutional neural network", "A first speech recognition system for Mandarin-English code-switch conversational speech", "A phone mapping technique for acoustic modeling of under-resourced languages", "A study of learning based beamforming methods for speech recognition", "A particle filter compensation approach to robust LVCSR", "Unsupervised domain adaptation via domain adversarial training for speaker recognition", "Joint nonnegative matrix factorization for exemplar-based voice conversion", "An expectation-maximization eigenvector clustering approach to direction of arrival estimation of multiple speech sources", "Kernel density-based acoustic model with cross-lingual bottleneck features for resource limited LVCSR", "I-vector based deep neural network acoustic model adaptation using multilingual language resource", "Broadcast news story segmentation using conditional random fields and multimodal features", "Local and Global Context Modeling with Relation Matching Task for Dialog Act Recognition", "Spex+: A complete time domain speaker extraction network", "A study on replay attack and anti-spoofing for text-dependent speaker verification", "The I4U system in NIST 2008 speaker recognition evaluation", "Detecting synthetic speech using long term magnitude and phase information", "On statistical machine translation method for lexicon refinement in speech recognition", "Fuzzy rule selection using Iterative Rule Learning for speech data classification", "Joint spectral and temporal normalization of features for robust recognition of noisy and reverberated speech", "An iterative approach to model merging for speech pattern discovery", "On the study of very low-resource language keyword search", "Fusion of acoustic and tokenization features for speaker recognition", "Named-entity tagging and domain adaptation for better customized translation", "Approximate search of audio queries by using DTW with phone time boundary and data augmentation", "Keyword search using query expansion for graph-based rescoring of hypothesized detections", "The I4U submission to the 2016 NIST speaker recognition evaluation", "Multi-stage speaker extraction with utterance and frame-level reference signals", "Efficient Repeating Segments Discovery in Music using Adaptive Motif Generation Algorithm", "Robust speech recognition using beamforming with adaptive microphone gains and multichannel noise reduction", "Independent language modeling architecture for end-to-end ASR", "Maximum likelihood adaptation of histogram equalization with constraint for robust speech recognition", "Improving air traffic control speech intelligibility by reducing speaking rate effectively", "Spoofing detection from a feature representation perspective", "Framewise Phone Classification Using Weighted Fuzzy Classification Rules", "A tree-construction search approach for multivariate time series motifs discovery", "Exemplar-based unit selection for voice conversion utilizing temporal information.", "The IIR NIST 2006 Speaker Recognition System: Fusion of Acoustic and Tokenization Features", "Named entity transliteration with sequence-to-sequence neural network", "Discriminative score normalization for keyword search decision", "Low-resource spoken keyword search strategies in georgian inspired by distinctive feature theory", "L-spex: Localized target speaker extraction", "Normalization of the speech modulation spectra for robust speech recognition", "Beamforming networks using spatial covariance features for far-field speech recognition", "Multilingual exemplar-based acoustic model for the NIST Open KWS 2015 evaluation", "Re-ranking spoken term detection with acoustic exemplars of keywords", "Exemplar-inspired strategies for low-resource spoken keyword search in Swahili", "A review of the Mandarin-English code-switching corpus: SEAME", "The IIR submission to CSLP 2006 speaker recognition evaluation", "Phoneme lattice based TextTiling towards multilingual story segmentation", "The development and analysis of a Malay broadcasr news corpus", "Improving N-gram language modeling for code-switching speech recognition", "Discriminative output coding features for speech recognition", "Proceedings of the 5th international conference on Chinese Spoken Language Processing", "Cross-lingual phone mapping for large vocabulary speech recognition of under-resourced languages", "Text-independent F0 transformation with non-parallel data for voice conversion", "Non-negative matrix factorization using stable alternating direction method of multipliers for source separation", "Improving language modeling by using distance and co-occurrence information of word-pairs and its application to LVCSR", "Speech modulation features for robust nonnative speech accent detection", "The IIR-NTU speaker diarization systems for RT 2009", "Learning to estimate reverberation time in noisy and reverberant rooms", "Single-channel dereverberation for distant-talking speech recognition by combining denoising autoencoder and temporal structure normalization", "The development of a Singapore English call resource", "Seame: a mandarin-english code-switching speech corpus in south-east asia", "An exemplar-based approach to frequency warping for voice conversion", "A vector-based approach to broadcast audio database indexing and retrieval", "The NTU-ADSC systems for reverberation challenge 2014", "Feature compensation using linear combination of speaker and environment dependent correction vectors", "Regularized non-negative matrix factorization using alternating direction method of multipliers and its application to source separation", "Exemplar-based voice conversion using joint nonnegative matrix factorization", "Language diarization for conversational code-switch speech with pronunciation dictionary adaptation", "Domain adversarial training for speech enhancement", "Distance metric learning for kernel density-based acoustic model under limited training data conditions", "Feature normalization using structured full transforms for robust speech recognition", "Neural networks based channel compensation for i-vector speaker verification", "Discriminative Feature Transformation using Output Coding for Speech Recognition", "Single channel speech separation with constrained utterance level permutation invariant training using grid lstm", "Development of hmm-based malay text-to-speech system", "Classification of speech transmission channels: Landline, GSM and VoIP networks", "Spex: Multi-scale time domain speaker extraction network", "Feature adaptation using linear spectro-temporal transform for robust speech recognition", "Semi-class-based N-gram Language Modeling for Chinese Dictation}}", "Error corrective classifier fusion for spoken Language Recognition", "Selecting phonotactic features for language recognition", "Multi-channel feature adaptation for robust speech recognition", "Mixture of factor analyzers using priors from non-parallel speech for voice conversion", "Error Corrective Fusion of Classifier Scores for Spoken Language Recognition", "Target-oriented phone tokenizers for spoken language recognition", "Speaker diarization: An emerging research", "A robust real-time sound source localization system for olivia robot", "Representation learning with spectro-temporal-channel attention for speech emotion recognition", "Spoken language recognition with relevance feedback", "Learning disentangled feature representations for speech enhancement via adversarial training", "Generalization of temporal filter and linear transformation for robust speech recognition", "I4U Submission for the 2008 NIST Speaker Recognition Evaluation Submission", "Towards better keyword search performance on Malay broadcast news data", "System and keyword dependent fusion for spoken term detection", "Optimization of speaker extraction neural network with magnitude and temporal spectrum approximation loss", "Temporal structure normalization of speech feature for robust speech recognition", "Target-aware lattice rescoring for dialect recognition", "Investigation of fixed-dimensional speech representations for real-time speech emotion recognition system", "Synthetic speech detection using temporal modulation feature", "An analysis of a Mandarin-English code-switching speech corpus: SEAME", "Exploiting prosodic information for Speaker Recognition", "MASS: A Malay language LVCSR corpus resource", "Conditional restricted boltzmann machine for voice conversion", "Subspace construction and selection for speaker recognition", "Time-domain speaker extraction network", "Multi-view features in a DNN-CRF model for improved sentence unit detection on English broadcast news", "An analysis of vector Taylor series model compensation for non-stationary noise in speech recognition", "Development of a malay LVCSR system", "Integration of language identification into a recognition system for spoken conversations containing code-switches", "A Discriminative Performance Metric for GMM-UBM Speaker Identification", "A deep neural network approach for sentence boundary detection in broadcast news", "Exemplar-based sparse representation with residual compensation for voice conversion", "Mandarin\u2013english code-switching speech corpus in south-east asia: Seame", "Language independent query-by-example spoken term detection using n-best phone sequences and partial matching", "TDTO Language Modeling with Feedforward Neural Networks", "Using Direction of Arrival Estimate and Acoustic Feature Information in Speaker Diarization", "Feature adaptation using linear spectro-temporal transform for robust speech recognition", "Classification of speech transmission channels: Landline, GSM and VoIP networks", "Topic embedding of sentences for story segmentation", "Normalization of the speech modulation spectra for robust speech recognition", "A phone mapping technique for acoustic modeling of under-resourced languages", "Collection and annotation of malay conversational speech corpus", "Multi-channel feature adaptation for robust speech recognition", "A learning-based approach to direction of arrival estimation in noisy and reverberant environments", "Beamforming networks using spatial covariance features for far-field speech recognition", "A study of learning based beamforming methods for speech recognition", "A particle filter compensation approach to robust LVCSR", "Multilingual exemplar-based acoustic model for the NIST Open KWS 2015 evaluation", "An expectation-maximization eigenvector clustering approach to direction of arrival estimation of multiple speech sources", "Semi-Supervised and Cross-Lingual Knowledge Transfer Learnings for DNN Hybrid Acoustic Models Under Low-Resource Conditions.", "Learning distributed sentence representations for story segmentation", "Re-ranking spoken term detection with acoustic exemplars of keywords", "Kernel density-based acoustic model with cross-lingual bottleneck features for resource limited LVCSR", "Exemplar-inspired strategies for low-resource spoken keyword search in Swahili", "Evaluating the temporal structure normalisation technique on the Aurora-4 task.", "Pruning strategies for partial search in spoken term detection", "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source.", "I-vector based deep neural network acoustic model adaptation using multilingual language resource", "The development and analysis of a Malay broadcasr news corpus", "The NNI Query-by-Example System for MediaEval 2014.", "On time-frequency mask estimation for MVDR beamforming with application in robust speech recognition", "Attribute-based histogram equalization (HEQ) and its adaptation for robust speech recognition.", "Generalization of temporal filter and linear transformation for robust speech recognition", "Cross-lingual phone mapping for large vocabulary speech recognition of under-resourced languages", "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis.", "Temporal structure normalization of speech feature for robust speech recognition", "Detecting synthetic speech using long term magnitude and phase information", "Dnn feature compensation for noise robust speaker verification", "Constrained adaptation of histogram equalization for robust speech recognition", "A comparative study of BNF and DNN multilingual training on cross-lingual low-resource speech recognition.", "On statistical machine translation method for lexicon refinement in speech recognition", "Speech modulation features for robust nonnative speech accent detection", "Joint spectral and temporal normalization of features for robust recognition of noisy and reverberated speech", "Context dependant phone mapping for cross-lingual acoustic modeling", "On the study of very low-resource language keyword search", "A study on the generalization capability of acoustic models for robust speech recognition", "Synthetic speech detection using temporal modulation feature", "Temporal filter design by minimum KL divergence criterion for robust speech recognition", "A hybrid neural network hidden Markov model approach for automatic story segmentation", "Single-channel dereverberation for distant-talking speech recognition by combining denoising autoencoder and temporal structure normalization", "Learning to estimate reverberation time in noisy and reverberant rooms", "Approximate search of audio queries by using DTW with phone time boundary and data augmentation", "A DNN-HMM Approach to Story Segmentation.", "A study on hidden Markov model's generalization capability for speech recognition", "MASS: A Malay language LVCSR corpus resource", "Keyword search using query expansion for graph-based rescoring of hypothesized detections", "The I4U submission to the 2016 NIST speaker recognition evaluation", "The NTU-ADSC systems for reverberation challenge 2014", "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation", "Robust speech recognition using beamforming with adaptive microphone gains and multichannel noise reduction", "Feature compensation using linear combination of speaker and environment dependent correction vectors", "A density peak clustering approach to unsupervised acoustic subword units discovery", "Multi-view features in a DNN-CRF model for improved sentence unit detection on English broadcast news", "Low-resource spoken keyword search strategies in georgian inspired by distinctive feature theory", "An analysis of vector Taylor series model compensation for non-stationary noise in speech recognition", "Maximum likelihood adaptation of histogram equalization with constraint for robust speech recognition", "Low-resource keyword search strategies for Tamil", "Spoofing detection from a feature representation perspective", "Development of a malay LVCSR system", "Hadoop framework: impact of data organization on performance", "A deep neural network approach for sentence boundary detection in broadcast news", "Effect of Feature Smoothing for Robust Speech Recognition", "Lasso environment model combination for robust speech recognition", "An end-to-end neural network approach to story segmentation", "Distance metric learning for kernel density-based acoustic model under limited training data conditions", "Feature normalization using structured full transforms for robust speech recognition", "Language independent query-by-example spoken term detection using n-best phone sequences and partial matching", "Spoofing speech detection using high dimensional magnitude and phase features: the NTU approach for ASVspoof 2015 challenge.", "Neural networks based channel compensation for i-vector speaker verification", "Single channel speech separation with constrained utterance level permutation invariant training using grid lstm", "Spoofing speech detection using temporal convolutional neural network", "Mixture of factor analyzers using priors from non-parallel speech for voice conversion", "Joint nonnegative matrix factorization for exemplar-based voice conversion", "Vulnerability of speaker verification systems against voice conversion spoofing attacks: The case of telephone speech", "Text-independent F0 transformation with non-parallel data for voice conversion", "A study on replay attack and anti-spoofing for text-dependent speaker verification", "Synthetic speech detection using temporal modulation feature", "An exemplar-based approach to frequency warping for voice conversion", "Conditional restricted boltzmann machine for voice conversion", "Exemplar-based voice conversion using joint nonnegative matrix factorization", "Sparse representation for frequency warping based voice conversion", "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition", "Spoofing detection from a feature representation perspective", "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case", "Vulnerability evaluation of speaker verification under voice conversion spoofing: the effect of text constraints.", "Exemplar-based sparse representation with residual compensation for voice conversion", "Exemplar-based unit selection for voice conversion utilizing temporal information.", "Local partial least square regression for spectral mapping in voice conversion", "Correlation-based frequency warping for voice conversion", "Development of hmm-based malay text-to-speech system", "Spex: Multi-scale time domain speaker extraction network", "L-spex: Localized target speaker extraction", "A study of learning based beamforming methods for speech recognition", "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source.", "Representation learning with spectro-temporal-channel attention for speech emotion recognition", "Learning disentangled feature representations for speech enhancement via adversarial training", "Spex+: A complete time domain speaker extraction network", "Optimization of speaker extraction neural network with magnitude and temporal spectrum approximation loss", "Investigation of fixed-dimensional speech representations for real-time speech emotion recognition system", "The I4U submission to the 2016 NIST speaker recognition evaluation", "Multi-stage speaker extraction with utterance and frame-level reference signals", "Time-domain neural network approach for speech bandwidth extension", "Multi-view features in a DNN-CRF model for improved sentence unit detection on English broadcast news", "A Shifted Delta Coefficient Objective for Monaural Speech Separation Using Multi-task Learning.", "Time-domain speaker extraction network", "Domain adversarial training for speech enhancement", "A deep neural network approach for sentence boundary detection in broadcast news", "Neural networks based channel compensation for i-vector speaker verification", "Single channel speech separation with constrained utterance level permutation invariant training using grid lstm", "Topic embedding of sentences for story segmentation", "ICASSP 2022 Organizing Committee", "A study of learning based beamforming methods for speech recognition", "Unsupervised domain adaptation via domain adversarial training for speaker recognition", "Learning distributed sentence representations for story segmentation", "Phoneme lattice based TextTiling towards multilingual story segmentation", "The NNI Query-by-Example System for MediaEval 2014.", "Broadcast news story segmentation using conditional random fields and multimodal features", "Non-negative matrix factorization using stable alternating direction method of multipliers for source separation", "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis.", "A DNN-HMM Approach to Story Segmentation.", "A hybrid neural network hidden Markov model approach for automatic story segmentation", "Approximate search of audio queries by using DTW with phone time boundary and data augmentation", "Study of semi-supervised approaches to improving english-mandarin code-switching speech recognition", "Regularized non-negative matrix factorization using alternating direction method of multipliers and its application to source separation", "Time-domain neural network approach for speech bandwidth extension", "Multi-view features in a DNN-CRF model for improved sentence unit detection on English broadcast news", "A density peak clustering approach to unsupervised acoustic subword units discovery", "A deep neural network approach for sentence boundary detection in broadcast news", "An end-to-end neural network approach to story segmentation", "Modeling broadcast news prosody using conditional random fields for story segmentation", "Language independent query-by-example spoken term detection using n-best phone sequences and partial matching", "The iscslp 2022 intelligent cockpit speech recognition challenge (icsrc): Dataset, tracks, baseline and results", "Towards age-friendly E-commerce through crowd-improved speech recognition, multimodal search, and personalized speech feedback", "Improving air traffic control speech intelligibility by reducing speaking rate effectively", "Spoofing detection from a feature representation perspective", "Spoofing speech detection using temporal convolutional neural network", "An exemplar-based approach to frequency warping for voice conversion", "A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data.", "Spoofing speech detection using high dimensional magnitude and phase features: the NTU approach for ASVspoof 2015 challenge.", "Local partial least square regression for spectral mapping in voice conversion", "Average Modeling Approach to Voice Conversion with Non-Parallel Data.", "Detecting synthetic speech using long term magnitude and phase information", "Novel Functional Technologies for Age-Friendly E-commerce", "High quality voice conversion using prosodic and high-resolution spectral features", "Investigation of fixed-dimensional speech representations for real-time speech emotion recognition system", "An investigation of spectral feature partitioning for replay attacks detection", "Sparse representation for frequency warping based voice conversion", "Correlation-based frequency warping for voice conversion", "Fractionally spaced blind equalization with low\u2010complexity concurrent constant modulus algorithm and soft decision\u2010directed scheme", "Minimum-BER linear-combiner DFE", "Regularized orthogonal least squares algorithm for constructing radial basis function networks", "Concurrent constant modulus algorithm and soft decision directed scheme for fractionally-spaced blind equalization", "Realizing the optimum decision boundary for channel equalisation using radial basis function and linear networks", "Reducing the computational requirement of the orthogonal least squares algorithm", "Gradient radial basis function networks for nonlinear and nonstationary time series prediction", "Space translation properties and the minimum-BER linear-combiner DFE", "Determining the optimal decision delay parameter for a linear equalizer", "Optimum delay order selection for linear equalization problems", "Improving the radial basis function network for homogeneous nonstationary time series prediction", "Efficient computational schemes for the orthogonal least squares algorithm", "Optimum lag and subset selection for a radial basis function equaliser", "Vulnerability of speaker verification systems against voice conversion spoofing attacks: The case of telephone speech", "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case", "Vulnerability evaluation of speaker verification under voice conversion spoofing: the effect of text constraints.", "Fusion of acoustic and tokenization features for speaker recognition", "Mixture of factor analyzers using priors from non-parallel speech for voice conversion", "ICASSP 2022 Organizing Committee", "Temporal discrete cosine transform: Towards longer term temporal features for speaker verification", "Text-independent F0 transformation with non-parallel data for voice conversion", "Exemplar-based unit selection for voice conversion utilizing temporal information.", "The IIR NIST 2006 Speaker Recognition System: Fusion of Acoustic and Tokenization Features", "The I4U submission to the 2016 NIST speaker recognition evaluation", "Exemplar-based voice conversion using non-negative spectrogram deconvolution.", "The IIR submission to CSLP 2006 speaker recognition evaluation", "Target-aware language models for spoken language recognition", "Vulnerability of speaker verification systems against voice conversion spoofing attacks: The case of telephone speech", "Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification", "Vulnerability evaluation of speaker verification under voice conversion spoofing: the effect of text constraints.", "Fusion of acoustic and tokenization features for speaker recognition", "I4U Submission for the 2008 NIST Speaker Recognition Evaluation Submission", "Probabilistic Back-ends for Online Speaker Recognition and Clustering", "The I4U system in NIST 2008 speaker recognition evaluation", "The I4U submission to the 2016 NIST speaker recognition evaluation", "The IIR NIST 2006 Speaker Recognition System: Fusion of Acoustic and Tokenization Features", "Neural networks based channel compensation for i-vector speaker verification", "Content-aware local variability vector for speaker verification with short utterance", "The IIR submission to CSLP 2006 speaker recognition evaluation", "Identify sports video shots with\" happy\" or\" sad\" emotions", "Automatic generation of personalized music sports video", "Soccer replay detection using scene transition structure analysis", "Event detection based on non-broadcast sports video", "Generation of personalized music sports video using multimodal cues", "Sports highlight detection from keyword sequences using HMM", "Automatic replay generation for soccer video broadcasting", "Fully and semi-automatic music sports video composition", "Automatic composition of broadcast sports video", "Automatic sports video genre classification using pseudo-2d-hmm", "Automatic generation of personalized music sports video", "Identify sports video shots with\" happy\" or\" sad\" emotions", "Soccer replay detection using scene transition structure analysis", "Event detection based on non-broadcast sports video", "Generation of personalized music sports video using multimodal cues", "Automatic replay generation for soccer video broadcasting", "Sports highlight detection from keyword sequences using HMM", "Fully and semi-automatic music sports video composition", "Automatic composition of broadcast sports video", "Automatic sports video genre classification using pseudo-2d-hmm", "Target-aware language models for spoken language recognition", "Integrating acoustic, prosodic and phonotactic features for spoken language identification", "Selecting phonotactic features for language recognition", "Spoken language recognition with relevance feedback", "Fusion of acoustic and tokenization features for speaker recognition", "I4U Submission for the 2008 NIST Speaker Recognition Evaluation Submission", "Target-oriented phone tokenizers for spoken language recognition", "The I4U system in NIST 2008 speaker recognition evaluation", "The IIR NIST 2006 Speaker Recognition System: Fusion of Acoustic and Tokenization Features", "Target-oriented phone selection from universal phone set for spoken language recognition", "The IIR submission to CSLP 2006 speaker recognition evaluation", "Target-aware lattice rescoring for dialect recognition", "A target-oriented phonotactic front-end for spoken language recognition", "Medbert: a pre-trained language model for biomedical named entity recognition", "Spex: Multi-scale time domain speaker extraction network", "A Shifted Delta Coefficient Objective for Monaural Speech Separation Using Multi-task Learning.", "Unsupervised domain adaptation via domain adversarial training for speaker recognition", "Neural networks based channel compensation for i-vector speaker verification", "The I4U submission to the 2016 NIST speaker recognition evaluation", "Optimization of speaker extraction neural network with magnitude and temporal spectrum approximation loss", "Single channel speech separation with constrained utterance level permutation invariant training using grid lstm", "Time-domain speaker extraction network", "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source.", "I-vector based deep neural network acoustic model adaptation using multilingual language resource", "On time-frequency mask estimation for MVDR beamforming with application in robust speech recognition", "Learning to estimate reverberation time in noisy and reverberant rooms", "A learning-based approach to direction of arrival estimation in noisy and reverberant environments", "A study of learning based beamforming methods for speech recognition", "An expectation-maximization eigenvector clustering approach to direction of arrival estimation of multiple speech sources", "The NTU-ADSC systems for reverberation challenge 2014", "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation", "Robust speech recognition using beamforming with adaptive microphone gains and multichannel noise reduction", "Minimum-BER linear-combiner DFE", "Backtracking orthogonal least squares algorithm for model selection", "Realizing the optimum decision boundary for channel equalisation using radial basis function and linear networks", "Gradient radial basis function networks for nonlinear and nonstationary time series prediction", "Space translation properties and the minimum-BER linear-combiner DFE", "Reducing the computational requirement of the orthogonal least squares algorithm", "Improving the radial basis function network for homogeneous nonstationary time series prediction", "Efficient computational schemes for the orthogonal least squares algorithm", "Optimum lag and subset selection for a radial basis function equaliser", "Language diarization for conversational code-switch speech with pronunciation dictionary adaptation", "Integration of language identification into a recognition system for spoken conversations containing code-switches", "A first speech recognition system for Mandarin-English code-switch conversational speech", "An analysis of a Mandarin-English code-switching speech corpus: SEAME", "Seame: a mandarin-english code-switching speech corpus in south-east asia", "Mandarin\u2013english code-switching speech corpus in south-east asia: Seame", "Language diarization for code-switch conversational speech", "The development and analysis of a Malay broadcasr news corpus", "L-spex: Localized target speaker extraction", "Multi-channel feature adaptation for robust speech recognition", "Learning affective representations based on magnitude and dynamic relative phase information for speech emotion recognition", "Single-channel dereverberation for distant-talking speech recognition by combining denoising autoencoder and temporal structure normalization", "Local and Global Context Modeling with Relation Matching Task for Dialog Act Recognition", "A study of learning based beamforming methods for speech recognition", "Spex+: A complete time domain speaker extraction network", "Multi-stage speaker extraction with utterance and frame-level reference signals", "Robust speech recognition using beamforming with adaptive microphone gains and multichannel noise reduction", "Representation learning with spectro-temporal-channel attention for speech emotion recognition", "Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition", "The IIR-NTU speaker diarization systems for RT 2009", "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention", "T-test distance and clustering criterion for speaker diarization", "Cluster criterion functions in spectral subspace and their application in speaker clustering", "Contrastive Speech Mixup for Low-Resource Keyword Spotting", "SPGM: Prioritizing Local Features for enhanced speech separation performance", "Are Soft Prompts Good Zero-shot Learners for Speech Recognition?", "Speaker Diarization Using Direction of Arrival Estimate and Acoustic Feature Information: The I2R-NTU Submission for the NIST RT 2007 Evaluation", "Speaker diarization: An emerging research", "A robust real-time sound source localization system for olivia robot", "Using Direction of Arrival Estimate and Acoustic Feature Information in Speaker Diarization", "Development of a malay LVCSR system", "Collection and annotation of malay conversational speech corpus", "An analysis of a Mandarin-English code-switching speech corpus: SEAME", "Seame: a mandarin-english code-switching speech corpus in south-east asia", "Mandarin\u2013english code-switching speech corpus in south-east asia: Seame", "MASS: A Malay language LVCSR corpus resource", "The development and analysis of a Malay broadcasr news corpus", "Speech Transformer with Speaker Aware Persistent Memory.", "A unified speaker adaptation approach for ASR", "The NNI Query-by-Example System for MediaEval 2014.", "Cross Attention with Monotonic Alignment for Speech Transformer.", "Preventing early endpointing for online automatic speech recognition", "Approximate search of audio queries by using DTW with phone time boundary and data augmentation", "Language independent query-by-example spoken term detection using n-best phone sequences and partial matching", "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis.", "Universal Speech Transformer.", "Low-resource keyword search strategies for Tamil", "Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory", "Enriching Under-Represented Named Entities for Improved Speech Recognition", "Minimum word error training for non-autoregressive Transformer-based code-switching ASR", "Intermediate-layer output regularization for attention-based speech recognition with shared decoder", "E2e-based multi-task learning approach to joint speech and accent recognition", "Approaches to improving recognition of underrepresented named entities in hybrid ASR systems", "The NTU-XJU System for the AP20-OLR Challenge", "Internal language model estimation based language model fusion for cross-domain code-switching speech recognition", "Semi-Supervised and Cross-Lingual Knowledge Transfer Learnings for DNN Hybrid Acoustic Models Under Low-Resource Conditions.", "Speech-text based multi-modal training with bidirectional attention for improved speech recognition", "Multilingual approach to joint speech and accent recognition with DNN-HMM framework", "I-vector based deep neural network acoustic model adaptation using multilingual language resource", "Image feature representation of the subband power distribution for robust sound event classification", "A discriminatively trained Hough Transform for frame-level phoneme recognition", "Overlapping sound event recognition using local spectrogram features and the generalised hough transform", "Analysis of spectrogram image methods for sound event classification", "Robust sound event recognition under TV playing conditions", "A robust sound event recognition framework under TV playing conditions", "Low-resource keyword search strategies for Tamil", "On the study of very low-resource language keyword search", "End-to-end speaker height and age estimation using attention mechanism with LSTM-RNN", "Multitask-based joint learning approach to robust asr for radio communication speech", "Approaches to improving recognition of underrepresented named entities in hybrid ASR systems", "Towards better keyword search performance on Malay broadcast news data", "System and keyword dependent fusion for spoken term detection", "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis.", "Re-ranking spoken term detection with acoustic exemplars of keywords", "Keyword search using query expansion for graph-based rescoring of hypothesized detections", "Speaker and Phoneme-Aware Speech Bandwidth Extension with Residual Dual-Path Network.", "Exemplar-inspired strategies for low-resource spoken keyword search in Swahili", "Independent language modeling architecture for end-to-end ASR", "Discriminative score normalization for keyword search decision", "Improving efficiency of sentence boundary detection by feature selection", "Pruning strategies for partial search in spoken term detection", "Low-resource spoken keyword search strategies in georgian inspired by distinctive feature theory", "E2e-based multi-task learning approach to joint speech and accent recognition", "Motion detection with adaptive background and dynamic thresholds", "Speech emotion recognition with co-attention based multi-level acoustic information", "Foreground motion detection by difference-based spatial temporal entropy image", "UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning", "Improved keypoint matching method for near-duplicate keyframe retrieval", "Optimum delay order selection for linear equalization problems", "Low-resource keyword search strategies for Tamil", "ICASSP 2022 Organizing Committee", "System and keyword dependent fusion for spoken term detection", "Keyword search using query expansion for graph-based rescoring of hypothesized detections", "Re-ranking spoken term detection with acoustic exemplars of keywords", "Exemplar-inspired strategies for low-resource spoken keyword search in Swahili", "Discriminative score normalization for keyword search decision", "Pruning strategies for partial search in spoken term detection", "Low-resource spoken keyword search strategies in georgian inspired by distinctive feature theory", "Integration of language identification into a recognition system for spoken conversations containing code-switches", "ICASSP 2022 Organizing Committee", "I4U Submission for the 2008 NIST Speaker Recognition Evaluation Submission", "The I4U system in NIST 2008 speaker recognition evaluation", "A first speech recognition system for Mandarin-English code-switch conversational speech", "An investigation of word embeddings with deep bidirectional lstm for sentence unit detection in automatic speech transcription", "Transfer learning for punctuation prediction", "Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin", "Improving efficiency of sentence boundary detection by feature selection", "Adapting Code-Switching Language Models with Statistical-Based Text Augmentation", "A review of the Mandarin-English code-switching corpus: SEAME", "An Empirical Study on Punctuation Restoration for English, Mandarin, and Code-Switching Speech", "A hybrid deep learning architecture for sentence unit detection", "Medbert: a pre-trained language model for biomedical named entity recognition", "High quality voice conversion using prosodic and high-resolution spectral features", "Correlation-based frequency warping for voice conversion", "Sparse representation for frequency warping based voice conversion", "An exemplar-based approach to frequency warping for voice conversion", "Maximum likelihood adaptation of histogram equalization with constraint for robust speech recognition", "A study on the generalization capability of acoustic models for robust speech recognition", "Lasso environment model combination for robust speech recognition", "Feature normalization using structured full transforms for robust speech recognition", "A study on hidden Markov model's generalization capability for speech recognition", "Feature compensation using linear combination of speaker and environment dependent correction vectors", "I4U Submission for the 2008 NIST Speaker Recognition Evaluation Submission", "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case", "The I4U submission to the 2016 NIST speaker recognition evaluation", "The I4U system in NIST 2008 speaker recognition evaluation", "Language-resource independent speech segmentation using cues from a spectrogram image", "Exemplar-inspired strategies for low-resource spoken keyword search in Swahili", "Low-resource keyword search strategies for Tamil", "A study on the generalization capability of acoustic models for robust speech recognition", "A first speech recognition system for Mandarin-English code-switch conversational speech", "Integration of language identification into a recognition system for spoken conversations containing code-switches", "Discriminative output coding features for speech recognition", "Error corrective classifier fusion for spoken Language Recognition", "Discriminative feature extraction for speech recognition using continuous output codes", "Framewise Phone Classification Using Weighted Fuzzy Classification Rules", "A Discriminative Performance Metric for GMM-UBM Speaker Identification", "Error Corrective Fusion of Classifier Scores for Spoken Language Recognition", "Discriminative Feature Transformation using Output Coding for Speech Recognition", "Fuzzy rule selection using Iterative Rule Learning for speech data classification", "Context dependant phone mapping for cross-lingual acoustic modeling", "A phone mapping technique for acoustic modeling of under-resourced languages", "On the study of very low-resource language keyword search", "Approximate search of audio queries by using DTW with phone time boundary and data augmentation", "Cross-lingual phone mapping for large vocabulary speech recognition of under-resourced languages", "Multilingual exemplar-based acoustic model for the NIST Open KWS 2015 evaluation", "Distance metric learning for kernel density-based acoustic model under limited training data conditions", "Kernel density-based acoustic model with cross-lingual bottleneck features for resource limited LVCSR", "Exemplar-inspired strategies for low-resource spoken keyword search in Swahili", "A comparative study of BNF and DNN multilingual training on cross-lingual low-resource speech recognition.", "Improving efficiency of sentence boundary detection by feature selection", "An empirical evaluation of stop word removal in statistical machine translation", "Decoupling word-pair distance and co-occurrence information for effective long history context language modeling", "Modeling of term-distance and term-occurrence information for improving n-gram language model performance", "Improving language modeling by using distance and co-occurrence information of word-pairs and its application to LVCSR", "TDTO Language Modeling with Feedforward Neural Networks", "Enriching Under-Represented Named Entities for Improved Speech Recognition", "Unsupervised language model adaptation by data selection for speech recognition", "Approaches to improving recognition of underrepresented named entities in hybrid ASR systems", "Independent language modeling architecture for end-to-end ASR", "Leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning", "High accuracy classification of EEG signal", "Spectral local harmonicity feature for voice activity detection", "Linear dynamic models for voice activity detection", "Spoken language recognition with relevance feedback", "Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition", "The IIR-NTU speaker diarization systems for RT 2009", "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention", "On time-frequency mask estimation for MVDR beamforming with application in robust speech recognition", "Learning to estimate reverberation time in noisy and reverberant rooms", "A learning-based approach to direction of arrival estimation in noisy and reverberant environments", "SPGM: Prioritizing Local Features for enhanced speech separation performance", "Are Soft Prompts Good Zero-shot Learners for Speech Recognition?", "A study of learning based beamforming methods for speech recognition", "An expectation-maximization eigenvector clustering approach to direction of arrival estimation of multiple speech sources", "A robust real-time sound source localization system for olivia robot", "The NTU-ADSC systems for reverberation challenge 2014", "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation", "Robust speech recognition using beamforming with adaptive microphone gains and multichannel noise reduction"], "link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:SP6oXDckpogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NJ774b8OgUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:7Hz3ACDFbsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:SpbeaW3--B0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:JoZmwDi-zQgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:4fKUyHm3Qg0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:LPZeul_q3PIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dshw04ExmUIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:dQ2og3OwTAUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:AXPGKjj_ei8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tS2w5q8j5-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XD-gHx7UXLsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OR75R8vi5nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:35r97b3x0nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NhqRSupF_l8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:EUQCXRtRnyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:SeFeTyx0c_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:BwyfMAYsbu0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:YOwf2qJgpHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:lmc2jWPfTJgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:rO6llkc54NcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OU6Ihb5iCvQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:eflP2zaiRacC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:hCrLmN-GePgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:L7CI7m0gUJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:MLfJN-KU85MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:kWvqk_afx_IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:AHdEip9mkN0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tKAzc9rXhukC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZfRJV9d4-WMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:pyW8ca7W8N0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:KxtntwgDAa4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ipzZ9siozwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:LO7wyVUgiFcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ML0RJ9NH7IQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:vbGhcppDl1QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:NyGDZy8z5eUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:ns9cj8rnVeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:zLWjf1WUPmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HeT0ZceujKMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_axFR9aDTf0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0KyAp5RtaNEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:p2g8aNsByqUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:YohjEiUPhakC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:eMMeJKvmdy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:5qfkUJPXOUwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:jgBuDB5drN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LI9QrySNdTsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:uWiczbcajpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:anf4URPfarAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:p__nRnzSRKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:rmuvC79q63oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_Qo2XoVZTnwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:86PQX7AUzd4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:ubry08Y2EpUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:-_dYPAW6P2MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Mojj43d5GZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:35N4QoGY0k4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bFI3QPDXJZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tzM49s52ZIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ye4kPcJQO24C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:xtoqd-5pKcoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:1qzjygNMrQYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:W5xh706n7nkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:tkaPQYYpVKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:5Ul4iDaHHb8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yFnVuubrUp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:vDijr-p_gm4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:kz9GbA2Ns4gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:pqnbT2bcN3wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bz8QjSJIRt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:BrmTIyaxlBUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:Bg7qf7VwUHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yB1At4FlUx8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:j7_hQOaDUrUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:NaGl4SEjCO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:M7yex6snE4oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:abG-DnoFyZgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:WqliGbK-hY8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:sJsF-0ZLhtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:dhFuZR0502QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tH6gc1N1XXoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Y5dfb0dijaUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:1yQoGdGgb4wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PR6Y55bgFSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:WJVC3Jt7v1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:ufrVoPGSRksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:kh2fBNsKQNwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:u9iWguZQMMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:fFSKOagxvKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:UxriW0iASnsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:PELIpwtuRlgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:HE397vMXCloC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:UHK10RUVsp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:uLbwQdceFCQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:_Re3VWB3Y0AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yB1At4FlUx8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:XoXfffV-tXoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OU6Ihb5iCvQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:SP6oXDckpogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:M7yex6snE4oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NJ774b8OgUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LI9QrySNdTsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:eflP2zaiRacC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:uWiczbcajpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:L7CI7m0gUJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:g3aElNc5_aQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HbR8gkJAVGIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:anf4URPfarAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:p__nRnzSRKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:0N-VGjzr574C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:foquWX3nUaYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:MLfJN-KU85MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:LjlpjdlvIbIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:SpbeaW3--B0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:mvPsJ3kp5DgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Y5dfb0dijaUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KUbvn5osdkgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:ufrVoPGSRksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tKAzc9rXhukC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:7T2F9Uy0os0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:LPZeul_q3PIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:2KloaMYe4IUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZfRJV9d4-WMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:35N4QoGY0k4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:pyW8ca7W8N0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dshw04ExmUIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ipzZ9siozwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tS2w5q8j5-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:WZBGuue-350C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ye4kPcJQO24C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tzM49s52ZIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ML0RJ9NH7IQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:tYavs44e6CUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:vbGhcppDl1QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:1qzjygNMrQYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XD-gHx7UXLsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:zLWjf1WUPmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:35r97b3x0nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:5qfkUJPXOUwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:UxriW0iASnsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:hkOj_22Ku90C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0KyAp5RtaNEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:PELIpwtuRlgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ruyezt5ZtCIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:vDijr-p_gm4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:uLbwQdceFCQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:kzcrU_BdoSEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:kz9GbA2Ns4gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bz8QjSJIRt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:lmc2jWPfTJgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:abG-DnoFyZgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:xtoqd-5pKcoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:u9iWguZQMMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:tkaPQYYpVKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:VL0QpB8kHFEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NhqRSupF_l8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0KyAp5RtaNEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:EUQCXRtRnyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:HE397vMXCloC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:p2g8aNsByqUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:geHnlv5EZngC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:5awf1xo2G04C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:BrmTIyaxlBUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:Bg7qf7VwUHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:jgBuDB5drN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:foquWX3nUaYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:sJsF-0ZLhtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tH6gc1N1XXoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:AHdEip9mkN0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:WJVC3Jt7v1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:kh2fBNsKQNwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:NyGDZy8z5eUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OR75R8vi5nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:0izLItjtcgwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:fFSKOagxvKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yFnVuubrUp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:PELIpwtuRlgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:kz9GbA2Ns4gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bz8QjSJIRt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:XoXfffV-tXoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:7Hz3ACDFbsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:hCrLmN-GePgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HbR8gkJAVGIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_Qo2XoVZTnwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:LjlpjdlvIbIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:-_dYPAW6P2MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KUbvn5osdkgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:tYavs44e6CUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:WZBGuue-350C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ML0RJ9NH7IQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:9pM33mqn1YgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:W5xh706n7nkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OR75R8vi5nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:35r97b3x0nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:PELIpwtuRlgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ruyezt5ZtCIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:SeFeTyx0c_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:uLbwQdceFCQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:hsZV8lGYWTMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:gsN89kCJA0AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_axFR9aDTf0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0KyAp5RtaNEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:lmc2jWPfTJgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:xtoqd-5pKcoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:4vMrXwiscB8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:kzcrU_BdoSEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:geHnlv5EZngC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:4fGpz3EwCPoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tKAzc9rXhukC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Dip1O2bNi0gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:VLnqNzywnoUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:kh2fBNsKQNwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PVjk1bu6vJQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:VL0QpB8kHFEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:5awf1xo2G04C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LkGwnXOMwfcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:zCSUwVk65WsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:2VqYfGB8ITEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:0CzhzZyukY4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:EUQCXRtRnyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:abG-DnoFyZgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:7Hz3ACDFbsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:p2g8aNsByqUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:sSrBHYA8nusC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:-nhnvRiOwuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:6_hjMsCP8ZoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:kz9GbA2Ns4gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:BwyfMAYsbu0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:dhFuZR0502QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:YOwf2qJgpHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:CdxZDUztZiMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:Bg7qf7VwUHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:0izLItjtcgwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:hCrLmN-GePgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:kz9GbA2Ns4gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:WJVC3Jt7v1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bz8QjSJIRt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:fFSKOagxvKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:foquWX3nUaYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:MLfJN-KU85MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:SpbeaW3--B0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tzM49s52ZIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NJ774b8OgUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:L7CI7m0gUJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:1qzjygNMrQYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XD-gHx7UXLsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:zLWjf1WUPmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:zCSUwVk65WsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:0CzhzZyukY4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:5Ul4iDaHHb8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:rO6llkc54NcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:UHK10RUVsp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:4fKUyHm3Qg0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:jgBuDB5drN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:M7yex6snE4oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:QD3KBmkZPeQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ye4kPcJQO24C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:kWvqk_afx_IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:AHdEip9mkN0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:NyGDZy8z5eUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:zLWjf1WUPmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:sJsF-0ZLhtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:-95Q15plzcUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bFI3QPDXJZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:NDuN12AVoxsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:OBSaB-F7qqsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:F2UWTTQJPOcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:3bvyWxjaHKcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:WqliGbK-hY8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:SP6oXDckpogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:UHK10RUVsp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:nZcligLrVowC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4xDN1ZYqzskC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:LjlpjdlvIbIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:GFxP56DSvIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:artPoR2Yc-kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ML0RJ9NH7IQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:uLbwQdceFCQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KUbvn5osdkgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:8xutWZnSdmoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:hkOj_22Ku90C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:O0nohqN1r9EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:1lhNe0rCu4AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PVgj2kMGcgYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:IaI1MmNe2tcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:F1b5ZUV5XREC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:6yz0xqPARnAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:Ak0FvsSvgGUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:FiytvqdAVhgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:g3aElNc5_aQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:Br1UauaknNIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:nRpfm8aw39MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:MLfJN-KU85MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:xtRiw3GOFMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:AXPGKjj_ei8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:1sJd4Hv_s6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_B80troHkn4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:fQNAKQ3IYiAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:D_sINldO8mEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:hkOj_22Ku90C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ipzZ9siozwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LgRImbQfgY4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:BJbdYPG6LGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:6yz0xqPARnAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:1yQoGdGgb4wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PR6Y55bgFSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KUbvn5osdkgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:anf4URPfarAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:vbGhcppDl1QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:2tRrZ1ZAMYUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:p__nRnzSRKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HeT0ZceujKMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:eMMeJKvmdy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:URolC5Kub84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:0N-VGjzr574C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:5qfkUJPXOUwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:F1b5ZUV5XREC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:l7t_Zn2s7bgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:U4n9YNQMCAIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:738O_yMBCRsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:-jrNzM816MMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:2VqYfGB8ITEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:hkOj_22Ku90C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:7Hz3ACDFbsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PR6Y55bgFSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:vbGhcppDl1QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:anf4URPfarAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:p__nRnzSRKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:eMMeJKvmdy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:0N-VGjzr574C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:5qfkUJPXOUwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:7Hz3ACDFbsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:rO6llkc54NcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4hFrxpcac9AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:3htObqc8RwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:3NQIlFlcGxIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:URolC5Kub84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:HhcuHIWmDEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:rmuvC79q63oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:VN7nJs4JPk0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:a9-T7VOCCH8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:CdxZDUztZiMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:VLnqNzywnoUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:5awf1xo2G04C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:VL0QpB8kHFEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:xtoqd-5pKcoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:EUQCXRtRnyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:Fu2w8maKXqMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:p__nRnzSRKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:hkOj_22Ku90C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:rO6llkc54NcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:NaGl4SEjCO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:pqnbT2bcN3wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dshw04ExmUIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OU6Ihb5iCvQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ipzZ9siozwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ML0RJ9NH7IQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:uWiczbcajpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:vDijr-p_gm4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:p__nRnzSRKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:2KloaMYe4IUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:URolC5Kub84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:umqufdRvDiIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:dQ2og3OwTAUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:B3FOqHPlNUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Mojj43d5GZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:_Re3VWB3Y0AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:1lhNe0rCu4AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:VaXvl8Fpj5cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:6yz0xqPARnAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HeT0ZceujKMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:gKiMpY-AVTkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:vRqMK49ujn8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:8AbLer7MMksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:dhFuZR0502QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:-95Q15plzcUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bFI3QPDXJZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:NDuN12AVoxsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:SpbeaW3--B0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tzM49s52ZIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NJ774b8OgUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:F2UWTTQJPOcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:3bvyWxjaHKcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:L7CI7m0gUJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:1qzjygNMrQYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XD-gHx7UXLsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:zLWjf1WUPmwC"]}, "published_by_year": {"Year": ["1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Publications": [3, 3, 8, 0, 1, 0, 0, 0, 0, 1, 7, 6, 9, 7, 12, 15, 15, 7, 17, 19, 22, 25, 24, 17, 10, 8, 10, 17, 32, 30, 19]}, "citations_by_year": {"Year": ["1994", "1995", "1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [0, 1, 6, 13, 32, 27, 32, 35, 42, 32, 53, 84, 106, 123, 134, 142, 160, 188, 220, 249, 342, 480, 548, 569, 564, 730, 627, 739, 815, 885, 26]}, "all_time_h_index": 47, "all_time_i10_index": 158, "h_index_by_publication_year": {"Publication Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 2, 6, 0, 1, 0, 0, 0, 0, 1, 6, 6, 6, 4, 7, 7, 9, 4, 10, 9, 12, 12, 15, 8, 6, 7, 7, 7, 10, 5]}, "avg_citations_by_publication_year": {"Publication Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [1.6666666666666667, 16.333333333333332, 81.875, 0.0, 81.0, 0.0, 0.0, 0.0, 0.0, 2.0, 65.42857142857143, 34.333333333333336, 26.77777777777778, 13.714285714285714, 21.083333333333332, 16.6, 21.066666666666666, 5.714285714285714, 60.76470588235294, 31.05263157894737, 31.818181818181817, 34.96, 18.541666666666668, 16.470588235294116, 36.3, 21.75, 28.8, 12.0, 9.3125, 2.6]}, "h_index_by_year": {"Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 7, 8, 10, 13, 14, 15, 17, 19, 19, 21, 25, 28, 30, 33, 34, 37, 39, 42, 45, 47]}, "h_index_by_years_from_publication_year": {"Publication Year": [1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1995, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1996, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 1, 2, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 2, 3, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 1, 3, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 1, 3, 4, 5, 6, 7, 7, 7, 7, 8, 8, 8, 9, 9, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 2, 6, 7, 9, 9, 10, 10, 10, 10, 10, 10, 10, 2, 5, 7, 8, 8, 8, 8, 8, 9, 9, 9, 2, 5, 8, 9, 10, 11, 11, 11, 12, 12, 3, 7, 10, 11, 11, 12, 12, 12, 12, 4, 7, 10, 11, 12, 13, 14, 15, 2, 4, 5, 6, 7, 8, 8, 2, 6, 6, 6, 6, 6, 2, 3, 4, 6, 7, 3, 4, 7, 7, 2, 5, 7, 4, 10, 5]}, "all_time_i20_index": 100, "publications": {"Publication Year": ["1996", "2015", "1996", "2012", "2012", "2014", "2014", "2012", "2015", "2018", "2013", "2012", "2004", "2015", "2020", "2010", "2012", "2017", "2006", "2004", "2013", "2020", "2008", "2013", "1998", "2004", "2004", "2018", "2004", "2006", "2015", "1996", "2021", "2013", "2009", "2012", "2005", "2016", "2013", "2008", "2018", "2005", "2005", "2019", "2022", "2017", "2014", "2007", "2018", "2014", "2010", "2019", "2016", "2010", "2016", "2014", "2013", "2008", "2015", "2009", "2016", "2009", "2013", "2009", "1995", "2014", "2015", "2007", "2013", "2015", "2022", "2021", "2019", "2022", "2016", "2010", "2009", "2006", "1996", "2021", "2019", "2018", "2015", "2008", "2017", "2016", "2014", "2012", "2022", "2016", "2012", "2022", "2016", "2005", "1996", "2022", "2021", "2020", "2017", "2016", "2011", "2016", "2015", "2015", "2017", "2016", "2012", "2010", "2006", "2005", "2020", "2016", "2016", "2016", "2012", "2022", "2020", "2017", "2010", "2018", "2016", "2014", "2014", "2014", "2014", "2010", "2008", "2022", "2022", "2016", "2015", "2014", "2004", "1995", "2020", "2015", "2013", "2009", "2008", "2023", "2023", "2021", "2015", "2006", "2022", "2019", "2019", "2014", "2010", "2010", "2009", "2022", "2022", "2021", "2020", "2017", "2012", "2012", "2017", "2014", "2013", "2012", "2023", "2022", "2019", "2017", "2015", "2015", "2010", "2006", "2023", "2022", "2022", "2021", "2020", "2016", "2015", "2013", "2013", "2012", "2012", "2008", "2023", "2021", "2021", "2018", "2017", "2017", "2015", "2011", "2009", "2009", "2007", "2007", "2005", "1996", "2023", "2023", "2022", "2022", "2021", "2020", "2018", "2016", "2016", "2016", "2014", "2009", "2008", "2006", "2023", "2023", "2023", "2022", "2022", "2022", "2021", "2018", "2017", "2016", "2015", "2014", "2014", "2011", "2011", "2010", "2009", "1996", "2022", "2022", "2022", "2021", "2021", "2021", "2017", "2017", "2017", "2017", "2015", "2014", "2014", "2013", "2012", "2012", "2011", "2011", "2009", "2008", "2008", "2007", "2006", "1994", "2023", "2023", "2023", "2022", "2022", "2022", "2021", "2021", "2019", "2016", "2015", "2014", "2013", "2013", "2013", "2010", "2004", "2003", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "2022", "2021", "2020", "2018", "2017", "2016", "2016", "2015", "2015", "2015", "2014", "2013", "2013", "2010", "2010", "2008", "2007", "2007", "1996", "1994", "1994", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "2022", "2015", "2015", "2014", "2013", "2011", "2010", "2009", "2009", "2009", "2008", "2006", "1995", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown"], "Title": ["Regularized orthogonal least squares algorithm for constructing radial basis function networks", "A learning-based approach to direction of arrival estimation in noisy and reverberant environments", "Gradient radial basis function networks for nonlinear and nonstationary time series prediction", "Vulnerability of speaker verification systems against voice conversion spoofing attacks: The case of telephone speech", "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition", "Exemplar-based sparse representation with residual compensation for voice conversion", "A study on replay attack and anti-spoofing for text-dependent speaker verification", "A first speech recognition system for Mandarin-English code-switch conversational speech", "Spoofing speech detection using high dimensional magnitude and phase features: the NTU approach for ASVspoof 2015 challenge.", "Unsupervised domain adaptation via domain adversarial training for speaker recognition", "Synthetic speech detection using temporal modulation feature", "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case", "Sports highlight detection from keyword sequences using HMM", "Modelling public sentiment in Twitter: using linguistic patterns to enhance supervised learning", "Spex: Multi-scale time domain speaker extraction network", "Seame: a mandarin-english code-switching speech corpus in south-east asia", "Image feature representation of the subband power distribution for robust sound event classification", "On time-frequency mask estimation for MVDR beamforming with application in robust speech recognition", "Integrating acoustic, prosodic and phonotactic features for spoken language identification", "Foreground motion detection by difference-based spatial temporal entropy image", "Overlapping sound event recognition using local spectrogram features and the generalised hough transform", "Spex+: A complete time domain speaker extraction network", "Normalization of the speech modulation spectra for robust speech recognition", "Conditional restricted boltzmann machine for voice conversion", "Space translation properties and the minimum-BER linear-combiner DFE", "Automatic replay generation for soccer video broadcasting", "High accuracy classification of EEG signal", "Single channel speech separation with constrained utterance level permutation invariant training using grid lstm", "Concurrent constant modulus algorithm and soft decision directed scheme for fractionally-spaced blind equalization", "Automatic sports video genre classification using pseudo-2d-hmm", "Mandarin\u2013english code-switching speech corpus in south-east asia: Seame", "Minimum-BER linear-combiner DFE", "Gdpnet: Refining latent multi-view graph for relation extraction", "Exemplar-based voice conversion using non-negative spectrogram deconvolution.", "MASS: A Malay language LVCSR corpus resource", "Integration of language identification into a recognition system for spoken conversations containing code-switches", "Motion detection with adaptive background and dynamic thresholds", "Spoofing detection from a feature representation perspective", "Exemplar-based unit selection for voice conversion utilizing temporal information.", "T-test distance and clustering criterion for speaker diarization", "Average Modeling Approach to Voice Conversion with Non-Parallel Data.", "Soccer replay detection using scene transition structure analysis", "Automatic generation of personalized music sports video", "Optimization of speaker extraction neural network with magnitude and temporal spectrum approximation loss", "Speech emotion recognition with co-attention based multi-level acoustic information", "An exemplar-based approach to frequency warping for voice conversion", "The NTU-ADSC systems for reverberation challenge 2014", "Generation of personalized music sports video using multimodal cues", "Study of semi-supervised approaches to improving english-mandarin code-switching speech recognition", "Severity-based adaptation with limited data for ASR to aid dysarthric speakers", "An analysis of a Mandarin-English code-switching speech corpus: SEAME", "Time-domain speaker extraction network", "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation", "Text-independent F0 transformation with non-parallel data for voice conversion", "A study of learning based beamforming methods for speech recognition", "A deep neural network approach for sentence boundary detection in broadcast news", "Language diarization for code-switch conversational speech", "Automatic composition of broadcast sports video", "Low-resource keyword search strategies for Tamil", "The I4U system in NIST 2008 speaker recognition evaluation", "Combining non-negative matrix factorization and deep neural networks for speech enhancement and automatic speech recognition", "A target-oriented phonotactic front-end for spoken language recognition", "Vulnerability evaluation of speaker verification under voice conversion spoofing: the effect of text constraints.", "The IIR-NTU speaker diarization systems for RT 2009", "Efficient computational schemes for the orthogonal least squares algorithm", "Correlation-based frequency warping for voice conversion", "A comparative study of BNF and DNN multilingual training on cross-lingual low-resource speech recognition.", "Temporal structure normalization of speech feature for robust speech recognition", "Hadoop framework: impact of data organization on performance", "Exemplar-based voice conversion using joint nonnegative matrix factorization", "Convmixer: Feature interactive convolution with curriculum learning for small footprint and noisy far-field keyword spotting", "Multi-stage speaker extraction with utterance and frame-level reference signals", "Transfer learning for punctuation prediction", "Interactive feature fusion for end-to-end noise-robust speech recognition", "High quality voice conversion using prosodic and high-resolution spectral features", "A tree-construction search approach for multivariate time series motifs discovery", "A study on the generalization capability of acoustic models for robust speech recognition", "Temporal discrete cosine transform: Towards longer term temporal features for speaker verification", "Reduced complexity implementation of Bayesian equaliser using local RBF network for channel equalisation problem", "Representation learning with spectro-temporal-channel attention for speech emotion recognition", "A Speaker-Dependent WaveNet for Voice Conversion with Non-Parallel Data.", "Named-entity tagging and domain adaptation for better customized translation", "Sparse representation for frequency warping based voice conversion", "Speaker Diarization Using Direction of Arrival Estimate and Acoustic Feature Information: The I2R-NTU Submission for the NIST RT 2007 Evaluation", "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source.", "An investigation of spoofing speech detection under additive noise and reverberant conditions", "The NNI Query-by-Example System for MediaEval 2014.", "Mixture of factor analyzers using priors from non-parallel speech for voice conversion", "Noise-robust speech recognition with 10 minutes unparalleled in-domain data", "A DNN-HMM Approach to Story Segmentation.", "Broadcast news story segmentation using conditional random fields and multimodal features", "Learning affective representations based on magnitude and dynamic relative phase information for speech emotion recognition", "Spoofing speech detection using temporal convolutional neural network", "A player-possession acquisition system for broadcast soccer video", "Orthogonal least-squares learning algorithm with local adaptation process for the radial basis function networks", "An embarrassingly simple model for dialogue relation extraction", "E2e-based multi-task learning approach to joint speech and accent recognition", "PyTrack: An end-to-end analysis toolkit for eye tracking", "A review of the Mandarin-English code-switching corpus: SEAME", "Semi-Supervised and Cross-Lingual Knowledge Transfer Learnings for DNN Hybrid Acoustic Models Under Low-Resource Conditions.", "Maximum likelihood adaptation of histogram equalization with constraint for robust speech recognition", "Toward High-Performance Language-Independent Query-by-Example Spoken Term Detection for MediaEval 2015: Post-Evaluation Analysis.", "Robust speech recognition using beamforming with adaptive microphone gains and multichannel noise reduction", "Language independent query-by-example spoken term detection using n-best phone sequences and partial matching", "Improving N-gram language modeling for code-switching speech recognition", "Exemplar-inspired strategies for low-resource spoken keyword search in Swahili", "An empirical evaluation of stop word removal in statistical machine translation", "Selecting phonotactic features for language recognition", "Fusion of acoustic and tokenization features for speaker recognition", "Fractionally spaced blind equalization with low\u2010complexity concurrent constant modulus algorithm and soft decision\u2010directed scheme", "Independent language modeling architecture for end-to-end ASR", "Beamforming networks using spatial covariance features for far-field speech recognition", "Single-channel dereverberation for distant-talking speech recognition by combining denoising autoencoder and temporal structure normalization", "Feature adaptation using linear spectro-temporal transform for robust speech recognition", "Discriminative feature extraction for speech recognition using continuous output codes", "Interactive audio-text representation for automated audio captioning with contrastive learning", "Speech Transformer with Speaker Aware Persistent Memory.", "An investigation of spectral feature partitioning for replay attacks detection", "A robust real-time sound source localization system for olivia robot", "A Shifted Delta Coefficient Objective for Monaural Speech Separation Using Multi-task Learning.", "Approximate search of audio queries by using DTW with phone time boundary and data augmentation", "Discriminative score normalization for keyword search decision", "Cross-lingual phone mapping for large vocabulary speech recognition of under-resourced languages", "Semi-supervised training for bottle-neck feature based DNN-HMM hybrid systems", "Analysis of spectrogram image methods for sound event classification", "Development of a malay LVCSR system", "Target-oriented phone tokenizers for spoken language recognition", "Self-critical sequence training for automatic speech recognition", "Dual-path style learning for end-to-end noise-robust speech recognition", "Content-aware local variability vector for speaker verification with short utterance", "Speech enhancement using beamforming and non negative matrix factorization for robust speech recognition in the CHiME-3 challenge", "Kernel density-based acoustic model with cross-lingual bottleneck features for resource limited LVCSR", "Event detection based on non-broadcast sports video", "Optimum lag and subset selection for a radial basis function equaliser", "Time-domain neural network approach for speech bandwidth extension", "Language-resource independent speech segmentation using cues from a spectrogram image", "Language diarization for conversational code-switch speech with pronunciation dictionary adaptation", "Cluster criterion functions in spectral subspace and their application in speaker clustering", "Efficient mobile phone Chinese optical character recognition systems by use of heuristic fuzzy rules and bigram Markov language models", "Leveraging modality-specific representations for audio-visual speech recognition via reinforcement learning", "Gradient remedy for multi-task learning in end-to-end noise-robust speech recognition", "Multitask-based joint learning approach to robust asr for radio communication speech", "Dnn feature compensation for noise robust speaker verification", "Identify sports video shots with\" happy\" or\" sad\" emotions", "Automated audio captioning using transfer learning and reconstruction latent space similarity regularization", "Audio codec simulation based data augmentation for telephony speech recognition", "Domain adversarial training for speech enhancement", "Joint nonnegative matrix factorization for exemplar-based voice conversion", "Modeling broadcast news prosody using conditional random fields for story segmentation", "The development of a Singapore English call resource", "Target-aware language models for spoken language recognition", "Efficient self-supervised learning representations for spoken language identification", "L-spex: Localized target speaker extraction", "Time domain speech enhancement with attentive multi-scale approach", "Multi-Task Learning for End-to-End Noise-Robust Bandwidth Extension.", "A hybrid neural network hidden Markov model approach for automatic story segmentation", "Joint spectral and temporal normalization of features for robust recognition of noisy and reverberated speech", "Lasso environment model combination for robust speech recognition", "Towards age-friendly E-commerce through crowd-improved speech recognition, multimodal search, and personalized speech feedback", "System and keyword dependent fusion for spoken term detection", "Modeling of term-distance and term-occurrence information for improving n-gram language model performance", "Context dependant phone mapping for cross-lingual acoustic modeling", "Unifying speech enhancement and separation with gradient modulation for end-to-end noise-robust speech separation", "Minimum word error training for non-autoregressive Transformer-based code-switching ASR", "Improving code-switching speech recognition with data augmentation and system combination", "Named entity transliteration with sequence-to-sequence neural network", "Non-negative matrix factorization using stable alternating direction method of multipliers for source separation", "Detecting synthetic speech using long term magnitude and phase information", "Phoneme lattice based TextTiling towards multilingual story segmentation", "Fully and semi-automatic music sports video composition", "Metric-oriented speech enhancement using diffusion probabilistic model", "Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model", "FMSG-NTU submission for DCASE 2022 Task 4 on sound event detection in domestic environments", "Leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning", "Cross Attention with Monotonic Alignment for Speech Transformer.", "Keyword search using query expansion for graph-based rescoring of hypothesized detections", "On the study of very low-resource language keyword search", "Temporal filter design by minimum KL divergence criterion for robust speech recognition", "Attribute-based histogram equalization (HEQ) and its adaptation for robust speech recognition.", "Collection and annotation of malay conversational speech corpus", "A phone mapping technique for acoustic modeling of under-resourced languages", "Target-oriented phone selection from universal phone set for spoken language recognition", "Unsupervised noise adaptation using data simulation", "Learning disentangled feature representations for speech enhancement via adversarial training", "End-to-end speaker height and age estimation using attention mechanism with LSTM-RNN", "Learning distributed sentence representations for story segmentation", "Investigation of fixed-dimensional speech representations for real-time speech emotion recognition system", "Improving air traffic control speech intelligibility by reducing speaking rate effectively", "Learning to estimate reverberation time in noisy and reverberant rooms", "Speech modulation features for robust nonnative speech accent detection", "Improved keypoint matching method for near-duplicate keyframe retrieval", "Exploiting prosodic information for Speaker Recognition", "An MCU description methodology for initialization code generation software", "Evaluating the temporal structure normalisation technique on the Aurora-4 task.", "Determining the optimal decision delay parameter for a linear equalizer", "Adaptive orthogonal least squares learning algorithm for the radial basis function network", "Speech-text based multi-modal training with bidirectional attention for improved speech recognition", "Leveraging Audio-Tagging Assisted Sound Event Detection using Weakified Strong Labels and Frequency Dynamic Convolutions", "Medbert: a pre-trained language model for biomedical named entity recognition", "Rainbow keywords: Efficient incremental learning for online spoken keyword spotting", "Preventing early endpointing for online automatic speech recognition", "Speaker and Phoneme-Aware Speech Bandwidth Extension with Residual Dual-Path Network.", "A hybrid deep learning architecture for sentence unit detection", "An expectation-maximization eigenvector clustering approach to direction of arrival estimation of multiple speech sources", "The I4U submission to the 2016 NIST speaker recognition evaluation", "Improving efficiency of sentence boundary detection by feature selection", "Multi-view features in a DNN-CRF model for improved sentence unit detection on English broadcast news", "Efficient sparse self-similarity matrix construction for repeating sequence detection", "MICRO-EBLOCK: A modular platform for embedded system education", "The IIR NIST 2006 Speaker Recognition System: Fusion of Acoustic and Tokenization Features", "Contrastive Speech Mixup for Low-Resource Keyword Spotting", "De\u2019hubert: Disentangling Noise in a Self-Supervised Model for Robust Speech Recognition", "Wav2code: Restore clean speech representations via codebook lookup for noise-robust asr", "Is your baby fine at home? Baby cry sound detection in domestic environments", "Continual learning for on-device environmental sound classification", "Internal language model estimation based language model fusion for cross-domain code-switching speech recognition", "A unified speaker adaptation approach for ASR", "Re-ranking spoken term detection with acoustic exemplars of keywords", "Low-resource spoken keyword search strategies in georgian inspired by distinctive feature theory", "Zero resource anti-spoofing detection for unit selection based synthetic speech using image spectrogram artifacts", "Distance metric learning for kernel density-based acoustic model under limited training data conditions", "Generalization of temporal filter and linear transformation for robust speech recognition", "Feature compensation using linear combination of speaker and environment dependent correction vectors", "An iterative approach to model merging for speech pattern discovery", "Feature normalization using structured full transforms for robust speech recognition", "Development of hmm-based malay text-to-speech system", "A generic MCU description methodology with dependency evaluation", "An on-line learning algorithm for blind equalization", "Punctuation Restoration for Singaporean Spoken Languages: English, Malay, and Mandarin", "I2cr: Improving noise robustness on keyword spotting using inter-intra contrastive regularization", "Small footprint multi-channel convmixer for keyword spotting with centroid based awareness", "End-to-end speaker age and height estimation using attention mechanism and triplet loss", "Learning speaker representation with semi-supervised learning approach for speaker profiling", "Overlapped Speech Detection Based on Spectral and Spatial Feature Fusion.", "An end-to-end neural network approach to story segmentation", "Pruning strategies for partial search in spoken term detection", "Unsupervised language model adaptation by data selection for speech recognition", "Novel Functional Technologies for Age-Friendly E-commerce", "Decoupling word-pair distance and co-occurrence information for effective long history context language modeling", "Speaker diarization: An emerging research", "A discriminatively trained Hough Transform for frame-level phoneme recognition", "Local partial least square regression for spectral mapping in voice conversion", "An analysis of vector Taylor series model compensation for non-stationary noise in speech recognition", "Spectral local harmonicity feature for voice activity detection", "Linear dynamic models for voice activity detection", "Target-aware lattice rescoring for dialect recognition", "A study on hidden Markov model's generalization capability for speech recognition", "Fuzzy rule selection using Iterative Rule Learning for speech data classification", "Classification of speech transmission channels: Landline, GSM and VoIP networks", "Spoken language recognition with relevance feedback", "The IIR submission to CSLP 2006 speaker recognition evaluation", "Reducing the computational requirement of the orthogonal least squares algorithm", "Hyporadise: An open baseline for generative speech recognition with large language models", "Noise-aware Speech Separation with Contrastive Learning", "Cross-Modal Global Interaction and Local Alignment for Audio-Visual Speech Recognition", "The iscslp 2022 intelligent cockpit speech recognition challenge (icsrc): Dataset, tracks, baseline and results", "DENT-DDSP: Data-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition", "Intermediate-layer output regularization for attention-based speech recognition with shared decoder", "Multilingual approach to joint speech and accent recognition with DNN-HMM framework", "Approaches to improving recognition of underrepresented named entities in hybrid ASR systems", "QASA: Advanced Document Retriever for Open-Domain Question Answering by Learning to Rank Question-Aware Self-Attentive Document Representations", "Neural networks based channel compensation for i-vector speaker verification", "A density peak clustering approach to unsupervised acoustic subword units discovery", "Towards better keyword search performance on Malay broadcast news data", "A particle filter compensation approach to robust LVCSR", "A robust sound event recognition framework under TV playing conditions", "Adaptive semi-supervised tree SVM for sound event recognition in home environments", "I4U Submission for the 2008 NIST Speaker Recognition Evaluation Submission", "A web based framework for e-learning: A model for online presentation authoring", "Optimum delay order selection for linear equalization problems", "Noise-aware Speech Enhancement using Diffusion Probabilistic Model", "A Neural State-Space Model Approach to Efficient Speech Separation", "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention", "Dual Acoustic Linguistic Self-supervised Representation Learning for Cross-Domain Speech Recognition", "Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement", "Vision Transformer based Audio Classification using Patch-level Feature Fusion", "Language-based audio retrieval with converging tied layers and contrastive loss", "Automated Audio Captioning with Epochal Difficult Captions for curriculum learning", "Enriching Under-Represented Named Entities for Improved Speech Recognition", "Universal Speech Transformer.", "An investigation of word embeddings with deep bidirectional lstm for sentence unit detection in automatic speech transcription", "Topic embedding of sentences for story segmentation", "I-vector based deep neural network acoustic model adaptation using multilingual language resource", "Multi-channel feature adaptation for robust speech recognition", "Multilingual exemplar-based acoustic model for the NIST Open KWS 2015 evaluation", "TDTO Language Modeling with Feedforward Neural Networks", "Regularized non-negative matrix factorization using alternating direction method of multipliers and its application to source separation", "Improving language modeling by using distance and co-occurrence information of word-pairs and its application to LVCSR", "The development and analysis of a Malay broadcasr news corpus", "Robust sound event recognition under TV playing conditions", "Error corrective classifier fusion for spoken Language Recognition", "A Discriminative Performance Metric for GMM-UBM Speaker Identification", "Discriminative output coding features for speech recognition", "A vector-based approach to broadcast audio database indexing and retrieval", "Using Direction of Arrival Estimate and Acoustic Feature Information in Speaker Diarization", "Adaptive local radial basis function network for nonstationary channel equalisation problem", "Improving the radial basis function network for homogeneous nonstationary time series prediction", "Backtracking orthogonal least squares algorithm for model selection", "Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification", "SPGM: Prioritizing Local Features for enhanced speech separation performance", "Are Soft Prompts Good Zero-shot Learners for Speech Recognition?", "Codec Data Augmentation for Time-domain Heart Sound Classification", "Adapting Code-Switching Language Models with Statistical-Based Text Augmentation", "An Empirical Study on Punctuation Restoration for English, Mandarin, and Code-Switching Speech", "Local and Global Context Modeling with Relation Matching Task for Dialog Act Recognition", "Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition", "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition", "Probabilistic Back-ends for Online Speaker Recognition and Clustering", "Improving Spoken Language Identification with Map-Mix", "UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning", "Exploring Speaker Age Estimation on Different Self-Supervised Learning Models", "Amino Acid Classification in 2D NMR Spectra via Acoustic Signal Embeddings", "Estimation of speaker characteristics from speech signal", "On the study of very low-resource language keyword search", "On statistical machine translation method for lexicon refinement in speech recognition", "A Bayesian performance bound for time-delay of arrival based acoustic source tracking in a reverberant environment", "Constrained adaptation of histogram equalization for robust speech recognition", "Error Corrective Fusion of Classifier Scores for Spoken Language Recognition", "Framewise Phone Classification Using Weighted Fuzzy Classification Rules", "Subspace construction and selection for speaker recognition", "Efficient Repeating Segments Discovery in Music using Adaptive Motif Generation Algorithm", "Discriminative Feature Transformation using Output Coding for Speech Recognition", "Effect of Feature Smoothing for Robust Speech Recognition", "Proceedings of the 5th international conference on Chinese Spoken Language Processing", "Realizing the optimum decision boundary for channel equalisation using radial basis function and linear networks", "ICNLP 2023", "A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions", "Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory", "Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition", "Small Footprint Multi-channel Network for Keyword Spotting with Centroid Based Awareness", "Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network", "\u9999\u6e2f 256IP \u5343\u5146\u7ad9\u7fa4\u670d\u52a1\u5668 BGP \u4e13\u7ebf 240 \u5143\u8d77! \u5883\u5185\u9ad8\u9632\u4e91\u670d\u52a1\u5668/\u4e91\u7269\u7406\u673a\u767e G \u9ad8\u9632\u5168\u9762\u4e0a\u65b0! \u534e\u4e3a\u4e91\u9999\u6e2f\u7269\u7406\u673a\u7cbe\u54c1\u7ebf\u8def\u5168\u9762\u4e0a\u7ebf![\u7279\u4ef7] \u4f01\u4e1a\u7ea7 CN2 GIA \u53cc\u7a0b\u4e13\u7ebf\u9ad8\u901f\u56de\u56fd T3 \u673a\u623f \u9999\u6e2f\u7f8e\u00a0\u2026", "\u53d1\u5e03\u65f6\u95f4: 2020-02-12 DIARIZATION", "ISSUE ON SELF-SUPERVISED LEARNING FOR SPEECH AND AUDIO PROCESSING (SLSAP)", "MISP CHALLENGE 2021: MULTIMODAL WAKEUP-WORD DETECTION FOR FAR-FIELD AND NOISY ENVIRONMENT\u2014TECHNICAL REPORT", "\u5343\u6b21\u9605\u8bfb", "Semi-class-based N-gram Language Modeling for Chinese Dictation}}", "ICASSP 2022 Organizing Committee", "diarization \u4e13\u680f\u6536\u5f55\u8be5\u5185\u5bb9", "\u6280\u672f\u6807\u7b7e: diarization", "\u9762\u5305\u5c51", "The NTU-XJU System for the AP20-OLR Challenge", "Adaptive orthogonal least squares learning algorithm for the Radial Basis Function network", "An investigation of spoofing speech detection under additive noise and reverberant conditions"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NJ774b8OgUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:NhqRSupF_l8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:HE397vMXCloC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:rO6llkc54NcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:kzcrU_BdoSEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:hCrLmN-GePgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:EUQCXRtRnyEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:t7zJ5fGR-2UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:Bg7qf7VwUHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:HDshCWvjkbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:xtRiw3GOFMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:SpbeaW3--B0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&citation_for_view=FJodrCcAAAAJ:738O_yMBCRsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:1sJd4Hv_s6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:AHdEip9mkN0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:u9iWguZQMMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:vRqMK49ujn8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bz8QjSJIRt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:UHK10RUVsp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:otzGkya1bYkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:sSrBHYA8nusC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:l7t_Zn2s7bgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0KyAp5RtaNEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:p2g8aNsByqUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:4fGpz3EwCPoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:zYLM7Y9cAGgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:WJVC3Jt7v1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:U4n9YNQMCAIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:xtoqd-5pKcoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:1qzjygNMrQYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:WF5omc3nYNoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:9pM33mqn1YgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:t6usbXjVLHcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:fFSKOagxvKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XD-gHx7UXLsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:FPJr55Dyh1AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:PELIpwtuRlgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:4fKUyHm3Qg0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:hkOj_22Ku90C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:_FxGoFyzp5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:EYYDruWGBe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:YOwf2qJgpHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:XiSMed-E-HIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bFI3QPDXJZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:5awf1xo2G04C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:2KloaMYe4IUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:ufrVoPGSRksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:tkaPQYYpVKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:buQ7SEKw-1sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:NyGDZy8z5eUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:3htObqc8RwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:uDGL6kOW6j0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:VLnqNzywnoUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:Y0pCki6q_DkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:sJsF-0ZLhtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:4vMrXwiscB8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:LO7wyVUgiFcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:VL0QpB8kHFEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:foquWX3nUaYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:gVv57TyPmFsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:LjlpjdlvIbIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:abG-DnoFyZgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:v1_lew4L6wgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:tYavs44e6CUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:QD3KBmkZPeQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:lmc2jWPfTJgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:MXK_kJrjxJIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:7BrZ7Jt4UNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:F1b5ZUV5XREC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:LhH-TYMQEocC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:rmuvC79q63oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=20&pagesize=80&citation_for_view=FJodrCcAAAAJ:g3aElNc5_aQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KUbvn5osdkgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:zLWjf1WUPmwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:uLbwQdceFCQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:86PQX7AUzd4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:p__nRnzSRKYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:umqufdRvDiIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LkGwnXOMwfcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HeT0ZceujKMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LI9QrySNdTsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ye4kPcJQO24C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yB1At4FlUx8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:BzfGm06jWhQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:nZcligLrVowC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PVjk1bu6vJQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3oQSFYPqjQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:0izLItjtcgwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ML0RJ9NH7IQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:eMMeJKvmdy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:JoZmwDi-zQgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_B80troHkn4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:RGFaLdJalmkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LdasjJ6CEcoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:43bX7VzcjpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:BwyfMAYsbu0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:epqYDVWIO7EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:3fE2CSJIrl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OR75R8vi5nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:Fu2w8maKXqMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:5Ul4iDaHHb8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:WC9gN4BGCRcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:1Ye0OR6EYb4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:BJbdYPG6LGMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:7T2F9Uy0os0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:vDZJ-YLwNdEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OcBU2YAGkTUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:yFnVuubrUp4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:SeFeTyx0c_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:XUvXOeBm_78C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:jgBuDB5drN8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ALROH1vI_8AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:Ehil0879vHcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:WZBGuue-350C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:pyW8ca7W8N0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dfsIfKJdRG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:gsN89kCJA0AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PR6Y55bgFSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:B3FOqHPlNUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:dshw04ExmUIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:jFemdcug13IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:PVgj2kMGcgYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ODE9OILHJdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:YohjEiUPhakC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:-_dYPAW6P2MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tKAzc9rXhukC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_Qo2XoVZTnwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:-7ulzOJl1JYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:rHJHxKgnXwkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:1taIhTC69MYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:gKiMpY-AVTkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:GFxP56DSvIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:vbGhcppDl1QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:ipzZ9siozwsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tS2w5q8j5-wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:mvPsJ3kp5DgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:SP6oXDckpogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:OU6Ihb5iCvQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:KaMxkj08jr0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tH6gc1N1XXoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:LgRImbQfgY4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HbR8gkJAVGIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:kh2fBNsKQNwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:_axFR9aDTf0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:tzM49s52ZIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:35N4QoGY0k4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:9ZlFYXVOiuMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:5icHVeHT4IsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:Br1UauaknNIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:HGTzPopzzJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:CdxZDUztZiMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=100&pagesize=100&citation_for_view=FJodrCcAAAAJ:kVjdVfd2voEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:artPoR2Yc-kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:2tRrZ1ZAMYUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:a9-T7VOCCH8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:L7CI7m0gUJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:q3CdL3IzO_QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:URolC5Kub84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZuybSZzF8UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:NMxIlDl6LWMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:70eg2SAEIzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4JMBOYKVnBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:OBSaB-F7qqsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:jE2MZjpN3IcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:MAUkC_7iAq8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:LPtt_HFRSbwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:PYBJJbyH-FwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:FiytvqdAVhgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4xDN1ZYqzskC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:anf4URPfarAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:5qfkUJPXOUwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:tuHXwOkdijsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:vDijr-p_gm4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Y5dfb0dijaUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:KxtntwgDAa4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:BrmTIyaxlBUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:3NQIlFlcGxIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:prdVHNxh-e8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:cK4Rrx0J3m0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:lgwcVrK6X84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:SjuI4pbJlxcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:PaBasH6fAo0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ruyezt5ZtCIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:0N-VGjzr574C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:VaXvl8Fpj5cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Dip1O2bNi0gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:dQ2og3OwTAUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:WqliGbK-hY8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:AXPGKjj_ei8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:geHnlv5EZngC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:UxriW0iASnsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:8AbLer7MMksC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:BqipwSGYUEgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:dhFuZR0502QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:w1MjKQ0l0TYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:PyEswDtIyv0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:QyXJ3EUuO1IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:hsZV8lGYWTMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:CB2v5VPnA5kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:IaI1MmNe2tcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:nRpfm8aw39MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:6yz0xqPARnAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:IUKN3-7HHlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:kz9GbA2Ns4gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:35r97b3x0nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:1yQoGdGgb4wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:eflP2zaiRacC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:D_sINldO8mEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:eJXPG6dFmWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:2VqYfGB8ITEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ce2CqMG-AY4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:sszUF3NjhM4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:NDuN12AVoxsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:KNjnJ3z-R6IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:uVUOdF_882EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:6bLC7aUMtPcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:kw52XkFRtyQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:MpfHP-DdYjUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:1lhNe0rCu4AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:8xutWZnSdmoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:4hFrxpcac9AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:XoXfffV-tXoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:MLfJN-KU85MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:M7yex6snE4oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:uWiczbcajpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:_Re3VWB3Y0AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:W5xh706n7nkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:Mojj43d5GZwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:N5tVd3kTz84C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:fQNAKQ3IYiAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:NaGl4SEjCO4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:e5wmG9Sq2KIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:0CzhzZyukY4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:-nhnvRiOwuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=200&pagesize=100&citation_for_view=FJodrCcAAAAJ:F2UWTTQJPOcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:3bvyWxjaHKcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:w0F2JDEymm0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:HhcuHIWmDEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:VN7nJs4JPk0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:kWvqk_afx_IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:RoXSNcbkSzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:mKu_rENv82IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:6_hjMsCP8ZoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:AXkvAH5U_nMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:-jrNzM816MMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:DBa1UEJaJKAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:jU7OWUQzBzMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:Ade32sEp0pkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:AvfA0Oy_GE0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:ZfRJV9d4-WMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:dTyEYWd-f8wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:LPZeul_q3PIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:ns9cj8rnVeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:pqnbT2bcN3wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:RYcK_YlVTxYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:ubry08Y2EpUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:zCSUwVk65WsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:FiDNX6EVdGUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:pAkWuXOU-OoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:O0nohqN1r9EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:-95Q15plzcUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:1DsIQWDZLl8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:U_HPUtbDl20C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:Dem6FJhTUoYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:1yWc8FF-_SYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:yxmsSjX2EkcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:-mN3Mh-tlDkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:L1USKYWJimsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:j7_hQOaDUrUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:7Hz3ACDFbsoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:CaZNVDsoPx4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:wMgC3FpKEyYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:4X0JR2_MtJMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:Ak0FvsSvgGUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:nVrZBo8bIpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=FJodrCcAAAAJ&cstart=300&pagesize=100&citation_for_view=FJodrCcAAAAJ:lvd772isFD0C"], "Topic": ["Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Signal Processing", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Audio Processing", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Audio Processing", "Audio Processing", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Audio Processing", "Audio Processing", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Signal Processing", "Federated Learning", "Audio Processing", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Signal Processing", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Audio Processing", "Audio Processing", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Audio Processing", "Audio Processing", "Audio Processing", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Federated Learning", "Audio Processing", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Audio Processing", "Audio Processing", "Audio Processing", "Federated Learning", "Audio Processing", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Signal Processing", "Audio Processing", "Audio Processing", "Audio Processing", "Audio Processing", "Audio Processing", "Geospatial Technology", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Audio Processing", "Audio Processing", "Artificial Intelligenc", "Biomedical Informatic", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Audio Processing", "Audio Processing", "Audio Processing", "Artificial Intelligenc", "Artificial Intelligenc", "Audio Processing", "Biomedical Informatic", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others"], "# of Citations": [305, 290, 232, 220, 209, 193, 192, 175, 151, 136, 139, 131, 124, 119, 111, 112, 108, 99, 99, 93, 90, 89, 89, 85, 81, 80, 79, 71, 66, 69, 68, 64, 67, 65, 56, 60, 59, 56, 53, 53, 53, 51, 51, 43, 49, 49, 49, 49, 47, 47, 47, 46, 46, 46, 44, 45, 45, 44, 42, 42, 41, 41, 38, 37, 35, 35, 33, 32, 31, 29, 28, 28, 28, 27, 27, 27, 26, 27, 27, 25, 25, 25, 25, 25, 24, 24, 24, 24, 23, 23, 23, 22, 22, 21, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 18, 18, 18, 18, 18, 18, 16, 17, 17, 17, 17, 16, 16, 16, 16, 15, 15, 15, 15, 15, 14, 14, 15, 14, 14, 11, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 10, 11, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["The paper presents a regularized orthogonal least squares learning algorithm for radial basis function networks. The proposed algorithm combines the advantages of both the orthogonal forward regression and regularization methods to provide an efficient and powerful procedure for constructing parsimonious network models that generalize well. Examples of nonlinear modelling and prediction are used to demonstrate better generalization performance of this regularized orthogonal least squares algorithm over the unregularized one.", "This paper presents a learning-based approach to the task of direction of arrival estimation (DOA) from microphone array input. Traditional signal processing methods such as the classic least square (LS) method rely on strong assumptions on signal models and accurate estimations of time delay of arrival (TDOA) . They only work well in relatively clean conditions, but suffer from noise and reverberation distortions. In this paper, we propose a learning-based approach that can learn from a large amount of simulated noisy and reverberant microphone array inputs for robust DOA estimation. Specifically, we extract features from the generalised cross correlation (GCC) vectors and use a multilayer perceptron neural network to learn the nonlinear mapping from such features to the DOA. One advantage of the learning based method is that as more and more training data becomes available, the DOA estimation will\u00a0\u2026", "We present a method of modifying the structure of radial basis function (RBF) network to work with nonstationary series that exhibit homogeneous nonstationary behavior. In the original RBF network, the hidden node's function is to sense the trajectory of the time series and to respond when there is a strong correlation between the input pattern and the hidden node's center. This type of response, however, is highly sensitive to changes in the level and trend of the time series. To counter these effects, the hidden node's function is modified to one which detects and reacts to the gradient of the series. We call this new network the gradient RBF (GRBF) model. Single and multistep predictive performance for the Mackey-Glass chaotic time series were evaluated using the classical RBF and GRBF models. The simulation results for the series without and with a tine-varying mean confirm the superior performance of the\u00a0\u2026", "Voice conversion - the methodology of automatically converting one's utterances to sound as if spoken by another speaker - presents a threat for applications relying on speaker verification. We study vulnerability of text-independent speaker verification systems against voice conversion attacks using telephone speech. We implemented a voice conversion systems with two types of features and nonparallel frame alignment methods and five speaker verification systems ranging from simple Gaussian mixture models (GMMs) to state-of-the-art joint factor analysis (JFA) recognizer. Experiments on a subset of NIST 2006 SRE corpus indicate that the JFA method is most resilient against conversion attacks. But even it experiences more than 5-fold increase in the false acceptance rate from 3.24 % to 17.33 %.", "Voice conversion techniques present a threat to speaker verification systems. To enhance the security of speaker verification systems, We study how to automatically distinguish natural speech and synthetic/converted speech. Motivated by the research on phase spectrum in speech perception, in this study, we propose to use features derived from phase spectrum to detect converted speech. The features are tested under three different training situations of the converted speech detector: a) only Gaussian mixture model (GMM) based converted speech data are available; b) only unit-selection based converted speech data are available; c) no converted speech data are available for training converted speech model. Experiments conducted on the National Institute of Standards and Technology (NIST) 2006 speaker recognition evaluation (SRE) corpus show that the performance of the features derived from phase\u00a0\u2026", "We propose a nonparametric framework for voice conversion, that is, exemplar-based sparse representation with residual compensation. In this framework, a spectrogram is reconstructed as a weighted linear combination of speech segments, called exemplars, which span multiple consecutive frames. The linear combination weights are constrained to be sparse to avoid over-smoothing, and high-resolution spectra are employed in the exemplars directly without dimensionality reduction to maintain spectral details. In addition, a spectral compression factor and a residual compensation technique are included in the framework to enhance the conversion performances. We conducted experiments on the VOICES database to compare the proposed method with a large set of state-of-the-art baseline methods, including the maximum likelihood Gaussian mixture model (ML-GMM) with dynamic feature constraint and the\u00a0\u2026", "Replay, which is to playback a pre-recorded speech sample, presents a genuine risk to automatic speaker verification technology. In this study, we evaluate the vulnerability of text-dependent speaker verification systems under the replay attack using a standard benchmarking database, and also propose an anti-spoofing technique to safeguard the speaker verification systems. The key idea of the spoofing detection technique is to decide whether the presented sample is matched to any previous stored speech samples based a similarity score. The experiments conducted on the RSR2015 database showed that the equal error rate (EER) and false acceptance rate (FAR) increased from both 2.92 % to 25.56 % and 78.36 % respectively as a result of the replay attack. It confirmed the vulnerability of speaker verification to replay attacks. On the other hand, our proposed spoofing countermeasure was able to reduce the\u00a0\u2026", "This paper presents first steps toward a large vocabulary continuous speech recognition system (LVCSR) for conversational Mandarin-English code-switching (CS) speech. We applied state-of-the-art techniques such as speaker adaptive and discriminative training to build the first baseline system on the SEAME corpus [1] (South East Asia Mandarin-English). For acoustic modeling, we applied different phone merging approaches based on the International Phonetic Alphabet (IPA) and Bhattacharyya distance in combination with discriminative training to improve accuracy. On language model level, we investigated statistical machine translation (SMT) - based text generation approaches for building code-switching language models. Furthermore, we integrated the provided information from a language identification system (LID) into the decoding process by using a multi-stream approach. Our best 2-pass system\u00a0\u2026", "Recent improvement in text-to-speech (TTS) and voice conversion (VC) techniques presents a threat to automatic speaker verification (ASV) systems. An attacker can use the TTS or VC systems to impersonate a target speaker\u2019s voice. To overcome such a challenge, we study the detection of such synthetic speech (called spoofing speech) in this paper. We propose to use high dimensional magnitude and phase based features and long term temporal information for the task. In total, 2 types of magnitude based features and 5 types of phase based features are used. For each feature type, we build a component system using a multilayer perceptron to predict the posterior probabilities of the input features extracted from spoofing speech. The probabilities of all component systems are averaged to produce the score for final decision. When tested on the ASVspoof 2015 benchmarking task, an equal error rate (EER) of 0.29% is obtained for known spoofing types, which demonstrates the highly effectiveness of the 7 features used. For unknown spoofing types, the EER is much higher at 5.23%, suggesting that future research should be focused on improving the generalization of the techniques.", "The i-vector approach to speaker recognition has achieved good performance when the domain of the evaluation dataset is similar to that of the training dataset. However, in realworld applications, there is always a mismatch between the training and evaluation datasets, that leads to performance degradation. To address this problem, this paper proposes to learn the domain-invariant and speaker-discriminative speech representations via domain adversarial training. Specifically, with domain adversarial training method, we use a gradient reversal layer to remove the domain variation and project the different domain data into the same subspace. Moreover, we compare the proposed method with other state-of-the-art unsupervised domain adaptation techniques for i-vector approach to speaker recognition (e.g. autoencoder based domain adaptation, inter dataset variability compensation, dataset-invariant\u00a0\u2026", "Voice conversion and speaker adaptation techniques present a threat to current state-of-the-art speaker verification systems. To prevent such spoofing attack and enhance the security of speaker verification systems, the development of anti-spoofing techniques to distinguish synthetic and human speech is necessary. In this study, we continue the quest to discriminate synthetic and human speech. Motivated by the facts that current analysis-synthesis techniques operate on frame level and make the frame-by-frame independence assumption, we proposed to adopt magnitude/phase modulation features to detect synthetic speech from human speech. Modulation features derived from magnitude/phase spectrum carry long-term temporal information of speech, and may be able to detect temporal artifacts caused by the frame-by-frame processing in the synthesis of speech signal. From our synthetic speech detection\u00a0\u2026", "Voice conversion technique, which modifies one speaker's (source) voice to sound like another speaker (target), presents a threat to automatic speaker verification. In this paper, we first present new results of evaluating the vulnerability of current state-of-the-art speaker verification systems: Gaussian mixture model with joint factor analysis (GMM-JFA) and probabilistic linear discriminant analysis (PLDA) systems, against spoofing attacks. The spoofing attacks are simulated by two voice conversion techniques: Gaussian mixture model based conversion and unit selection based conversion. To reduce false acceptance rate caused by spoofing attack, we propose a general anti-spoofing attack framework for the speaker verification systems, where a converted speech detector is adopted as a post-processing module for the speaker verification system's acceptance decision. The detector decides whether the accepted\u00a0\u2026", "Sports video highlight detection is a popular topic. A multi-layer sport event detection framework is described. In the mid-level of this framework, visual and audio keywords are created from low-level features and the original video is converted into a keyword sequence. In the high-level, the temporal pattern of keyword sequences is analyzed by an HMM classifier. The creation of visual and audio keywords can help to bridge the gap between low-level features and high-level semantics. The use of the HMM classifier can automatically find the temporal change character of the event instead of rule based heuristic modeling to map certain keyword sequences into events. Experiments using our model on soccer games produced some promising results.", "This paper describes a Twitter sentiment analysis system that classifies a tweet as positive or negative based on its overall tweet-level polarity. Supervised learning classifiers often misclassify tweets containing conjunctions such as \u201cbut\u201d and conditionals such as \u201cif\u201d, due to their special linguistic characteristics. These classifiers also assign a decision score very close to the decision boundary for a large number tweets, which suggests that they are simply unsure instead of being completely wrong about these tweets. To counter these two challenges, this paper proposes a system that enhances supervised learning for polarity classification by leveraging on linguistic rules and sentic computing resources. The proposed method is evaluated on two publicly available Twitter corpora to illustrate its effectiveness.", "Speaker extraction aims to mimic humans' selective auditory attention by extracting a target speaker's voice from a multi-talker environment. It is common to perform the extraction in frequency-domain, and reconstruct the time-domain signal from the extracted magnitude and estimated phase spectra. However, such an approach is adversely affected by the inherent difficulty of phase estimation. Inspired by Conv-TasNet, we propose a time-domain speaker extraction network (SpEx) that converts the mixture speech into multi-scale embedding coefficients instead of decomposing the speech signal into magnitude and phase spectra. In this way, we avoid phase estimation. The SpEx network consists of four network components, namely speaker encoder, speech encoder, speaker extractor, and speech decoder. Specifically, the speech encoder converts the mixture speech into multi-scale embedding coefficients, the\u00a0\u2026", "In Singapore and Malaysia, people often speak a mixture of Mandarin and English within a single sentence. We call such sentences intra-sentential code-switch sentences. In this paper, we report on the development of a Mandarin-English codeswitching spontaneous speech corpus: SEAME. The corpus is developed as part of a multilingual speech recognition project and will be used to examine how Mandarin-English codeswitch speech occurs in the spoken language in South-East Asia. Additionally, it can provide insights into the development of large vocabulary continuous speech recognition (LVCSR) for code-switching speech. The corpus collected consists of intra-sentential code-switching utterances that are recorded under both interview and conversational settings. This paper describes the corpus design and the analysis of collected corpus.", "The ability to automatically recognize a wide range of sound events in real-world conditions is an important part of applications such as acoustic surveillance and machine hearing. Our approach takes inspiration from both audio and image processing fields, and is based on transforming the sound into a two-dimensional representation, then extracting an image feature for classification. This provided the motivation for our previous work on the spectrogram image feature (SIF). In this paper, we propose a novel method to improve the sound event classification performance in severe mismatched noise conditions. This is based on the subband power distribution (SPD) image - a novel two-dimensional representation that characterizes the spectral power distribution over time in each frequency subband. Here, the high-powered reliable elements of the spectrogram are transformed to a localized region of the SPD, hence\u00a0\u2026", "Acoustic beamforming has played a key role in the robust automatic speech recognition (ASR) applications. Accurate estimates of the speech and noise spatial covariance matrices (SCM) are crucial for successfully applying the minimum variance distortionless response (MVDR) beamforming. Reliable estimation of time-frequency (TF) masks can improve the estimation of the SCMs and significantly improve the performance of the MVDR beamforming in ASR tasks. In this paper, we focus on the TF mask estimation using recurrent neural networks (RNN). Specifically, our methods include training the RNN to estimate the speech and noise masks independently, training the RNN to minimize the ASR cost function directly, and performing multiple passes to iteratively improve the mask estimation. The proposed methods are evaluated individually and overally on the CHiME-4 challenge. The results show that the\u00a0\u2026", "The fundamental issue of the automatic language identification is to explore the effective discriminative cues for languages. This paper studies the fusion of five features at different level of abstraction for language identification, including spectrum, duration, pitch, n-gram phonotactic, and bag-of- sounds features. We build a system and report test results on NIST 1996 and 2003 LRE datasets. The system is also built to participate in NIST 2005 LRE. The experiment results show that different levels of information provide complementary language cues. The prosodic features are more effective for shorter utterances while the phonotactic features work better for longer utterances. For the task of 12 languages, the system with fusion of five features achieved 2.38% EER for 30-sec speech segments on NIST 1996 dataset.", "Human motion detection is a fundamental task for many computer vision tasks. The most popular method for motion detection is background subtraction where a background model needs to be maintained. In this paper an entropy based method for human motion detection is described which makes no use of background model. The difference image between consecutive images are calculated and at each particular pixel, and a spatio-temporal histogram is generated by accumulating pixels in difference image. This histogram is then normalized to calculate entropy and the magnitude of entropy is used to denote the significance of motion. Experiment results demonstrate that our method can detect motion object effectively and reliably.", "In this paper, we address the challenging task of simultaneous recognition of overlapping sound events from single channel audio. Conventional frame-based methods are not well suited to the problem, as each time frame contains a mixture of information from multiple sources. Missing feature masks are able to improve the recognition in such cases, but are limited by the accuracy of the mask, which is a non-trivial problem. In this paper, we propose an approach based on Local Spectrogram Features (LSFs) which represent local spectral information that is extracted from the two-dimensional region surrounding \u201ckeypoints\u201d detected in the spectrogram. The keypoints are designed to locate the sparse, discriminative peaks in the spectrogram, such that we can model sound events through a set of representative LSF clusters and their occurrences in the spectrogram. To recognise overlapping sound events, we use a\u00a0\u2026", "Speaker extraction aims to extract the target speech signal from a multi-talker environment given a target speaker's reference speech. We recently proposed a time-domain solution, SpEx, that avoids the phase estimation in frequency-domain approaches. Unfortunately, SpEx is not fully a time-domain solution since it performs time-domain speech encoding for speaker extraction, while taking frequency-domain speaker embedding as the reference. The size of the analysis window for time-domain and the size for frequency-domain input are also different. Such mismatch has an adverse effect on the system performance. To eliminate such mismatch, we propose a complete time-domain speaker extraction solution, that is called SpEx+. Specifically, we tie the weights of two identical speech encoder networks, one for the encoder-extractor-decoder pipeline, another as part of the speaker encoder. Experiments show that the SpEx+ achieves 0.8dB and 2.1dB SDR improvement over the state-of-the-art SpEx baseline, under different and same gender conditions on WSJ0-2mix-extr database respectively.", "In this paper, we study a novel technique that normalizes the modulation spectra of speech signals for robust speech recognition. The modulation spectra of a speech signal are the power spectral density (PSD) functions of the feature trajectories generated from the signal, hence they describe the temporal structure of the features. The modulation spectra are distorted when the speech signal is corrupted by noise. We propose the temporal structure normalization (TSN) filter to reduce the noise effects by normalizing the modulation spectra to reference spectra. The TSN filter is different from other feature normalization methods such as the histogram equalization (HEQ) that only normalize the probability distributions of the speech features. Our previous work showed promising results of TSN on a small vocabulary Aurora-2 task. In this paper, we conduct an inquiry into the theoretical and practical issues of the TSN filter\u00a0\u2026", "The conventional statistical-based transformation functions for voice conversion have been shown to suffer over-smoothing and over-fitting problems. The over-smoothing problem arises because of the statistical average during estimating the model parameters for the transformation function. In addition, the large number of parameters in the statistical model cannot be well estimated from the limited parallel training data, which will result in the over-fitting problem. In this work, we investigate a robust transformation function for voice conversion using conditional restricted Boltzmann machine. Conditional restricted Boltzmann machine, which performs linear and non-linear transformations simultaneously, is proposed to learn the relationship between source and target speech. CMU ARCTIC corpus is adopted in the experimental validations. The number of parallel training utterances is varied from 2 to 40. For these\u00a0\u2026", "Decision feedback in a decision feedback equaliser (DFE) performs a space translation that maps the DFE onto a transversal equaliser in the translated observation space. Properties of DFEs can therefore be analysed more easily by exploiting this geometric translation property. This approach is used to analyse the conventional DFE that employs a linear combination of the channel observations and the past decisions (the linear-combiner DFE). It is demonstrated that the usual minimum mean square error (MMSE) solution does not achieve the full performance potential of the linear-combiner DFE structure. A bit error rate (BER) expression for the linear-combiner DFE with binary signalling is obtained, and a method is proposed to optimally set the coefficients of the linear-combiner DFE. The performance of this minimum-BER (MBER) linear-combiner DFE is much closer to that of the optimal Bayesian DFE\u00a0\u2026", "While most current approaches for sports video analysis are based on broadcast video, in this paper, we present a novel approach for highlight detection and automatic replay generation for soccer videos taken by the main camera. This research is important as current soccer highlight detection and replay generation from a live game is a labor-intensive process. A robust multi-level, multi-model event detection framework is proposed to detect the event and event boundaries from the video taken by the main camera. This framework explores the possible analysis cues, using a mid-level representation to bridge the gap between low-level features and high-level events. The event detection results and mid-level representation are used to generate replays which are automatically inserted into the video. Experimental results are promising and found to be comparable with those generated by broadcast professionals.", "Improving classification accuracy is a key issue to advancing brain computer interface (BCI) research from laboratory to real world applications. This work presents a high accuracy EEG signal classification method using single trial EEC signal to detect left and right finger movement. We apply an optimal temporal filter to remove irrelevant signal and subsequently extract key features from spatial patterns of EEG signal to perform classification. Specifically, the proposed method transforms the original EEG signal into a spatial pattern and applies the RBF feature selection method to generate robust feature. Classification is performed by the SVM and our experimental result shows that the classification accuracy of the proposed method reaches 90% as compared to the current reported best accuracy of 84%.", "Utterance level permutation invariant training (uPIT) technique is a state-of-the-art deep learning architecture for speaker independent multi-talker separation. uPIT solves the label ambiguity problem by minimizing the mean square error (MSE) over all permutations between outputs and targets. However, uPIT may be sub-optimal at segmental level because the optimization is not calculated over the individual frames. In this paper, we propose a constrained uPIT (cuPIT) to solve this problem by computing a weighted MSE loss using dynamic information (i.e., delta and acceleration). The weighted loss ensures the temporal continuity of output frames with the same speaker. Inspired by the heuristics (i.e., vocal tract continuity) in computational auditory scene analysis, we then extend the model by adding a Grid LSTM layer, that we name it as cuPIT-Grid LSTM, to automatically learn both temporal and spectral patterns\u00a0\u2026", "The paper proposes a concurrent constant modulus algorithm (CMA) and soft decision-directed (SDD) scheme for low-complexity blind equalization of high-order quadrature amplitude modulation channels. Simulation using a fractionally spaced equalization setting is used to compare the proposed scheme with the recently introduced state-of-art concurrent CMA and decision-directed (DD) scheme. The proposed CMA+SDD blind equalizer is shown to have simpler computational complexity per weight update, faster convergence speed, and slightly improved steady-state equalization performance, compared with the CMA+DD blind equalizer.", "Building a generic content-based sports video analysis system remains a challenging problem because of the diversity in sports rules and game features which makes it difficult to discover generic low-level features or high-level modeling algorithms. One possible alternative is to first classify the sports genre and then apply specific sports domain knowledge to perform analysis. In this paper we describe a multi-level framework to automatically recognize the genre of the sports video. The system consists of a Pseudo-2D-HMM classifier using low-level visual/audio features to evaluate the video clips. The experimental results are satisfactory and extension of the framework to a generic sports video analysis system is being implemented.", "This paper introduces the South East Asia Mandarin\u2013English corpus, a 63-h spontaneous Mandarin\u2013English code-switching transcribed speech corpus suitable for LVCSR and language change detection/identification research. The corpus is recorded under unscripted interview and conversational settings from 157 Singaporean and Malaysian speakers who spoke a mixture of Mandarin and English within a single sentence. About 82\u00a0% of the transcribed utterances are intra-sentential code-switching speech and the corpus will be release by LDC in 2015. This paper presents an analysis of the code-switching statistics of the corpus, such as the duration of monolingual segments and the frequency of language turns in code-switch utterances. We also summarize the development effort, details such as the processing time for transcription, validation and language boundary labelling. Lastly, we present textual\u00a0\u2026", "The paper derives a minimum bit error rate (BER) solution for the decision feedback equaliser (DFE) that employs a linear combination of the channel observations and the past decisions. We show that by using a geometric translation the DFE is reduced to a simpler linear equaliser. A BER expression for the linear equaliser is obtained under the assumption of linearly separable decision regions, and a method is proposed to optimally set the linear-combiner coefficients of the DFE. This minimum BER solution is superior to the usual minimum mean square error (MSE) solution. The channel is modelled as a finite impulse response (FIR) filter.", "Relation Extraction (RE) is to predict the relation type of two entities that are mentioned in a piece of text, eg, a sentence or a dialogue. When the given text is long, it is challenging to identify indicative words for the relation prediction. Recent advances on RE task are from BERT-based sequence modeling and graph-based modeling of relationships among the tokens in the sequence. In this paper, we propose to construct a latent multi-view graph to capture various possible relationships among tokens. We then refine this graph to select important words for relation prediction. Finally, the representation of the refined graph and the BERT-based sequence representation are concatenated for relation extraction. Specifically, in our proposed GDPNet (Gaussian Dynamic Time Warping Pooling Net), we utilize Gaussian Graph Generator (GGG) to generate edges of the multi-view graph. The graph is then refined by Dynamic Time Warping Pooling (DTWPool). On DialogRE and TACRED, we show that GDPNet achieves the best performance on dialogue-level RE, and comparable performance with the state-of-the-arts on sentence-level RE. Our code is available at https://github. com/XueFuzhao/GDPNet.", "In the traditional voice conversion, converted speech is generated using statistical parametric models (for example Gaussian mixture model) whose parameters are estimated from parallel training utterances. A well-known problem of the statistical parametric methods is that statistical average in parameter estimation results in the over-smoothing of the speech parameter trajectories, and thus leads to low conversion quality. Inspired by recent success of so-called exemplar-based methods in robust speech recognition, we propose a voice conversion system based on non-negative spectrogram deconvolution with similar ideas. Exemplars, which are able to capture temporal context, are employed to generate converted speech spectrogram convolutely. The exemplar-based approach is seen as a data-driven, non-parametric approach as an alternative to the traditional parametric approaches to voice conversion. Experiments on VOICES database indicate that the proposed method outperforms the conventional joint density Gaussian mixture model by a wide margin in terms of both objective and subjective evaluations.", "This paper presents the development of the speech, text and pronunciation dictionary resources required to build a large vocabulary speech recognizer for the Malay language. This project is a collaboration project among three universities: USM, MMU from Malaysia and NTU from Singapore. The Malay speech corpus consists of read speech (speaker independent/ dependent and accent independent/ dependent) and broadcast news. To date, 90 speakers have been recorded which is equal to a total of nearly 70 hours of read speech, and 10 hours of broadcast news from local TV stations in Malaysia was transcribed. The text corpus consists of 700Mbytes of data extracted from Malaysia's local news Web pages from 1998-2008 and a rule based G2P tool is develop to generate the pronunciation dictionary.", "This paper describes the integration of language identification (LID) into a multilingual automatic speech recognition (ASR) system for spoken conversations containing code-switches between Mandarin and English. We apply a multistream approach to combine at frame level the acoustic model score and the language information, where the latter is provided by an LID component. Furthermore, we advance this multistream approach by a new method called \u201cLanguage Lookahead\u201d, in which the language information of subsequent frames is used to improve accuracy. Both methods are evaluated using a set of controlled LID results with varying frame accuracies. Our results show that both approaches improve the ASR performance by at least 4% relative if the LID achieves a minimum frame accuracy of 85%.", "Human motion detection is a fundamental part for many computer vision tasks and various methods have been proposed. Background subtraction is a very popular method where the classification of pixels into motion pixels and background pixels is based on thresholding the difference image between a background image and a current image. The choice of threshold is crucial and many methods have been employed, from a preset threshold for the whole image to adaptive thresholds for each pixel. In this paper, a motion detection method by dynamically thresholding the difference image is proposed. We keep a static background image that is updated periodically, and compare the incoming frame with it to obtain the absolute difference image. If through a preliminary judgement that there are moving objects in the scene, Otsu's threshoding method is used to find the threshold to binarize the difference image\u00a0\u2026", "Spoofing detection, which discriminates the spoofed speech from the natural speech, has gained much attention recently. Low-dimensional features that are used in speaker recognition/verification are also used in spoofing detection. Unfortunately, they don't capture sufficient information required for spoofing detection. In this work, we investigate the use of high-dimensional features for spoofing detection, that maybe more sensitive to the artifacts in the spoofed speech. Six types of high-dimensional feature are employed. For each kind of feature, four different representations are extracted, i.e. the original high-dimensional feature, corresponding low-dimensional feature, the low- and the high-frequency regions of the original high-dimensional feature. Dynamic features are also calculated to assess the effectiveness of the temporal information to detect the artifacts across frames. A neural network-based classifier is\u00a0\u2026", "Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker\u2019s training data for synthesizing the converted speech. Both objective and subjective\u00a0\u2026", "In this paper, we present an application of student\u2019s t-test to measure the similarity between two speaker models. The measure is evaluated by comparing with other distance metrics: the Generalized Likelihood Ratio, the Cross Likelihood Ratio and the Normalized Cross Likelihood Ratio in speaker detection task. We also propose an objective criterion for speaker clustering. The criterion deduces the number of speakers automatically by maximizing the separation between intra-speaker distances and inter-speaker distances. It requires no development data and works well with various distance metrics. We then report the performance of our proposed similarity distance measure and objective criterion in speaker diarization task. The system produces competitive results: low speaker diarization error rate and high accuracy in detecting number of speakers.", "Voice conversion techniques typically require source-target parallel speech data for model training. Such parallel data may not be available always in practice. This paper presents a nonparallel data approach, that we call average modeling approach. The proposed approach makes use of a multi-speaker average model that maps speaker-independent linguistic features to speaker dependent acoustic features. In particular, we present two practical implementations, 1) to adapt the average model towards target speaker with a small amount of target data, 2) to present speaker identity as an additional input to the average model to generate target speech. As the linguistic feature and the acoustic feature can be extracted from the same utterance, the proposed approach doesn\u2019t require parallel data in either average model training or adaptation. We report the experiments on the voice conversion challenge 2018\u00a0\u2026", "Replay scene detection is a useful technique for content based sports video analysis. Most current researchers try to find suitable visual and/or compressed domain features to detect the replay scene from a broadcast video. We present a novel approach using context information from the concurrence of replay and other types of shots to detect the replay scenes. We first perform a shot classification and then a scene transition structure analysis on the generated shot label sequence to extract the replay scene. The proposed model is computationally fast and some promising results were obtained.", "In this paper, we propose a novel automatic approach for personalized music sports video generation. Two research challenges, semantic sports video content selection and automatic video composition, are addressed. For the first challenge, we propose to use multi-modal (audio, video and text) feature analysis and alignment to detect the semantic of events in sports video. For the second challenge, we propose video-centric and music-centric music video composition schemes to automatically generate personalized music sports video based on user's preference. The experimental results and user evaluations are promising and show that our system's generated music sports video is comparable to manually generated ones. The proposed approach greatly facilitates the automatic music sports video generation for both professionals and amateurs.", "The SpeakerBeam-FE (SBF) method is proposed for speaker extraction. It attempts to overcome the problem of unknown number of speakers in an audio recording during source separation. The mask approximation loss of SBF is sub-optimal, which doesn't calculate direct signal reconstruction error and consider the speech context. To address these problems, this paper proposes a magnitude and temporal spectrum approximation loss to estimate a phase sensitive mask for the target speaker with the speaker characteristics. Moreover, this paper explores a concatenation framework instead of the context adaptive deep neural network in the SBF method to encode a speaker embedding into the mask estimation network. Experimental results under open evaluation condition show that the proposed method achieves 70.4% and 17.7% relative improvement over the SBF baseline on signal-to-distortion ratio (SDR) and\u00a0\u2026", "Speech Emotion Recognition (SER) aims to help the machine to understand human\u2019s subjective emotion from only audio in-formation. However, extracting and utilizing comprehensive in-depth audio information is still a challenging task. In this paper, we propose an end-to-end speech emotion recognition system using multi-level acoustic information with a newly designed co-attention module. We firstly extract multi-level acoustic information, including MFCC, spectrogram, and the embedded high-level acoustic information with CNN, BiL-STM and wav2vec2, respectively. Then these extracted features are treated as multimodal inputs and fused by the pro-posed co-attention mechanism. Experiments are carried on the IEMOCAP dataset, and our model achieves competitive performance with two different speaker-independent cross-validation strategies. Our code is available on GitHub.", "The voice conversion's task is to modify a source speaker's voice to sound like that of a target speaker. A conversion method is considered successful when the produced speech sounds natural and similar to the target speaker. This paper presents a new voice conversion framework in which we combine frequency warping and exemplar-based method for voice conversion. Our method maintains high-resolution details during conversion by directly applying frequency warping on the high-resolution spectrum to represent the target. The warping function is generated by a sparse interpolation from a dictionary of exemplar warping functions. As the generated warping function is dependent only on a very small set of exemplars, we do away with the statistical averaging effects inherited from Gaussian mixture models. To compensate for the conversion error, we also apply residual exemplars into the conversion process\u00a0\u2026", "This paper describes our speech enhancement and recognition systems developed for the Reverberation Challenge 2014. To enhance the noisy and reverberant speech for human listening, besides using conventional methods such as delay and sum beamformer and late reverberation reduction by spectral subtraction, we also studied a novel learning-based speech enhancement. Specifically, we train deep neural networks (DNN) to map reverberant spectrogram to the corresponding clean spectrogram by using parallel data of clean and reverberant speech. Results show that the trained DNN is able to reduce reverberation significantly for unseen test data. For the speech recognition task, when parallel data is available, we train a DNN to map reverberant features to clean features, following the same spirit as the DNN-based speech enhancement. Results show that the DNN-based feature compensation improves speech recognition performance even when a DNN acoustic model is already used, showing the benefit of explicitly cleansing the features. When parallel data is not available in the clean condition training scheme, we focus on reducing the training-test mismatch by using our proposed cross transform feature adaptation that uses both temporal and spectral information. The cross transform works complementarily with traditional model adaptation.", "In this paper, we propose a novel automatic approach for personalized music sports video generation. Two research challenges are addressed, specifically the semantic sports video content extraction and the automatic music video composition. For the first challenge, we propose to use multimodal (audio, video, and text) feature analysis and alignment to detect the semantics of events in broadcast sports video. For the second challenge, we introduce the video-centric and music-centric music video composition schemes and proposed a dynamic-programming based algorithm to perform fully or semi-automatic generation of personalized music sports video. The experimental results and user evaluations are promising and show that our systems generated music sports video is comparable to professionally generated ones. Our proposed system greatly facilitates the music sports video editing task for both\u00a0\u2026", "In this paper, we present our overall efforts to improve the performance of a code-switching speech recognition system using semi-supervised training methods from lexicon learning to acoustic modeling, on the South East Asian Mandarin-English (SEAME) data. We first investigate semi-supervised lexicon learning approach to adapt the canonical lexicon, which is meant to alleviate the heavily accented pronunciation issue within the code-switching conversation of the local area. As a result, the learned lexicon yields improved performance. Furthermore, we attempt to use semi-supervised training to deal with those transcriptions that are highly mismatched between human transcribers and ASR system. Specifically, we conduct semi-supervised training assuming those poorly transcribed data as unsupervised data. We found the semi-supervised acoustic modeling can lead to improved results. Finally, to make up for the limitation of the conventional n-gram language models due to data sparsity issue, we perform lattice rescoring using neural network language models, and significant WER reduction is obtained.", "Automatic speech recognition (ASR) is currently used in many assistive technologies, such as helping individuals with speech impairment in their communication ability. One challenge in ASR for speech-impaired individuals is the difficulty in obtaining a good speech database of impaired speakers for building an effective speech acoustic model. Because there are very few existing databases of impaired speech, which are also limited in size, the obvious solution to build a speech acoustic model of impaired speech is by employing adaptation techniques. However, issues that have not been addressed in existing studies in the area of adaptation for speech impairment are as follows: (1) identifying the most effective adaptation technique for impaired speech; and (2) the use of suitable source models to build an effective impaired-speech acoustic model. This research investigates the above-mentioned two issues on dysarthria, a type of speech impairment affecting millions of people. We applied both unimpaired and impaired speech as the source model with well-known adaptation techniques like the maximum likelihood linear regression (MLLR) and the constrained-MLLR(C-MLLR). The recognition accuracy of each impaired speech acoustic model is measured in terms of word error rate (WER), with further assessments, including phoneme insertion, substitution and deletion rates. Unimpaired speech when combined with limited high-quality speech-impaired data improves performance of ASR systems in recognising severely impaired dysarthric speech. The C-MLLR adaptation technique was also found to be better than MLLR in recognising\u00a0\u2026", "SEAME (South East Asia Mandarin-English) is a 30 hours spontaneous Mandarin-English code-switching speech corpus recorded from Singapore and Malaysia speakers. In this paper, we report a series of analyses on the recording, processing time and voice activity rate (VAR) of the speech recording, transcription, validation and language boundaries labeling processes. In addition, the duration of the monolingual segment in the code-switching utterance and the analysis of the speakers\u201f behavior in language switching during conversation are also described. The results of the analysis show that 80% and 72% monolingual segments of English and Mandarin in the code-switching utterance are shorter than one second. In over 80% of the cases, speakers directly switch language without any short pause and discourse particle between two adjacent different languages.", "Speaker extraction is to extract a target speaker's voice from multi-talker speech. It simulates humans' cocktail party effect or the selective listening ability. The prior work mostly performs speaker extraction in frequency domain, then reconstructs the signal with some phase approximation. The inaccuracy of phase estimation is inherent to the frequency domain processing, that affects the quality of signal reconstruction. In this paper, we propose a time-domain speaker extraction network (TseNet) that doesn't decompose the speech signal into magnitude and phase spectrums, therefore, doesn't require phase estimation. The TseNet consists of a stack of dilated depthwise separable convolutional networks, that capture the long-range dependency of the speech signal with a manageable number of parameters. It is also conditioned on a reference voice from the target speaker, that is characterized by speaker i-vector, to\u00a0\u2026", "This paper investigates deep neural networks (DNN) based on nonlinear feature mapping and statistical linear feature adaptation approaches for reducing reverberation in speech signals. In the nonlinear feature mapping approach, DNN is trained from parallel clean/distorted speech corpus to map reverberant and noisy speech coefficients (such as log magnitude spectrum) to the underlying clean speech coefficients. The constraint imposed by dynamic features (i.e., the time derivatives of the speech coefficients) are used to enhance the smoothness of predicted coefficient trajectories in two ways. One is to obtain the enhanced speech coefficients with a least square estimation from the coefficients and dynamic features predicted by DNN. The other is to incorporate the constraint of dynamic features directly into the DNN training process using a sequential cost function. In the linear feature adaptation approach, a sparse linear transform, called cross transform, is used to transform multiple frames of speech coefficients to a new feature space. The transform is estimated to maximize the likelihood of the transformed coefficients given a model of clean speech coefficients. Unlike the DNN approach, no parallel corpus is used and no assumption on distortion types is made. The two approaches are evaluated on the REVERB Challenge 2014 tasks. Both speech enhancement and automatic speech recognition (ASR) results show that the DNN-based mappings significantly reduce the reverberation in speech and improve both speech quality and ASR performance. For the speech enhancement task, the proposed dynamic feature constraint help to improve\u00a0\u2026", "In voice conversion, frame-level mean and variance normalization is typically used for fundamental frequency (F0) transformation, which is text-independent and requires no parallel training data. Some advanced methods transform pitch contours instead, but require either parallel training data or syllabic annotations. We propose a method which retains the simplicity and text-independence of the frame-level conversion while yielding high-quality conversion. We achieve these goals by (1) introducing a text-independent tri-frame alignment method,(2) including delta features of F0 into Gaussian mixture model (GMM) conversion and (3) reducing the well-known GMM oversmoothing effect by F0 histogram equalization. Our objective and subjective experiments on the CMU Arctic corpus indicate improvements over both the mean/variance normalization and the baseline GMM conversion.", "This paper presents a comparative study of three learning based beamforming methods that are specifically designed for robust speech recognition. The three methods are 1) neural network that predicts beamforming weights from generalized cross correlation (GCC) features; 2) neural network that predicts timefrequency (TF) mask which is used to estimate MVDR (minimum variance distortionless response) beamforming weights; 3) maximum likelihood estimation of beamforming weights to fit enhanced features to clean trained Gaussian mixture model. All three methods operate in frequency domain. They are evaluated on the CHiME-4 benchmarking speech recognition task and compared with traditional delay-and-sum and MVDR beamforming methods on the same speech recognition task. Discussions and future research directions are presented.", "This paper presents a deep neural network (DNN) approach to sentence boundary detection in broadcast news. We extract prosodic and lexical features at each inter-word position in the transcripts and learn a sequential classifier to label these positions as either boundary or non-boundary. This work is realized by a hybrid DNN-CRF (conditional random field) architecture. The DNN accepts prosodic feature inputs and non-linearly maps them into boundary/non-boundary posterior probability outputs. Subsequently, the posterior probabilities are combined with lexical features and the integrated features are modeled by a linear-chain CRF. The CRF finally labels the inter-word positions as boundary or non-boundary by Viterbi decoding. Experiments show that, as compared with the state-of-the-art DTCRF approach [1], the proposed DNN-CRF approach achieves 16.7% and 4.1% reduction in NIST boundary detection\u00a0\u2026", "This paper examines the process of language diarization, the process to perform language segmentation and recognition, in a code-switched speech. Towards this task, we have developed a 63 hours conversational code-switch corpus recorded from Singapore/Malaysia speakers. We show that code-switching can occur frequently and the average language interval may be as short as one second. As such, language diarization is challenging task. To process such short segments, we propose a language diarization system using long term context feature across several phone-based segments and the combination of acoustics and phonotactic information. We achieved a frame error rate of 14.7% for language diarization performance on a Mandarin-English code-switch corpus. To evaluate our system, we measured the language recognition performance on monolingual segments extracted from the code-switch\u00a0\u2026", "This study examines an automatic broadcast soccer video composition system. The research is important as the ability to automatically compose broadcast sports video will not only improve broadcast video generation efficiency, but also provides the possibility to customize sports video broadcasting. We present a novel approach to the two major issues required in the system\u2019s implementation, specifically the camera view selection/switching module and the automatic replay generation module. In our implementation, we use multi-modal framework to perform video content analysis, event and event boundary detection from the raw unedited main/sub-camera captures. This framework explores the possible cues using mid-level representations to bridge the gap between low-level features and high-level semantics. The video content analysis results are utilized for camera view selection/switching in the\u00a0\u2026", "We propose strategies for a state-of-the-art keyword search (KWS) system developed by the SINGA team in the context of the 2014 NIST Open Keyword Search Evaluation (OpenKWS14) using conversational Tamil provided by the IARPA Babel program. To tackle low-resource challenges and the rich morphological nature of Tamil, we present highlights of our current KWS system, including: (1) Submodular optimization data selection to maximize acoustic diversity through Gaussian component indexed N-grams; (2) Keywordaware language modeling; (3) Subword modeling of morphemes and homophones.", "This paper describes the performance of the I4U speaker recognition system in the NIST 2008 Speaker Recognition Evaluation. The system consists of seven subsystems, each with different cepstral features and classifiers. We describe the I4U Primary system and report on its core test results as they were submitted, which were among the best-performing submissions. The I4U effort was led by the Institute for Infocomm Research, Singapore (IIR), with contributions from the University of Science and Technology of China (USTC), the University of New South Wales, Australia (UNSW), Nanyang Technological University, Singapore (NTU) and Carnegie Mellon University, USA (CMU).", "Sparse Non-negative Matrix Factorization (SNMF) and Deep Neural Networks (DNN) have emerged individually as two efficient machine learning techniques for single-channel speech enhancement. Nevertheless, there are only few works investigating the combination of SNMF and DNN for speech enhancement and robust Automatic Speech Recognition (ASR). In this paper, we present a novel combination of speech enhancement components based-on SNMF and DNN into a full-stack system. We refine the cost function of the DNN to back-propagate the reconstruction error of the enhanced speech. Our proposal is compared with several state-of-the-art speech enhancement systems. Evaluations are conducted on the data of CHiME-3 challenge which consists of real noisy speech recordings captured under challenging noisy conditions. Our system yields significant improvements for both objective quality speech\u00a0\u2026", "This paper presents a strategy to optimize the phonotactic front-end for spoken language recognition. This is achieved by selecting a subset of phones from an existing phone recognizer's phone inventory such that only the phones that best discriminate each of the target languages are selected. Each such phone subset will be used to construct a target-oriented phone tokenizer (TOPT). In this study, we examine different approaches to construct such phone tokenizers for the front-end of a parallel phone recognizers followed  by  vector space modeling (PPR-VSM) system. We show that the target-oriented phone tokenizers derived from language-specific phone recognizers are more effective than the original parallel phone recognizers. Our experimental results also show that the target-oriented phone tokenizers derived from universal phone recognizers achieve better performance than those derived from language\u00a0\u2026", "Voice conversion, a technique to change one\u2019s voice to sound like that of another, poses a threat to even high performance speaker verification system. Vulnerability of text-independent speaker verification systems under spoofing attack, using statistical voice conversion technique, was evaluated and confirmed in our previous work. In this paper, we further extend the study to text-dependent speaker verification systems. In particular, we compare both joint density Gaussian mixture model (JD-GMM) and unit-selection (US) spoofing methods and, for the first time, the performances of text-independent and text-dependent speaker verification systems in a single study. We conduct the experiments using RSR2015 database which is recorded using multiple mobile devices. The experimental results indicate that text-dependent speaker verification system tolerates spoofing attacks better than the text-independent\u00a0\u2026", "# histogram bins a. Generate the 36 MFCC features (12 MFCC plus their first and second order derivatives) 30ms window with 20ms overlap b. Select 10% highest energy with relative high zero cross rates frames as the initial speech guessing. c. Select 20% lowest energy with relative low zero cross rates frames as the initial noise guessing d. Train initial speech model \u03bb S and Non-speech model \u03bb NS via EM, the speech and noise model sizes are set to be 16 and 4, respectively. e. Frame-wise maximum likelihood evaluation against \u03bb S and \u03bb NS f. Retrain speech model \u03bb S and non-speech model \u03bb NS via MAP. g. Compute the speech/non-speech frame ratio, if the percentage change of the ratio (compared to its previous ratio) is less than 1%, stop, otherwise go to step e.", "The orthogonal least squares (OLS) algorithm is an efficient implementation of the forward selection method for subset model selection. The ability to find good subset parameters with only a linearly increasing computational requirement makes this method attractive for practical implementations. We examine the computational complexity of the algorithm and present a preprocessing method for reducing the computational requirement.< >", "Frequency warping (FW) based voice conversion aims to modify the frequency axis of source spectra towards that of the target. In previous works, the optimal warping function was calculated by minimizing the spectral distance of converted and target spectra without considering the spectral shape. Nevertheless, speaker timbre and identity greatly depend on vocal tract peaks and valleys of spectrum. In this paper, we propose a method to define the warping function by maximizing the correlation between the converted and target spectra. Different from the conventional warping methods, the correlation-based optimization is not determined by the magnitude of the spectra. Instead, both spectral peaks and valleys are considered in the optimization process, which also improves the performance of amplitude scaling. Experiments were conducted on VOICES database, and the results show that after amplitude scaling\u00a0\u2026", "This paper presents a comparative study of bottle-neck feature (BNF) and Deep Neural Network (DNN) multilingual training for low-resource language speech recognition. Besides, we also compared the system performances after fine-tuning. The evaluation was conducted on the IARPA Babel data. The source languages are Cantonese, Pashto, Tagalog, and Turkish, while the target languages are Vietnamese and Tamil. As compared to the monolingual baseline systems, the BNF and DNN methods similarly achieved relative WER reductions of 3.0-5.1% on the Limited Language Pack (LLP) data and 6.1-8.6% on the Very LLP (VLLP) data. By fine-tuning, the BNF method further gained significant performance improvement, while the DNN method obtained marginal gains. Overall, we observed the DNN method performs worse on the smaller size data (VLLP), as well as on the noisier data (Tamil) of the target\u00a0\u2026", "This letter presents a new feature normalization technique to normalize the temporal structure of speech features. The temporal structure of the features is partially represented by its power spectral density (PSD). We observed that the PSD of the features varies with the corrupting noise and signal-to-noise ratio. To reduce the PSD variation due to noise, we propose to normalize the PSD of features to a reference function by filtering the features. Experimental results on the AURORA-2 task show that the proposed approach when combined with the mean and variance normalization improves the speech recognition accuracy significantly; the system achieves 69.11% relative error rate reduction over the baseline.", "Hadoop, based on the popular MapReduce framework, is an open\u2010source distributed computing framework that has been gaining much popularity and usage. It aims to allow programmers to focus on building applications that deals with processing large amount of data, without having to handle other issues when performing parallel computations. However, tuning the performance of Hadoop applications is not an easy task due to the level of abstraction of the framework. In this paper, we present three case studies and some of the challenges and issues that are to be considered in performance tuning when running applications in Hadoop. The focus is mainly on the impact of input data on Hadoop's performance and how they can be tuned. Copyright \u00a9 2011 John Wiley & Sons, Ltd.", "Exemplar-based sparse representation is a nonparametric framework for voice conversion. In this framework, a target spectrum is generated as a weighted linear combination of a set of basis spectra, namely exemplars, extracted from the training data. This framework adopts coupled source-target dictionaries consisting of acoustically aligned source-target exemplars, and assumes they can share the same activation matrix. At runtime, a source spectrogram is factorized as a product of the source dictionary and the common activation matrix, which is applied to the target dictionary to generate the target spectrogram. In practice, either low-resolution mel-scale filter bank energies or high-resolution spectra are adopted in the source dictionary. Low-resolution features are flexible in capturing the temporal information without increasing the computational cost and the memory occupation significantly, while high\u00a0\u2026", "Building efficient architecture in neural speech processing is paramount to success in keyword spotting deployment. However, it is very challenging for lightweight models to achieve noise robustness with concise neural operations. In a real-world application, the user environment is typically noisy and may contain reverberations. We proposed a novel feature interactive convolutional model with merely 100K parameters to tackle this under the noisy far-field condition. The interactive unit is proposed in place of the attention module that promotes the flow of information with more efficient computations. Moreover, curriculum-based multi-condition training is adopted to attain better noise robustness. Our model achieves 98.2% top-1 accuracy on Google Speech Command V2-12 and is competitive against large transformer models under the designed noise condition.", "Speaker extraction requires a sample speech from the target speaker as the reference. However, enrolling a speaker with a long speech is not practical. We propose a speaker extraction technique, that performs in multiple stages to take full advantage of short reference speech sample. The extracted speech in early stages is used as the reference speech for late stages. For the first time, we use frame-level sequential speech embedding as the reference for target speaker. This is a departure from the traditional utterance-based speaker embedding reference. In addition, a signal fusion scheme is proposed to combine the decoded signals in multiple scales with automatically learned weights. Experiments on WSJ0-2mix and its noisy versions (WHAM! and WHAMR!) show that SpEx++ consistently outperforms other state-of-the-art baselines.", "The output from most of the Automatic Speech Recognition system is a continuous sequence of words without proper punctuation. This decreases human readability and the performance of downstream natural language processing tasks on ASR text. We treat the punctuation prediction task as a sequence tagging task and propose an architecture that uses pre-trained BERT embeddings. Our model significantly improves the state of art on the IWSLT dataset. We achieve an overall F1 of 81.4% on the joint prediction of period, comma and question mark.", "Speech enhancement (SE) aims to suppress the additive noise from noisy speech signals to improve the speech\u2019s perceptual quality and intelligibility. However, the over-suppression phenomenon in the enhanced speech might degrade the performance of downstream automatic speech recognition (ASR) task due to the missing latent information. To alleviate such problem, we propose an interactive feature fusion network (IFF-Net) for noise-robust speech recognition to learn complementary information from the enhanced feature and original noisy feature. Experimental results show that the proposed method achieves absolute word error rate (WER) reduction of 4.1% over the best baseline on RATS Channel-A corpus. Our further analysis indicates that the proposed IFF-Net can complement some missing information in the over-suppressed enhanced feature.", "Voice conversion methods have advanced rapidly over the last decade. Studies have shown that speaker characteristics are captured by spectral feature as well as various prosodic features. Most existing conversion methods focus on the spectral feature as it directly represents the timbre characteristics, while some conversion methods have focused only on the prosodic feature represented by the fundamental frequency. In this paper, a comprehensive framework using deep neural networks to convert both timbre and prosodic features is proposed. The timbre feature is represented by a high-resolution spectral feature. The prosodic features include F0, intensity and duration. It is well known that DNN is useful as a tool to model high-dimensional features. In this work, we show that DNN initialized by our proposed autoencoder pretraining yields good quality DNN conversion models. This pretraining is tailor\u00a0\u2026", "This paper examines an unsupervised search method to discover motifs from multivariate time series data. Our method first scans the entire series to construct a list of candidate motifs in linear time, the list is then used to populate a sparse self-similarity matrix for further processing to generate the final selections. The proposed algorithm is efficient in both running time and memory storage. To demonstrate its effectiveness, we applied it to search for repeating segments in both music and sensory data sets. The experimental results showed that the proposed method can efficiently detect repeating segments as compared to well-known methods such as self-similarity matrix search and symbolic aggregation approximation approaches.", "In this paper, we explore the generalization capability of acoustic model for improving speech recognition robustness against noise distortions. While generalization in statistical learning theory originally refers to the model's ability to generalize well on unseen testing data drawn from the same distribution as that of the training data, we show that good generalization capability is also desirable for mismatched cases. One way to obtain such general models is to use margin-based model training method, e.g., soft-margin estimation (SME), to enable some tolerance to acoustic mismatches without a detailed knowledge about the distortion mechanisms through enhancing margins between competing models. Experimental results on the Aurora-2 and Aurora-3 connected digit string recognition tasks demonstrate that, by improving the model's generalization capability through SME training, speech recognition\u00a0\u2026", "In this paper, we propose the temporal discrete cosine transform (TDCT) feature for the speaker verification task. The TDCT feature captures temporal information from a longer time context beyond the conventional delta and double-delta coefficients. We evaluate the effectiveness of the TDCT feature on the NIST 2001, NIST 2004, and NIST 2005 speaker recognition benchmark corpora by using a standard GMM-UBM recognizer. We compare our results against the standard MFCC+ \u0394+ \u0394\u0394 front end, and with the shifted delta cepstrum (SDC) feature which is commonly used in the language identification task. The results indicate that the TDCT and SDC give similar accuracy, and that the TDCT feature outperforms MFCC+ \u0394+ \u0394\u0394 in most of the cases.", "The authors examine a method for reducing the implementation complexity of the RBF Bayesian equaliser using model selection. The selection process is based on finding a subset model to approximate the response of the full RBF model for the current input vector, and not for the entire input space. Using such a scheme, for cases in which the channel equalisation problem is non-stationary, the requirement for updating all the centre locations is removed, and the implementation complexity is reduced. Using computer simulations, we show that the number of centres can be greatly reduced without compromising classification performance.", "Convolutional neural network (CNN) is found to be effective in learning representation for speech emotion recognition. CNNs do not explicitly model the associations or relative importance of features in the spectral/temporal/channel-wise axes. In this paper, we propose an attention module, named spectro-temporal-channel (STC) attention module that is integrated with CNN to improve representation learning ability. Our module infers an attention map along the three dimensions, namely time, frequency, and CNN channel. Experiments are conducted on the IEMOCAP database to evaluate the effectiveness of the proposed representation learning method. The results demonstrate that the proposed method outperforms the traditional CNN method by an absolute increase of 3.13% in terms of F1 score.", "In a typical voice conversion system, vocoder is commonly used for speech-to-features analysis and features-to-speech synthesis. However, vocoder can be a source of speech quality degradation. This paper presents a novel approach to voice conversion using WaveNet for non-parallel training data. Instead of reconstructing speech with intermediate features, the proposed approach utilizes the WaveNet to map the Phonetic Posterior-Grams (PPGs) to the waveform samples directly. In this way, we avoid the estimation errors arising from vocoding and feature conversion. Additionally, as PPG is assumed to be speaker independent, the proposed approach also reduces the feature mismatch problem in WaveNet vocoder based solutions. Experimental results conducted on the CMU-ARCTIC database show that the proposed approach significantly outperforms the traditional vocoder and WaveNet Vocoder baselines\u00a0\u2026", "Customized translation need pay spe-cial attention to the target domain ter-minology especially the named-entities for the domain. Adding linguistic features to neural machine translation (NMT) has been shown to benefit translation in many studies. In this paper, we further demonstrate that adding named-entity (NE) feature with named-entity recognition (NER) into the source language produces better translation with NMT. Our experiments show that by just including the different NE classes and boundary tags, we can increase the BLEU score by around 1 to 2 points using the standard test sets from WMT2017. We also show that adding NE tags using NER and applying in-domain adaptation can be combined to further improve customized machine translation.", "This paper presents a sparse representation framework for weighted frequency warping based voice conversion. In this method, a frame-dependent warping function and the corresponding spectral residual vector are first calculated for each source-target spectrum pair. At runtime conversion, a source spectrum is factorised as a linear combination of a set of source spectra in the training data. The linear combination weight matrix, which is constrained to be sparse, is used to interpolate the frame-dependent warping functions and spectral residual vectors. In this way, the proposed method not only avoids the statistical averaging caused by GMM but also preserves the high-resolution spectral details for high-quality converted speech. Experiments are conducted on the VOICES database. Both objective and subjective results confirmed the effectiveness of the proposed method. In particular, the spectral distortion\u00a0\u2026", "This paper describes the I2R/NTU system submitted for the NIST Rich Transcription 2007 (RT-07) Meeting Recognition evaluation Multiple Distant Microphone (MDM) task. In our system, speaker turn detection and clustering is done using Direction of Arrival (DOA) information. Purification of the resultant speaker clusters is then done by performing GMM modeling on acoustic features. As a final step, non-speech & silence removal is done. Our system achieved a competitive overall DER of 15.32% for the NIST Rich Transcription 2007 evaluation task.", "We study the estimation of time difference of arrival (TDOA) under noisy and reverberant conditions. Conventional TDOA estimation methods such as MUltiple SIgnal Classification (MUSIC) are not robust to noise and reverberation due to the distortion in the spatial covariance matrix (SCM). To address this issue, this paper proposes a robust SCM estimation method, called weighted SCM (WSCM). In the WSCM estimation, each time-frequency (TF) bin of the input signal is weighted by a TF mask which is 0 for non-speech TF bins and 1 for speech TF bins in ideal case. In practice, the TF mask takes values between 0 and 1 that are predicted by a long short term memory (LSTM) network trained from a large amount of simulated noisy and reverberant data. The use of mask weights significantly reduces the contribution of low SNR TF bins to the SCM estimation, hence improves the robustness of MUSIC. Experimental\u00a0\u2026", "Spoofing detection for automatic speaker verification (ASV), which is to discriminate between live and artificial speech, has received increasing attentions recently. However, the previous studies have been done on the clean data without significant noise. It is still not clear whether the spoofing detectors trained on clean speech can generalise well under noisy conditions. In this work, we perform an investigation of spoofing detection under additive noise and reverberant conditions. In particular, we consider five difference additive noises at three different signalto-noise ratios (SNR), and a reverberation noise with different reverberation time (RT). Our experimental results reveal that additive noises degrade the spoofing detectors trained on clean speech significantly. However, the reverberation does not hurt the performance too much.", "In this paper we describe the system proposed by NNI (NWPUNTU-I2R) team for the QUESST task within the Mediaeval 2014 evaluation. To solve the problem, we used both dynamic time warping (DTW) and symbolic search (SS) based approaches. The DTW system performs template matching using subsequence DTW algorithm and posterior representations. The symbolic search is performed on phone sequences generated by phone recognizers. For both symbolic and DTW search, partial sequence matching is performed to reduce missing rate, especially for query type 2 and 3. After fusing 9 DTW systems, 7 symbolic systems, and query length side information, we obtained 0.6023 actual normalized cross entropy (actCnxe) for all queries combined. For type 3 complex queries, we achieved 0.7252 actCnxe.", "A robust voice conversion function relies on a large amount of parallel training data, which is difficult to collect in practice. To tackle the sparse parallel training data problem in voice conversion, this paper describes a mixture of factor analyzers method which integrates prior knowledge from non-parallel speech into the training of conversion function. The experiments on CMU ARCTIC corpus show that the proposed method improves the quality and similarity of converted speech. With both objective and subjective evaluations, we show the proposed method outperforms the baseline GMM method.", "Noise-robust speech recognition systems require large amounts of training data including noisy speech data and corresponding transcripts to achieve state-of-the-art performances in face of various practical environments. However, such plenty of in-domain data is not always available in the real-life world. In this paper, we propose a generative adversarial network to simulate noisy spectrum from the clean spectrum (SimuGAN), where only 10 minutes of unparalleled in-domain noisy speech data is required as labels. Furthermore, we also propose a dual-path speech recognition system to improve the robustness of the system under noisy conditions. Experimental results show that the proposed speech recognition system achieves 7.3% absolute improvement with simulated noisy data by Simu-GAN over the best baseline in terms of word error rate (WER).", "Hidden Markov model (HMM) is one of the popular techniques for story segmentation, where hidden Markov states represent the topics, and the emission distributions of n-gram language model (LM) are dependent on the states. Given a text document, a Viterbi decoder finds the hidden story sequence, with a change of topic indicating a story boundary. In this paper, we propose a discriminative approach to story boundary detection. In the HMM framework, we use deep neural network (DNN) to estimate the posterior probability of topics given the bag-ofwords in the local context. We call it the DNN-HMM approach. We consider the topic dependent LM as a generative modeling technique, and the DNN-HMM as the discriminative solution. Experiments on topic detection and tracking (TDT2) task show that DNN-HMM outperforms traditional n-gram LM approach significantly and achieves state-of-the-art performance.", "In this paper, we propose integration of multimodal features using conditional random fields (CRFs) for the segmentation of broadcast news stories. We study story boundary cues from lexical, audio and video modalities, where lexical features consist of lexical similarity, chain strength and overall cohesiveness; acoustic features involve pause duration, pitch, speaker change and audio event type; and visual features contain shot boundaries, anchor faces and news title captions. These features are extracted in a sequence of boundary candidate positions in the broadcast news. A linear-chain CRF is used to detect each candidate as boundary/non-boundary tags based on the multimodal features. Important interlabel relations and contextual feature information are effectively captured by the sequential learning framework of CRFs. Story segmentation experiments show that the CRF approach outperforms other\u00a0\u2026", "The complete acoustic features include magnitude and phase information. However, traditional speech emotion recognition methods only focus on the magnitude information and ignore the phase data, and will inevitably miss some information. This study explores the accurate extraction and effective use of phase features for speech emotion recognition. First, the reflection of speech emotion in the phase spectrum is analyzed, and a quantitative analysis shows that phase data contain information that can be used to distinguish emotions. A dynamic relative phase (DRP) feature extraction method is then proposed to solve the problem in which the original relative phase (RP) has difficulty determining the base frequency and further alleviating the dependence of the phase on the clipping position of the frame. Finally, a single-channel model (SCM) and a multi-channel model with an attention mechanism (MCMA) are\u00a0\u2026", "Spoofing speech detection aims to differentiate spoofing speech from natural speech. Frame-based features are usually used in most of previous works. Although multiple frames or dynamic features are used to form a super-vector to represent the temporal information, the time span covered by these features are not sufficient. Most of the systems failed to detect the non-vocoder or unit selection based spoofing attacks. In this work, we propose to use a temporal convolutional neural network (CNN) based classifier for spoofing speech detection. The temporal CNN first convolves the feature trajectories with a set of filters, then extract the maximum responses of these filters within a time window using a max-pooling layer. Due to the use of max-pooling, we can extract useful information from a long temporal span without concatenating a large number of neighbouring frames, as in feedforward deep neural network (DNN\u00a0\u2026", "A semi-auto system is developed to acquire player possession for broadcast soccer video, whose objective is to minimize the manual work. This research is important because acquiring player-possession by pure manual work is very time-consuming. For completeness, this system integrates the ball detection-and-tracking algorithm, view classification algorithm, and play/break analysis algorithm. First, it produces the ball locations, play/break structure, and the view classes of frames. Then it finds the touching points based on ball locations and player detection. Next it estimates the touching-place in the field for each touching point based on the view-class of the touching frame. Last, for each touching-point it acquires the touching-player candidates based on the touching-place and the roles of players. The system provides the graphical user interfaces to verify touching-points and finalize the touching-player for each\u00a0\u2026", "We introduce a local adaptation process in the orthogonal least squares (OLS) learning algorithm for the selection of radial basis function (RBF) networks. Using simulation results, we show that the proposed algorithm can find significantly better subset models than the OLS algorithm.", "Dialogue relation extraction (RE) is to predict the relation type of two entities mentioned in a dialogue. In this paper, we propose a simple yet effective model named SimpleRE for the RE task. SimpleRE captures the interrelations among multiple relations in a dialogue through a novel input format named BERT Relation Token Sequence (BRS). In BRS, multiple [CLS] tokens are used to capture possible relations between different pairs of entities mentioned in the dialogue. A Relation Refinement Gate (RRG) is then designed to extract relation-specific semantic representation in an adaptive manner. Experiments on the DialogRE dataset show that SimpleRE achieves the best performance, with much shorter training time. Further, SimpleRE outperforms all direct baselines on sentence-level RE without using external resources.", "In this paper, we propose a single multi-task learning framework to perform End-to-End (E2E) speech recognition (ASR) and accent recognition (AR) simultaneously. The proposed framework is not only more compact but can also yield comparable or even better results than standalone systems. Specifically, we found that the overall performance is predominantly determined by the ASR task, and the E2E-based ASR pretraining is essential to achieve improved performance, particularly for the AR task. Additionally, we conduct several analyses of the proposed method. First, though the objective loss for the AR task is much smaller compared with its counterpart of ASR task, a smaller weighting factor with the AR task in the joint objective function is necessary to yield better results for each task. Second, we found that sharing only a few layers of the encoder yields better AR results than sharing the overall encoder. Experimentally, the proposed method produces WER results close to the best standalone E2E ASR ones, while it achieves 7.7% and 4.2% relative improvement over standalone and single-task-based joint recognition methods on test set for accent recognition respectively.", "Eye tracking is a widely used tool for behavioral research in the field of psychology. With technological advancement, we now have specialized eye-tracking devices that offer high sampling rates, up to 2000 Hz, and allow for measuring eye movements with high accuracy. They also offer high spatial resolution, which enables the recording of very small movements, like drifts and microsaccades. Features and parameters of interest that characterize eye movements need to be algorithmically extracted from raw data as most eye trackers identify only basic parameters, such as blinks, fixations, and saccades. Eye-tracking experiments may investigate eye movement behavior in different groups of participants and in varying stimuli conditions. Hence, the analysis stage of such experiments typically involves two phases, (i) extraction of parameters of interest and (ii) statistical analysis between different participants\u00a0\u2026", "In this paper, we report the development of the South East Asia Mandarin-English (SEAME) corpus, including 63 hours of transcribed spontaneous Mandarin-English code-switching speech in its first release, and an update of additional 129 transcribed hours of speech. The corpus was developed for code-switching speech recognition research, such as LVCSR, language recognition, and language segmentation. It was made publicly available through LDC since 2015. The corpus was recorded under unscripted interview and conversation settings, therefore, consisting of spontaneous speech. This paper seeks to present a comprehensive statistics and analysis of the corpus after the update in term of its composition, speaker profile and code-switch characteristics. This paper will also review its suitability for various code-switch related researches and possible further developments.", "Semi-supervised and cross-lingual knowledge transfer learnings are two strategies for boosting performance of lowresource speech recognition systems. In this paper, we propose a unified knowledge transfer learning method to deal with these two learning tasks. Such a knowledge transfer learning is realized by fine-tuning of Deep Neural Network (DNN). We demonstrate its effectiveness in both monolingual based semisupervised learning task and cross-lingual knowledge transfer learning task. We then combine these two learning strategies to obtain further performance improvement.", "In this paper, we propose a novel feature space adaptation technique to improve the robustness of speech recognition in noisy environments. Histogram equalization (HEQ) is an effective technique for improving robustness by reducing the difference between clean and noisy features. A weakness of HEQ is that it does not take into account acoustic model, resulting in possible mismatch between HEQ processed features and the acoustic model. In this paper, we propose to adapt HEQ to maximize the likelihood of HEQ-processed features on the acoustic model, with a constraint on the parameters of HEQ. In addition, we use a Gaussian mixture model (GMM) to represent the clean feature space rather than using the acoustic model itself, and this results in both simpler implementation and better results. Experimental results show that HEQ with adaptation reduces word error rate by 7.5% and 5.7% respectively on\u00a0\u2026", "This paper documents the significant components of a state-ofthe-art language-independent query-by-example spoken term detection system designed for the Query by Example Search on Speech Task (QUESST) in MediaEval 2015. We developed exact and partial matching DTW systems, and WFST based symbolic search systems to handle different types of search queries. To handle the noisy and reverberant speech in the task, we trained tokenizers using data augmented with different noise and reverberation conditions. Our postevaluation analysis showed that the phone boundary label provided by the improved tokenizers brings more accurate speech activity detection in DTW systems. We argue that acoustic condition mismatch is possibly a more important factor than language mismatch for obtaining consistent gain from stacked bottleneck features. Our post-evaluation system, involving a smaller number of component systems, can outperform our submitted systems, which performed the best for the task.", "This paper presents a robust speech recognition system using a microphone array for the 3rd CHiME Challenge. A minimum variance distortionless response (MVDR) beamformer with adaptive microphone gains is proposed for robust beamforming. Two microphone gain estimation methods are studied using the speech-dominant time-frequency bins. A multichannel noise reduction (MCNR) postprocessing is also proposed to further reduce the interference in the MVDR processed signal. Experimental results for the ChiME-3 challenge show that both the proposed MVDR beamformer with microphone gains and the MCNR postprocessing improve the speech recognition performance significantly. With the state-of-the-art deep neural network (DNN) based acoustic model, our system achieves a word error rate (WER) of 11.67% on the real test data of the evaluation set.", "In this paper, we propose a partial sequence matching based symbolic search (SS) method for the task of language independent query-by-example spoken term detection. One main drawback of conventional SS approach is the high miss rate for long queries. This is due to high variations in symbol representation of query and search audios, especially in language independent scenario. The successful matching of a query with its instances in search audio becomes exponentially more difficult as the query grows longer. To reduce miss rate, we propose a partial matching strategy, in which all partial phone sequences of a query are used to search for query instances. The partial matching is also suitable for real life applications where exact match is usually not necessary and word prefix, suffix, and order should not affect the search result. When applied to the QUESST 2014 task, results show the partial matching of\u00a0\u2026", "Code-switching language modeling is challenging due to statistics of each individual language, as well as statistics of cross-lingual language are insufficient. To compensate for the issue of statistical insufficiency, in this paper we propose a word-class n-gram language modeling approach of which only infrequent words are clustered while most frequent words are treated as singleton classes themselves. We first demonstrate the effectiveness of the proposed method on our English-Mandarin code-switching SEAME data in terms of perplexity. Compared with the conventional word n-gram language models, as well as the word-class n-gram language models of which entire vocabulary words are clustered, the proposed word-class n- gram language modeling approach can yield lower perplexity on our SEAME dev data sets. Additionally, we observed further perplexity reduction by interpolating the word n-gram\u00a0\u2026", "We present exemplar-inspired low-resource spoken keyword search strategies for acoustic modeling, keyword verification, and system combination. This state-of-the-art system was developed by the SINGA team in the context of the 2015 NIST Open Keyword Search Evaluation (OpenKWS15) using conversational Swahili provided by the IARPA Babel program. In this work, we elaborate on the following: (1) exploiting exemplar training samples to construct a non-parametric acoustic model using kernel density estimation at test time; (2) rescoring hypothesized keyword detections through quantifying their acoustic similarity with exemplar training samples; (3 ) extending our previously proposed system combination approach to incorporate prosody features of exemplar keyword samples.", "In this paper we evaluate the possibility of improving the performance of a statistical machine translation system by relaxing the complexity of the translation task by removing the most frequent and predictable terms from the target language vocabulary. Afterwards, the removed terms are inserted back in the relaxed output by using an n-gram based word predictor. Empirically, we have found that when these words are omitted from the text, the perplexity of the text decreases, which may imply the reduction of confusion in the text. We conducted some machine translation experiments to see if this perplexity reduction produced a better translation output. While the word prediction results exhibits 77% accuracy in predicting 40% of the most frequent words in the text, the perplexity reduction did not help to produce better translations.", "This paper studies feature selection in phonotactic language recognition. The phonotactic feature is presented by n-gram statistics derived from one or more phone recognizers in the form of high dimensional feature vectors. Two feature selection strategies are proposed to select the n-gram statistics for reducing the dimension of feature vectors, so that higher order n-gram features can be adopted in language recognition. With the proposed feature selection techniques, we achieved equal error rates (EERs) of 1.84% with 4-gram statistics on the 2007 NIST Language Recognition Evaluation 30s closed test sets.", "This paper describes our recent efforts in exploring effective discriminative features for speaker recognition. Recent researches have indicated that the appropriate fusion of features is critical to improve the performance of speaker recognition system. In this paper we describe our approaches for the NIST 2006 Speaker Recognition Evaluation. Our system integrated the cepstral GMM modeling, cepstral SVM modeling and tokenization at both phone level and frame level. The experimental results on both NIST 2005 SRE corpus and NIST 2006 SRE corpus are presented. The fused system achieved 8.14% equal error rate on 1conv4w-1conv4w test condition of the NIST 2006 SRE.", "The paper proposes a low\u2010complexity concurrent constant modulus algorithm (CMA) and soft decision\u2010directed (SDD) scheme for fractionally spaced blind equalization of high\u2010order quadrature amplitude modulation channels. We compare our proposed blind equalizer with the recently introduced state\u2010of\u2010art concurrent CMA and decision\u2010directed (DD) scheme. The proposed CMA+SDD blind equalizer is shown to have simpler computational complexity per weight update, faster convergence speed, and slightly improved steady\u2010state equalization performance, compared with the existing CMA+DD blind equalizer. Copyright \u00a9 2004 John Wiley & Sons, Ltd.", "The attention-based end-to-end (E2E) automatic speech recognition (ASR) architecture allows for joint optimization of acoustic and language models within a single network. However, in a vanilla E2E ASR architecture, the decoder sub-network (subnet), which incorporates the role of the language model (LM), is conditioned on the encoder output. This means that the acoustic encoder and the language model are entangled that doesn\u2019t allow language model to be trained separately from external text data. To address this problem, in this work, we propose a new architecture that separates the decoder subnet from the encoder output. In this way, the decoupled subnet becomes an independently trainable LM subnet, which can easily be updated using the external text data. We study two strategies for updating the new architecture. Experimental results show that, 1) the independent LM architecture benefits from\u00a0\u2026", "Recently, a deep beamforming (BF) network was proposed to predict BF weights from phase-carrying features, such as generalized cross correlation (GCC). The BF network is trained jointly with the acoustic model to minimize automatic speech recognition (ASR) cost function. In this paper, we propose to replace GCC with features derived from input signals' spatial covariance matrices (SCM), which contain the phase information of individual frequency bands. Experimental results on the AMI meeting transcription task shows that the BF network using SCM features significantly reduces the word error rate to 44.1% from 47.9% obtained with the conventional ASR pipeline using delay-and-sum BF. Also compared with GCC features, we have observed small but steady gain by 0.6% absolutely. The use of SCM features also facilitate the implementation of more advanced BF methods within a deep learning framework\u00a0\u2026", "In this paper, we propose a robust distant-talking speech recognition by combining cepstral domain denoising autoencoder (DAE) and temporal structure normalization (TSN) filter. As DAE has a deep structure and nonlinear processing steps, it is flexible enough to model highly nonlinear mapping between input and output space. In this paper, we train a DAE to map reverberant and noisy speech features to the underlying clean speech features in the cepstral domain. For the proposed method, after applying a DAE in the cepstral domain of speech to suppress reverberation, we apply a post-processing technology based on temporal structure normalization (TSN) filter to reduce the noise and reverberation effects by normalizing the modulation spectra to reference spectra of clean speech. The proposed method was evaluated using speech in simulated and real reverberant environments. By combining a\u00a0\u2026", "Spectral information represents short-term speech information within a frame of a few tens of milliseconds, while temporal information captures the evolution of speech statistics over consecutive frames. Motivated by the findings that human speech comprehension relies on the integrity of both the spectral content and temporal envelope of speech signal, we study a spectro-temporal transform framework that adapts run-time speech features to minimize the mismatch between run-time and training data, and its implementation that includes cross transform and cascaded transform. A Kullback-Leibler divergence based cost function is proposed to estimate the transform parameters. We conducted experiments on the REVERB Challenge 2014 task, where clean and multi-condition trained acoustic models are tested with real reverberant and noisy speech. We found that temporal information is important for reverberant\u00a0\u2026", "Feature transformation techniques have been widely investigated to reduce feature redundancy and to introduce additional discriminative information with the aim to improve the performance of automatic speech recognition (ASR). In this paper, we propose a novel method to obtain discriminative feature transformation based on output coding technique for speech recognition. The output coding transformation projects the speech features from their original space to a new one where each dimension of the features captures information to distinguish different phones. Using polynomial expansion, the short-time spectral features are first expanded to a high-dimensional space where the generalized linear discriminant sequence kernel is applied on the sequences of input feature vectors. Then, the output coding transformation formulated via a set of linear SVMs projects the sequences of high dimensional vectors into a\u00a0\u2026", "Automated Audio captioning (AAC) is a cross-modal task that generates natural language to describe the content of input audio. Most prior works usually extract single-modality acoustic features and are therefore sub-optimal for the cross-modal decoding task. In this work, we propose a novel AAC system called CLIP-AAC to learn interactive cross-modality representation with both acoustic and textual information. Specifically, the proposed CLIP-AAC introduces an audio-head and a text-head in the pre-trained encoder to extract audio-text information. Furthermore, we also apply contrastive learning to narrow the domain difference by learning the correspondence between the audio signal and its paired captions. Experimental results show that the proposed CLIP-AAC approach surpasses the best baseline by a significant margin on the Clotho dataset in terms of NLP evaluation metrics. The ablation study indicates that both the pre-trained model and contrastive learning contribute to the performance gain of the AAC model.", "End-to-end models have been introduced into automatic speech recognition (ASR) successfully and achieved superior performance compared with conventional hybrid systems, especially with the newly proposed transformer model. However, speaker mismatch between training and test data remains a problem, and speaker adaptation for transformer model can be further improved. In this paper, we propose to conduct speaker aware training for ASR in transformer model. Specifically, we propose to embed speaker knowledge through a persistent memory model into speech transformer encoder at utterance level. The speaker information is represented by a number of static speaker i-vectors, which is concatenated to speech utterance at each encoder self-attention layer. Persistent memory is thus formed by carrying speaker information through the depth of encoder. The speaker knowledge is captured from self\u00a0\u2026", "Replay attacks from unseen utterances poses a significant challenge in Anti-Spoofing Detection. In this paper, we propose a statistical measure based on the Rayleigh Quotient in order to investigate a feature partition capable of discerning genuine and playback speech under unseen conditions. The Log- Magnitude Spectrum (LMS) of the utterances is used in this study. Using the proposed measure, we analyze the frequency bands of the LMS based on the amount of discriminative information between the scatter matrices of the genuine and spoof utterances. This allows us to determine the optimal frequency bands required for replay attacks detection. In addition, we further investigate the effects of training our models using voiced and unvoiced portions of the utterances. We conducted our experiments based on the ASVspoof 2017 database. On the development set, our partitioned LMS feature based on the\u00a0\u2026", "In this paper, we present a robust real-time sound source localization system implemented on a social robot platform developed in Institute for Infocomm Research, Singapore. The audio localization system provides the robot with auditory senses and enables the robot to direct its face to a speaker outside its frontal vision system. As the localization system exploits time difference of arrival (TDOA), the placement of the 8 microphone system array is crucial. This paper discusses the configuration and implementation of our system for the Olivia robot platform for accurate 3D localization under high babble noise condition.", "This paper addresses the problem of monaural speech separation for simultaneous speakers. Recent studies such as uPIT, cuPIT-Grid LSTM and their variants have advanced the stateof-the-art separation models. Delta and acceleration coefficients are typically used in the objective function to capture short time dynamics. We consider that such coefficients don\u2019t benefit from the temporal information over a long range such as phoneme and syllable. In this paper, we propose a shifted delta coefficient (SDC) objective to explore the temporal information over a long range of the spectral dynamics. The SDC ensures the temporal continuity of output frames within the same speaker. In addition, we propose a novel multi-task learning framework, that we call SDC-MTL, by extending the SDC objective with a subtask of predicting the time-frequency labels ({silence, single, overlapped}) of the mixture. The experimental results show 11.7% and 3.9% relative improvements on WSJ0-2mix dataset under open conditions over the uPIT and cuPIT-Grid LSTM baselines. A further analysis shows 17.8% and 6.2% relative improvements with speakers of same gender.", "Dynamic Time Warping (DTW) is widely used in language independent query-by-example (QbE) spoken term detection (STD) tasks due to its high performance. However, there are two limitations of DTW based template matching, 1) it is not straightforward to perform approximate match of audio queries; 2) DTW is sensitive to the mismatch of signal conditions between the query and the speech search data. To allow approximate search, we propose a partial template matching strategy using phone time boundary information generated by a phone recognizer. To have more invariant representation of audio signals, we use bottleneck features (BNF) as the input of DTW. The BNF network is trained from augmented data, which is generated by adding reverberation and additive noises to the clean training data. Experimental results on QUESST 2015 task shows the effectiveness of the proposed methods for QbE-STD\u00a0\u2026", "Many keyword search (KWS) systems make \u201chit/false alarm (FA)\u201d decisions based on the lattice-based posterior probability, which is incomparable across keywords. Therefore, score normalization is essential for a KWS system. In this paper, we investigate the integration of two novel features, ranking-score and relative-to-max, into a discriminative score normalization method. These features are extracted by considering all competing hypotheses of a putative detection. A metric-based normalization method is also applied as a post-processing step to further optimize the term-weighted value (TWV) evaluation metric. We report empirical improvements over standard baselines using the Vietnamese data from IARPA's Babel program in the NIST OpenKWS13 Evaluation setup.", "This paper presents a novel acoustic modeling technique of large vocabulary automatic speech recognition for under-resourced languages by leveraging well-trained acoustic models of other languages (called source languages). The idea is to use source language acoustic model to score the acoustic features of the target language, and then map these scores to the posteriors of the target phones using a classifier. The target phone posteriors are then used for decoding in the usual way of hybrid acoustic modeling. The motivation of such a strategy is that human languages usually share similar phone sets and hence it may be easier to predict the target phone posteriors from the scores generated by source language acoustic models than to train from scratch an under-resourced language acoustic model. The proposed method is evaluated using on the Aurora-4 task with less than 1 hour of training data. Two types\u00a0\u2026", "In this paper, we investigate semi-supervised training (SST) method in various state-of-the-art acoustic modeling techniques, using bottle-neck and corresponding tandem features. These techniques include subspace GMM, tanh-neuron deep neural network (DNN), and a generalized soft-maxout (p-norm) DNN. We demonstrate that SST may lead up to 2% Word Error Rate (WER) reduction using all these techniques in each case, and the best one comes from tandem feature based p-norm DNN system. In addition to recognition performance, effectiveness of the SST on keyword search performance is also investigated. Results on Actual Term Weighted Value (ATWV) are reported, with an analysis on lattice density. It is shown that SST may not necessarily increase ATWV due to the shrink of lattices size.", "The time-frequency spectrogram representation of an audio signal can be visually analysed by a trained researcher to recognise any underlying sound events in a process called \u201cspectrogram reading\u201d. However, this has not become a popular approach for automatic classification, as the field is driven by Automatic Speech Recognition (ASR) where frame-based features are popular. As opposed to speech, sound events typically have a more distinctive time-frequency representation, with the energy concentrated in a small number of spectral components. This makes them more suitable for classification based on their visual signature, and enables inspiration to be found in techniques from the related field of image processing. Recently, there have been a range of techniques that extract image processing-inspired features from the spectrogram for sound event classification. In this paper, we introduce the idea and\u00a0\u2026", "Recently, we have collected a Malay read speech corpus and news text corpus. In this paper, we develop a large vocabulary continuous speech recognition (LVCSR) system based on this corpus. To the best of our knowledge, there is very few study on Malay LVCSR. In this paper, we discuss one aspect of a Malay LVCSR system, ie the amount of training data required to train robust acoustic and langauge models for the Malay language. This results may be of interest to reader for future study in Malay LVCSR system development.", "This paper presents a new strategy for designing the parallel phone recognizers for spoken language recognition. Given a collection of parallel phone recognizers, we select a subset of phones from each phone recognizer for each target language to construct a target-oriented phone tokenizer (TOPT). As a result, the collection of target-oriented phone tokenizers is more effective than the original parallel phone recognizers. This approach improves system performance significantly without requesting for additional transcribed training samples. We validate the effectiveness of the proposed strategy within the framework of the parallel phone recognizer followed by vector space modeling backend, or PPR-VSM. We achieve equal-error-rate of 2.21% and 3.65% on the 2003 and 2005 NIST LRE databases, respectively, for 30-second trials.", "Although automatic speech recognition (ASR) task has gained remarkable success by sequence-to-sequence models, there are two main mismatches between its training and testing that might lead to performance degradation: 1) The typically used cross-entropy criterion aims to maximize log-likelihood of the training data, while the performance is evaluated by word error rate (WER), not log-likelihood; 2) The teacher-forcing method leads to the dependence on ground truth during training, which means that model has never been exposed to its own prediction before testing. In this paper, we propose an optimization method called self-critical sequence training (SCST) to make the training procedure much closer to the testing phase. As a reinforcement learning (RL) based method, SCST utilizes a customized reward function to associate the training criterion and WER. Furthermore, it removes the reliance on teacher\u00a0\u2026", "Automatic speech recognition (ASR) systems degrade significantly under noisy conditions. Recently, speech enhancement (SE) is introduced as front-end to reduce noise for ASR, but it also suppresses some important speech information, i.e., over-suppression. To alleviate this, we propose a dual-path style learning approach for end-to-end noise-robust speech recognition (DPSL-ASR). Specifically, we first introduce clean speech feature along with the fused feature from IFF-Net as dual-path inputs to recover the suppressed information. Then, we propose style learning to map the fused feature close to clean feature, in order to learn latent speech information from the latter, i.e., clean \"speech style\". Furthermore, we also minimize the distance of final ASR outputs in two paths to improve noise-robustness. Experiments show that the proposed approach achieves relative word error rate (WER) reductions of 10.6% and 8.6% over the best IFF-Net baseline, on RATS and CHiME-4 datasets respectively.", "I-vector has shown to be very effective in speaker verification with long-duration speech utterances. But when test utterances are of short duration, content mismatch between the enrollment and test utterances limit the performance of i-vector system. This paper proposes to extract local session variability vectors on different phonetic classes from the utterances instead of estimating the session variability across the whole utterance as i-vector does. Using the posteriors given by a deep neural network (DNN) trained for phone state classification, the local vectors represent the session variability contained in specific phonetic content. Our experiments show that the content-aware local vectors are better at coping with the content mismatch between training and test utterances of short durations for text-independent, text-constrained and text-dependent tasks.", "In this paper we present our contribution to the third CHiME challenge on speech separation and recognition for noisy multi-channel recordings. The use-case of the challenge consists in single speaker utterances recorded in highly non-stationary noisy environments using a 6-microphone array mounted on a tablet computer. The front-end of our system is performing speech enhancement by cascading a cross-correlation-based channel selection, Signal Dependent MVDR beamforming and online source separation based on sparse NMF. The back-end module is a state-of-the-art speech recognition system with DNN acoustic models trained on fMLLR features and a RNN Language Model. Our system reaches an overall WER of 11.94% on real test recordings, achieving a relative improvement of 65% compared to the baseline system.", "Conventional acoustic models, such as Gaussian mixture models (GMM) or deep neural networks (DNN), cannot be reliably estimated when there are very little speech training data, eg less than 1 hour. In this paper, we investigate the use of a non-parametric kernel density estimation method to predict the emission probability of HMM states. In addition, we introduce a discriminative score calibrator to improve the speech class posteriors generated by the kernel density for speech recognition task. Experimental results on the Wall Street Journal task show that the proposed acoustic model using cross-lingual bottleneck features significantly outperforms GMM and DNN models for limited training data case.", "Most recent research work on sports video analysis focuses on broadcast video. The broadcast video is a post-produced video and has much additional editing information inserted. In this paper, we propose a novel event detection framework in sports game only based on the video taken by single (main) camera. Compared with event detection from broadcast video, the proposed framework is more challenging as interesting events needs to be automatically detected. The results of the research could be used for automatic replay generation in sports video. In this paper, a mid-level representation is first created based on low-level audio/visual feature and events related to the replay are then detected from this representation. We experiment our framework and some promising results are obtained.", "This paper examines the application of the radial basis function (RBF) network to the modelling of the Bayesian equaliser. In particular, the authors study the effects of delay order d on decision boundary and attainable bit error rate (BER) performance. To determine the optimum delay parameter for minimum BER performance, a simple BER estimator is proposed. The implementation complexity of the RBF network grows exponentially with respect to the number of input nodes. As such, the full implementation of the RBF network to realise the Bayesian solution may not be feasible. To reduce some of the implementation complexity, the authors propose an algorithm to perform subset model selection. The authors' results indicate that it is possible to reduce model size without significant degradation in BER performance.", "In this paper, we study the time-domain neural network approach for speech bandwidth extension. We propose a network architecture, named multi-scale fusion neural network (MfNet), that gradually restores the low-frequency signal and predicts the high-frequency signal through the exchange of information across different scale representations. We propose a training scheme to optimize the network with a combination of perceptual loss and time-domain adversarial loss. Experiments show the proposed multi-scale fusion network consistently outperforms the competing methods in terms of perceptual evaluation of speech quality (PESQ), signal to distortion rate (SDR), signal to noise ratio (SNR), log-spectral distance (LSD) and word error rate (WER). More promisingly, the multi-scale fusion network requires only 10% of the parameters of the time-domain reference baseline.", "In this paper, we use image processing techniques on the speech spectrogram to perform speech phoneme segmentation. The proposed method relies solely on visual cues on the spectrogram, without the need for language-specific training data. The results are evaluated on the TIMIT corpus, and compared to other unsupervised speech segmentation techniques, with comparable results obtained. We also fuse the results with those obtained by hidden Markov models (HMM) and HMM-based forced alignment to investigate if image features can provide an additional feature representation for speech processing tasks. With the fusion, up to 10% absolute improvement in segmentation accuracy over the HMM baselines can be obtained. Results are promising and suggests a strong potential for image-based features applying to speech processing.", "Language diarization is the task to perform automatic language segmentation and recognition in a code-switch speech. Towards this task, we developed a conversational Mandarin-English code-switch corpus spoken by Singaporean/Malaysian speakers. We also developed a Singapore accent specific pronunciation dictionary, with which we built a Singapore accent phone recognizer to extract long term context phonotactic feature. Our experiment shows that accent-specific phone recognizer is essential to improve language diarization performance. Specifically, the language diarization experiment, the phonotactic features generated by the Singapore accent phone recognizer has a 6.5% relative frame error rate reduction over the phone recognizer using the CMU dictionary. In addition, the ASR performance using this dictionary on the Singapore English corpus achieved 21% relative word error rate reduction over\u00a0\u2026", "In this paper, we propose two cluster criterion functions which aim to maximize the separation between intra-cluster distances and inter-cluster distances. These criteria can automatically deduce the desired number of clusters based on their extremized values. We then propose an algorithm to apply our criterion functions in conjunction with spectral clustering. By exploiting the characteristic of spectral subspace, we show that the speakers are more separable in this subspace which will further enhance the effectiveness of our proposed criteria. The algorithm is used in our agglomerative hierarchical speaker diarization system to test on Rich Transcription 2007 conference data set and obtains very good results.", "Statistical language models are very useful tools to improve the recognition accuracy of optical character recognition (OCR) systems. In previous systems, segmentation by maximum word matching, semantic class segmentation, or trigram language models have been used. However, these methods have some disadvantages, such as inaccuracies due to a preference for longer words (which may be erroneous), failure to recognize word dependencies, complex semantic training data segmentation, and a requirement of high memory. To overcome these problems, we propose a novel bigram Markov language model in this paper. This type of model does not have large word preferences and does not require semantically segmented training data. Furthermore, unlike trigram models, the memory requirement is small. Thus, the scheme is suitable for handheld and pocket computers, which are expected to be a major\u00a0\u2026", "Audio-visual speech recognition (AVSR) has gained remarkable success for ameliorating the noise-robustness of speech recognition. Mainstream methods focus on fusing audio and visual inputs to obtain modality-invariant representations. However, such representations are prone to over-reliance on audio modality as it is much easier to recognize than video modality in clean conditions. As a result, the AVSR model underestimates the importance of visual stream in face of noise corruption. To this end, we leverage visual modality-specific representations to provide stable complementary information for the AVSR task. Specifically, we propose a reinforcement learning (RL) based framework called MSRL, where the agent dynamically harmonizes modality-invariant and modality-specific representations in the auto-regressive decoding process. We customize a reward function directly related to task-specific metrics (ie, word error rate), which encourages the MSRL to effectively explore the optimal integration strategy. Experimental results on the LRS3 dataset show that the proposed method achieves state-of-the-art in both clean and various noisy conditions. Furthermore, we demonstrate the better generality of MSRL system than other baselines when test set contains unseen noises.", "Speech enhancement (SE) is proved effective in reducing noise from noisy speech signals for downstream automatic speech recognition (ASR), where multi-task learning strategy is employed to jointly optimize these two tasks. However, the enhanced speech learned by SE objective may not always yield good ASR results. From the optimization view, there sometimes exists interference between the gradients of SE and ASR tasks, which could hinder the multi-task learning and finally lead to sub-optimal ASR performance. In this paper, we propose a simple yet effective approach called gradient remedy (GR) to solve interference between task gradients in noise-robust speech recognition, from perspectives of both angle and magnitude. Specifically, we first project the SE task's gradient onto a dynamic surface that is at acute angle to ASR gradient, in order to remove the conflict between them and assist in ASR\u00a0\u2026", "To realize robust End-to-end Automatic Speech Recognition (E2E ASR) under radio communication condition, we propose a multitask-based method to jointly train a Speech Enhancement (SE) module as the front-end and an E2E ASR model as the back-end in this paper. One of the advantages of the proposed method is that the entire system can be trained from scratch. Different from prior works, either component here doesn't need to perform pre-training and fine-tuning processes separately. Through analysis, we found that the success of the proposed method lies in the following aspects. First, multitask learning is essential, that is, the SE network is not only learned to produce more intelligible speech, it is also aimed to generate speech that is beneficial to recognition. Secondly, we also found speech phase preserved from noisy speech is critical for an improved ASR performance. Thirdly, we propose a dual\u00a0\u2026", "The speaker verification (SV) task has been an active area of research in the last thirty years. One of the recent research topics is on improving the robustness of SV system in challenging environments. This paper examines the robustness of current state of the art SV system against background noise corruptions. Specifically, we consider the scenario where the SV system is trained from noise free speech and tested on background noise corrupted speech. To improve robustness of the system, a deep neural networks (DNN) based feature compensation is proposed to enhance the cepstral features before the evaluation. The DNN is trained from parallel data of clean and noise corrupted speech which are aligned in the frame level. The training is achieved by minimizing the mean square error (MSE) between the DNN's prediction and the target clean features. The trained network could predict the underlying clean\u00a0\u2026", "Semantic video content extraction and selection are critical steps in sports video analysis and editing. The identification of video segments can be from various semantic perspectives, e.g. certain event, player or emotional state. In this paper, we examined the possibility of automatically identifying shots with \"happy\" or \"sad\" emotion from broadcast sports video. Our proposed model first performs the sports highlight extraction to obtain candidate shots that possibly contain emotion information and then classifies these shots into either \"happy\" or \"sad\" emotion groups using hidden Markov model based method. The final experimental results are satisfactory", "In this paper, we examine the use of Transfer Learning using Pretrained Audio Neural Networks (PANNs) [1], and propose an architecture that is able to better leverage the acoustic features provided by PANNs for the Automated Audio Captioning Task [2]. We also introduce a novel self-supervised objective, Reconstruction Latent Space Similarity Regularization (RLSSR). The RLSSR module supplements the training of the model by maximizing the similarity between the encoder and decoder embedding. The combination of both methods allows us to surpass state of the art results by a significant margin on the Clotho dataset [3] across several metrics and benchmarks.", "Real telephony speech recognition task is challenging due to 1) diversified channel distortions and 2) limited access to the real data because of the data privacy consideration. In this paper, assuming no real telephony data are available, we employ diversified audio codecs simulation based data augmentation method to train telephony speech recognition system. Specifically, we assume only wide-band 16 kHz data are available, and we first down-sample the 16 kHz data to the 8 kHz data; we then pass the down-sampled data through various categories of audio codecs to simulate the real channel distortion. As a result, we train our speech recognition with such distorted data. To analyze the effectiveness of different audio codec simulation methods, we classify them into three main categories according to their distortion severity, in terms of their spectrogram analysis. We conduct experiments on various real\u00a0\u2026", "The performance of deep learning approaches to speech enhancement degrades significantly in face of mismatch between training and testing. In this paper, we propose a domain adversarial training technique for unsupervised domain transfer, that 1) overcomes domain mismatch, and 2) provides a solution to the scenario where we only have noisy speech data, and we don't have clean-noisy parallel data in the new domain. Specifically, our method includes two parts that are jointly trained, 1) an enhancement net to map noisy speech to clean speech by indirectly estimating a mask with a spectrum approximation loss, and 2) a domain predictor to distinguish between domains. As the proposed approach is able to adapt to a new domain only with noisy speech data in target domain, we call it an unsupervised learning technique. Experiments suggest that our approach delivers voice quality comparable with other\u00a0\u2026", "Recently, exemplar-based sparse representation methods have been proposed for voice conversion. These methods reconstruct a target spectrum through a weighted linear combination from a set of basis spectra, called exemplars. To include temporal constraint, multiple-frame exemplars are employed when estimating the linear combination weights, namely activations, by the nonnegative matrix factorization technique with a sparsity constraint. In practice, low-resolution mel-scale filter bank energies rather than high-resolution spectra are employed to estimate the activations in order to reduce computational cost and memory usages. However, the conversion performance degrades due to the loss of the spectral details in the low-resolution representations. In this study, we propose a joint nonnegative matrix factorization technique to estimate the activations using both the low-and high-resolution features\u00a0\u2026", "This paper proposes to model broadcast news prosody using conditional random fields (CRF) for news story segmentation. Broadcast news has both editorial prosody and speech prosody that convey essential structural information for story segmentation. Hence we extract prosodic features, including pause duration, pitch, intensity, rapidity, speaker change and music, for a sequence of boundary candidates. A linearchain CRF is used to label each candidate with boundary/nonboundary tags based on the prosodic features. Important interlabel relations and contextual feature interactions are effectively captured by CRF\u2019s sequential learning framework. Experiments show that the CRF approach outperforms decision tree (DT), support vector machines (SVM) and maximum entropy (ME) classifiers in prosody-based story segmentation. I. INTRODUCTIONSpoken documents, eg, broadcast news, meetings and lectures, usually have multiple topics or sub-topics. For example, a one-hour broadcast news episode often includes a series of news stories, each addressing a central topic. The task of automatic story segmentation is to divide the spoken documents into topically homogeneous segments, known as stories. Segmentation is an important precursor that facilitates efficient information extraction, topic tracking, summarization, browsing, indexing and retrieval [1]. With the ever-increasing volumes of spoken content, automatic story segmentation techniques are highly in demand. Automatic story segmentation approaches have focused on generative topic modeling [2] and story boundary detection [1],[3],[4],[5],[6]. The former category treats the word\u00a0\u2026", "This paper describes the development of an English Computer Assisted Language Learning (CALL) resource for the Singapore environment in Nanyang Technological University, Singapore. Specifically, this paper describes the development of a Singapore English-based pronunciation lexicon, the collection of a 44-speaker and a 39-speaker audio corpus for pronunciation scoring and acoustic model training respectively, and the development of a software for manual prosodic scoring. These resources can be used for the development of a pronunciation scoring system for Singapore users.", "This paper studies a new way of constructing multiple phone tokenizers for language recognition. In this approach, each phone tokenizer for a target language will share a common set of acoustic models, while each tokenizer will have a unique phone-based language model (LM) trained for a specific target language. The target-aware language models (TALM) are constructed to capture the discriminative ability of individual phones for the desired target languages. The parallel phone tokenizers thus formed are shown to achieve better performance than the original phone recognizer. The proposed TALM is very different from the LM in the traditional PPRLM technique. First of all, the TALM applies the LM information in the front-end as opposed to PPRLM approach which uses a LM in the system back-end; Furthermore, the TALM exploits the discriminative phones occurrence statistics, which are different from the\u00a0\u2026", "Self-supervised learning has been widely exploited to learn powerful speech representations. The premise of this paper is that these learned self-supervised representations contain irrelevant information for a particular downstream task. Hence, we investigate efficient methods to compute reliable representations and discard redundant information for language identification (LID) using a pre-trained multilingual wav2vec 2.0 model. To determine an optimal basic system, we compare the performance of wav2vec features extracted from different inner layers of the context network. For this approach, the x-vector self-attention LID (XSA-LID) model forms the backbone used to discriminate between distinct languages. We then propose to employ two mechanisms to reduce irrelevant information of the representations in LID. The first is the attentive squeeze-and-excitation (SE) block for dimension-wise scaling and the\u00a0\u2026", "Speaker extraction aims to extract the target speaker\u2019s voice from a multi-talker speech mixture given an auxiliary reference utterance. Recent studies show that speaker extraction benefits from the location or direction of the target speaker. However, these studies assume that the target speaker\u2019s location is known in advance or detected by an extra visual cue, e.g., face image or video. In this paper, we propose an end-to-end localized target speaker extraction on pure speech cues, that is called L-SpEx. Specifically, we design a speaker localizer driven by the target speaker\u2019s embedding to extract the spatial features, including direction-of-arrival (DOA) of the target speaker and beamforming output. Then, the spatial cues and target speaker\u2019s embedding are both used to form a top-down auditory attention to the target speaker. Experiments on the multi-channel reverberant dataset called MCLibri2Mix show that our L\u00a0\u2026", "Speech enhancement aims to suppress the additive noise from noisy speech signals to improve the speech quality. It is believed that multi-scale temporal information learned from the speech inputs strengthens the mask prediction or noise sup-pression especially for the encoder-mask-decoder like structure in time-domain speech enhancement techniques. In this paper, we propose a multi-scale encoding and decoding scheme that captures multiple temporal resolutions for improving speech quality. We also propose an attention module to capture the global temporal information of each-scale embedding in the encoding layer. The experiments show that the proposed approach achieves 9.0% and 2.9% relative improvements over the best baseline in terms of perceptual evaluation of the speech quality (PESQ) and signal-to-distortion ratio (SDR), respectively.", "Bandwidth extension aims to reconstruct wideband speech signals from narrowband inputs to improve perceptual quality. Prior studies mostly perform bandwidth extension under the assumption that the narrowband signals are clean without noise. The use of such extension techniques is greatly limited in practice when signals are corrupted by noise. To alleviate such problem, we propose an end-to-end time-domain framework for noise-robust bandwidth extension, that jointly optimizes a mask-based speech enhancement and an ideal bandwidth extension module with multi-task learning. The proposed framework avoids decomposing the signals into magnitude and phase spectra, therefore, requires no phase estimation. Experimental results show that the proposed method achieves 14.3% and 15.8% relative improvements over the best baseline in terms of perceptual evaluation of speech quality (PESQ) and log-spectral distortion (LSD), respectively. Furthermore, our method is 3 times more compact than the best baseline in terms of the number of parameters.", "We propose a hybrid neural network hidden Markov model (NN-HMM) approach for automatic story segmentation. A story is treated as an instance of an underlying topic (a hidden state) and words are generated from the distribution of the topic. The transition from one topic to another indicates a story boundary. Different from the traditional HMM approach, in which the emission probability of each state is calculated from a topic-dependent language model, we use deep neural network (DNN) to directly map the word distribution into topic posterior probabilities. DNN is known to be able to learn meaningful continuous features for words and hence has better discriminative and generalization capability than n-gram models. Specifically, we investigate three neural network structures: a feed-forward neural network, a recurrent neural network with long short-term memory cells (LSTM-RNN) and a modified LSTM\u00a0\u2026", "In this paper, we propose a framework for joint normalization of spectral and temporal statistics of speech features for robust speech recognition. Current feature normalization approaches normalize the spectral and temporal aspects of feature statistics separately to overcome noise and reverberation. As a result, the interaction between the spectral normalization (e.g. mean and variance normalization, MVN) and temporal normalization (e.g. temporal structure normalization, TSN) is ignored. We propose a joint spectral and temporal normalization (JSTN) framework to simultaneously normalize these two aspects of feature statistics. In JSTN, feature trajectories are filtered by linear filters and the filters' coefficients are optimized by maximizing a likelihood-based objective function. Experimental results on Aurora-5 benchmark task show that JSTN consistently out-performs the cascade of MVN and TSN on test data\u00a0\u2026", "In this paper, we propose a novel acoustic model adaptation method for noise robust speech recognition. Model combination is a common way to adapt acoustic models to a target test environment. For example, the mean supervectors of the adapted model are obtained as a linear combination of mean supervectors of many pre-trained environment-dependent acoustic models. Usually, the combination weights are estimated using a maximum likelihood (ML) criterion and the weights are nonzero for all the mean supervectors. We propose to estimate the weights by using Lasso (least absolute shrinkage and selection operator) which imposes an L 1  regularization term in the weight estimation problem to shrink some weights to exactly zero. Our study shows that Lasso usually shrinks to zero the weights of those mean supervectors not relevant to the test environment. By removing some nonrelevant supervectors, the\u00a0\u2026", "This paper presents an age-friendly system for improving the elderly's online shopping experience. Different from most related studies focusing on website design and content organization, we propose to integrate three assistive techniques to facilitate the elderly's browsing of products in E-commerce platforms, including the crowd-improved speech recognition, the multimodal search, and the personalized speech feedback. The first two techniques, namely, the crowd-improved speech recognition and the multimodal search, work together to allow the elderly search for desired products flexibly using either speech, an image, text, or any combination of them whichever are convenient for the elderly. The personalized speech feedback provides a speech summary of search result in a personalized voice. That is, the elderly are allowed to choose or even create their desired voices, and also can customize the voices in\u00a0\u2026", "System combination (or data fusion1) is known to provide significant improvement for spoken term detection (STD). The key issue of the system combination is how to effectively fuse the various scores of participant systems. Currently, most system combination methods are system and keyword independent, i.e. they use the same arithmetic functions to combine scores for all keywords. Although such strategy improve keyword search performance, the improvement is limited. In this paper we first propose an arithmetic-based system combination method to incorporate the system and keyword characteristics into the fusion procedure to enhance the effectiveness of system combination. The method incorporates a system-keyword dependent property, which is the number of acceptances in this paper, into the combination procedure. We then introduce a discriminative model to combine various useful system and\u00a0\u2026", "In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence.", "This paper presents a novel method for acoustic modeling with limited training data. The idea is to leverage on a well-trained acoustic model of a source language. In this paper, a conventional HMM/GMM triphone acoustic model of the source language is used to derive likelihood scores for each feature vector of the target language. These scores are then mapped to triphones of the target language using neural networks. We conduct a case study where Malay is the source language while English (Aurora-4 task) is the target language. Experimental results on the Aurora-4 (clean test set) show that by using only 7, 16, and 55 minutes of English training data, we achieve 21.58%, 17.97%, and 12.93% word error rate, respectively. These results outperform the conventional HMM/GMM and hybrid systems significantly.", "Recent studies in neural network-based monaural speech separation (SS) have achieved a remarkable success thanks to increasing ability of long sequence modeling. However, they would degrade significantly when put under realistic noisy conditions, as the background noise could be mistaken for speaker\u2019s speech and thus interfere with the separated sources. To alleviate this problem, we propose a novel network to unify speech enhancement and separation with gradient modulation to improve noise-robustness. Specifically, we first build a unified network by combining speech enhancement (SE) and separation modules, with multi-task learning for optimization, where SE is supervised by parallel clean mixture to reduce noise for downstream speech separation. Furthermore, in order to avoid suppressing valid speaker information when reducing noise, we propose a gradient modulation (GM) strategy to\u00a0\u2026", "Non-autoregressive end-to-end ASR framework might be potentially appropriate for code-switching recognition task thanks to its inherent property that present output token being independent of historical ones. However, it still under-performs the state-of-the-art autoregressive ASR frameworks. In this paper, we propose various approaches to boosting the performance of a CTC-mask-based non-autoregressive Transformer under code-switching ASR scenario. To begin with, we attempt diversified masking method that are closely related with code-switching point, yielding an improved baseline model. More importantly, we employ Minimum Word Error (MWE) criterion to train the model. One of the challenges is how to generate a diversified hypothetical space, so as to obtain the average loss for a given ground truth. To address such a challenge, we explore different approaches to yielding desired N-best-based\u00a0\u2026", "We focused on a study of comprehensive approaches to an improved code-switching speech recognition, using data augmentation and system combination methods. For data augmentation, we not only use speech speed perturbation based method, but we also attempt to add diversified room impulse response based reverberate noise, as well as music, babble, and white noise based additive noise. It is found we still can achieve significant performance improvement with such noise-corrupted data augmentation methods, though our SEAME code-switching data belongs to a clean corpus. In addition to data augmentation methods, we also adopt Minimum Bayesian risk-based lattice combination method to further improve our recognition results. We achieve significant word error rate (WER) reduction on lattice combination with/without recurrent neural network language model based lattice rescoring. Compared with\u00a0\u2026", "Named Entities are often rare words, and their transliteration across languages has been a challenging task. In this paper, we study a novel technique that segments a named entity into a sequence sub-words or characters. We propose to learn the transliteration mechanism using a sequence-to-sequence neural network. Applying the proposed technique to personal named transliteration on LDC dataset, we show impressive results with more than 10 BLEU score improvement over the competing statistic method on the same corpus.", "Nonnegative matrix factorization (NMF) is a popular method for source separation. In this paper, an alternating direction method of multipliers (ADMM) for NMF is studied, which deals with the NMF problem using the cost function of beta-divergence. Our study shows that this algorithm outperforms state-of-the-art algorithms on synthetic data sets, but it presents unstable behavior and low accuracy on real data sets. Therefore, we propose two different stable ADMM algorithms for NMF to solve this problem. They differ slightly in the multiplicative factor utilized in the update rules. One algorithm is to adapt the step size to guarantee the convergence while the other minimizes the beta-divergence with a pivot element weighting iterative method (PEWI). Experimental results demonstrate that the proposed algorithms are more stable and accurate. Particularly, PEWI based ADMM shows superior performance in the source\u00a0\u2026", "Synthetic speech is speech signals generated by text-to-speech (TTS) and voice conversion (VC) techniques. They impose a threat to speaker verification (SV) systems as an attacker may make use of TTS or VC to synthesize a speakers voice to cheat the SV system. To address this challenge, we study the detection of synthetic speech using long term magnitude and phase information of speech. As most of the TTS and VC techniques make use of vocoders for speech analysis and synthesis, we focus on differentiating speech signals generated by vocoders from natural speech. Log magnitude spectrum and two phase-based features, including instantaneous frequency derivation and modified group delay, were studied in this work. We conducted experiments on the CMU-ARCTIC database using various speech features and a neural network classifier. During training, the synthetic speech detection is formulated as\u00a0\u2026", "This paper proposes a phoneme lattice based TextTiling approach towards multilingual story segmentation. The phoneme is the smallest segmental unit in a language and the number of phonemes in a language is usually far smaller than the number of words. Furthermore, many phonemes are shared by different languages. These properties make phonemes particularly appropriate for representing multilingual speech. As phoneme recognition is far from perfect, phoneme lattices, which carry much richer statistics than the 1-best hypotheses, are adopted in this paper as the input to the TextTiling approach. The term frequencies used in traditional TextTiling are replaced by the expected counts of phoneme n-gram units calculated from phoneme lattices. Experiments on TDT2 English and Mandarin corpora show that the phoneme lattice based TextTiling outperforms the phoneme 1-best based TextTiling and word\u00a0\u2026", "Video composition is important for music video production. In this paper we propose an automatic method to assist the music sports video composition operation. Our approach is based on dynamic programming algorithm which finds a set of video shots that best matches the music. The method by default is fully-automatic, and users specification could be inserted to control the composition process, making it a semiautomatic system. This research has obvious importance to reduce manual processing, and enables the generation of high quality personalized music sports video. The proposed method is generic and fast. The experimental results are satisfactory", "Deep neural network based speech enhancement technique focuses on learning a noisy-to-clean transformation supervised by paired training data. However, the task-specific evaluation metric (e.g., PESQ) is usually non-differentiable and can not be directly constructed in the training criteria. This mismatch between the training objective and evaluation metric likely results in sub-optimal performance. To alleviate it, we propose a metric-oriented speech enhancement method (MOSE), which leverages the recent advances in the diffusion probabilistic model and integrates a metric-oriented training strategy into its reverse process. Specifically, we design an actor-critic based framework that considers the evaluation metric as a posterior reward, thus guiding the reverse process to the metric-increasing direction. The experimental results demonstrate that MOSE obviously benefits from metric-oriented training and\u00a0\u2026", "The estimation of speaker characteristics such as age and height is a challenging task, having numerous applications in voice forensic analysis. In this work, we propose a bi-encoder transformer mixture model for speaker age and height estimation. Considering the wide differences in male and female voice characteristics such as differences in formant and fundamental frequencies, we propose the use of two separate transformer encoders for the extraction of specific voice features in the male and female gender, using wav2vec 2.0 as a common-level feature extractor. This architecture reduces the interference effects during backpropagation and improves the generalizability of the model. We perform our experiments on the TIMIT dataset and significantly outperform the current state-of-the-art results on age estimation. Specifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49 years for male and female age estimation, respectively. Further experiment to evaluate the relative importance of different phonetic types for our task demonstrate that vowel sounds are the most distinguishing for age estimation.", "In this work, we describe the jointly submitted systems by Fortemedia Singapore (FMSG) and Nanyang Technological University (NTU) for DCASE 2022 Task 4: sound event detection in domestic environments. The proposed framework is divided into two stages: Stage-1 focuses on the audio-tagging system, which assists the sound event detection system in Stage-2. We train the Stage-1 utilizing a strongly labeled set converted into weak predictions, a weakly labeled set, and an unlabeled set to develop an effective audio-tagging system. This audio-tagging system is then used to infer on the unlabeled set to generate reliable pseudo-weak labels, which are used together with the strongly labeled set and weakly labeled set to train the sound event detection system at Stage-2. In Stage-1, we used two different networks, which are frequency dynamic (FDY)-convolutional recurrent neural network (CRNN) and convolutional neural network (CNN)-14 based pretrained audio neural networks (PANNs) for our developed systems. While the system at Stage-2 is based on FDY-CRNN for all the systems submitted to the challenge. It is noted that the systems at both stages employ data augmentation to reduce the risk of overfitting, and apply adaptive post-processing techniques to further enhance the performance. On the DESED real validation dataset, we obtain the highest PSDS1 and PSDS2 of 0.474 and 0.840, respectively.", "In this work, we study leveraging extra text data to improve low- resource end-to-end ASR under cross-lingual transfer learning setting. To this end, we extend the prior work [1], and propose a hybrid Transformer-LSTM based architecture. This architecture not only takes advantage of the highly effective encoding capacity of the Transformer network but also benefits from extra text data due to the LSTM-based independent language model network. We conduct experiments on our in-house Malay corpus which contains limited labeled data and a large amount of extra text. Results show that the proposed architecture outperforms the previous LSTM-based architecture [1] by 24.2% relative word error rate (WER) when both are trained using limited labeled data. Starting from this, we obtain further 25.4% relative WER reduction by transfer learning from another resource-rich language. Moreover, we obtain additional 13.6\u00a0\u2026", "Transformer, a state-of-the-art neural network architecture, has been used successfully for different sequence-to-sequence transformation tasks. This model architecture disperses the attention distribution over entire input to learn long-term dependencies, which is important for some sequence-to-sequence tasks, such as neural machine translation and text summarization. However, automatic speech recognition (ASR) has a characteristic to have monotonic alignment between text output and speech input. Techniques like Connectionist Temporal Classification (CTC), RNN Transducer (RNN-T) and Recurrent Neural Aligner (RNA) build on top of this monotonic alignment and use local encoded speech representations for corresponding token prediction. In this paper, we present an effective cross attention biasing technique in transformer that takes monotonic alignment between text output and speech input into consideration by making use of cross attention weights. Specifically, a Gaussian mask is applied on cross attention weights to limit the input speech context range locally given alignment information. We further introduce a regularizer for alignment regularization. Experiments on LibriSpeech dataset find that our proposed model can obtain improved output-input alignment for ASR, and yields 14.5%-25.0% relative word error rate (WER) reductions.", "In this work, we propose a novel framework for rescoring keyword search (KWS) detections using acoustic samples extracted from the training data. We view the keyword rescoring task as an information retrieval task and adopt the idea of query expansion. We expand a textual keyword with multiple speech keyword samples extracted from the training data. In this way, the hypothesized detections are compared with the multiple keywords using non-parametric approaches such as dynamic time warping (DTW). The obtained similarity scores are used in a graph based method to re-rank the original confidence scores estimated by the automatic speech recognition (ASR) systems. Experimental results on the NIST OpenKWS15 Evaluation show that our rescoring method is effective, especially for the subword system. For subword experiments, the graph-based rescoring with training samples obtains 5.1% and 1.5\u00a0\u2026", "In this paper we report our approaches to accomplishing the very limited resource keyword search (KWS) task in the NIST Open Keyword Search 2015 (OpenKWS15) Evaluation. We devised the methods, first, to attain better acoustic modeling, multilingual and semi-supervised acoustic model training as well as the examplar-based acoustic model training; second, to address the overwhelming out-of-vocabulary (OOV) KWS issue. Finally, we proposed a neural network (NN) framework to fuse diversified component systems, yielding improved combination results. Experimental results demonstrated the effectiveness of these approaches.", "In this paper, we propose a new temporal filter design method based on minimum KL divergence criterion for robust recognition of noisy and reverberant speech. The main idea is to optimize the filter parameters by minimizing the KL divergence of two distributions, of which one is the feature distribution in the test environment, and another is the feature distribution represented by the acoustic model. The minimization of the KL divergence reduces the mismatch between the acoustic model and the test data. Experimental results on Aurora-5 task shows that the new filter design outperforms other filter design methods significantly in noisy and reverberant test conditions. In addition, the proposed filtering of feature trajectories is shown to be complementary to linear transformation of feature vectors, which is popular in feature processing.", "Histogram equalization (HEQ) is a simple and effective feature normalization technique for robust speech recognition. Recently, we proposed to adapt HEQ transform to each test utterance using a maximum likelihood (ML) criterion and observed improved performance. In this paper, we further the study by applying attribute-based HEQ and its ML adaptation. Instead of applying a global HEQ transform to the test utterance, we propose to apply different HEQ transforms to the 6 manners of speech, eg vowel and fricative. We also developed the ML adaptation algorithm of the attribute-based HEQ. Experimental results show that the attribute-based HEQ adaptation obtained 21.8% and 19.5% relative error rate reduction over the global HEQ baseline on the Aurora-2 and Aurora-4 benchmarking tasks, respectively.", "We report the development of a Malay conversational speech corpus as part of our research in spontaneous conversational speech LVCSR. This corpus development effort is the collaboration between NTU and USM. The goal is to collect, transcribe, and annotate 50 hours of conversational Malay speech. The conversation is recorded from both close-talk and telephone channels, and both speakers' utterances are kept in separate tracks. Besides the word transcription, we also annotate linguistics phenomena such as fillers and disfluencies. To date, 20 hours have been recorded, transcribed and analyzed. The details of our analysis will be presented in this report.", "This paper presents a novel method for acoustic modeling of a new language with a limited amount of training data. In this approach, we use well-trained acoustic models of a foreign language to generate acoustic scores for each feature vector of the target language. These scores are then used as the input for mapping to context dependent triphones of the target language using a limited amount of training data. With this approach, we do not need to modify or have a special requirement for the foreign acoustic models. In this paper, English is used as the foreign language while Malay is used as the target language. Experiments on a Malay large vocabulary continuous speech recognition (LVCSR) task show that with using only few minutes of training data we can achieve a low word error rate which outperforms the best monolingual baseline acoustic model significantly.", "This paper studies target-oriented phone selection strategy for constructing phone tokenizers in the Parallel Phone Recognizers followed by Vector Space Model (PPR-VSM) paradigm of spoken language recognition. With this phone selection strategy, one derives a set of target-oriented phone tokenizers (TOPT), each having a subset of phones that have high discriminative ability for a target language. Two phone selection methods are proposed to derive such phone subsets from a phone recognizer. We show that the TOPTs derived from a universal phone recognizer (UPR) outperform those derived from language specific phone recognizers. The TOPT front-end derived from a UPR also consistently outperforms the UPR front-end without involving additional acoustic modeling. We achieve an equal error rates (EERs) of 1.33%, 1.75% and 2.80% on NIST 1996, 2003 and 2007 LRE databases respectively for 30\u00a0\u2026", "Deep neural network based speech enhancement approaches aim to learn a noisy-to-clean transformation using a supervised learning paradigm. However, such a trained-well transformation is vulnerable to unseen noises that are not included in training set. In this work, we focus on the unsupervised noise adaptation problem in speech enhancement, where the ground truth of target domain data is completely unavailable. Specifically, we propose a generative adversarial network based method to efficiently learn a converse clean-to-noisy transformation using a few minutes of unpaired target domain data. Then this transformation is utilized to generate sufficient simulated data for domain adaptation of the enhancement model. Experimental results show that our method effectively mitigates the domain mismatch between training and test sets, and surpasses the best baseline by a large margin.", "Neural speech enhancement degrades significantly in face of unseen noise. To address such mismatch, we propose to learn noise-agnostic feature representations by disentanglement learning, which removes the unspecified noise factor, while keeping the specified factors of variation associated with the clean speech. Specifically, a discriminator module is introduced to distinguish the type of noises, which is referred to as the disentangler. With the adversarial training strategy, a gradient reversal layer seeks to disentangle the noise factor and remove it from the feature representation. Experiment results show that the proposed approach achieves 5.8% and 5.2% relative improvements over the best baseline in terms of perceptual evaluation of the speech quality (PESQ) and segmental signal-to-noise ratio (SSNR), respectively. The ablation study indicates that the proposed disentangler module is also effective in\u00a0\u2026", "Automatic height and age estimation of speakers using acoustic features is widely used for the purpose of human-computer interaction, forensics, etc. In this work, we propose a novel approach of using attention mechanism to build an end-to-end architecture for height and age estimation. The attention mechanism is combined with Long Short-Term Memory(LSTM) encoder which is able to capture long-term dependencies in the input acoustic features. We modify the conventionally used Attention -- which calculates context vectors the sum of attention only across timeframes -- by introducing a modified context vector which takes into account total attention across encoder units as well, giving us a new cross-attention mechanism. Apart from this, we also investigate a multi-task learning approach for jointly estimating speaker height and age. We train and test our model on the TIMIT corpus. Our model outperforms several approaches in the literature. We achieve a root mean square error (RMSE) of 6.92cm and6.34cm for male and female heights respectively and RMSE of 7.85years and 8.75years for male and females ages respectively. By tracking the attention weights allocated to different phones, we find that Vowel phones are most important whistlestop phones are least important for the estimation task.", "Traditional sentence representations such as bag-of-words (BOW) and term frequency-inverse document frequency (tf-idf) face the problem of data sparsity and may not generalize well. Neural network based representations such as word/sentence vectors are usually trained in an unsupervised way and lack the topic information which is important for story segmentation. In this paper, we propose to learn sentence representation by using deep neural network (DNN) to directly predict the topic class of the input sentence. By using supervised training, the learned vector representation of sentences contains more topic information and is more suitable for the story segmentation task. The input of the DNN is BOW vector computed from a context window. Multiple time resolution BOW and bottleneck features (BNF) are also introduced to enhance the performance of story segmentation. As text data labeled with topic\u00a0\u2026", "The real-time speech emotion recognition system is not only required to achieve the high accuracy, but also is needed to consider the memory requirement and running time in the practical application. This paper focuses on exploring the effective features with lower memory requirement and running time for the real-time speech emotion recognition system. To this end, the fixed-dimensional speech representations are considered because of its lower memory requirement and less computation cost. This paper investigates two types of fixed-dimensional speech representations which are high level descriptors and i-vectors and compares them with the conventional frame-based features low level descriptors in terms of accuracy and computation cost. Experimental results on IEMOCAP database show that although high level descriptors and i-vectors only contain the compact information comparing with low level\u00a0\u2026", "Low intelligibility of Air Traffic Control (ATC) speech is one major cause of aircraft accidents every year. Many factors can affect speech intelligibility, among which the most prominent aspects is the high speaking rate commonly present in ATC speech. Hence, a possible solution would be to improve intelligibility by artificially lengthening the spoken utterance to lower the speaking rate. In this work, we explore the lengthening of clean recorded ATC utterances by first identifying phoneme sequences in a given utterance. Such identified phoneme segments can then be lengthened. We will examine effects of lengthening vowels-only, consonants-only, or homogeneous lengthening. To verify our approach, we will conduct human listening test to evaluate the intelligibility. The results show 74.67% was obtained in AB preference test.", "The reverberation time, T60, is an important indicator of the reverberation strength in a room and has many applications in speech processing, such as dereverberation. However, the T60 must be blindly estimated if only reverberant speech is available. In this paper, we provide a learning based approach for T60 estimation. We treat the T60 estimation as a classification problem by dividing the T60 range into countable bins (eg 19 bins covering 0.1 s to 1s with a bin width of 0.05 s) and the estimation becomes predicting which bin the true T60 falls into for a given speech. We use deep neural networks (DNN) to learn such a mapping from speech to the T60. The DNN is trained on a large amount of reverberant and noisy speech signals generated from various simulated rooms with known reverberations. After training, we observe that the DNN can learn highly sensible features for the T60 estimation task. Experimental\u00a0\u2026", "In this paper, we propose to use speech modulation features for robust nonnative accent detection. Modulation spectrum carries long term temporal information of speech and may discriminate accents of native and nonnative speakers. For each speech segment to be tested, we extract a 10 dimension feature vector from modulation spectrum and use it for model training and testing. The proposed modulation features are compared with other popular features such as pitch and formant on a nonnative French accent detection task. Results show that the modulation features produce good detection performance and are quite robust to channel distortions. In addition, when combine test scores of modulation features and pitch features, performance is further significantly reduced. The best equal error rate is 13.1% by fusing pitch and modulation-based systems.", "We propose a Near-Duplicate Keyframe (NDK) retrieval method that can handle extreme zooming and significant object motion. The first stage consists of eliminating false keypoint matches using symmetric property and a ratio of nearest and second-nearest neighbor distances. Then, a pattern coherency score is assigned to each pair of keyframes. These two features are combined through linear discriminant analysis (LDA) and the separating boundary is trained using SVM. Experiments are carried out for NDK retrieval on the Columbia and NTU datasets. The promising results confirm the effectiveness of our keypoint matching algorithm and show distinguishing power of our proposed features and feature weighting role in NDK retrieval.", "In this paper, we study speaker characterization using prosodic supervectors with negative within-class covariance normalization (NWCCN) projection and speaker modeling with support vector regression (SVR). We also propose a segmental weight fusion (SWF) technique that combines acoustic and prosodic subsystems effectively, despite the big performance gap between the subsystems. We validate the effectiveness of our proposed techniques on the NIST 2006 Speaker Recognition Evaluation (SRE) in comparison with other prominent solutions. The experiments have reported competitive results of 17.72% Equal Error Rate for the prosodic subsystem alone and 4.50% for the fusion system on NIST 2006 SRE core test condition.", "Due to the widespread use of microcontroller unit (MCU) in application and education areas, there is a need for initialization code generation for MCUs to reduce developers' effort and to aid beginners' learning of MCU programming. A common realization for initialization code generation software is a graphical user interface (GUI) with all the available initialization settings for user selection, and the initialization code can be generated based on the user's configuration. This has been achieved by several software using different implementations. This paper evaluates two current initialization code generation software to identify the existing problems and requirements. It then presents a methodology to describe MCU structure to support the software's initialization code generation functionality to achieve flexibility and effectiveness. Our approach observes low development cost, enables user contribution to define and\u00a0\u2026", "We evaluate the temporal structure normalisation (TSN), a feature normalisation technique for robust speech recognition, on the large vocabulary Aurora-4 task. The TSN technique operates by normalising the trend of the feature\u2019s power spectral density (PSD) function to a reference function using finite impulse response (FIR) filters. The features are the cepstral coefficients and the normalisation procedure is performed on every cepstral channel of each utterance. Experimental results show that the TSN reduces the average word error rate (WER) by 7.20% and 8.16% relatively over the mean-variance normalisation (MVN) and the histogram equalisation (HEQ) baselines respectively. We further evaluate two other state-of-the-art temporal filters. Experimental results show that among the three evaluated temporal filters, the TSN filter performs the best. Lastly, our results also demonstrates that fixed smoothing filters\u00a0\u2026", "The achievable bit error rate of a linear equalizer is crucially determined by the choice of a decision delay parameter. This brief paper presents a simple method for the efficient determination of the optimal decision delay parameter that results in the best bit error rate performance for a linear equalizer.", "This paper presents an algorithm to select the parameters of a radial basis function network based on the orthogonal least squares (OLS) learning algorithm. To improve the OLS learning process, an additional procedure to modify the selected node's parameter during training is introduced. Using simulation results, we show that significant improvement to the selected model's performance can be achieved by the proposed algorithm.", "To let the state-of-the-art end-to-end ASR model enjoy data efficiency, as well as much more unpaired text data by multi-modal training, one needs to address two problems: 1) the synchronicity of feature sampling rates between speech and language (aka text data); 2) the homogeneity of the learned representations from two encoders. In this paper we propose to employ a novel bidirectional attention mechanism (BiAM) to jointly learn both ASR encoder (bottom layers) and text encoder with a multi-modal learning method. The BiAM is to facilitate feature sampling rate exchange, realizing the quality of the transformed features for the one kind to be measured in another space, with diversified objective functions. As a result, the speech representations are enriched with more linguistic information, while the representations generated by the text encoder are more similar to corresponding speech ones, and therefore the\u00a0\u2026", "Jointly learning from a small labeled set and a larger unlabeled set is an active research topic under semi-supervised learning (SSL). In this paper, we propose a novel SSL method based on a two-stage framework for leveraging a large unlabeled in-domain set. Stage-1 of our proposed framework focuses on audio-tagging (AT), which assists the sound event detection (SED) system in Stage-2. The AT system is trained utilizing a strongly labeled set converted into weak predictions referred to as weakified set, a weakly labeled set, and an unlabeled set. This AT system then infers on the unlabeled set to generate reliable pseudo-weak labels, which are used with the strongly and weakly labeled set to train a frequency dynamic convolutional recurrent neural network-based SED system at Stage-2 in a supervised manner. Our system outperforms the baseline by 45.5% in terms of polyphonic sound detection score on the DESED real validation set.", "This paper introduces MedBERT, a new pre-trained transformer-based model for biomedical named entity recognition. MedBERT is trained with 57.46M tokens collected from biomedical-related data sources, i.e. datasets acquired from N2C2, BioNLP, CRAFT challenges, and biomedical-related articles crawled from Wikipedia. We validate the effectiveness of MedBERT by comparing it with four publicly available pre-trained models on ten biomedical datasets from BioNLP and CRAFT shared tasks. Our experimental results show that models fine-tuned on MedBERT achieve state-of-the-art performance in nine datasets that predict Protein, Gene, Chemical, Cellular/Component, Gene Ontology, and Taxonomy entities. Specifically, the model achieved an average of 84.04% F1-micro score on ten test sets from BioNLP and CRAFT challenges with an improvement of 3.7% and 7.83% as compared to models that were\u00a0\u2026", "Catastrophic forgetting is a thorny challenge when updating keyword spotting (KWS) models after deployment. This problem will be more challenging if KWS models are further required for edge devices due to their limited memory. To alleviate such an issue, we propose a novel diversity-aware incremental learning method named Rainbow Keywords (RK). Specifically, the proposed RK approach introduces a diversity-aware sampler to select a diverse set from historical and incoming keywords by calculating classification uncertainty. As a result, the RK approach can incrementally learn new tasks without forgetting prior knowledge. Besides, the RK approach also proposes data augmentation and knowledge distillation loss function for efficient memory management on the edge device. Experimental results show that the proposed RK approach achieves 4.2% absolute improvement in terms of average accuracy over the best baseline on Google Speech Command dataset with less required memory. The scripts are available on GitHub.", "With the recent development of end-to-end models in speech recognition, there have been more interests in adapting these models for online speech recognition. However, using end-to-end models for online speech recognition is known to suffer from an early endpointing problem, which brings in many deletion errors. In this paper, we propose to address the early endpointing problem from the gradient perspective. Specifically, we leverage on the recently proposed ScaleGrad technique, which was proposed to mitigate the text degeneration issue. Different from ScaleGrad, we adapt it to discourage the early generation of the end-of-sentence () token. A scaling term is added to directly maneuver the gradient of the training loss to encourage the model to learn to keep generating non- tokens. Compared with previous approaches such as voice-activity-detection and end-of-query detection, the proposed method does\u00a0\u2026", "Speech bandwidth extension aims to generate a wideband signal from a narrowband (low-band) input by predicting the missing high-frequency components. It is believed that the general knowledge about the speaker and phonetic content strengthens the prediction. In this paper, we propose to augment the low-band acoustic features with i-vector and phonetic posteriorgram (PPG), which represent speaker and phonetic content of the speech, respectively. We also propose a residual dual-path network (RDPN) as the core module to process the augmented features, which fully utilizes the utterance-level temporal continuity information and avoids gradient vanishing. Experiments show that the proposed method achieves 20.2% and 7.0% relative improvements over the best baseline in terms of log-spectral distortion (LSD) and signal-to-noise ratio (SNR), respectively. Furthermore, our method is 16 times more compact than the best baseline in terms of the number of parameters.", "Automatic speech recognition systems currently deliver an unpunctuated sequence of words which is hard to peruse for human and degrades the performance of the downstream natural language processing tasks. In this paper, we propose a hybrid approach for Sentence Unit Detection, in which the focus is on adding the full stop [.]to the unstructured text. Our model profits from the advantage of two dominant deep learning architectures: (i)the ability to learn the long dependencies in both directions of a bidirectional Long Short-Term Memory; (ii)the ability to capture the local context with Convolutional Neural Networks. We also empirically study the training objective of our networks using extra-loss and further investigate the impacts of each model component on the overall result. Experiments conducted on two large-scale datasets demonstrated that the proposed architecture outperforms previous separated\u00a0\u2026", "This paper presents an eigenvector clustering approach for estimating the direction of arrival (DOA) of multiple speech signals using a microphone array. Existing clustering approaches usually only use low frequencies to avoid spatial aliasing. In this study, we propose a probabilistic eigenvector clustering approach to use all frequencies. In our work, time-frequency (TF) bins dominated by only one source are first detected using a combination of noise-floor tracking, onset detection and coherence test. For each selected TF bin, the largest eigenvector of its spatial covariance matrix is extracted for clustering. A mixture density model is introduced to model the distribution of the eigenvectors, where each component distribution corresponds to one source and is parameterized by the source DOA. To use eigenvectors of all frequencies, the steering vectors of all frequencies of the sources are used in the distribution\u00a0\u2026", "The I4U\u2019s submission to SRE\u201916 was a result from the collaboration and active exchange of information among researchers across sixteen Institutes and Universities across 4 continents. The submitted results were based on the fusion of multiple classifiers. A lot of efforts have been devoted to two major challenges, namely, test duration variability and dataset shift from Switchboard and Mixer corpora to the new Call My Net dataset.", "The goal of sentence boundary detection (SBD) is to predict the presence/absence of sentence boundary in an unstructured word sequence, where there is no punctuation presented. In this paper, we propose a feature selection approach to obtain more effective features used for the SBD classifier. Specifically, the observed words are considered its correlation with the sentence boundary based on the pointwise mutual information before being used as the feature of the classifier. By using the linear chain CRF model to predict sentence boundaries of a text sequence, the experimental results on a part of the English Gigaword  Edition corpus show that the proposed method helps to reduce the number of model parameters up\u00a0to 44.87\u00a0% while maintaining a comparable F1-score to the original model.", "This paper presents a deep neural network-conditional random field (DNN-CRF) system with multi-view features for sentence unit detection on English broadcast news. We proposed a set of multi-view features extracted from the acoustic, articulatory, and linguistic domains, and used them together in the DNN-CRF model to predict the sentence boundaries. We tested the accuracy of the multi-view features on the standard NIST RT-04 English broadcast news speech data. Experiments show that the best system outperforms the state-of-the-art sentence unit detection system significantly by 13.2% absolute NIST sentence error rate reduction using the reference transcription. However, the performance gain is limited on the recognized transcription partly due to the high word error rate.", "This paper presents an efficient way to construct the self-similarity matrix, a popular approach, to detect repeating segments in music. Our proposed method extends the sparse suffix tree construction algorithm to accept vectors as input to construct an initial selection of repeating sequences to generate a sparse self-similarity matrix. Our proposed insertion criterion does not only rely on vector-to-vector similarity but also measures the similarity between two subsequences in its insertion criteria. As such, our method is more robust as compared to approaches that simply quantize the input vectors into symbols for suffix tree construction. In addition, the proposed method is efficient in both computation and memory storage. Our experimental results showed that the proposed approach obtains similar average F1 score as compared to the traditional self-similarity approach with much less computational cost and memory\u00a0\u2026", "The advancement of technology in recent decades has brought computing beyond the desktop computer. With the focus on portable and interactive devices, it is necessary to equip the next generation of engineers with the necessary programming skills and a robust set of tools to create a wider range of applications. In the School of Computer Engineering at Nanyang Technological University (NTU), Singapore, we are developing a scalable microcontroller peripheral system called Micro-eBlocks to facilitate the studentpsilas learning of microcontroller based modules. The Micro-eBlocks are a suite of mix and match and plug-and-play input-output (I/O), sensor, communication and processor modules, which can be used to address a wide variety of embedded systems application needs. This paper describes the design philosophy and how it can be used to train embedded system engineers.", "This paper describes our recent efforts in exploring effective discriminative features for speaker recognition. There is an obvious trend in the past few years that the information fusion from multiple resources is critical to improve the performance of speaker recognition system. The extracted information for speaker recognition varies from acoustic features to high dimensional vectors with different levels of tokenization. In the IIR NIST 2006 Speaker Recognition System, we integrated cepstral GMM modeling, cepstral SVM modeling and tokenization at both phone level and frame level. The experimental results on both NIST 2005 SRE corpus and NIST 2006 SRE corpus are presented.", "Most of the existing neural-based models for keyword spotting (KWS) in smart devices require thousands of training samples to learn a decent audio representation. However, with the rising demand for smart devices to become more person-alized, KWS models need to adapt quickly to smaller user samples. To tackle this challenge, we propose a contrastive speech mixup (CosMix) learning algorithm for low-resource KWS. CosMix introduces an auxiliary contrastive loss to the existing mixup augmentation technique to maximize the relative similarity between the original pre-mixed samples and the augmented samples. The goal is to inject enhancing constraints to guide the model towards simpler but richer content-based speech representations from two augmented views (i.e. noisy mixed and clean pre-mixed utterances). We conduct our experiments on the Google Speech Command dataset, where we trim the size\u00a0\u2026", "Existing self-supervised pre-trained speech models have offered an effective way to leverage massive unannotated corpora to build good automatic speech recognition (ASR). However, many current models are trained on a clean corpus from a single source, which tends to do poorly when noise is present during testing. Nonetheless, it is crucial to overcome the adverse influence of noise for real-world applications. In this work, we propose a novel training framework, called deHuBERT, for noise reduction encoding inspired by H. Barlow\u2019s redundancy-reduction principle. The new framework improves the HuBERT training algorithm by introducing auxiliary losses that drive the self- and cross-correlation matrix between pairwise noise-distorted embeddings towards identity matrix. This encourages the model to produce noise- agnostic speech representations. With this method, we report improved robustness in noisy\u00a0\u2026", "Automatic speech recognition (ASR) has gained a remarkable success thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be completely eliminated. In this paper, we propose a self-supervised framework named Wav2code to implement a generalized SE without distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the original clean representations, in order to store them in codebook as prior. Second, during finetuning we propose a Transformer-based code predictor to accurately predict clean codes by modeling the global dependency of input noisy representations, which enables discovery and restoration of high-quality clean representations without distortions. Furthermore, we propose an interactive feature fusion network to combine original noisy and the restored clean representations to consider both fidelity and quality, resulting in even more informative features for downstream ASR. Finally, experiments on both synthetic and real noisy datasets demonstrate\u00a0\u2026", "Baby cry detection in domestic environments is an essential component in baby monitoring systems, studying sleep cycle patterns in infants, and developing other diagnostic tools. In this work, we explore the state-of-the-art convolutional recurrent neural network (CRNN) and proposed replacement using depth-wise-separable (DWS) convolutions to have a low-complexity model for baby cry sound detection application. The studies are carried out on a dataset curated from AudioSet, which contains baby cry sounds and several other sounds that occur commonly in a domestic household environment. We also perform various data augmentation methods, and a few post-processing techniques to enhance the robustness of our baby cry sound detection system. The studies show that our low-complexity model developed using DWS achieves promising results by only using 3% of the parameters of the standard CRNN\u00a0\u2026", "Continuously learning new classes without catastrophic forgetting is a challenging problem for on-device environmental sound classification given the restrictions on computation resources (e.g., model size, running memory). To address this issue, we propose a simple and efficient continual learning method. Our method selects the historical data for the training by measuring the per-sample classification uncertainty. Specifically, we measure the uncertainty by observing how the classification probability of data fluctuates against the parallel perturbations added to the classifier embedding. In this way, the computation cost can be significantly reduced compared with adding perturbation to the raw data. Experimental results on the DCASE 2019 Task 1 and ESC-50 dataset show that our proposed method outperforms baseline continual learning methods on classification accuracy and computational efficiency, indicating our method can efficiently and incrementally learn new classes without the catastrophic forgetting problem for on-device environmental sound classification.", "Internal Language Model Estimation (ILME) based language model (LM) fusion has been shown significantly improved recognition results over conventional shallow fusion in both intra-domain and cross-domain speech recognition tasks. In this paper, we attempt to apply our ILME method to cross-domain code-switching speech recognition (CSSR) work. Specifically, our curiosity comes from several aspects. First, we are curious about how effective the ILME-based LM fusion is for both intra-domain and cross-domain CSSR tasks. We verify this with or without merging two code-switching domains. More importantly, we train an end-to-end (E2E) speech recognition model by means of merging two monolingual data sets and observe the efficacy of the proposed ILME-based LM fusion for CSSR. Experimental results on SEAME that is from Southeast Asian and another Chinese Mainland CS data set demonstrate the effectiveness of the proposed ILME-based LM fusion method.", "Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.58% relative WER reduction, and surpasses the finetuning method by up to relative 2.54%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53% with only a few epochs of training.", "Spoken term detection (STD) systems rank hypothesized detections by scores, which indicate how confident a hypothesized detection is a true instance of the keyword. Many STD systems rely on automatic speech recognition (ASR) to transcribe the speech content into the lattice representation. In such STD systems, the detection scores are usually estimated as the posterior probabilities of the keyword in the decoding lattices. Such scores may be inaccurate, e.g. due to the imperfect modeling of speech and noise. To improve the ranking of hypothesized detections, we propose to directly utilize the acoustic similarity scores between the speech signal of hypothesized detections and that of the keyword exemplars. A keyword exemplar is a true instance of the keyword obtained from an annotated speech corpus. When no exemplar is available, we propose to synthesize exemplars from the annotated speech corpus\u00a0\u2026", "We present low-resource spoken keyword search (KWS) strategies guided by distinctive feature theory in linguistics to conduct data selection, feature selection, and transcription augmentation. These strategies were employed in the context of the 2016 NIST Open Keyword Search Evaluation (OpenKWS16) using conversational Georgian from the IARPA Babel program. In particular, we elaborate on the following: (1) We exploit glottal-source-related acoustic features that characterize Georgian ejective phonemes ([+constricted glottis], [+raised larynx ejective] specified in distinctive feature theory). These features complement standard acoustic features, leading to a relative fusion gain of 11.9%. (2) We use noisy channel models to incorporate probabilistic phonetic transcriptions from mismatched crowdsourcing to conduct transfer learning to improve KWS for extremely under-resourced conditions (24 min of\u00a0\u2026", "Synthesized speech poses a serious threat to speaker verification systems, which is aggravated by speech synthesis systems becoming more freely available and easily adaptable to a target speaker. This motivated research into synthetic speech detection to circumvent the threat. Although current algorithms are effective in the detection of HMM-based speech synthesizers, unit selection based speech synthesizers remain a serious threat due to its ability to generate spoofing speech which easily overcame existing detectors. Current error rates for their detection is a lot higher than that obtained for other spoofing methods. This paper proposes a detection algorithm to counter unit selection based synthesis speech. It is free of training and exploits presence of artifacts in image spectrogram to perform detection. To the best of our knowledge, this is the first attempt targeted for unit selection based synthesis speech\u00a0\u2026", "Kernel density model works well for limited training data in acoustic modeling. In this paper, we improve the kernel density-based acoustic model for low resource language speech recognition. In our previous study, we demonstrated the effectiveness of the kernel density-based acoustic model on discriminative features such as cross-lingual bottleneck features. In this paper, we propose to learn a Mahalanobis-based distance, which is equivalent to a full rank linear feature transformation, to minimize training data frame classification error. Experimental results on the Wall Street Journal (WSJ) task show that the proposed Mahalanobis-based distance learning results in significant improvements over the Euclidean distance. The kernel density acoustic model with the Mahalanobis-based distance also outperforms deep neural network acoustic model significantly in limited training data cases.", "Temporal filtering of feature trajectories and linear transformation of feature vectors are two effective ways to compensate the speech features to achieve robust speech recognition in noisy and reverberant environments. In the previous studies, as the two methods are usually applied in sequence, the interaction between the two methods is not optimized. In this paper, we propose a generalized transform which integrates temporal filter and linear transformation into a single process. The new transform parameters are optimized to minimize an approximated Kullback-Leibler divergence between the distribution of the compensated features and the distribution represented by a clean reference model. The proposed method is evaluated on the Aurora-5 clean condition training task. The experiments show that the generalized transform significantly outperforms the simple cascade of temporal filtering and linear\u00a0\u2026", "In this paper, we study a novel way to compensate speech features to counter the effects of speaker variations and environment distortions in speech recognition. For each homogeneous cluster of speech data, e.g. a specific speaker and environment combination, a set of correction vectors are learnt. A correction vector measures the deviation of features in a small region of feature space due to the speaker and environment effects. From a heterogenous training set, dozens of sets of correction vectors are learnt, each from a homogenous subset of the data. During testing, those correction vector sets are linearly combined to compensate test feature vectors. The combination weights are estimated by maximizing the likelihood (ML) of the compensated features with respect to a reference model, which is a simplified version of the acoustic model used for speech recognition. In addition, variance compensation is applied\u00a0\u2026", "This paper introduces a novel approach to auto-matically discover recurrent speech patterns from multi-speaker corpus without a priori knowledge. The proposed approach is based on the sub-word acoustic units and it iteratively concatenates the most-likely joint sub-word units to produce a longer acoustic unit till our proposed stop criterion is satisfied. Among the resulting acoustic units, the units with the most stable number of occurrences are selected as the lexicon. The proposed approach has been applied to automatically discover English words from TIDIGIT corpus. The experimental results measured by F1 score showed the proposed approach can effectively detect and extract the recurrent patterns. This technique can be used for lexicon generation from an unknown speech corpus or in audio content summarization.", "Classical mean and variance normalization (MVN) uses a diagonal transform and a bias vector to normalize the mean and variance of noisy features to reference values. As MVN uses diagonal transform, it ignores correlation between feature dimensions. Although full transform is able to make use of feature correlation, its large amount of parameters may not be estimated reliably from a short observation, eg 1 utterance. We propose a novel structured full transform that has the same amount of free parameters as diagonal transform while being able to capture correlation between feature dimensions. The proposed structured transform can be estimated reliably from one utterance by maximizing the likelihood of the normalized features on a reference Gaussian mixture model. Experimental results on Aurora-4 task show that the structured transform produces consistently better speech recognition results than diagonal\u00a0\u2026", "This paper presents the development of a hidden Markov model (HMM)-based Malay text-to-speech (TTS) system. To our knowledge, this is the first report on the development of the HMM-based speech synthesis system for the Malay language. In this paper, We first discuss the Malay speech characteristics, specifically, on Malay phonological system and syllable structure. In the Malay phonological system, 37 phonemes are adopted as the phonemic representations. Then, we describe a HMM-based TTS framework and language specific knowledge such as phonological, linguistic information, and utterance structure, which is used in context dependent continuous HMM and treebased clustering. After that, we report the development of Malay TTS corpora. Finally, a male and a female HMM-based Malay TTS systems are developed and evaluated. We further conduct listening test based on the Mean Opinion Score (MOS), and the results show that the developed HMM-based Malay TTS system can generate speech with acceptable quality in terms of naturalness and intelligibility.", "The widespread use of micro-controllers in the fields of education and application development have led to the need for a generic initialization code generator tool to aid tertiary students as well as developers. With this aim a generic description methodology for MCUs was developed by the authors of this paper. The description methodology bundled with a Java based MCU initialization code generator software was put to test using data from the H8S-2377 MCU. This paper presents a thorough analysis of the structural aspects of an MCU, the various features of MCU peripherals and initialization sequences to evaluate the completeness of the MCU description methodology. To this effect we identified the dependency problem whereby the setting for certain registers in an MCU is a mathematical function of other register settings. This feature of an MCU register, referred to as a dependency, is not supported by the\u00a0\u2026", "An on-line algorithm for blind equalization of an FIR channel is proposed by minimizing the mutual information of the output. The algorithm is closely related to the blind separation algorithm based on independent component analysis. It is assumed that the channel impulse responses are unknown and the channel may be non-minimum phase. The algorithm is implemented on a linear neural network in which the weight matrix is updated by the proposed algorithm. The simulation results are used to demonstrate the e ectiveness of the algorithm.", "This paper presents the work of restoring punctuation for ASR transcripts generated by multilingual ASR systems. The focus languages are English, Mandarin, and Malay which are three of the most popular languages in Singapore. To the best of our knowledge, this is the first system that can tackle punctuation restoration for these three languages simultaneously. Traditional approaches usually treat the task as a sequential labeling task, however, this work adopts a slot-filling approach that predicts the presence and type of punctuation marks at each word boundary. The approach is similar to the Masked-Language Model approach employed during the pre-training stages of BERT, but instead of predicting the masked word, our model predicts masked punctuation. Additionally, we find that using Jieba 1 1 https://github.com/fxsjy/jieba instead of only using the built-in SentencePiece tokenizer of XLM-R can\u00a0\u2026", "Noise robustness in keyword spotting remains a challenge as many models fail to overcome the heavy influence of noises, causing the deterioration of the quality of feature embeddings. We proposed a contrastive regularization method called Inter-Intra Contrastive Regularization (I2CR) to improve the feature representations by guiding the model to learn the fundamental speech information specific to the cluster. This involves maximizing the similarity across Intra and Inter samples of the same class. As a result, it pulls the instances closer to more generalized representations that form more prominent clusters and reduces the adverse impact of noises. We show that our method provides consistent improvements in accuracy over different backbone model architectures under different noise environments. We also demonstrate that our proposed framework has improved the accuracy of unseen out-of-domain noises\u00a0\u2026", "It is critical for a keyword spotting model to have a small footprint as it typically runs on-device with low computational resources. However, maintaining the previous SOTA performance with reduced model size is challenging. In addition, a far-field and noisy environment with multiple signals interference aggravates the problem causing the accuracy to degrade significantly. In this paper, we present a multi-channel ConvMixer for speech command recognitions. The novel architecture introduces an additional audio channel mixing for channel audio interaction in a multi-channel audio setting to achieve better noise-robust features with more efficient computation. Besides, we proposed a centroid based awareness component to enhance the system by equipping it with additional spatial geometry information in the latent feature projection space. We evaluate our model using the new MISP challenge 2021 dataset. Our model achieves significant improvement against the official baseline with a 55% gain in the competition score (0.152) on raw microphone array input and a 63% (0.126) boost upon front-end speech enhancement.", "Automatic age and height estimation of speakers using acoustic features is widely used for the purpose of human-computer interaction, forensics, etc. In this work, we study end-to-end framework for age and height estimation. We first propose a novel attention mechanism, named cross-attention. Different from conventionally used attention, which calculates context vector as the sum of attention only across timeframes, the proposed approach introduces a modified context vector which takes into account total attention across both time-frames and encoder units. We further propose using triplet loss to enhance the discriminative power of the encoder. We evaluate the Root Mean Square Error (RMSE) of pro-posed approaches on the TIMIT corpus. The proposed cross-attention outperforms the conventional counterpart for both age and height estimation while the triplet loss brings 8% relative improvement for age\u00a0\u2026", "Speaker profiling, which aims to estimate speaker characteristics such as age and height, has a wide range of applications inforensics, recommendation systems, etc. In this work, we propose a semisupervised learning approach to mitigate the issue of low training data for speaker profiling. This is done by utilizing external corpus with speaker information to train a better representation which can help to improve the speaker profiling systems. Specifically, besides the standard supervised learning path, the proposed framework has two more paths: (1) an unsupervised speaker representation learning path that helps to capture the speaker information; (2) a consistency training path that helps to improve the robustness of the system by enforcing it to produce similar predictions for utterances of the same speaker.The proposed approach is evaluated on the TIMIT and NISP datasets for age, height, and gender estimation, while the Librispeech is used as the unsupervised external corpus. Trained both on single-task and multi-task settings, our approach was able to achieve state-of-the-art results on age estimation on the TIMIT Test dataset with Root Mean Square Error(RMSE) of6.8 and 7.4 years and Mean Absolute Error(MAE) of 4.8 and5.0 years for male and female speakers respectively.", "Overlapped speech is widely present in conversations and can cause significant performance degradation on speech processing such as diarization, enhancement, and recognition. Detection of overlapped speech, in particular when the speakers are in the far-field, is a challenging task as the overlapped part is usually short, and heavy reverberation and noise may present in the conversation scenario. Existing solutions overwhelmingly rely on spectral features extracted from single microphone signal to perform the detection. In this paper, we propose a novel detection approach which is able to use a microphone array and fuse the spatial and spectral features extracted from multi-channel array signal. Two categories of spatial features, directional statistics which are projected to spherical location grids and generalized cross-correlation function based on phase transform (GCC-PHAT), are considered to model the\u00a0\u2026", "This paper proposes an end-to-end story segmentation approach based on long short-term memory (LSTM) - recurrent neural network (RNN). Traditional story segmentation approaches are a two-stage pipeline consisting of feature extraction and segmentation, each of which has its individual objective function. In other words, the objective function used to extract features is different from the true performance measure of story segmentation, which may degrade the segmentation results. In this paper, we combine the two components and optimize them jointly, using an LSTM-RNN. Specifically, one LSTM layer is used to extract sentence vectors, and another LSTM layer is used to predict story boundaries by taking as input of the sentence vectors. Importantly, the whole network is optimized directly to predict story boundaries. We also investigate bi-directional LSTM (BLSTM) that can utilize past and future information\u00a0\u2026", "In this paper, we propose a partial search approach for subword-based keyword search (KWS) systems. The proposed approach addresses the problem of high miss rate in the conventional full sequence matching approach by retaining detections that only contain some partial sequences. To control the increase of false alarm (FA), we propose two pruning methods, global threshold pruning (GTP) and keyword-specific pruning (KSP). The former uses the same threshold for all keywords; while the latter gradually adjusts a specific threshold for each keyword. Experimental results on the NIST OpenKWS15 and OpenKWS16 Evaluations show that partial search using KSP is effective. For out-of-vocabulary keywords, our proposed approach reduces the miss rate up to 21.0% absolute and improves the Actual Term Weighted Value (ATWV) up to 19.2% absolute over the full search baseline", "In this paper, we present a language model (LM) adaptation framework based on data selection to improve the recognition accuracy of automatic speech recognition systems. Previous approaches of LM adaptation usually require additional data to adapt the existing background LM. In this work, we propose a novel two-pass decoding approach that uses no additional data, but instead, selects relevant data from the existing background corpus that is used to train the background LM. The motivation is that the background corpus consists of data from the different domains and as such, the LM trained from it is generic and not discriminative. To make the LM more discriminative, we will select sentences from the background corpus that are similar in some linguistic characteristics to the utterances recognized in the first-pass and use them to train a new LM which is employed during the second-pass decoding. In this\u00a0\u2026", "In this paper, we present an age-friendly E-commerce system with novel assistive functional technologies, aiming at providing a comfortable online shopping environment for the elderly. Besides incorporating human factors for the elderly into the design of user interface, we build an age-friendly system by improving the functional usability. First, to improve the searching experience, we design a multimodal product search function, which accepts image, speech, text and the combination of them as inputs to help the elderly find products easily and accurately. Second, we develop a product reputation function to provide an objective evaluation of products\u2019 quality, which helps the elderly filter out low-quality products while saves their energy in product comparison. Additionally, to reduce the elderly\u2019s visual burden when browsing the Web, a personalized speech feedback function is designed to provide speech\u00a0\u2026", "In this paper, we propose the use of distance and co-occurrence information of word-pairs to improve language modeling. We have empirically shown that, for history-context sizes of up to ten words, the extracted information about distance and co-occurrence complements the n-gram language model well, for which learning long-history contexts is inherently difficult. Evaluated on the Wall Street Journal and the Switchboard corpora, our proposed model reduces the trigram model perplexity by up to 11.2% and 6.5%, respectively. As compared to the distant bigram model and the trigger model, our proposed model offers a more effective manner of capturing far context information, as verified in terms of perplexity and computational efficiency, i.e., fewer free parameters to be fine-tuned. Experiments using the proposed model for speech recognition, text classification and word prediction tasks showed improved\u00a0\u2026", "Speaker diarization is the task of determining \u201cWho spoke when?\u201d, where the objective is to annotate a continuous audio recording with appropriate speaker labels corresponding to the time regions where they spoke. The labels are not necessarily the actual speaker identities, i.e. speaker identification, as long as the same labels are assigned to the regions uttered by the same speakers. These regions may overlap as multiple speakers could talk simultaneously. Speaker diarization is thus essentially the combination of two different processes: segmentation, in which the speaker turns are detected, and unsupervised clustering, in which segments of the same speakers are grouped. The clustering process is considered as unsupervised problem since there is no prior information about the number of speakers, their identities or acoustic conditions (Meignier et al., Comput Speech Lang 20(2\u20133):303\u00a0\u2026", "Despite recent advances in the use of Artificial Neural Network (ANN) architectures for automatic speech recognition (ASR), relatively little attention has been given to using feature inputs beyond MFCCs in such systems. In this paper, we propose an alternative to conventional MFCC or filterbank features, using an approach based on the Generalised Hough Transform (GHT). The GHT is a common approach used in the field of image processing for the task of object detection, where the idea is to learn the spatial distribution of a codebook of feature information relative to the location of the target class. During recognition, a simple weighted summation of the codebook activations is commonly used to detect the presence of the target classes. Here we propose to learn the weighting discriminatively in an ANN, where the aim is to optimise the static phone classification error at the output of the network. As such an ANN\u00a0\u2026", "Joint density Gaussian mixture model (JD-GMM) based method has been widely used in voice conversion task due to its flexible implementation. However, the statistical averaging effect during estimating the model parameters will result in over-smoothing the target spectral trajectories. Motivated by the local linear transformation method, which uses neighboring data rather than all the training data to estimate the transformation function for each feature vector, we proposed a local partial least square method to avoid the over-smoothing problem of JD-GMM and the over-fitting problem of local linear transformation when training data are limited. We conducted experiments using the VOICES database and measure both spectral distortion and correlation coefficient of the spectral parameter trajectory. The experimental results show that our proposed method obtain better performance as compared to baseline methods.", "In this paper, we investigate a feature conditioning method for the VTS-based model compensation. The VTS is a technique that predicts noisy acoustic model from clean acoustic model and noise model. It is noted that most of the previous studies use a single Gaussian noise model, which is unable to model noise statistics well, especially in non-stationary noisy environments. In this paper, we propose a combination of feature processing and VTS model compensation to handle non-stationary noise more efficiently. In the feature processing stage, the non-stationary characteristics of noise is reduced, hence the processed features is more suitable for VTS model compensation using single Gaussian noise model. Experimental analysis on the AURORA2 task shows that the proposed method has the potential to improve the performance of VTS method in non-stationary environments if good noise estimation is available.", "In this paper, we propose a method to exploit the harmonicity of human voiced speech using only the most harmonic sub-part of the spectrum. This technique searches for all the potential sub-windows of the spectrum, and measures their local harmonicity, using a newly proposed metric, which works in the spectral autocorrelation domain and employs a novel sinusoidal fitting approach. Experiments show that the new feature can be used to detect noisy voiced speech frames heavily corrupted by non-stationary noise even at 0dB SNR with high precision and recall, which gives better results than the Windowed Autocorrelation Lag Energy (WALE), a recently proposed voicing features, under a complex factory noise scenarios.", "In this paper, we propose a robust voice activity detection method based on long-term stationarity (LTS) of the speech signal. The approach is motivated by the fact that noise, in timedomain, is relatively more stationary as compared to speech. We describe the use of Linear dynamic models (LDMs) as a measure of calculating the long-term stationarity of the signal and propose a voice activity detector by comparing the degree of stationarity at different times in the signal. We evaluate the proposed approach in presence of five types of noises at various SNR levels. Comparison with G. 729-Annex B, order statistics filters (OSF) VAD, windowed autocorrelation lag energy (WALE), and autocorrelation zero-crossing rate (AZR) schemes demonstrates that the accuracy of the LTS-based VAD scheme averaged over all noises and all SNRs is 3.94% better than that obtained by the best among the considered VAD schemes.", "We observed that human listeners distinguish one dialect from another by paying special attention to some particular phonetic and/or phonotactic patterns. Motivated by this observation, we propose a technique that emulates this process. We explore a target-aware lattice rescoring (TALR) process that revises the n-gram statistics in a lattice with target dialect information. We then derive n-gram statistics as the phonotactic features from the lattice and develop a system under the vector space modeling framework. The experiment results show that the proposed technique consistently improves dialect recognition performance on 30-second test utterances. We achieved equal error rates (EERs) of 4.57% and 13.28% with 3-gram statistics for Chinese and English dialect recognition in 2007 NIST Language Recognition Evaluation 30-second closed test sets.", "From statistical learning theory, the generalization capability of a model is the ability to generalize well on unseen test data which follow the same distribution as the training data. This paper investigates how generalization capability can also improve robustness when testing and training data are from different distributions in the context of speech recognition. Two discriminative training (DT) methods are used to train the hidden Markov model (HMM) for better generalization capability, namely the minimum classification error (MCE) and the soft-margin estimation (SME) methods. Results on Aurora-2 task show that both SME and MCE are effective in improving one of the measures of acoustic model's generalization capability, i.e. the margin of the model, with SME be moderately more effective. In addition, the better generalization capability translates into better robustness of speech recognition performance, even\u00a0\u2026", "Fuzzy rule-based systems have been successfully used for pattern classification. These systems focus on generating a rule-base from numerical input data. The resulting rule-base can be applied on classification problems. However, we are faced with some challenges when generating and selecting the appropriate rules to create final rule-base. In this paper, a novel approach for rule selection is proposed. The proposed algorithm makes the use of Iterative Rule Learning (IRL) to reduce the search space of the classification problem in hand for rule-base extraction. The major element of our proposed approach is an evaluation metric which is able to accurately estimate the degree of cooperation of the candidate rule with current rules in the rule-base. Finally, fine-tuning of the selected rules is handled by employing a proposed rule-weighting mechanism. To evaluate the performance of the proposed scheme, TIMIT\u00a0\u2026", "In this paper, we study the classification of three speech transmission channels: landline telephone, mobile phone and voice over Internet protocol (VoIP), based on speech signals collected from these channels. The problem is formulated as a three-class statistical pattern classification problem. The Mel-frequency cepstral coefficients (MFCC) are used as the features for classification and the Gaussian mixture model (GMM) is used to model the distribution of the features. The maximum likelihood (ML) is used as the decision rule for the classification. Our major contribution is that we use different databases for training and testing, so the evaluation tests are completely open. In such tests, high classification accuracy around 95% is obtained which indicates that the classification of speech transmission channels using training data is possible. We also consider factors that may influence the performance of the\u00a0\u2026", "This paper applies relevance feedback technique in spoken language recognition task, in which we consider a test utterance as a test query. Assuming that we have a labeled multilingual corpus, we exploit the retrieved utterances from such a reference corpus to automatically augment the test query. Note that successful spoken language recognition relies on sufficient query data. The proposed method is especially effective for short query by expanding the query at a low cost. Experiments show that unsupervised relevance feedback reduces the relative equal-error-rate by 16.2%, 4.9% and 10.2% on NIST LRE 1996, 2003 and 2005 databases respectively for 3-second trials.", "This paper describes the design and implementation of a practical automatic speaker recognition system for the CSLP speaker recognition evaluation (SRE). The speaker recognition system is built upon four subsystems using speaker information from acoustic spectral features. In addition to the conventional spectral features, a novel temporal discrete cosine transform (TDCT) feature is introduced in order to capture long-term speech dynamic. The speaker information is modeled using two complementary speaker modeling techniques, namely, Gaussian mixture model (GMM) and support vector machine (SVM). The resulting subsystems are then integrated at the score level through a multilayer perceptron (MLP) neural network. Evaluation results confirm that the feature selection, classifier design, and fusion strategy are successful, giving rise to an effective speaker recognition system.", "The orthogonal, least squares (OLS) algorithm is an efficient implementation of the forward regression procedure for subset model selection. The ability to find good subset parameters with only linear increase in computational complexity makes this method attractive for practical implementations. We examine the computation requirement of the OLS algorithm to reduce a model of K terms to a subset model of R terms when the number of training data available is N. We show that in the case where N/spl Gt/K, we can reduce the computation requirement by introducing an unitary transformation on the problem.< >", "Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, eg, background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the output transcription. The proposed benchmark contains a novel dataset,\u201cHyPoradise\u201d(HP), encompassing more than 334,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains a significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. More surprisingly, LLM with reasonable prompt\u00a0\u2026", "Recently, speech separation (SS) task has achieved remarkable progress driven by deep learning technique. However, it is still challenging to separate target signals from noisy mixture, as neural model is vulnerable to assign background noise to each speaker. In this paper, we propose a noise-aware SS method called NASS, which aims to improve the speech quality of separated signals in noisy conditions. Specifically, NASS views background noise as an independent speaker and predicts it with other speakers in a mask-based manner. Then we conduct patch-wise contrastive learning on feature level to minimize the mutual information between the predicted noise-speaker and other speakers, which suppresses the noise information in separated signals. The experimental results show that NASS effectively improves the noise-robustness for different mask-based separation backbones with less than 0.1M parameter increase. Furthermore, SI-SNRi results demonstrate that NASS achieves state-of-the-art performance on WHAM! dataset.", "Audio-visual speech recognition (AVSR) research has gained a great success recently by improving the noise-robustness of audio-only automatic speech recognition (ASR) with noise-invariant visual information. However, most existing AVSR approaches simply fuse the audio and visual features by concatenation, without explicit interactions to capture the deep correlations between them, which results in sub-optimal multimodal representations for downstream speech recognition task. In this paper, we propose a cross-modal global interaction and local alignment (GILA) approach for AVSR, which captures the deep audio-visual (A-V) correlations from both global and local perspectives. Specifically, we design a global interaction model to capture the A-V complementary relationship on modality level, as well as a local alignment approach to model the A-V temporal consistency on frame level. Such a holistic view of cross-modal correlations enable better multimodal representations for AVSR. Experiments on public benchmarks LRS3 and LRS2 show that our GILA outperforms the supervised learning state-of-the-art.", "This paper summarizes the outcomes from the ISCSLP 2022 Intelligent Cockpit Speech Recognition Challenge (ICSRC). We first address the necessity of the challenge and then introduce the associated dataset collected from a new-energy vehicle (NEV) covering a variety of cockpit acoustic conditions and linguistic contents. We then describe the track arrangement and the baseline system. Specifically, we set up two tracks in terms of allowed model/system size to investigate resource-constrained and -unconstrained setups, targeting to vehicle embedded as well as cloud ASR systems respectively. Finally we summarize the challenge results and provide the major observations from the submitted systems.", "The performances of automatic speech recognition (ASR) systems degrade drastically under noisy conditions. Explicit distortion modelling (EDM), as a feature compensation step, is able to enhance ASR systems under such conditions by simulating the in-domain noisy speeches from the clean counterparts. Yet, existing distortion models are either non-trainable or unexplainable and often lack controllability and generalization ability. In this paper, we propose a fully explainable and controllable model: DENT-DDSP to achieve EDM. DENT-DDSP utilizes novel differentiable digital signal processing (DDSP) components and requires only 10 seconds of training data to achieve high fidelity. The experiment shows that the simulated noisy data from DENT-DDSP achieves the highest simulation fidelity compared to other baseline models in terms of multi-scale spectral loss (MSSL). Moreover, to validate whether the data simulated by DENT-DDSP are able to replace the scarce in-domain noisy data in the noise-robust ASR tasks, several downstream ASR models with the same architecture are trained using the simulated data and the real data. The experiment shows that the model trained with the simulated noisy data from DENT-DDSP achieves similar performances to the benchmark with a 2.7\\% difference in terms of word error rate (WER). The code of the model is released online.", "Intermediate layer output (ILO) regularization by means of multitask training on encoder side has been shown to be an effective approach to yielding improved results on a wide range of end-to-end ASR frameworks. In this paper, we propose a novel method to do ILO regularized training differently. Instead of using conventional multitask methods that entail more training overhead, we directly make the intermediate layer output as input to the decoder, that is, our decoder not only accepts the output of the final encoder layer as input, it also takes the output of the encoder ILO as input during training. With the proposed method, as both encoder and decoder are simultaneously \"regularized\", the network is more sufficiently trained, consistently leading to improved results, over the ILO-based CTC method, as well as over the original attention-based modeling method without the proposed method employed.", "Human can recognize speech, as well as the pe-culiar accent of the speech simultaneously. However, present state-of-the-art ASR system can rarely do that. In this paper, we propose a multilingual approach to recognizing English speech, and related accent that speaker conveys using DNN-HMM framework. Specifically, we assume different accents of English as different languages. We then merge them together and train a multilingual ASR system. During decoding, we conduct two experiments. One is a monolingual ASR-based decoding, with the accent information embedded at phone level, realizing word-based accent recognition (AR), and the other is a multilingual ASR-based decoding, realizing an approximated utterance-based AR. Experimental results on an 8-accent English speech recognition show both methods can yield WERs close to the conventional ASR systems that completely ignore the accent\u00a0\u2026", "In this paper, we present a series of complementary approaches to improve the recognition of underrepresented named entities (NE) in hybrid ASR systems without compromising overall word error rate performance. The underrepresented words correspond to rare or out-of-vocabulary (OOV) words in the training data, and thereby can't be modeled reliably. We begin with graphemic lexicon which allows to drop the necessity of phonetic models in hybrid ASR. We study it under different settings and demonstrate its effectiveness in dealing with underrepresented NEs. Next, we study the impact of neural language model (LM) with letter-based features derived to handle infrequent words. After that, we attempt to enrich representations of underrepresented NEs in pretrained neural LM by borrowing the embedding representations of rich-represented words. This let us gain significant performance improvement on\u00a0\u2026", "For information consumers, being able to obtain a short and accurate answer for a query is one of the most desirable features. This motivation, along with the rise of deep learning, has led to a boom in open-domain Question Answering (QA) research. While the problem of machine comprehension has received multiple success with the help of large training corpora and the emergence of attention mechanism, the development of document retrieval in open-domain QA is lagged behind. In this work, we propose a novel encoding method for learning question-aware self-attentive document representations. By applying pair-wise ranking approach to these encodings, we build a Document Retriever, called QASA, which is then integrated with a machine reader to form a complete open-domain QA system. Our system is thoroughly evaluated using QUASAR-T dataset and shows surpassing results compared to other state\u00a0\u2026", "Linear discriminant analysis (LDA) and Gaussian probabilistic LDA (PLDA) have been shown to effectively suppress channel- and session-variability of i-vectors. But they suffer the following limitations: 1) In LDA, a single linear transformation may not be adequate to describe the nonlinear relationship of features and 2) Gaussian-PLDA assumes the speaker and channel factors follow a Gaussian distribution, but they are actually non-Gaussians. We consider neural networks (NN) as a way to overcome the limitations, that captures the nonlinear relationship of features and does not require prior assumptions. This paper investigates three NN based channel compensation methods: deep metric learning, NN classifier, and deep denoising autoencoder and compares their performance with LDA and PLDA. Experiments conducted on NIST 2010 speaker recognition evaluation suggest that NN-based channel\u00a0\u2026", "This paper studies unsupervised acoustic units discovery from unlabelled speech data. This task is usually approached by two steps, i.e., partitioning speech utterances into segments and clustering these segments into subword categories. In previous approaches, the clustering step usually assumes the number of subword units are known beforehand, which is unreasonable for zero-resource languages. Moreover, the previously-used clustering methods are not able to detect non-spherical clusters that are often present in real-world speech data. We address the two problems by a brand new clustering method, called density peak clustering (DPC), which is motivated by the observation that cluster centers are characterized by a higher density than their neighbors and by a relatively large distance from other points of a higher density in the space. Experiments on unsupervised acoustic units discovery demonstrate\u00a0\u2026", "In this paper we describe approaches to building our recent Malay broadcast news audio retrieval system. This system contains speech-to-text and keyword search subsystems. The speech-to-text system is built aiming at two folds: hybrid vocabulary recognition to tackle out-of-vocabulary keyword search issue and diversified acoustic modeling for effective system combination in keyword searching afterwards. For keyword search part we focus on the strategies such as keyword expansion to alleviate the out-of-vocabulary search issue and system combination to attain better search performance. Last we also introduce our data driven based keyword selection recipe to evaluate our system performance using actual term weighted value (ATWV) metric.", "We extend our previous work on particle filter compensation (PFC) to large vocabulary continuous speech recognition (LVCSR) and conduct the experiments on Aurora-4 database. Obtaining an accurately aligned state and mixture sequence of hidden Markov models (HMMs) that describe the underlying clean speech features being estimated in noise is a challenging task for sub-word based LVCSR because the total number of triphone models involved can be very large. In this paper, we show that by using separate sets of HMMs for recognition and compensation, we can simplify the models used for PFC to a great extent and thus facilitate the estimation of the side information offered in the state and mixture sequences. When the missing side information for PFC is available, a large word error reduction of 28.46% from multi-condition training is observed. In the actual scenarios, an error reduction of only 5.3% is\u00a0\u2026", "In this paper, we address the problem of performing sound event recognition tasks in the presence of television playing in a home environment. Our proposed framework consist of two modules: (1) a novel regression-based noise cancellation (RNC), a preprocessing which utilises a addition reference microphone placed near the television to reduce the noise. RNC learns an empirical mapping instead of the convention adaptive methods to achieve better noise reduction. (2) An improved subband power distribution image feature (iSPD-IF) which build on our existing classification framework by enhancing the feature extraction. A comprehensive experiment is carried out on our recorded data, which demonstrates high classification accuracy under severe television noise.", "This paper addresses a problem in sound event recognition, more specifically for home environments in which training data is not readily available. Our proposed method is an extension of our previous method based on a robust semi-supervised Tree-SVM classifier. The key step in this paper is that the MFCC features are adapted using custom filters constructed at each classification node of the tree. This is shown to significantly improve the discriminative capability. Experimental results under realistic noisy environments demonstrate that our proposed framework outperforms conventional methods.", "The I4U team is a consortium of one institute and four universities comprising of Institute for Infocomm Research (IIR), University of Science and Technology of China (USTC), University of New South Wales (UNSW), Nanyang Technological University (NTU), and Carnegie Mellon University (CMU). The I4U team submitted two systems for the 2008 NIST Speaker Recognition Evaluation (LRE), namely the I4U-Primary and I4U-Contrast, as described in this file I4U_SystemDescription. pdf. The primary and contrast systems are based on the combination of multiple classifiers and acoustic features with different fusion strategies. Both submissions include the results for eight train-test conditions as indicated in Table I. In particular, included in this submission (I4U. zip) are:", "A recent trend in the E-learning community is the emergence of Learning Content Management System (LCMS). An LCMS is generally defined as a system which combines content creation and course administration tools. A model for online presentation authoring system is presented in this work, following the notion of LCMS. It serves as a framework for people to author and manage their presentation over the Internet. In particular, this project explores the potentials of Flash-based authoring tools and lays a foundation for further developments. The key advantage of the system is the convenience of online access. Simplification of the authoring tasks has also been achieved by separating the roles of the layout designer and content author. Furthermore, the system provides learners with the flexibility to view any contents using any defined layouts.", "This paper shows that the BER performance using linear equalizer for channel equalization problem is significantly dependent on delay order. To obtain optimum performance, the equalizer output should be derived from the equalizer with delay order having the best BER performance. An efficient method to evaluate the upper bound BER performance of a linear equalizer to find the optimum delay is proposed. The method is novel as the evaluation is performed using only the channel statistics and the equalizer\u2019s weights.", "With recent advances of diffusion model, generative speech enhancement (SE) has attracted a surge of research interest due to its great potential for unseen testing noises. However, existing efforts mainly focus on inherent properties of clean speech for inference, underexploiting the varying noise information in real-world conditions. In this paper, we propose a noise-aware speech enhancement (NASE) approach that extracts noise-specific information to guide the reverse process in diffusion model. Specifically, we design a noise classification (NC) model to produce acoustic embedding as a noise conditioner for guiding the reverse denoising process. Meanwhile, a multi-task learning scheme is devised to jointly optimize SE and NC tasks, in order to enhance the noise specificity of extracted noise conditioner. Our proposed NASE is shown to be a plug-and-play module that can be generalized to any diffusion SE models. Experiment evidence on VoiceBank-DEMAND dataset shows that NASE achieves significant improvement over multiple mainstream diffusion SE models, especially on unseen testing noises.", "In this work, we introduce S4M, a new efficient speech separation framework based on neural state-space models (SSM). Motivated by linear time-invariant systems for sequence modeling, our SSM-based approach can efficiently model input signals into a format of linear ordinary differential equations (ODEs) for representation learning. To extend the SSM technique into speech separation tasks, we first decompose the input mixture into multi-scale representations with different resolutions. This mechanism enables S4M to learn globally coherent separation and reconstruction. The experimental results show that S4M performs comparably to other separation backbones in terms of SI-SDRi, while having a much lower model complexity with significantly fewer trainable parameters. In addition, our S4M-tiny model (1.8M parameters) even surpasses attention-based Sepformer (26.0M parameters) in noisy conditions with only 9.2 of multiply-accumulate operation (MACs).", "In this paper, we propose ACA-Net, a lightweight, global context-aware speaker embedding extractor for Speaker Verification (SV) that improves upon existing work by using Asymmetric Cross Attention (ACA) to replace temporal pooling. ACA is able to distill large, variable-length sequences into small, fixed-sized latents by attending a small query to large key and value matrices. In ACA-Net, we build a Multi-Layer Aggregation (MLA) block using ACA to generate fixed-sized identity vectors from variable-length inputs. Through global attention, ACA-Net acts as an efficient global feature extractor that adapts to temporal variability unlike existing SV models that apply a fixed function for pooling over the temporal dimension which may obscure information about the signal's non-stationary temporal variability. Our experiments on the WSJ0-1talker show ACA-Net outperforms a strong baseline by 5\\% relative improvement in EER using only 1/5 of the parameters.", "The integration of well-pre-trained acoustic and linguistic representations boosts the performance of speech-to-text crossmodality tasks. However, the potential of fine-tuning crossmodality integrated model on accented and noisy corpus is still under-explored. To address this gap, we propose an endto-end acoustic and linguistic integrated representation learning model, namely Dual-w2v-BART. Our model incorporates acoustic representations from wav2vec2. 0 and linguistic information from BART model by utilizing the cross-attention mechanism in the decoder, with paired speech-text dual inputs. To enhance model robustness on accent and noise, we propose a text-centric representation consistency component that helps to gain the similarity between different modality inputs while representing the same content. The results on accented and noisy speech recognition tasks demonstrate the effectiveness of the\u00a0\u2026", "Enabling continual learning (CL) from an ever-changing environment is highly valuable, but it poses significant challenges for spoken keyword spotting (KWS), which simultaneously deals with both variability in acoustic characteristics of speech signals and catastrophic forgetting issues. In this paper, we propose a novel framework for replay-based CL in KWS that uses a Dual-Memory Multi-Modal (DM3) structure to enhance generalizability and robustness. Our approach leverages short-term and long-term models to learn near-term and long-term knowledge in an adaptive manner with a dual-memory structure, while also exploiting the consistency of multiple speech perturbations to improve the robustness with a multi-modal structure. Additionally, we introduce a class-balanced selection strategy that uses confidence scores to sort training samples. Experiments demonstrate the effectiveness of our method over\u00a0\u2026", "Recently, transformer based model has been widely employed for audio classification due to its capability of modelling long feature dependencies in audio representations such as spectrograms and Mel-Frequency Cepstral Coefficients (MFCC). However, the transformer model focuses on learning attention coefficients between features of each patch and using it only is incapable of identifying the informative patches in the audio representations and obtain the corresponding patch information of an audio event. In this paper, in addition to fuse different features of each patch using a vision transformer (ViT) encoder, a patch-level feature fusion scheme is proposed to fuse the feature information learned from another ViT encoder. In essence, an audio representation is fed into a ViT to generate a feature map such that the merit of the network pre-trained on a large amount of data can be exploited. Rather than outputting\u00a0\u2026", "In this paper, we tackle the new language-based audio retrieval task proposed in DCASE 2022 1 1 https://dcase.community/challenge2022/task-language-based-audio-retrieval. Firstly, we introduce a simple, scalable architecture which ties both the audio and text encoder together. Our approach requires very minimal training, and allows us to use many publicly available models without needing to fine-tune them. Secondly, we show that using this architecture along with contrastive loss allows the model to beat the performance of the baseline model. Finally, in addition to having an extremely low training memory requirement, we are able to utilize pretrained models as it is without needing to finetune them. We test our methods and show that using a combination of our methods beats the baseline scores by 0.08 in R@1 and 0.13 in mAP 10 .", "In this paper, we propose an algorithm, Epochal Difficult Captions, to supplement the training of any model for the Automated Audio Captioning task. Epochal Difficult Captions is an elegant evolution to the keyword estimation task that previous work have used to train the encoder of the AAC model. Epochal Difficult Captions modifies the target captions based on a curriculum and a difficulty level determined as a function of current epoch. Epochal Difficult Captions can be used with any model architecture and is a lightweight function that does not increase training time. We test our results on three systems and show that using Epochal Difficult Captions consistently improves performance by up to 0.013 SPIDEr score.", "Automatic speech recognition (ASR) for under-represented named-entity (UR-NE) is challenging due to such named-entities (NE) have insufficient instances and poor contextual coverage in the training data to learn reliable estimates and representations 1 1 In this paper, UR-NE refers to the named-entity (NE) words that have low-frequency count, say, the count is in [1], [9] in this work, or do not appear in the training data at all, i.e. the count is 0.. In this paper, we propose approaches to enriching UR-NEs to improve speech recognition performance. Specifically, our first priority is to ensure those UR-NEs to appear in the word lattice if there is any. To this end, we employ class-based language model (LM) philosophy, making exemplar utterances for those UR-NEs according to their classes (e.g, location, person, organization, etc.), ending up with an improved LM that boosts the UR-NE occurrence in the word lattice\u00a0\u2026", "Transformer model has made great progress in speech recognition. However, compared with models with iterative computation, transformer model has fixed encoder and decoder depth, thus losing the recurrent inductive bias. Besides, finding the optimal number of layers involves trial-and-error attempts. In this paper, the universal speech transformer is proposed, which to the best of our knowledge, is the first work to use universal transformer for speech recognition. It generalizes the speech transformer with dynamic numbers of encoder/decoder layers, which can relieve the burden of tuning depth related hyperparameters. Universal transformer adds the depth and positional embeddings repeatedly for each layer, which dilutes the acoustic information carried by hidden representation, and it also performs a partial update of hidden vectors between layers, which is less efficient especially on the very deep models. For better use of universal transformer, we modify its processing framework by removing the depth embedding and only adding the positional embedding once at transformer encoder frontend. Furthermore, to update the hidden vectors efficiently, especially on the very deep models, we adopt a full update. Experiments on LibriSpeech, Switchboard and AISHELL-1 datasets show that our model outperforms a baseline by 3.88%-13.7%, and surpasses other model with less computation cost.", "This work investigates the effectiveness of using the word based and sub-word based embedding representations as input for a deep bidirectional Long Short-Term Memory Network for Sentence Unit Detection in Automatic Speech Recognition transcription. Our experimental results show that using sub-word based embedding can significantly improve the SUD performance when a limited text is used to train both the word embedding and the SUD model. The SUD model using the sub-word based embedding gains up to 2.07% absolute improvement in F1-score as compared to the best model trained with the word-based embedding. When tested on a domain-mismatch condition, the SUD model with sub-word based embedding trained from the in-domain data gives an approximate 2 % and 1 % improvement over the best model using out-of-domain embedding with reference and ASR transcription with 29.5\u00a0\u2026", "In this paper, we propose to embed sentences into fixed-dimensional vectors that carry the topic information for story segmentation. As a sentence comprises of a sequence of words and may have different lengths, we use long short-term memory recurrent neural network (LSTM-RNN) to summarize the information of the whole sentence and only predict the topic class at the last word in the sentence. The output of the network at the last word can be used as an embedding of the sentence in the topic space. We used the obtained sentence embeddings in the HMM-based story segmentation framework and obtained promising results. On the TDT2 corpus, the F1 measure is improved to 0.789 from 0.765 which is obtained by a competitive system using DNN and bag-of-words features.", "I-vector adaptation of DNN-HMM acoustic models has shown clear performance improvement for speech recognition. In this paper, we study this technique on Babel task. we use Swahili as target language (training data of 50 hours) and another 6 languages as multilingual resources to train i-vector extractors respectively. Our study shows that i-vector extractors trained with more multilingual data only produce slightly improved results. Moreover, we compared two i-vectors adaptation methods, 1) concatenate i-vectors with spectral features; 2) predict a bias term adding it to spectral features from i-vectors using a NN. When DNN is trained from scratch, the two methods perform similarly. However, only the second method is appropriate in a cross-lingual transfer learning scenario. We investigate it as well, and results show further word error rate reduction can be gained.", "In this paper, we propose a feature adaptation method that combines speech features from multiple microphone channels for robust automatic speech recognition (ASR). The proposed method first transforms the features in all channels using channel-dependent linear transforms, and then sum the channels into one channel for acoustic modeling. The transform parameters are estimated by maximizing the likelihood of the transformed features on a Gaussian mixture model (GMM) trained from clean features. To use diagonal covariance matrices for efficient estimation algorithm, the likelihood function is evaluated in the cepstral domain, while the transformation is in the log Mel filterbank domain. We evaluate the proposed feature adaptation on the 6-channel evaluation data in the CHiME-3 task. Results show that the proposed feature adaptation method with diagonal channel-dependent transforms reduces word error\u00a0\u2026", "In this paper, we investigate the use of the proposed non-parametric exemplar-based acoustic modeling for the NIST Open Keyword Search 2015 Evaluation. Specifically, kernel-density model is used to replace GMM in HMM/GMM (Hidden Markov Model / Gaussian Mixture Model) or DNN in HMM/DNN (Hidden Markov Model / Deep Neural Network) acoustic model to predict the emission probability of HMM states. To get further improvement, likelihood score generated by the kernel-density model is discriminatively tuned by the score tuning module realized by a neural network. Various configurations for score tuning module have been examined to show that simple neural network with 1 hidden layer is sufficient to fine tune the likelihood score generated by the kernel-density model. With this architecture, our exemplar-based model outperforms the 9-layer-DNN acoustic model significantly for both the speech\u00a0\u2026", "In this paper, we describe the use of feedforward neural networks to improve the term-distance term-occurrence (TDTO) language model, previously proposed in [1]\u2212[3]. The main idea behind the TDTO model proposition is to model separately both position and occurrence information of words in the history-context to better estimate n-gram probabilities. Neural networks have been shown to offer a better generalization property than other conventional smoothing methods. We take advantage of such property for a better smoothing mechanism for the TDTO model, referred to as the continuous space TDTO (cTDTO). The newly proposed model has reported an improved perplexity over the baseline TDTO model of up to 9.2%, at history length of ten, as evaluated on the Wall Street Journal (WSJ) corpus. Also, in the Aurora-4 speech recognition N-best re-ranking task, the cTDTO outperformed the TDTO model by\u00a0\u2026", "Non-negative matrix factorization (NMF) aims at finding nonnegative representations of nonnegative data. Among different NMF algorithms, alternating direction method of multipliers (ADMM) is a popular one with superior performance. However, we find that ADMM shows instability and inferior performance on real-world data like speech signals. In this paper, to solve this problem, we develop a class of advanced regularized ADMM algorithms for NMF. Efficient and robust learning rules are achieved by incorporating l1-norm and the Frobenius norm regularization. The prior information of Laplacian distribution of data is used to solve the problem with a unique solution. We evaluate this class of ADMM algorithms using both synthetic and real speech signals for a source separation task at different cost functions, ie, Euclidean distance (EUD), Kullback-Leibler (KL) divergence and Itakura-Saito (IS) divergence. Results\u00a0\u2026", "This paper reports our study in exploiting the distance and co-occurrence information of word-pairs to improve the n-gram language model. We used these two types of information for modeling the distant context, up to history length of ten. Also we show that the proposed model provides complementary information about the n-gram's context that is unable to be captured by the n-gram model due to data scarcity. Evaluated on the WSJ and SWB-1 corpora, the proposed model reduced the trigram perplexity up to 11.2% and 6.5% respectively. In an N-best re-ranking task of the Aurora-4 database, our model aided a hexagram model to perform ~9% relatively better in terms of WER.", "This paper presents our effort in collecting a Malay broadcast news (BN) speech corpus to support our research in Malay LVCSR. The 53 hours corpus is recorded from the TV channels in both Singapore and Malaysia over a 9-month period. To facilitate various researches in LVCSR, besides of orthographic transcription, the corpus provides other metadata such as speaking environment type, speaker identity information, language identity, and topic descriptions. In the orthographic transcription, we also tagged various linguistic phenomena such as disfluencies, code switched words, and proper nouns. We trained an ASR system and achieved a word error rate of 8.5% for anchor speech and 17.1% overall (including reporter and other speakers speech) on 27 hours of test data.", "The ability to automatically recognize sound events in real-life conditions is an important part of applications such as acoustic surveillance and smart home automation. The main challenge of these applications is that the sound sources often come from unknown distances under different acoustic environments, which are also noisy and reverberant. Among the noises in the home, the most difficult to deal with are non-stationary interference, such as TV, radio or music playing. In this paper, we address one of the hardest situations of sound event recognition: the presence of interference under reverberant conditions. Our system is a dual microphone approach and consists of a comprehensive combination of several modules: first, a novel regression-based noise cancellation (RNC), to reduce the interference, and second, an improved subband power distribution image feature (iSPD-IF) to classify the noise cancelled\u00a0\u2026", "A number of effective classification algorithms have been developed for spoken language recognition, and it has been a common practice in the NIST Language Recognition Evaluations (LREs) that an information fusion is applied to boost the performance of the recognition system. This paper investigates the fusion of multiple output scores generated using different classifiers that complement to further reduce the classification error rate in spoken language recognition. We introduce a local performance metric to optimize the performance of the classifier fusion. The experiments are conducted on the 2009 NIST LRE corpus. The experimental results show that the proposed fusion effectively improves the performance over individual classifiers.", "Gaussian mixture modeling with universal background model (GMM-UBM) is a widely used method for speaker identification, where the GMM model is used to characterize a specific speaker\u2019s voice. The estimation of model parameters is generally performed based on the maximum likelihood (ML) or maximum a posteriori (MAP) criteria. In this way, interspeaker information that discriminates between different speakers is not taken into account. To overcome this limitation, we design a discriminative performance metric to capture interspeaker variabilities leading to improve the classification capability of speaker models. A learning algorithm is presented to tune the Gaussian mixture weights by optimizing the frame classification accuracy of GMM classifiers. We design an objective function to directly relate the model parameters to the performance metric. The comparative study of the proposed method is done with the\u00a0\u2026", "This paper presents a novel approach of discriminative acoustic feature extraction for speech recognition using output coding technique. A high dimensional feature space for higher discriminative capability is constructed by expanding MFCC coefficients with polynomial expansion. In order to fit the discriminative features in the hidden Markov model structure of speech recognition, the high dimensional feature vectors are further projected into a low dimensional feature space using the output scores of a set of SVMs. Each of the SVMs is trained in one phone versus the rest manner so that each of the resulting feature dimensions can provide effective information to differ one phone from the others. The discriminative features have been evaluated in the speech recognition task of the TIMIT corpus, and 72.18% phone accuracy has been achieved.", "This paper proposes a novel framework to index and retrieve audio content from broadcast database that contains both speech and music. In this framework, we model the acoustic events using hidden Markov models, which are then used to decode the audio content. The decoding results in the form of acoustic token sequence and acoustic lattice are used to generate features for indexing and retrieval with the vector space model. Experiments were carried out on the TRECVID database and the results showed that the proposed framework is effective in audio information retrieval. The results also showed that the features generated from the acoustic lattice provide more accurate information than token sequence.", "This paper describes the I 2 R/NTU system submitted for the NIST Rich Transcription 2007 (RT-07) Meeting Recognition evaluation Multiple Distant Microphone (MDM) task. In our implementation, the Direction of Arrival (DOA) information is specifically used to perform speaker turn detection and clustering. Cluster purification is then carried out by performing GMM modeling on acoustic features. Finally, non-speech & silence removal is effected to remove unwanted segments. The system achieved an overall DER of 31.02% on the NIST Rich Transcription Spring 2006 evaluation tasks.", "The computational requirement to implement the optimal Bayesian symboldecision equaliser using RBF network 1] can be very high as the full RBF Bayesian solution usually requires a large number of centres. To reduce the implementation complexity, we propose to use a subset number of the full RBF network's centres to generate a subset equaliser. The centres to be selected for the subset equaliser are those that have their Euclidean distance close to the equaliser's current input vector. Our results show that the number of centres can be greatly reduced without signi cant degradation in classi cation performance.", "The radial basis function (RBF) network has become a popular choice of neural network to be used for nonlinear time series prediction 1 {3]. Although the results have been encouraging for modelling time invariant nonlinear systems, it is difficult to achieve the same level of success for tracking nonstationary signals 4]. In this article, we present a method of modifying the classical RBF networks, which improves the predictive accuracy for nonlinear and nonstationary data.", "The orthogonal least squares (OLS) algorithm is an efficient implementation of the forward-selection method for subset model selection. The ability to find good subset parameters with only a linearly increasing computational requirement makes this method attractive for practical implementations. This paper examines why forward-selection technique can fail to find optimum subset models and presents a modification scheme to improve the selection process.< >", "Knowledge distillation (KD) is used to enhance automatic speaker verification performance by ensuring consistency between large teacher networks and lightweight student networks at the embedding level or label level. However, the conventional label-level KD overlooks the significant knowledge from non-target speakers, particularly their classification probabilities, which can be crucial for automatic speaker verification. In this paper, we first demonstrate that leveraging a larger number of training non-target speakers improves the performance of automatic speaker verification models. Inspired by this finding about the importance of non-target speakers' knowledge, we modified the conventional label-level KD by disentangling and emphasizing the classification probabilities of non-target speakers during knowledge distillation. The proposed method is applied to three different student model architectures and achieves an average of 13.67% improvement in EER on the VoxCeleb dataset compared to embedding-level and conventional label-level KD methods.", "Dual-path is a popular architecture for speech separation models (e.g. Sepformer) which splits long sequences into overlapping chunks for its intra- and inter-blocks that separately model intra-chunk local features and inter-chunk global relationships. However, it has been found that inter-blocks, which comprise half a dual-path model's parameters, contribute minimally to performance. Thus, we propose the Single-Path Global Modulation (SPGM) block to replace inter-blocks. SPGM is named after its structure consisting of a parameter-free global pooling module followed by a modulation module comprising only 2% of the model's total parameters. The SPGM block allows all transformer layers in the model to be dedicated to local feature modelling, making the overall model single-path. SPGM achieves 22.1 dB SI-SDRi on WSJ0-2Mix and 20.4 dB SI-SDRi on Libri2Mix, exceeding the performance of Sepformer by 0.5 dB and 0.3 dB respectively and matches the performance of recent SOTA models with up to 8 times fewer parameters.", "Large self-supervised pre-trained speech models require computationally expensive fine-tuning for downstream tasks. Soft prompt tuning offers a simple parameter-efficient alternative by utilizing minimal soft prompt guidance, enhancing portability while also maintaining competitive performance. However, not many people understand how and why this is so. In this study, we aim to deepen our understanding of this emerging method by investigating the role of soft prompts in automatic speech recognition (ASR). Our findings highlight their role as zero-shot learners in improving ASR performance but also make them vulnerable to malicious modifications. Soft prompts aid generalization but are not obligatory for inference. We also identify two primary roles of soft prompts: content refinement and noise information enhancement, which enhances robustness against background noise. Additionally, we propose an effective modification on noise prompts to show that they are capable of zero-shot learning on adapting to out-of-distribution noise environments.", "Heart auscultations are a low-cost and effective way of detecting valvular heart diseases early, which can save lives. Nevertheless, it has been difficult to scale this screening method since the effectiveness of auscultations is dependent on the skill of doctors. As such, there has been increasing research interest in the automatic classification of heart sounds using deep learning algorithms. However, it is currently difficult to develop good heart sound classification models due to the limited data available for training. In this work, we propose a simple time domain approach, to the heart sound classification problem with a base classification error rate of 0.8 and show that augmentation of the data through codec simulation can improve the classification error rate to 0.2. With data augmentation, our approach outperforms the existing time-domain CNN-BiLSTM baseline model. Critically, our experiments show that codec data augmentation is effective in getting around the data limitation.", "This paper introduces a statistical augmentation approach to generate code-switched sentences for code-switched language modeling. The proposed technique converts monolingual sentences from a particular domain into their corresponding code-switched versions using pretrained monolingual Part-of-Speech tagging models. The work also showed that adding 150 handcrafted formal to informal word replacements can further improve the naturalness of augmented sentences. When tested on an English-Malay code-switching corpus, a relative decrease of 9.7% in perplexity for ngram language model interpolated with the language model trained with augmented texts and other monolingual texts was observed, and 5.9% perplexity reduction for RNNLMs.", "Punctuation restoration is a crucial task in enriching automated transcripts produced by Automatic Speech Recognition (ASR) systems. This paper presents an empirical study on the impact of employing different data acquisition and training strategies on the performance of punctuation restoration models for multilingual and codeswitching speech. The study focuses on two of the most popular Singaporean spoken languages, namely English and Mandarin in both monolingual and codeswitching forms. Specifically, we experimented with in-domain and out-of-domain evaluation for multilingual and codeswitching speech. Subsequently, we enlarge the training data by sampling the codeswitching corpus by reordering the conversational transcripts. We also proposed to ensemble the predicting models by averaging saved model checkpoints instead of using the last checkpoint to improve the model performance. The\u00a0\u2026", "In dialog act recognition (DAR) of an utterance in a conversation, the prior studies have focused either on the global context using the whole utterances in the dialog, or the local context using the neighbouring utterance flow in the dialog. However, their methods attempt to deal with all types of dialogs indiscriminately. In this study, we propose a model to extract the local context information by an inter-utterance relation matching task (RMT), and a DAR framework to incorporate the local context information into a hierarchical network to fulfil both local and global context modeling. Extensive evaluations were conducted on a Mandarin dialog corpus and two benchmark English corpora. It is found that the different dialog types possess different window lengths for RMT, which is related to the length of subtopics in a given type of dialog. According to ablation experiments, the global information contributed more to the DAR\u00a0\u2026", "Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises. In this work, we investigate the noise-invariant visual modality to strengthen robustness of AVSR, which can adapt to any testing noises while without dependence on noisy training data, a.k.a., unsupervised noise adaptation. Inspired by human perception mechanism, we propose a universal viseme-phoneme mapping (UniVPM) approach to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noisy conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach achieves the state-of-the-art under various noisy as well as clean conditions. In addition, we also outperform previous state-of-the-arts on visual speech recognition task.", "Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine frame-level modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts.", "This paper focuses on multi-enrollment speaker recognition which naturally occurs in the task of online speaker clustering, and studies the properties of different scoring back-ends in this scenario. First, we show that popular cosine scoring suffers from poor score calibration with a varying number of enrollment utterances. Second, we propose a simple replacement for cosine scoring based on an extremely constrained version of probabilistic linear discriminant analysis (PLDA). The proposed model improves over the cosine scoring for multi-enrollment recognition while keeping the same performance in the case of one-to-one comparisons. Finally, we consider an online speaker clustering task where each step naturally involves multi-enrollment recognition. We propose an online clustering algorithm allowing us to take benefits from the PLDA model such as the ability to handle uncertainty and better score calibration\u00a0\u2026", "The pre-trained multi-lingual XLSR model generalizes well for language identification after fine-tuning on unseen languages. However, the performance significantly degrades when the languages are not very distinct from each other, for example, in the case of dialects. Low resource dialect classification remains a challenging problem to solve. We present a new data augmentation method that leverages model training dynamics of individual data points to improve sampling for the latent mixup. The method works well in low-resource settings where generalization is paramount. Our datamaps-based mixup technique, which we call Map-Mix, improves weighted F1 scores by 2% compared to the random mixup baseline and results in a significantly well-calibrated model. The code for our method is open-sourced on github.", "Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. Specifically, we first capture task-related unimodal representations and the unimodal predictions from the introduced unimodal predicting task. Then the unimodal representations are aligned with the more effective one by the designed multimodal contrastive method under the supervision of the unimodal predictions. Experimental results with fused features on two image-text classification benchmarks UPMC-Food-101 and N24News show that our proposed Unimodality-Supervised MultiModal Contrastive UniS-MMC learning method outperforms current state-of-the-art multimodal methods. The detailed ablation study and analysis further demonstrate the advantage of our proposed method.", "Self-supervised learning (SSL) has played an important role in various tasks in the field of speech and audio processing. However, there is limited research on adapting these SSL models to predict the speaker's age and gender using speech signals. In this paper, we investigate seven SSL models, namely PASE+, NPC, wav2vec 2.0, XLSR, HuBERT, WavLM, and data2vec in the joint age estimation and gender classification task on the TIMIT corpus. Additionally, we also study the effect of using different hidden encoder layers within these models on the age estimation result. Furthermore, we evaluate how the performance of different SSL models varies in predicting the speaker's age under simulated noisy conditions. The simulated noisy speech is created by mixing the clean utterance from the TIMIT test set with random noises from the Music and Noise category of the MUSAN corpus on multiple levels of signal-to\u00a0\u2026", "Nuclear Magnetic Resonance (NMR) is used in structural biology to experimentally determine the structure of proteins, which is used in many areas of biology and is an important part of drug development. Unfortunately, NMR data can cost thousands of dollars per sample to collect and it can take a specialist weeks to assign the observed resonances to specific chemical groups. There has thus been growing interest in the NMR community to use deep learning to automate NMR data annotation. Due to similarities between NMR and audio data, we propose that methods used in acoustic signal processing can be applied to NMR as well. Using a simulated amino acid dataset, we show that by swapping out filter banks with a trainable convolutional encoder, acoustic signal embeddings from speaker verification models can be used for amino acid classification in 2D NMR spectra by treating each amino acid as a unique speaker. On an NMR dataset comparable in size with of 46 hours of audio, we achieve a classification performance of 97.7% on a 20-class problem. We also achieve a 23% relative improvement by using an acoustic embedding model compared to an existing NMR-based model.", "Estimating speaker attributes like age and height is a difficult task with sev eral applications in speech forensic analysis and potential applications in speaker verification and speaker adaptation techniques. We present a bi-encoder (Mixture of Experts (MoE) inspired) transformer mixture model for estimating speaker age and height in this project. For the extraction of specific male and female voice characteristic features, we suggest the use of two different transformer encoders, while making use of wav2vec 2.0 as a common feature extraction method. The bi encoder architecture is chosen due to the significant variances in male and female voice characteristics. This architecture increases the model\u2019s generalizability by re ducing interference effects during the model training. We conduct our tests using the TIMIT corpus and find that our results on age estimation surpass the present state-of-the-art. For male and female age estimation, we obtain 5.54 years and 6.49 years as root mean squared error (RMSE), respectively. Further research into the relative impact of various phonetic sound kinds for speaker profiling reveals that vowel phonemes are the most distinctive for age estimate.", "In this paper we report our approaches to accomplishing the very limited resource keyword search (KWS) task in the NIST Open Keyword Search 2015 (OpenKWS15) Evaluation. We devised the methods, first, to attain better acoustic modeling, multilingual and semi-supervised acoustic model training as well as the examplar-based acoustic model training; second, to address the overwhelming out-of-vocabulary (OOV) KWS issue. Finally, we proposed a neural network (NN) framework to fuse diversified component systems, yielding improved combination results. Experimental results demonstrated the effectiveness of these approaches.", "In low resource Automatic Speech Recognition (ASR), one usually resorts to the Statistical Machine Translation (SMT) technique to learn transform rules to refine grapheme lexicon. To do this, we face two challenges. One is to generate grapheme sequences from the training data as the targets, which is paired with the original transcripts to train SMT models; the other is to effectively prune the learned rules from the translation model. In this paper we further this study. First we propose a simple but effective pruning method; second, to see in which case we are able to learn better rules, different setups with various acoustic and language model combinations are investigated; finally, to examine if the rules in different setups are complementary, lexicons generated via different rule tables are merged in ASR experiments. We report a WER reduction of up to 6.2% with the proposed technique.", "Acoustic source tracking in a room environment based on a number of distributed microphone pairs has been widely studied in the past. Based on the received microphone pair signals, the time-delay of arrival (TDOA) measurement is easily accessible. Bayesian tracking approaches such as extended Kalman filter (EKF) and particle filtering (PF) are subsequently applied to estimate the source position. In this paper, the Bayesian performance bound, namely posterior Cram\u00e9r-Rao bound (PCRB) is derived for such a tracking scheme. Since the position estimation is indirectly related to the received signal, a two-stage approach is developed to formulate the Fisher information matrix (FIM). First, the Cram\u00e9r-Rao bound (CRB) of the TDOA measurement in the noisy and reverberant environment is calculated. The CRB is then regarded as the variance of the TDOAs in the measurement function to obtain the PCRB. Also\u00a0\u2026", "Parametric histogram equalization (PHEQ) is a simple and effective way to reduce noise and channel effects in automatic speech recognition. Recently, we proposed to adapt the PHEQ transform towards individual test utterance to further improve the performance of PHEQ. In this paper, we improve the adaptation of PHEQ by exploiting the correlation between speech feature elements. Previously, PHEQ is applied to each feature element independently. In the case of Mel-frequency cepstral features (MFCC), this may not be optimal as it ignores the fact that the delta and acceleration features are derived from the static features and they are correlated. In this paper, we proposed to maintain the relationship between static, delta, and acceleration features to impose a constraint in PHEQ adaptation such that the PHEQ parameters will be more reliably estimated for short utterances. Experiments on both Aurora-2 and\u00a0\u2026", "This paper investigates a new method for fusion of scores generated by multiple classification sub-systems that help to further reduce the classification error rate in Spoken Language Recognition (SLR). In recent studies, a variety of effective classification algorithms have been developed for SLR. Hence, it has been a common practice in the National Institute of Standards and Technology (NIST) Language Recognition Evaluations (LREs) to fuse the results from several classification sub-systems to boost the performance of the SLR systems. In this work, we introduce a discriminative performance measure to optimize the performance of the fusion of 7 language classifiers developed as IIR's submission to the 2009 NIST LRE. We present an Error Corrective Fusion (ECF) method in which we iteratively learn the fusion weights to minimize error rate of the fusion system. Experiments conducted on the 2009 NIST LRE\u00a0\u2026", "Our aim in this paper is to propose a rule-weight learning algorithm in fuzzy rule-based classifiers. The proposed algorithm is presented in two modes: first, all training examples are assumed to be equally important and the algorithm attempts to minimize the error-rate of the classifier on the training data by adjusting the weight of each fuzzy rule in the rule-base, and second, a weight is assigned to each training example as the cost of misclassification of it using the class distribution of its neighbors. Then, instead of minimizing the error-rate, the learning algorithm is modified to minimize the sum of costs for misclassified examples. Using six data sets from UCI-ML repository and the TIMIT speech corpus for frame wise phone classification, we show that our proposed algorithm considerably improves the prediction ability of the classifier.", "In this paper, we propose a subspace construction and selection strategy (SUBS) for speaker recognition with limited training and testing speech data. Based on the individual Gaussian distributions of Gaussian mixture model (GMM), each speaker's characteristic subspace is constructed by training an SVM using the corresponding Gaussian mean vectors from the GMMs of both enrollment and imposter speakers. A subspace selection based on the structure risk criterion is used to select those subspaces with lower structure risks. The selected subspaces are then combined and used to evaluate the test utterances. We evaluate this subspace strategy on the 10sec-10sec test condition in 2008 NIST speaker recognition evaluations, achieving a relative 12.16% equal error rate reduction over the GMM supervector baseline system.", "This paper introduces an efficient unsupervised algorithm to discover motifs in multivariate data sequence. Specifically, we apply our proposed work to detect repeating segments on music feature vectors. The proposed algorithm, namely Adaptive Motif Generation, scans the music features online to construct a list of repeating candidate segments in linear time. The candidate list is then used to populate a sparse self-similarity matrix for further processing to generate the final selections. The experimental results showed that the proposed approach was able to obtain similar average F1 score compared to the traditional self-similarity approach with significant reduction in computational cost and memory usage.", "In this paper, we present a new mechanism to extract discriminative acoustic features for speech recognition using continuous output coding (COC) based feature transformation. Our proposed method first expands the short-time spectral features into a higher dimensional feature space to improve its discriminative capability. The expansion is performed by employing the polynomial expansion. The high dimension features are then projected into lower dimension space using continuous output coding technique implemented by a set of linear SVMs. The resulting feature vectors are designed to encode the difference between phones. The generated features are shown to be more discriminative than MFCCs and experimental results on both TIMIT and NTIMIT corpus showed better phone recognition accuracy with the proposed features.", "One class of feature enhancement techniques improve features robustness by performing temporal filtering to smooth the feature trajectories. While smoothing can enhance the features robustness by reducing the intra-class variation of the features, it also compromises the features discriminative power by reducing their inter-class distance. In this paper, we investigate the effect of feature smoothing on speech recognition performance. To evaluate how different degrees of smoothing will affect the performance, the speech features are low-pass filtered with different cut-off frequencies and then used for model training and recognition. From the experimental results, we have two observations: 1) the noisy speech needs more aggressive feature smoothing; 2) the large vocabulary Aurora-4 task prefers less smoothing than the small vocabulary Aurora-2 task.", "Proceedings of the 5th international conference on Chinese Spoken Language Processing | \nGuide Proceedings ACM Digital Library home ACM home Google, Inc. (search) Advanced \nSearch Browse About Sign in Register Advanced Search Journals Magazines Proceedings \nBooks SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced \nSearch Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsISCSLP'06 \nABSTRACT No abstract available. Comments Login options Check if you have access \nthrough your login credentials or your institution to get full access on this article. Sign in Full \nAccess Get this Publication Information Contributors Published in Guide Proceedings cover \nimage ISCSLP'06: Proceedings of the 5th international conference on Chinese Spoken \nLanguage Processing December 2006 804 pages ISBN:3540496653 Editors: Qiang Huo , Bin \u2026", "Realizing the optimum decision boundary for channel equalisation using radial basis function \nand linear networks - ePrints Soton The University of Southampton Courses University life \nResearch Business Global About Visit Alumni Departments News Events Contact \u00d7 Search the \nSite Search Filter your search: All Courses Projects Staff University of Southampton Institutional \nRepository Search Advanced Search Policies & Help Latest Download Statistics Browse by \nYear Browse by Divisions LeftRight Realizing the optimum decision boundary for channel \nequalisation using radial basis function and linear networks Chng, ES, Mulgrew, B., Chen, \nSheng and Gibson, G. (1995) Realizing the optimum decision boundary for channel \nequalisation using radial basis function and linear networks. In Mathematics for neural networks \nand applications. pp . Record type: Conference or Workshop Item (Paper) Text manna_submit - \u2026", "Reviewers Page 1 Reviewers ICNLP 2023 R. Saini Jatinderkumar, Symbiosis Institute of \nComputer Studies and Research, India Hanne Thomas, University of Applied Sciences and \nArts Northwestern Switzerland, Switzerland Bin Dong, Ricoh Software Research Center Beijing \nCo., Ltd., China Jin Wu, Xi'an University of Posts and Telecommunications, China Weigang Li, \nUniversity of Brasilia, Brazil Yanling Li, Inner Mongolia Normal University, China Muralidhar \nKurni, GITAM (Deemed to be University), India Asfand-e-yar Muhammad, Bahria University, \nPakistan Rongheng Lin, Beijing University of Posts and Telecommunications, China Jie \nFang, Xi\u2019an University of Posts and Telecommunications, China Ana Luisa V. Leal, University \nof Macau, Macau, China Rong Lan, Xi\u2019an University of Posts and Telecommunications, China \nAmando P. Singun Jr., University of Technology and Applied Sciences in Muscat, Oman \u2026", "Automatic speech recognition (ASR) and its post-processing, such as recognition error correction, are usually cascaded in a pipeline ignoring their strong interconnection. Inspired by the recent progress of leveraging text data to improve linguistic modeling, we propose a Unified ASR and error Correction framework (UAC), coupling speech recognition and error correction to capture richer semantic information for improving the performance of speech recognition. The proposed framework established interaction between speech and textual representations via explicitly fusing their uni-modal embeddings in a shared encoder. Additionally, the proposed framework is flexible to operate in either synchronous or asynchronous variant and could be equipped with modality and task tags enhancing its adaptation to heterogeneous inputs. Experimental results on accented and noisy speech datasets demonstrate that our\u00a0\u2026", "Accent recognition (AR) is challenging due to the lack of training data as well as the accents are entangled with speakers and regional characteristics. This paper aims to improve AR performance from two perspectives. First, to alleviate the data insufficiency problem, we employ the self-supervised learning representations (SSLRs) extracted from a pre-trained model to build the AR models. With the help of SSLRs, it gains significant performance improvement compared with the traditional acoustic features. Secondly, we proposed a persistent accent memory (PAM) as contextual knowledge to bias the AR models. The accent embeddings that are extracted from all training data by the encoder of AR models are clustered to form an accent codebook, ie PAM. In addition, we propose diverse attention mechanisms to investigate the optimal utilization of PAM. We observe that the best performance is obtained by selecting\u00a0\u2026", "The use of self-supervised pre-trained speech models has greatly improved speech tasks in low-resource settings. However, fine-tuning the entire model can be computationally expensive and not scalable for multiple tasks (eg, personalized ASR). While recent approaches have tried to solve this issue by training adapters, they fail to match the performance of full finetuning models, possibly due to the challenge of task domain transferability. Our proposed method enhances the performance of vanilla adapter tuning for ASR by using a simple yet effective token-dependent bias. This approach adds a token-specific representation shift (bias) to the intermediate representations of a pre-trained model, which better maps the latent features of the frozen network to the task domain. Our approach yields better recognition results with the adapter tuning strategy and achieves the performance of a full fine-tuning model on\u00a0\u2026", "Spoken Keyword Spotting (KWS) in noisy far-field environments is challenging for small-footprint models, given the restrictions on computational resources (eg, model size, running memory). This is even more intricate when handling noises from multiple microphones. To address this, we present a new multi-channel model that uses a CNN-based network with a linear mixing unit to achieve local-global dependency representations. Our method enhances noise-robustness while ensuring more efficient computation. Besides, we propose an end-toend centroid-based awareness module that provides class similarity awareness at the bottleneck level to correct ambiguous cases during prediction. We conducted experiments using real noisy far-field data from the MISP challenge 2021 and achieved SOTA results compared to existing small-footprint KWS models. Our best score of 0.126 is highly competitive against\u00a0\u2026", "This paper presents a generative neural network to estimate room impulse response (RIR) directly from the received reverberant speech in single-channel scenario. Complex spectrogram of the reverberant speech is used as the input of an encoder to produce the compact acoustic embedding, which is then fed to a generator to construct the related time-domain acoustic response. To avoid a large model to generate the RIR with long taps, we propose SG-RIR, a novel segmental generative network that splits the RIR into segments and shares the network parameters across segments for blind RIR estimation. Experimental results show that the proposed model is capable of estimating the time-domain RIR with mean error of 0.008 in terms of both simulated and measured RIR test sets. The effectiveness is further verified by the achieved competitive estimation accuracy of two key room acoustic parameters (the reverberation time RT and the direct-to-reverberant ratio DRR) as compared to state-of-the-art approaches that are specific for RT and DRR estimation.", "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker\u2019s speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker\u2019s speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification sys-tem. Experimental results show that the proposed approach sig-nificantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline.", "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker\u2019s speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker\u2019s speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification sys-tem. Experimental results show that the proposed approach sig-nificantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline.", "Presents the table of contents for this issue of the publication.", "This technical report describes the details to the strategy we adopted for Task 1 in the MISP Challenge 2021. In this work, we proposed a multimodal wake-up word detection model that handles the audio and the visual input to determine the presence of a predefined word. This involves the use of the extracted latent representation from a trained multi-channel audio keyword spotting and a video classification model with a shallow fusion to predict the outcome of our task. To allow a more robust performance under the noisy and far-field environment, curriculum learning based on the distance and the level of the augmented noise is exploited with increasing difficulty in multiple stages. Moreover, we applied the weighted prediction error (WPE) for speech dereverberation to enhance our utterance for a better learning. Lastly, we built a separate of two system with different audio model, namely with temporal-convolution (TC-ResNet) and ConvMixer and we take the average of the predictive score on two system. Our experimental result for team ALISPEECH has achieved top 6 on the leaderboard with a score of 0.109.", "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker\u2019s speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker\u2019s speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification sys-tem. Experimental results show that the proposed approach sig-nificantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline.", "In this paper, we propose a novel semi-class-based n-gram language modeling. The proposed modeling estimates the n-gram probability from the observed frequencies of word-class n-tuples, constituted by the (n-1) classes of preceding (n-1) words of the utterance and the current word itself. Three kinds of language modeling, word-based, class-based and semi-class-based n-gram modeling are implemented to build bi-gram and tri-gram models for a vocabulary of 50k words over a corpus of over 200 millions Chinese words. The parameter numbers and LM perplexities among the three models have been studied and compared. Our experiments show that our proposal of using the semi-class language modeling is a good tradeoff between the number of parameters and LM perplexity.", "Provides a listing of current committee members and society officers.", "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker\u2019s speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker\u2019s speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification sys-tem. Experimental results show that the proposed approach sig-nificantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline.", "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker\u2019s speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker\u2019s speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification sys-tem. Experimental results show that the proposed approach sig-nificantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline.", "The performance of speaker verification degrades significantly when the test speech is corrupted by interference from non-target speakers. Speaker diarization separates speakers well only if the speakers are not overlapped. However, if multiple talkers speak at the same time, we need a technique to separate the speech in the spectral domain. In this paper, we study a way to extract the target speaker\u2019s speech from an overlapped multi-talker speech. Specifically, given some reference speech samples from the target speaker, the target speaker\u2019s speech is firstly extracted from the overlapped multi-talker speech, then the extracted speech is processed in the speaker verification sys-tem. Experimental results show that the proposed approach sig-nificantly improves the performance of overlapped multi-talker speaker verification and achieves 64.4% relative EER reduction over the zero-effort baseline.", "In this paper, we present our NTU-XJU system for the oriental language recognition (OLR) challenge, AP20-OLR. The challenge this year contained three tasks:(1) cross-channel LID,(2) dialect identification and (3) noisy LID. We implemented only one network for all Tasks, which is x-vector, with pseudo labeling techniques and multiple data augmentation methods. For task 1, we achieved Cavg values of 0.169 on given dev set defined by official committee of AP20-OLR, and Cavg value of 0.074 on our own defined dev set from experimental data of former challenges for task 2.", null, null]}}