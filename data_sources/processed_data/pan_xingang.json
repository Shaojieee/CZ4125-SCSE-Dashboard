{"full_name": "Pan Xingang", "designation": "Assistant Professor, School of Computer Science and Engineering", "email": "xingang.pan@ntu.edu.sg", "image_path": "./profile_img/pan_xingang.jpg", "biography": "Xingang Pan is an Assistant Professor at the School of Computer Science and Engineering, Nanyang Technological University, Singapore. Prior to joining NTU, he was a postdoc researcher at Max Planck Institute for Informatics, advised by Prof. Christian Theobalt. He received his Ph.D. degree at MMLab of The Chinese University of Hong Kong in 2021, supervised by Prof. Xiaoou Tang. He obtained his Bachelor\u2019s degree from Tsinghua University in 2016. His research interests include computer vision, machine learning, and computer graphics, with a focus on generative AI and neural rendering. He has published over 20 papers on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, SIGGRAPH, ICLR, NeurIPS, ICML, AAAI, T-PAMI, etc. He received the Hong Kong PhD Fellowship Scheme (HKFPS) award from the Hong Kong government. He won the WAD Drivable Area Segmentation Challenge in 2018 and the Tusimple Lane Detection Challenge in 2017. His main works include DragGAN, GAN2Shape, and Deep Generative Prior.", "grants": "AI-Human Collaborative High-Quality 3D Content Generation", "google_scholar": "https://scholar.google.com/citations?hl=en&user=uo0q9WgAAAAJ", "orcid": null, "github": "https://xingangpan.github.io/", "scopus": null, "web_of_science": null, "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp02182", "other_websites": [], "interests": ["Computer Vision", "Machine Learning", "Computer Graphics"], "bachelor_degree": null, "masters": null, "phd": "MMLab of The Chinese University of Hong Kong", "collaboration_network": {"target": ["Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Jianping Shi", "Jianping Shi", "Jianping Shi", "Jianping Shi", "Jianping Shi", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Christian Theobalt", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Chen Change Loy", "Chen Change Loy", "Chen Change Loy", "Chen Change Loy", "Chen Change Loy", "Chen Change Loy", "Xiaogang Wang", "Ayush Tewari", "Ayush Tewari", "Ayush Tewari", "Ayush Tewari", "Ayush Tewari", "Lingjie Liu", "Lingjie Liu", "Mohamed Elgharib", "Mohamed Elgharib", "Mohamed Elgharib", "Mohamed Elgharib", "Maneesh Agrawala", "Karol Myszkowski", "Karol Myszkowski", "Hans-Peter Seidel", "Hans-Peter Seidel"], "target_id": ["aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "eIWg8NMAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "559LF80AAAAJ", "559LF80AAAAJ", "559LF80AAAAJ", "559LF80AAAAJ", "559LF80AAAAJ", "559LF80AAAAJ", "-B5JgjsAAAAJ", "pDnzpeoAAAAJ", "pDnzpeoAAAAJ", "pDnzpeoAAAAJ", "pDnzpeoAAAAJ", "pDnzpeoAAAAJ", "HZPnJ9gAAAAJ", "HZPnJ9gAAAAJ", "e1WLgm8AAAAJ", "e1WLgm8AAAAJ", "e1WLgm8AAAAJ", "e1WLgm8AAAAJ", "YPzKczYAAAAJ", "eZ40F-MAAAAJ", "eZ40F-MAAAAJ", "s2Ibok8AAAAJ", "s2Ibok8AAAAJ"], "type": ["Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU"], "location": ["University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Chinese University of HongKong", "MIT", "MIT", "MIT", "MIT", "MIT", "UPenn", "UPenn", "Unknown", "Unknown", "Unknown", "Unknown", "Stanford University", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics", "Max Planck Institute for Informatics"], "year": [2022, 2021, 2021, 2021, 2018, 2018, 2020, 2019, 2021, 2023, 2020, 2022, 2021, 2021, 2022, 2023, 2021, "unknown", 2020, 2023, 2019, 2023, 2021, 2022, 2023, 2021, 2023, "unknown", 2020, 2021, 2021, 2021, 2021, 2018, 2018, 2019, 2021, 2021, 2018, 2018, 2019, 2023, 2023, 2022, 2022, 2023, 2023, 2022, 2022, 2021, 2023, 2023, 2023, 2020, 2022, 2021, 2021, 2019, 2020, 2019, 2020, 2021, 2020, 2023, 2019, 2021, 2021, 2021, 2020, 2021, 2019, 2021, 2021, 2018, 2022, 2023, 2022, 2022, 2023, 2023, 2022, 2023, 2023, 2023, 2022, 2022, 2023, 2023, 2023, 2023], "title": ["Network Training Method, Electronic Device And Storage Medium", "Method and Device for Image Processing, and Computer Storage Medium", "Method and Device for Image Processing, Electronic Device, and Storage Medium", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Spatial as Deep: Spatial CNN for Traffic Scene Understanding", "Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net", "Channel Equilibrium Networks for Learning Deep Representation", "Switchable Whitening for Deep Representation Learning", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation", "Open Compound Domain Adaptation", "Network Training Method, Electronic Device And Storage Medium", "Image Processing Method and Device, and Network Training Method and Device", "Generative Occupancy Fields for 3D Surface-Aware Image Synthesis", "BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering", "Grid-guided Neural Radiance Fields for Large Urban Scenes", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Supplementary for AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation", "Self-Supervised Scene De-occlusion", "Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction", "Self-Supervised Learning via Conditional Motion Propagation", "AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation", "Generative Occupancy Fields for 3D Surface-Aware Image Synthesis", "BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering", "Grid-guided Neural Radiance Fields for Large Urban Scenes", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR", "Supplementary for AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation", "Self-Supervised Scene De-occlusion", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Method and Device for Image Processing, and Computer Storage Medium", "Method and Device for Image Processing, Electronic Device, and Storage Medium", "Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net", "Spatial as Deep: Spatial CNN for Traffic Scene Understanding", "Switchable Whitening for Deep Representation Learning", "Method and Device for Image Processing, and Computer Storage Medium", "Method and Device for Image Processing, Electronic Device, and Storage Medium", "Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net", "Spatial as Deep: Spatial CNN for Traffic Scene Understanding", "Switchable Whitening for Deep Representation Learning", "HQ3DAvatar: High Quality Controllable 3D Head Avatar", "GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild", "gCoRF: Generative Compositional Radiance Fields", "BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering", "Grid-guided Neural Radiance Fields for Large Urban Scenes", "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold", "GAN2X: Non-Lambertian Inverse Rendering of Image GANs", "Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction", "AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars", "GVP: Generative Volumetric Primitives", "Open Compound Domain Adaptation", "Network Training Method, Electronic Device And Storage Medium", "Image Processing Method and Device, and Network Training Method and Device", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Switchable Whitening for Deep Representation Learning", "Self-Supervised Scene De-occlusion", "Self-Supervised Learning via Conditional Motion Propagation", "Open Compound Domain Adaptation", "Image Processing Method and Device, and Network Training Method and Device", "Self-Supervised Scene De-occlusion", "Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction", "Self-Supervised Learning via Conditional Motion Propagation", "Talk-To-Edit: Fine-Grained Facial Editing via Dialog", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Self-Supervised Scene De-occlusion", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Self-Supervised Learning via Conditional Motion Propagation", "Talk-To-Edit: Fine-Grained Facial Editing via Dialog", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Spatial as Deep: Spatial CNN for Traffic Scene Understanding", "gCoRF: Generative Compositional Radiance Fields", "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold", "GAN2X: Non-Lambertian Inverse Rendering of Image GANs", "Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images", "AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars", "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold", "GAN2X: Non-Lambertian Inverse Rendering of Image GANs", "HQ3DAvatar: High Quality Controllable 3D Head Avatar", "AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars", "GVP: Generative Volumetric Primitives", "gCoRF: Generative Compositional Radiance Fields", "Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images", "GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild", "A Neural Implicit Representation for the Image Stack: Depth, All in Focus, and High Dynamic Range", "GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild", "A Neural Implicit Representation for the Image Stack: Depth, All in Focus, and High Dynamic Range"], "link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:mB3voiENLucC"]}, "published_by_year": {"Year": ["2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Publications": [2, 2, 3, 9, 5, 10, 1]}, "citations_by_year": {"Year": ["2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [17, 100, 231, 505, 791, 857, 11]}, "all_time_h_index": 15, "all_time_i10_index": 16, "h_index_by_publication_year": {"Publication Year": [2018, 2019, 2020, 2021, 2022, 2023], "h-index": [2, 2, 3, 5, 3, 2]}, "avg_citations_by_publication_year": {"Publication Year": [2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [726.0, 61.5, 84.0, 55.666666666666664, 24.2, 5.2]}, "h_index_by_year": {"Year": [2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 3, 6, 8, 12, 15]}, "h_index_by_years_from_publication_year": {"Publication Year": [2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 3, 3, 2, 5, 5, 2, 3, 2]}, "all_time_i20_index": 15, "publications": {"Publication Year": ["2018", "2018", "2021", "2020", "2020", "2021", "2022", "2019", "2021", "2021", "2021", "2019", "2022", "2023", "2023", "2020", "2022", "2023", "2021", "2021", "2023", "2023", "2022", "2021", "2023", "2023", "2023", "2023", "2023", "2022", "2021", "Unknown"], "Title": ["Spatial as Deep: Spatial CNN for Traffic Scene Understanding", "Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Open Compound Domain Adaptation", "Self-Supervised Scene De-occlusion", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering", "Switchable Whitening for Deep Representation Learning", "Talk-To-Edit: Fine-Grained Facial Editing via Dialog", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Generative Occupancy Fields for 3D Surface-Aware Image Synthesis", "Self-Supervised Learning via Conditional Motion Propagation", "Disentangled3D: Learning a 3D Generative Model with Disentangled Geometry and Appearance from Monocular Images", "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold", "Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction", "Channel Equilibrium Networks for Learning Deep Representation", "GAN2X: Non-Lambertian Inverse Rendering of Image GANs", "HQ3DAvatar: High Quality Controllable 3D Head Avatar", "Method and Device for Image Processing, and Computer Storage Medium", "Method and Device for Image Processing, Electronic Device, and Storage Medium", "AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation", "Grid-guided Neural Radiance Fields for Large Urban Scenes", "gCoRF: Generative Compositional Radiance Fields", "Image Processing Method and Device, and Network Training Method and Device", "MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR", "AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars", "GVP: Generative Volumetric Primitives", "A Neural Implicit Representation for the Image Stack: Depth, All in Focus, and High Dynamic Range", "GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild", "Network Training Method, Electronic Device And Storage Medium", "Deep Learning for Generalizable Visual Understanding and Editing", "Supplementary for AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:2osOgNQ5qMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:YsMSGLbcyi4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:ULOm3_A8WrAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:kNdYIx-mwKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:W7OEmFMy1HYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:7PzlFSSx8tAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:hqOjcs7Dif8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&citation_for_view=uo0q9WgAAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:4DMP91E08xMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=uo0q9WgAAAAJ&cstart=20&pagesize=80&citation_for_view=uo0q9WgAAAAJ:qUcmZB5y_30C"], "Topic": ["Others", "Others", "Others", "Others", "Others", "Others", "Computer Scienc", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Artificial Intelligenc", "Federated Learning", "Others", "Federated Learning", "Computer Scienc", "Others", "Others", "Federated Learning", "Computer Scienc", "Federated Learning", "Others", "Federated Learning", "Computer Scienc", "Computer Scienc", "Federated Learning", "Federated Learning", "Artificial Intelligenc", "Others", "Others"], "# of Citations": [810, 642, 247, 129, 109, 97, 85, 85, 62, 57, 41, 40, 28, 26, 22, 15, 7, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-by-slice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+ CNN (MRFNet) in the lane detection dataset by 8.7% and 4.6% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53%.", "Convolutional neural networks (CNNs) have achieved great successes in many computer vision problems. Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNN\u2019s modeling ability on one domain (eg Cityscapes) as well as its generalization capacity on another domain (eg GTA5) without finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances. This work has three key contributions.(1) By delving into IN and BN, we disclose that IN learns features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, while BN is essential for preserving content related information.(2) IBN-Net can be applied to many advanced deep architectures, such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their performance without increasing computational cost.(3) When applying the trained networks to new domains, eg from GTA5 to Cityscapes, IBN-Net achieves comparable improvements as domain adaptation methods, even without using data from the target domain. With IBN-Net, we won the 1st place on the WAD 2018 Challenge Drivable Area track, with an mIoU of 86.18%.", "Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig. 1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible restoration and manipulation are made possible through relaxing the assumption of existing GAN inversion methods, which tend to fix the generator\u00a0\u2026", "A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (eg, sunny weather) for achieving high performance on the test data in a target domain (eg, rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (eg, changes in weather). We study an open compound domain adaptation (OCDA) problem, in which the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model's agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.", null, "Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code is available at https://github.com/XingangPan/GAN2Shape.", "Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering\u00a0\u2026", "Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig. 1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset.", "Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (eg, editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual\"\" semantic field\"\" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field. We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants.", "The advancement of generative radiance fields has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code is available at https://github. com/XingangPan/ShadeGAN.", "The advent of generative radiance fields has significantly promoted the development of 3D-aware image synthesis. The cumulative rendering process in radiance fields makes training these generative models much easier since gradients are distributed over the entire volume, but leads to diffused object surfaces. In the meantime, compared to radiance fields occupancy representations could inherently ensure deterministic surfaces. However, if we directly apply occupancy representations to generative models, during training they will only receive sparse gradients located on object surfaces and eventually suffer from the convergence problem. In this paper, we propose Generative Occupancy Fields (GOF), a novel model based on generative radiance fields that can learn compact object surfaces without impeding its training convergence. The key insight of GOF is a dedicated transition from the cumulative rendering in radiance fields to rendering with only the surface points as the learned surface gets more and more accurate. In this way, GOF combines the merits of two representations in a unified framework. In practice, the training-time transition of start from radiance fields and march to occupancy representations is achieved in GOF by gradually shrinking the sampling region in its rendering process from the entire volume to a minimal neighboring region around the surface. Through comprehensive experiments on multiple datasets, we demonstrate that GOF can synthesize high-quality images with 3D consistency and simultaneously learn compact and smooth object surfaces. Our code is available at https://github. com/SheldonTsui\u00a0\u2026", "Intelligent agent naturally learns from motion. Various self-supervised algorithms have leveraged the motion cues to learn effective visual representations. The hurdle here is that motion is both ambiguous and complex, rendering previous works either suffer from degraded learning efficacy, or resort to strong assumptions on object motions. In this work, we design a new learning-from-motion paradigm to bridge these gaps. Instead of explicitly modeling the motion probabilities, we design the pretext task as a conditional motion propagation problem. Given an input image and several sparse flow guidance on it, our framework seeks to recover the full-image motion. Compared to other alternatives, our framework has several appealing properties:(1) Using sparse flow guidance during training resolves the inherent motion ambiguity, and thus easing feature learning.(2) Solving the pretext task of conditional motion propagation encourages the emergence of kinematically-sound representations that poss greater expressive power. Extensive experiments demonstrate that our framework learns structural and coherent features; and achieves state-of-the-art self-supervision performance on several downstream tasks including semantic segmentation, instance segmentation and human parsing. Furthermore, our framework is successfully extended to several useful applications such as semi-automatic pixel-level annotation.", "Learning 3D generative models from a dataset of monocular images enables self-supervised 3D reasoning and controllable synthesis. State-of-the-art 3D generative models are GANs which use neural 3D volumetric representations for synthesis. Images are synthesized by rendering the volumes from a given camera. These models can disentangle the 3D scene from the camera viewpoint in any generated image. However, most models do not disentangle other factors of image formation, such as geometry and appearance. In this paper, we design a 3D GAN which can learn a disentangled model of objects, just from monocular observations. Our model can disentangle the geometry and appearance variations in the scene, ie, we can independently sample from the geometry and appearance spaces of the generative model. This is achieved using a novel non-rigid deformable scene formulation. A 3D volume which represents an object instance is computed as a non-rigidly deformed canonical 3D volume. Our method learns the canonical volume, as well as its deformations, jointly during training. This formulation also helps us improve the disentanglement between the 3D scene and the camera viewpoints using a novel pose regularization loss defined on the 3D deformation field. In addition, we further model the inverse deformations, enabling the computation of dense correspondences between images generated by our model. Finally, we design an approach to embed real images onto the latent space of our disentangled generative model, enabling editing of real images.", "Synthesizing visual content that meets users\u2019 needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to \"drag\" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can\u00a0\u2026", "Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model, and they require hours of training for a single scene. Recent efforts explore the explicit volumetric representation, which substantially accelerates the optimization process by memorizing significant information in learnable voxel grids. However, these voxel-based methods often struggle in reconstructing fine-grained geometry. Through empirical studies, we found that high-quality surface reconstruction hinges on two key factors: the capability of constructing a coherent shape and the precise modeling of color-geometry dependency. In particular, the latter is the key to the accurate reconstruction of fine details. Inspired by these findings, we develop Voxurf, a voxel-based approach for efficient and accurate neural surface reconstruction, which consists of two stages: 1) leverage a learnable feature grid to construct the color field and obtain a coherent coarse shape, and 2) refine detailed geometry with a dual color network that captures precise color-geometry dependency. We further introduce a hierarchical geometry feature to enable information sharing across voxels. Our experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality compared to state-of-the-art methods, with 20x speedup in training.", "Convolutional Neural Networks (CNNs) are typically constructed by stacking multiple building blocks, each of which contains a normalization layer such as batch normalization (BN) and a rectified linear function such as ReLU. However, this work shows that the combination of normalization and rectified linear function leads to inhibited channels, which have small magnitude and contribute little to the learned feature representation, impeding the generalization ability of CNNs. Unlike prior arts that simply removed the inhibited channels, we propose to \u201cwake them up\u201d during training by designing a novel neural building block, termed Channel Equilibrium (CE) block, which enables channels at the same layer to contribute equally to the learned representation. We show that CE is able to prevent inhibited channels both empirically and theoretically. CE has several appealing benefits.(1) It can be integrated into many advanced CNN architectures such as ResNet and MobileNet, outperforming their original networks.(2) CE has an interesting connection with the Nash Equilibrium, a well-known solution of a non-cooperative game.(3) Extensive experiments show that CE achieves state-of-the-art performance on various challenging benchmarks such as ImageNet and COCO.", "2D images are observations of the 3D physical world depicted with the geometry, material, and illumination components. Recovering these underlying intrinsic components from 2D images, also known as inverse rendering, usually requires a supervised setting with paired images collected from multiple viewpoints and lighting conditions, which is resource-demanding. In this work, we present GAN2X, a new method for unsupervised inverse rendering that only uses unpaired images for training. Unlike previous Shape-from-GAN approaches that mainly focus on 3D shapes, we take the first attempt to also recover non-Lambertian material properties by exploiting the pseudo paired data generated by a GAN. To achieve precise inverse rendering, we devise a specularity-aware neural surface representation that continuously models the geometry and material properties. A shading-based refinement technique is\u00a0\u2026", "Multi-view volumetric rendering techniques have recently shown great potential in modeling and synthesizing high-quality head avatars. A common approach to capture full head dynamic performances is to track the underlying geometry using a mesh-based template or 3D cube-based graphics primitives. While these model-based approaches achieve promising results, they often fail to learn complex geometric details such as the mouth interior, hair, and topological changes over time. This paper presents a novel approach to building highly photorealistic digital head avatars. Our method learns a canonical space via an implicit function parameterized by a neural network. It leverages multiresolution hash encoding in the learned feature space, allowing for high-quality, faster training and high-resolution rendering. At test time, our method is driven by a monocular RGB video. Here, an image encoder extracts face-specific features that also condition the learnable canonical space. This encourages deformation-dependent texture variations during training. We also propose a novel optical flow based loss that ensures correspondences in the learned canonical space, thus encouraging artifact-free and temporally consistent renderings. We show results on challenging facial expressions and show free-viewpoint renderings at interactive real-time rates for medium image resolutions. Our method outperforms all existing approaches, both visually and numerically. We will release our multiple-identity dataset to encourage further research. Our Project page is available at: https://vcai.mpi-inf.mpg.de/projects/HQ3DAvatar/", "A method and device for image recognition and a storage medium are provided. The method includes: a target image is acquired; feature extraction processing is performed on the target image through convolutional layers in a neural network model to obtain feature maps, and Instance Normalization (IN) and Batch Normalization (BN) processing is performed on the feature maps to obtain a recognition result of the target image; and the recognition result of the target image is output.", "A method for image processing, an electronic device, and a storage medium are provided. The method includes the following. For each processing method in a preset set of processing methods, a first feature parameter and a second feature parameter are determined according to image data to-be-processed, where the preset set includes at least two processing methods selected from whitening methods and/or normalization methods, and the image data to-be-processed includes at least one image data. A first weighted average of the first feature parameters is determined according to a weight coefficient of each first feature parameter, and a second weighted average of the second feature parameters is determined according to a weight coefficient of each second feature parameter. The image data to-be-processed is whitened according to the first weighted average and the second weighted average.", "Both indoor and outdoor environments are inherently structured and repetitive. Traditional modeling pipelines keep an asset library storing unique object templates, which is both versatile and memory efficient in practice. Inspired by this observation, we propose AssetField, a novel neural scene representation that learns a set of object-aware ground feature planes to represent the scene, where an asset library storing template feature patches can be constructed in an unsupervised manner. Unlike existing methods which require object masks to query spatial points for object editing, our ground feature plane representation offers a natural visualization of the scene in the bird-eye view, allowing a variety of operations (e.g. translation, duplication, deformation) on objects to configure a new scene. With the template feature patches, group editing is enabled for scenes with many recurring items to avoid repetitive work on object individuals. We show that AssetField not only achieves competitive performance for novel-view synthesis but also generates realistic renderings for new scene configurations.", "Purely MLP-based neural radiance fields (NeRF-based methods) often suffer from underfitting with blurred renderings on large-scale scenes due to limited model capacity. Recent approaches propose to geographically divide the scene and adopt multiple sub-NeRFs to model each region individually, leading to linear scale-up in training costs and the number of sub-NeRFs as the scene expands. An alternative solution is to use a feature grid representation, which is computationally efficient and can naturally scale to a large scene with increased grid resolutions. However, the feature grid tends to be less constrained and often reaches suboptimal solutions, producing noisy artifacts in renderings, especially in regions with complex geometry and texture. In this work, we present a new framework that realizes high-fidelity rendering on large urban scenes while being computationally efficient. We propose to use a compact multi-resolution ground feature plane representation to coarsely capture the scene, and complement it with positional encoding inputs through another NeRF branch for rendering in a joint learning fashion. We show that such an integration can utilize the advantages of two alternative solutions: a light-weighted NeRF is sufficient, under the guidance of the feature grid representation, to render photorealistic novel views with fine details; and the jointly optimized ground feature planes, can meanwhile gain further refinements, forming a more accurate and compact feature space and output much more natural rendering results.", "3D generative models of objects enable photorealistic image synthesis with 3D control. Existing methods model the scene as a global scene representation, ignoring the compositional aspect of the scene. Compositional reasoning can enable a wide variety of editing applications, in addition to enabling generalizable 3D reasoning. In this paper, we present a compositional generative model, where each semantic part of the object is represented as an independent 3D representation learned from only in-the-wild 2D data. We start with a global generative model (GAN) and learn to decompose it into different semantic parts using supervision from 2D segmentation masks. We then learn to composite independently sampled parts in order to create coherent global scenes. Different parts can be independently sampled while keeping the rest of the object fixed. We evaluate our method on a wide variety of objects and parts and demonstrate editing applications.", "An image processing method and a device, and a network training method and a device are provided. The image processing method includes determining a guide group arranged on an image to be processed and directed at a target object, the guide group comprising at least one guide point, and the guide point being used to indicate the position of a sampling pixel, and the magnitude and direction of the motion speed of the sampling pixel; and on the basis of the guide point in the guide group and the image to be processed, performing optical flow prediction to obtain the motion of the target object in the image to be processed.", "Based on powerful text-to-image diffusion models, text-to-3D generation has made significant progress in generating compelling geometry and appearance. However, existing methods still struggle to recover high-fidelity object materials, either only considering Lambertian reflectance, or failing to disentangle BRDF materials from the environment lights. In this work, we propose Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR (\\textbf{MATLABER}) that leverages a novel latent BRDF auto-encoder for material generation. We train this auto-encoder with large-scale real-world BRDF collections and ensure the smoothness of its latent space, which implicitly acts as a natural distribution of materials. During appearance modeling in text-to-3D generation, the latent BRDF embeddings, rather than BRDF parameters, are predicted via a material network. Through exhaustive experiments, our approach demonstrates the superiority over existing ones in generating realistic and coherent object materials. Moreover, high-quality materials naturally enable multiple downstream tasks such as relighting and material editing. Code and model will be publicly available at \\url{https://sheldontsui.github.io/projects/Matlaber}.", "Capturing and editing full head performances enables the creation of virtual characters with various applications such as extended reality and media production. The past few years witnessed a steep rise in the photorealism of human head avatars. Such avatars can be controlled through different input data modalities, including RGB, audio, depth, IMUs and others. While these data modalities provide effective means of control, they mostly focus on editing the head movements such as the facial expressions, head pose and/or camera viewpoint. In this paper, we propose AvatarStudio, a text-based method for editing the appearance of a dynamic full head avatar. Our approach builds on existing work to capture dynamic performances of human heads using neural radiance field (NeRF) and edits this representation with a text-to-image diffusion model. Specifically, we introduce an optimization strategy for incorporating\u00a0\u2026", "Advances in 3D-aware generative models have pushed the boundary of image synthesis with explicit camera control. To achieve high-resolution image synthesis, several attempts have been made to design efficient generators, such as hybrid architectures with both 3D and 2D components. However, such a design compromises multiview consistency, and the design of a pure 3D generator with high resolution is still an open problem. In this work, we present Generative Volumetric Primitives (GVP), the first pure 3D generative model that can sample and render 512-resolution images in real-time. GVP jointly models a number of volumetric primitives and their spatial information, both of which can be efficiently generated via a 2D convolutional network. The mixture of these primitives naturally captures the sparsity and correspondence in the 3D volume. The training of such a generator with a high degree of freedom is made possible through a knowledge distillation technique. Experiments on several datasets demonstrate superior efficiency and 3D consistency of GVP over the state-of-the-art.", "A Neural Implicit Representation for the Image Stack: Depth, All in Focus, and High \nDynamic Range :: MPG.PuRe English Help Privacy Policy Disclaimer Include files \nAdvanced SearchBrowse START BASKET (0)Tools Item ITEM ACTIONSEXPORT Add to \nBasket Local TagsRelease HistoryDetailsSummary Released Journal Article A Neural \nImplicit Representation for the Image Stack: Depth, All in Focus, and High Dynamic Range \nMPS-Authors /persons/resource/persons263980 Wang, Chao Computer Graphics, MPI for \nInformatics, Max Planck Society; /persons/resource/persons282904 Pan, Xingang Visual \nComputing and Artificial Intelligence, MPI for Informatics, Max Planck Society; /persons/resource/persons221198 \nWolski, Krzysztof Computer Graphics, MPI for Informatics, Max Planck Society; /persons/resource/persons263978 \n\u2026", "Most in-the-wild images are stored in Low Dynamic Range (LDR) form, serving as a partial observation of the High Dynamic Range (HDR) visual world. Despite limited dynamic range, these LDR images are often captured with different exposures, implicitly containing information about the underlying HDR image distribution. Inspired by this intuition, in this work we present, to the best of our knowledge, the first method for learning a generative model of HDR images from in-the-wild LDR image collections in a fully unsupervised manner. The key idea is to train a generative adversarial network (GAN) to generate HDR images which, when projected to LDR under various exposures, are indistinguishable from real LDR images. Experiments show that our method GlowGAN can synthesize photorealistic HDR images in many challenging cases such as landscapes, lightning, or windows, where previous supervised generative models produce overexposed images. With the assistance of GlowGAN, we showcase the innovative application of unsupervised inverse tone mapping (GlowGAN-ITM) that sets a new paradigm in this field. Unlike previous methods that gradually complete information from LDR input, GlowGAN-ITM searches the entire HDR image manifold modeled by GlowGAN for the HDR images which can be mapped back to the LDR input. GlowGAN-ITM method achieves more realistic reconstruction of overexposed regions compared to state-of-the-art supervised learning models, despite not requiring HDR images or paired multi-exposure images for training.", "The present disclosure relates to a network training method, an electronic device and a storage medium. The network training method includes the following steps. At least one implicit vector may be input into at least one pre-trained generative network to obtain a first generated image; the generative network may be obtained with a discriminative network through adversarial trainings with a plurality of natural images. A degradation process may be performed on the first generated image to obtain a first degraded image of the first generated image. The implicit vector and the generative network may be trained according to the first degraded image and a second degraded image of at least one target image; the trained generative network and the trained implicit vector may be used to generate at least one reconstructed image of the target image.", "Deep learning in visual understanding and editing tasks has witnessed great success in recent years, thanks to the strong modeling capacity of deep neural networks. A standard pipeline for these tasks is to train a deep learning model on a training set, and evaluate it on a testing set. Typically, the studies are conducted in a controlled setting, where the testing set would fall in the same domain as the training set, ie, they follow the same distribution.", "This supplementary is organized as follows:(1) In the first section we show some concrete examples of the useful asset library in industry when building indoor and outdoor environments.(2) The second part elaborate our training details, including model adaptation and hyper-parameter setting.(3) The third part includes ablation studies on the number of clusters to discretize ground feature plane; and alternations decode 2D ground feature into 3D.(4) Next we show some special cases which might require extra cautions or post-processing.(5) Finally we include some applications to demonstrate the flexibility of AssetField and its potential to cooperate with physical rendering pipelines. Video demos are also provided to show various editing effects using the asset library extracted by AssetField, spanning instance-level, category-level to scene-level manipulation."]}}