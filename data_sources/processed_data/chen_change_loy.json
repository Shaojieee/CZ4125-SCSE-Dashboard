{"full_name": "Chen Change Loy", "designation": "Professor, School of Computer Science and Engineering", "email": "ccloy@ntu.edu.sg", "image_path": "./profile_img/chen_change_loy.jpg", "biography": "Chen Change Loy is a Professor with the School of Computer Science and Engineering, Nanyang Technological University, Singapore. He received his PhD (2010) in Computer Science from the Queen Mary University of London. Prior to joining NTU, he served as a Research Assistant Professor at the MMLab of the Chinese University of Hong Kong, from 2013 to 2018.  He was a postdoctoral researcher at Queen Mary University of London and Vision Semantics Limited (acquired by Veritone), from 2010 to 2013. His research interests include computer vision and deep learning with a focus on image/video restoration and enhancement, generative tasks, and representation learning. He and his research group pioneer the research in face detection, face alignment, and image super-resolution by deep learning. His journal paper on image super-resolution was selected as the `Most Popular Article' by IEEE Transactions on Pattern Analysis and Machine Intelligence in 2016. It remains as one of the top 10 non-survey articles to date.He is recognized as one of the 100 most influential scholars in computer vision from 2020 to 2023 by AMiner. He received the Nanyang Associate Professorship Award in 2019. He was selected as the outstanding reviewer of ACCV 2014, BMVC 2017, and CVPR 2017.He serves as an Associate Editor of the Computer Vision and Image Understanding (CVIU), International Journal of Computer Vision (IJCV) and IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). He also serves/served as the Area Chair of top conferences such as ICCV, CVPR, ECCV, and NeurIPS. He has co-organized several workshops and challenges at major computer vision conferences. He is a senior member of IEEE.Check out MMLab@NTU.", "grants": "3D Geometry and Semantic Modeling for Human\u2010Scene InteractionCollaborative AI \u201cHuman-AI collaborative systems for multi-media content creation, Intelligent systems to improve human's decision with clear interpretation\u201dDeep Generative Modeling of 3D DataDeep Surround Visual Reasoning for Autonomous DrivingGoogle PhD FellowshipOpen-vocabulary visual recognition, Scene understanding, Foundation modelsResource-Efficient AI \u201cAI Understanding of Creation and Design, Learning from Small Datasets\u201dResource-Efficient AI \u201cHuman AI Co-Design, Resource Efficient Content Creation, AI Understanding of Design and Creation\u201dResource-Efficient AI \u201cLightweight model design for video restoration, Learn video restoration from limited data, Efficient blind video restoration\u201dSelf-Supervised Visual Representation LearningSpatial-Temporal Patch Graph for Fine-grained Action Analysis in the Real World", "google_scholar": "https://scholar.google.com/citations?hl=en&user=559LF80AAAAJ", "orcid": null, "github": null, "scopus": null, "web_of_science": null, "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp00574", "other_websites": ["https://personal.ntu.edu.sg/ccloy"], "interests": ["Computer Vision", "Image Processing", "Machine Learning"], "bachelor_degree": null, "masters": null, "phd": "Queen Mary University of London", "collaboration_network": {"target": ["Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Xiaoou Tang", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Ziwei Liu", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Dahua Lin", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Chao Dong", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Ping Luo (\u7f85\u5e73)", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Shaogang Gong", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Chongyi Li", "Ke Yu", "Ke Yu", "Ke Yu", "Ke Yu", "Ke Yu", "Ke Yu", "Ke Yu", "Ke Yu", "Ke Yu", "Ke Yu", "Xiaoxiao Li", "Xiaoxiao Li", "Xiaoxiao Li", "Xiaoxiao Li", "Xiaoxiao Li", "Xiaoxiao Li", "Xiaoxiao Li", "Xiaoxiao Li", "Shuo Yang", "Shuo Yang", "Shuo Yang", "Shuo Yang", "Shuo Yang", "Shuo Yang", "Shuo Yang", "Shuo Yang", "Shuo Yang", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Bo Dai", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Chen Qian", "Jianping Shi", "Jianping Shi", "Jianping Shi", "Jianping Shi", "Jianping Shi", "Jianping Shi", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Xintao Wang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Chen Huang", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Kelvin CK Chan", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Liming Jiang", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Cheng Li", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Chunxiao Liu", "Yuhang Cao", "Yuhang Cao", "Yuhang Cao", "Yuhang Cao", "Wanli Ouyang (\u6b27\u9633\u4e07\u91cc)", "Wanli Ouyang (\u6b27\u9633\u4e07\u91cc)", "Wanli Ouyang (\u6b27\u9633\u4e07\u91cc)", "Wanli Ouyang (\u6b27\u9633\u4e07\u91cc)", "Xiaogang Wang", "Xiaogang Wang", "Xiaogang Wang", "Xiaogang Wang", "Xiaogang Wang", "Xiaogang Wang", "Xiaogang Wang", "Xiaogang Wang", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Xiaohang Zhan", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Wenwei Zhang", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Shangchen Zhou", "Chunle Guo", "Chunle Guo", "Chunle Guo", "Chunle Guo", "Chunle Guo", "Chunle Guo", "Chunle Guo", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Kaiyang Zhou", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Jiangmiao Pang (\u5e9e\u6c5f\u6dfc)", "Zhanpeng Zhang (\u5f20\u5c55\u9e4f)", "Zhanpeng Zhang (\u5f20\u5c55\u9e4f)", "Zhanpeng Zhang (\u5f20\u5c55\u9e4f)", "Zhanpeng Zhang (\u5f20\u5c55\u9e4f)", "Zhanpeng Zhang (\u5f20\u5c55\u9e4f)", "Zhanpeng Zhang (\u5f20\u5c55\u9e4f)", "Yu Qiao", "Yu Qiao", "Yu Qiao", "Yu Qiao", "Kaiming He", "Tak Wai Hui", "Tak Wai Hui", "Tak Wai Hui", "Jiaqi Wang", "Jiaqi Wang", "Jiaqi Wang", "Jiaqi Wang", "Jiaqi Wang", "Jiaqi Wang", "Jiaqi Wang", "Jiaqi Wang", "Shuyang Sun", "Shuyang Sun", "Jinwei Gu", "Jinwei Gu", "Jinwei Gu", "Jinwei Gu", "Jinwei Gu", "Jinwei Gu", "Jinwei Gu", "Zhe Wang", "Zhe Wang", "Zhe Wang", "Zhe Wang", "Zhe Wang", "Zhe Wang", "Xiangyu Xu", "Xiangyu Xu", "Xiangyu Xu", "Xiangyu Xu", "Xiangyu Xu", "Xiangyu Xu", "Xiangyu Xu", "Yining Li", "Yining Li", "Yining Li", "Yining Li", "Yining Li", "Yu Xiong (\u718a\u5b87)", "Yuenan Hou", "Yuenan Hou", "Yuenan Hou", "Yuenan Hou", "Yuenan Hou", "Yuenan Hou", "Rui Xu", "Rui Xu", "Rui Xu", "Rui Xu", "Rui Xu", "Timothy M. Hospedales", "Timothy M. Hospedales", "Timothy M. Hospedales", "Timothy M. Hospedales", "Timothy M. Hospedales", "Pan Xingang", "Pan Xingang", "Pan Xingang", "Pan Xingang", "Pan Xingang", "Pan Xingang", "Jingkang Yang", "Jingkang Yang", "Jingkang Yang", "Jingkang Yang", "Ke Chen", "Ke Chen", "Yuhang Zang", "Yuhang Zang", "Yuhang Zang", "Yuhang Zang", "Yuhang Zang", "Yuhang Zang", "Yuhang Zang", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Shuai Yang (\u6768\u5e05)", "Lei Yang", "Lei Yang", "Lei Yang", "Lei Yang", "Lei Yang", "Lei Yang", "Lei Yang", "Zhongang Cai (\u8521\u4e2d\u6602)", "Zhongang Cai (\u8521\u4e2d\u6602)", "Zhongang Cai (\u8521\u4e2d\u6602)", "Zhongang Cai (\u8521\u4e2d\u6602)", "Zhongang Cai (\u8521\u4e2d\u6602)", "Zhongang Cai (\u8521\u4e2d\u6602)", "Zhongang Cai (\u8521\u4e2d\u6602)", "Zhongang Cai (\u8521\u4e2d\u6602)", "Bolei Zhou", "Bolei Zhou", "Bolei Zhou", "Bolei Zhou", "Jiahao Xie", "Jiahao Xie", "Jiahao Xie", "Jiahao Xie", "Jiahao Xie", "Jiahao Xie", "Yuming Jiang", "Yuming Jiang", "Yuming Jiang", "Yuming Jiang", "Yuming Jiang", "Yuming Jiang", "Ong Yew Soon", "Ong Yew Soon", "Ong Yew Soon", "Ong Yew Soon", "Ong Yew Soon", "Yonglong Tian", "Yonglong Tian", "Xingyu Zeng", "Hongsheng Li (\u674e\u9e3f\u5347)", "Hongsheng Li (\u674e\u9e3f\u5347)", "Shi Qiu", "Kaidi Cao", "Kaidi Cao", "Kaidi Cao", "Kaidi Cao", "Kaidi Cao", "Jinjin GU", "Xiatian Zhu", "Xiatian Zhu", "Xiatian Zhu", "Xiatian Zhu", "Xiatian Zhu", "Linsen Song", "Linsen Song", "Linsen Song", "Yuanjun Xiong", "Yuanjun Xiong", "Wei Li (\u674e\u5a01)", "Wei Li (\u674e\u5a01)", "Wei Li (\u674e\u5a01)", "Wei Li (\u674e\u5a01)", "Wei Li (\u674e\u5a01)", "Wei Li (\u674e\u5a01)", "Wei Li (\u674e\u5a01)", "Ruicheng Feng", "Ruicheng Feng", "Ruicheng Feng", "Ruicheng Feng", "Ruicheng Feng", "Ruicheng Feng", "Ruicheng Feng", "Saihui Hou", "Saihui Hou", "Xinyu Pan", "Xinyu Pan", "Li Siyao", "Li Siyao", "Li Siyao", "Li Siyao", "Li Siyao", "Xiangtai Li", "Xiangtai Li", "Xiangtai Li", "Xiangtai Li", "Xiangtai Li", "Xiangtai Li", "Xiangtai Li", "Kai Kang", "Kai Kang", "Kai Kang", "Yu Rong", "Yu Rong", "Yu Rong", "Yu Rong", "Lu Sheng\uff08\u76db\u5f8b\uff09", "Lu Sheng\uff08\u76db\u5f8b\uff09", "Lu Sheng\uff08\u76db\u5f8b\uff09", "Ren Daxuan", "Ren Daxuan", "Ren Daxuan", "Ren Daxuan", "Ren Daxuan", "Thomas S. Huang", "Yuchen Fan", "Ding Liu", "Yi-Zhe Song", "Yi-Zhe Song", "Shuai Yi", "Shuai Yi", "Shuai Yi", "Shuai Yi", "Zhang Junzhe", "Zhang Junzhe", "Zhang Junzhe", "Zhang Junzhe", "CP Lim", "CP Lim", "CP Lim", "WENG KIN LAI", "WENG KIN LAI", "WENG KIN LAI", "Shu Liu", "Hengshuang Zhao", "Jiaya Jia", "Yi Zhang", "Guodong Xu", "Guodong Xu", "Guodong Xu", "Xingcheng ZHANG", "Xingcheng ZHANG", "Linjie Yang", "Dapeng Chen", "Dapeng Chen", "Hang Zhou", "Hang Zhou", "Bihan Wen", "Liang Pan", "Liang Pan", "Liang Pan", "Qian Yu (\u4e8e\u831c)", "Feng Liu", "Liang Lin", "Liang Lin", "Chong Zhou", "Chong Zhou", "Mingyuan Zhang", "Mingyuan Zhang", "Mingyuan Zhang", "Mingyuan Zhang", "Weilin Huang", "Pan He", "Zhizhong Li", "Sanja Fidler", "Raquel Urtasun", "Huaijin Chen", "Huaijin Chen", "Sifei Liu", "Fangzhou Hong", "Fangzhou Hong", "Fangzhou Hong", "Haiyu Zhao", "Haiyu Zhao", "Fei Wang", "Henghui Ding", "Henghui Ding", "Henghui Ding", "Henghui Ding", "Ming-Ming Cheng", "Rui Zhao", "Yuekun Dai", "Yuekun Dai", "Yuekun Dai", "Wu Shi", "Marco Cristani", "Marco Cristani", "Kenneth Shum", "Jianyi Wang", "Jianyi Wang", "Jianyi Wang", "Wing Cheong Lau", "Wing Cheong Lau", "Zhibo Yang", "Zhibo Yang", "Ziqi Huang", "Ziqi Huang", "Kwan-Yee Lin (\u6797\u541b\u5100)", "Kwan-Yee Lin (\u6797\u541b\u5100)", "Kwan-Yee Lin (\u6797\u541b\u5100)", "Hongyang Li", "Dimitris N. Metaxas", "Wenxiu Sun", "Henry Gouk", "Linus Ericsson", "Hao Zhu", "Hao Zhu", "Jingbo Wang", "Jingbo Wang", "Jingbo Wang", "Xinchi Zhou", "Dongzhan Zhou", "Shuai Li", "Shuai Li", "Zhiyi CHENG", "Zhiyi CHENG", "Yuankai Qi", "Jianglin Fu", "Zongsheng Yue", "Zongsheng Yue", "Zongsheng Yue", "Xudong XU", "Christian Theobalt", "Haonan Qiu", "Zhexin Liang", "Zhexin Liang", "Chee Seng Chan", "Ven Jyn Kok", "Mei Kuan Lim", "Heng Yang", "Yuanbo Xiangli", "Yuen Fei Wong", "Guangcong Wang", "Guangcong Wang", "Weidong Yin", "Yanbo Xu", "Size Wu", "Size Wu", "Wentao Liu", "Li Bo", "Li Bo", "Peiqing Yang", "Peiqing Yang", "Wayne Zhang", "Zhaoxi Chen", "Yuanhan Zhang", "Quanzhou Li", "Qingyi Tao", "Qingyi Tao", "Davide Moltisanti", "Xiaoming Li", "Chenyang Si (\u53f8\u6668\u9633)", "Ceyuan Yang", "Tianxing Wu", "Yuhan Wang", "Yuxin Jiang", "Jing Shao", "Jing Shao", "Jing Shao", "Jing Shao", "Jing Shao", "Jing Shao", "Jing Shao", "Jing Shao", "Junjie Yan", "Junjie Yan", "Junjie Yan", "Hui Zhou", "Ramalingam Chellappa"], "target_id": ["qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "qpBtpGsAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "lc45xlcAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "GMzzRRUAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "OSDCB0UAAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "aXdjxb4AAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "Nhi0I8cAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "1_I0P-AAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "GatMy5UAAAAJ", "udZam0oAAAAJ", "udZam0oAAAAJ", "udZam0oAAAAJ", "udZam0oAAAAJ", "udZam0oAAAAJ", "udZam0oAAAAJ", "udZam0oAAAAJ", "udZam0oAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "U3KDpBUAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "KNWTvgEAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "AerkT0YAAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "mwsxrm4AAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "FQgZpQoAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QZ-JKOUAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "QYTu_KQAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "v_D9J7kAAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "F5rVlz0AAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "4m061tYAAAAJ", "sJkqsqkAAAAJ", "sJkqsqkAAAAJ", "sJkqsqkAAAAJ", "sJkqsqkAAAAJ", "pw_0Z_UAAAAJ", "pw_0Z_UAAAAJ", "pw_0Z_UAAAAJ", "pw_0Z_UAAAAJ", "-B5JgjsAAAAJ", "-B5JgjsAAAAJ", "-B5JgjsAAAAJ", "-B5JgjsAAAAJ", "-B5JgjsAAAAJ", "-B5JgjsAAAAJ", "-B5JgjsAAAAJ", "-B5JgjsAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QfquhDEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "QDXADSEAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "suaDwBQAAAAJ", "RZLYwR0AAAAJ", "RZLYwR0AAAAJ", "RZLYwR0AAAAJ", "RZLYwR0AAAAJ", "RZLYwR0AAAAJ", "RZLYwR0AAAAJ", "RZLYwR0AAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "gRIejugAAAAJ", "ssSfKpAAAAAJ", "ssSfKpAAAAAJ", "ssSfKpAAAAAJ", "ssSfKpAAAAAJ", "ssSfKpAAAAAJ", "ssSfKpAAAAAJ", "ssSfKpAAAAAJ", "ssSfKpAAAAAJ", "cVvfP4oAAAAJ", "cVvfP4oAAAAJ", "cVvfP4oAAAAJ", "cVvfP4oAAAAJ", "cVvfP4oAAAAJ", "cVvfP4oAAAAJ", "gFtI-8QAAAAJ", "gFtI-8QAAAAJ", "gFtI-8QAAAAJ", "gFtI-8QAAAAJ", "DhtAFkwAAAAJ", "U210xGIAAAAJ", "U210xGIAAAAJ", "U210xGIAAAAJ", "GDvt570AAAAJ", "GDvt570AAAAJ", "GDvt570AAAAJ", "GDvt570AAAAJ", "GDvt570AAAAJ", "GDvt570AAAAJ", "GDvt570AAAAJ", "GDvt570AAAAJ", "PoAvGRMAAAAJ", "PoAvGRMAAAAJ", "k_T8t30AAAAJ", "k_T8t30AAAAJ", "k_T8t30AAAAJ", "k_T8t30AAAAJ", "k_T8t30AAAAJ", "k_T8t30AAAAJ", "k_T8t30AAAAJ", "546GPMoAAAAJ", "546GPMoAAAAJ", "546GPMoAAAAJ", "546GPMoAAAAJ", "546GPMoAAAAJ", "546GPMoAAAAJ", "Ec5Biz4AAAAJ", "Ec5Biz4AAAAJ", "Ec5Biz4AAAAJ", "Ec5Biz4AAAAJ", "Ec5Biz4AAAAJ", "Ec5Biz4AAAAJ", "Ec5Biz4AAAAJ", "y_cp1sUAAAAJ", "y_cp1sUAAAAJ", "y_cp1sUAAAAJ", "y_cp1sUAAAAJ", "y_cp1sUAAAAJ", "7P30Es0AAAAJ", "o9mX9sUAAAAJ", "o9mX9sUAAAAJ", "o9mX9sUAAAAJ", "o9mX9sUAAAAJ", "o9mX9sUAAAAJ", "o9mX9sUAAAAJ", "1xn5GHgAAAAJ", "1xn5GHgAAAAJ", "1xn5GHgAAAAJ", "1xn5GHgAAAAJ", "1xn5GHgAAAAJ", "nHhtvqkAAAAJ", "nHhtvqkAAAAJ", "nHhtvqkAAAAJ", "nHhtvqkAAAAJ", "nHhtvqkAAAAJ", "uo0q9WgAAAAJ", "uo0q9WgAAAAJ", "uo0q9WgAAAAJ", "uo0q9WgAAAAJ", "uo0q9WgAAAAJ", "uo0q9WgAAAAJ", "S-YjbUYAAAAJ", "S-YjbUYAAAAJ", "S-YjbUYAAAAJ", "S-YjbUYAAAAJ", "pbNCoTwAAAAJ", "pbNCoTwAAAAJ", "hW23VKIAAAAJ", "hW23VKIAAAAJ", "hW23VKIAAAAJ", "hW23VKIAAAAJ", "hW23VKIAAAAJ", "hW23VKIAAAAJ", "hW23VKIAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "_1UDYowAAAAJ", "jZH2IPYAAAAJ", "jZH2IPYAAAAJ", "jZH2IPYAAAAJ", "jZH2IPYAAAAJ", "jZH2IPYAAAAJ", "jZH2IPYAAAAJ", "jZH2IPYAAAAJ", "WrDKqIAAAAAJ", "WrDKqIAAAAAJ", "WrDKqIAAAAAJ", "WrDKqIAAAAAJ", "WrDKqIAAAAAJ", "WrDKqIAAAAAJ", "WrDKqIAAAAAJ", "WrDKqIAAAAAJ", "9D4aG8AAAAAJ", "9D4aG8AAAAAJ", "9D4aG8AAAAAJ", "9D4aG8AAAAAJ", "yA9qseUAAAAJ", "yA9qseUAAAAJ", "yA9qseUAAAAJ", "yA9qseUAAAAJ", "yA9qseUAAAAJ", "yA9qseUAAAAJ", "rU4OT14AAAAJ", "rU4OT14AAAAJ", "rU4OT14AAAAJ", "rU4OT14AAAAJ", "rU4OT14AAAAJ", "rU4OT14AAAAJ", "h9oWOsEAAAAJ", "h9oWOsEAAAAJ", "h9oWOsEAAAAJ", "h9oWOsEAAAAJ", "h9oWOsEAAAAJ", "OsP7JHAAAAAJ", "OsP7JHAAAAAJ", "4XyqsFwAAAAJ", "BN2Ze-QAAAAJ", "BN2Ze-QAAAAJ", "JtPSC9sAAAAJ", "4Zw1PJ8AAAAJ", "4Zw1PJ8AAAAJ", "4Zw1PJ8AAAAJ", "4Zw1PJ8AAAAJ", "4Zw1PJ8AAAAJ", "uMQ-G-QAAAAJ", "ZbA-z1cAAAAJ", "ZbA-z1cAAAAJ", "ZbA-z1cAAAAJ", "ZbA-z1cAAAAJ", "ZbA-z1cAAAAJ", "kII4yekAAAAJ", "kII4yekAAAAJ", "kII4yekAAAAJ", "ojKsx6AAAAAJ", "ojKsx6AAAAAJ", "41KAd6AAAAAJ", "41KAd6AAAAAJ", "41KAd6AAAAAJ", "41KAd6AAAAAJ", "41KAd6AAAAAJ", "41KAd6AAAAAJ", "41KAd6AAAAAJ", "nDrw-wwAAAAJ", "nDrw-wwAAAAJ", "nDrw-wwAAAAJ", "nDrw-wwAAAAJ", "nDrw-wwAAAAJ", "nDrw-wwAAAAJ", "nDrw-wwAAAAJ", "6gnHaLcAAAAJ", "6gnHaLcAAAAJ", "HxS8Bx0AAAAJ", "HxS8Bx0AAAAJ", "83WWEs4AAAAJ", "83WWEs4AAAAJ", "83WWEs4AAAAJ", "83WWEs4AAAAJ", "83WWEs4AAAAJ", "FL3ReD0AAAAJ", "FL3ReD0AAAAJ", "FL3ReD0AAAAJ", "FL3ReD0AAAAJ", "FL3ReD0AAAAJ", "FL3ReD0AAAAJ", "FL3ReD0AAAAJ", "brEBMIkAAAAJ", "brEBMIkAAAAJ", "brEBMIkAAAAJ", "ql1bcCwAAAAJ", "ql1bcCwAAAAJ", "ql1bcCwAAAAJ", "ql1bcCwAAAAJ", "_8lB7xcAAAAJ", "_8lB7xcAAAAJ", "_8lB7xcAAAAJ", "CJ5dYFwAAAAJ", "CJ5dYFwAAAAJ", "CJ5dYFwAAAAJ", "CJ5dYFwAAAAJ", "CJ5dYFwAAAAJ", "rGF6-WkAAAAJ", "BlfdYL0AAAAJ", "PGtHUI0AAAAJ", "irZFP_AAAAAJ", "irZFP_AAAAAJ", "afbbNmwAAAAJ", "afbbNmwAAAAJ", "afbbNmwAAAAJ", "afbbNmwAAAAJ", "wupwVHAAAAAJ", "wupwVHAAAAAJ", "wupwVHAAAAAJ", "wupwVHAAAAAJ", "an_4IJkAAAAJ", "an_4IJkAAAAJ", "an_4IJkAAAAJ", "klKYui4AAAAJ", "klKYui4AAAAJ", "klKYui4AAAAJ", "BUEDUFkAAAAJ", "4uE10I0AAAAJ", "XPAkzTEAAAAJ", "rYaif-cAAAAJ", "r7QElw8AAAAJ", "r7QElw8AAAAJ", "r7QElw8AAAAJ", "3L8CsIIAAAAJ", "3L8CsIIAAAAJ", "XptEO8oAAAAJ", "-Wpd7FcAAAAJ", "-Wpd7FcAAAAJ", "iqbKD9UAAAAJ", "iqbKD9UAAAAJ", "ypkClpwAAAAJ", "lSDISOcAAAAJ", "lSDISOcAAAAJ", "lSDISOcAAAAJ", "mmm90qgAAAAJ", "zCN0gesAAAAJ", "Nav8m8gAAAAJ", "Nav8m8gAAAAJ", "1mCYe_oAAAAJ", "1mCYe_oAAAAJ", "2QLD4fAAAAAJ", "2QLD4fAAAAAJ", "2QLD4fAAAAAJ", "2QLD4fAAAAAJ", "78vU1IUAAAAJ", "Y_ABdTgAAAAJ", "bS0_aIUAAAAJ", "CUlqK5EAAAAJ", "jyxO2akAAAAJ", "6E9elWUAAAAJ", "6E9elWUAAAAJ", "j4pcHV4AAAAJ", "mhaiL5MAAAAJ", "mhaiL5MAAAAJ", "mhaiL5MAAAAJ", "sMQV1ecAAAAJ", "sMQV1ecAAAAJ", "ljt16JkAAAAJ", "WI_flSwAAAAJ", "WI_flSwAAAAJ", "WI_flSwAAAAJ", "WI_flSwAAAAJ", "huWpVyEAAAAJ", "1c9oQNMAAAAJ", "UyKX7ZsAAAAJ", "UyKX7ZsAAAAJ", "UyKX7ZsAAAAJ", "pTH7MA4AAAAJ", "LbgTPRwAAAAJ", "LbgTPRwAAAAJ", "DdNBcogAAAAJ", "Fq9SgKYAAAAJ", "Fq9SgKYAAAAJ", "Fq9SgKYAAAAJ", "iM0lua0AAAAJ", "iM0lua0AAAAJ", "Q88BI2QAAAAJ", "Q88BI2QAAAAJ", "Y3h_pzMAAAAJ", "Y3h_pzMAAAAJ", "beGt3cAAAAAJ", "beGt3cAAAAAJ", "beGt3cAAAAAJ", "Hfrih1EAAAAJ", "a7VNhCIAAAAJ", "X9lE6O4AAAAJ", "i1bzlyAAAAAJ", "QRW9NN0AAAAJ", "sntA9FgAAAAJ", "sntA9FgAAAAJ", "GStTsxAAAAAJ", "GStTsxAAAAAJ", "GStTsxAAAAAJ", "AUFcWtwAAAAJ", "Ox6SxpoAAAAJ", "mFG5Sv4AAAAJ", "mFG5Sv4AAAAJ", "vGaTPe0AAAAJ", "vGaTPe0AAAAJ", "mLqg5hYAAAAJ", "jplwHW8AAAAJ", "F554LkQAAAAJ", "F554LkQAAAAJ", "F554LkQAAAAJ", "D8VMkA8AAAAJ", "eIWg8NMAAAAJ", "8Ss6ahEAAAAJ", "19Ovo9AAAAAJ", "19Ovo9AAAAAJ", "hKfga9oAAAAJ", "dEEwwEoAAAAJ", "u5VRx_kAAAAJ", "UalYAksAAAAJ", "S6tTC-oAAAAJ", "MR24IpgAAAAJ", "dk8EnkoAAAAJ", "dk8EnkoAAAAJ", "kHblmyAAAAAJ", "8bX2roMAAAAJ", "y2S02IcAAAAJ", "y2S02IcAAAAJ", "KZn9NWEAAAAJ", "1_zc1-IAAAAJ", "1_zc1-IAAAAJ", "s2yHgo8AAAAJ", "s2yHgo8AAAAJ", "5GtyVooAAAAJ", "HsV0WbwAAAAJ", "g6grFy0AAAAJ", "7bk53mEAAAAJ", "fMXnSGMAAAAJ", "fMXnSGMAAAAJ", "Ual305IAAAAJ", "tmT_voUAAAAJ", "XdahAuoAAAAJ", "Rfj4jWoAAAAJ", "CE8KY0MAAAAJ", "TGXfPg4AAAAJ", "BXpcvSMAAAAJ", "VU5ObUwAAAAJ", "VU5ObUwAAAAJ", "VU5ObUwAAAAJ", "VU5ObUwAAAAJ", "VU5ObUwAAAAJ", "VU5ObUwAAAAJ", "VU5ObUwAAAAJ", "VU5ObUwAAAAJ", "rEYarG0AAAAJ", "rEYarG0AAAAJ", "rEYarG0AAAAJ", "i35tdbMAAAAJ", "L60tuywAAAAJ"], "type": ["Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "NTU", "NTU", "NTU", "NTU", "NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside NTU", "Unknown", "Outside SCSE", "Outside SCSE", "Unknown", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU"], "location": ["Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "University of Hong Kong", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Queen Mary University of London", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Apple", "Apple", "Apple", "Apple", "Apple", "Apple", "Apple", "Apple", "Apple", "Apple", "Apple", "Apple", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nankai University", "Nankai University", "Nankai University", "Nankai University", "Nankai University", "Nankai University", "Nankai University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Hong Kong Bapist University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Shenzhen Institutes of Advanced Technology", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "University of Oxford", "University of Oxford", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "University of Edinburgh", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "UCLA", "UCLA", "UCLA", "UCLA", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "MIT", "MIT", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Facebook", "Stanford University", "Stanford University", "Stanford University", "Stanford University", "Stanford University", "University of Sydney", "University of Surrey", "University of Surrey", "University of Surrey", "University of Surrey", "University of Surrey", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Institute of Automation, Chinese Academy of Sciences", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Beijing Normal University", "Beijing Normal University", "Chinese University of HongKong", "Chinese University of HongKong", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Apple", "Apple", "Apple", "Unknown", "Unknown", "Unknown", "Unknown", "Beihang University, Beijing", "Beihang University, Beijing", "Beihang University, Beijing", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "University of Illinois", "University of Illinois", "Unknown", "University of Surrey", "University of Surrey", "Unknown", "Unknown", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Deakin University", "Deakin University", "Deakin University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Huawei", "Huawei", "Chinese University of HongKong", "Chinese University of HongKong", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Beihang University, Beijing", "SEU", "Unknown", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Auburn University", "Unknown", "University of Toronto", "University of Toronto", "Unknown", "Unknown", "NVIDIA", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "University of Science and Technology of China", "Unknown", "Unknown", "Unknown", "Unknown", "Nankai University", "Unknown", "Unknown", "Unknown", "Unknown", "Shenzhen Institutes of Advanced Technology", "Universit\u00e0 degli Studi di Verona", "Universit\u00e0 degli Studi di Verona", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Chinese University of HongKong", "Chinese University of HongKong", "Stony Brook", "Stony Brook", "Nanyang Technological University", "Nanyang Technological University", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "Unknown", "Rutgers University", "HKUST", "University of Edinburgh", "University of Edinburgh", "Unknown", "Unknown", "Chinese University of HongKong", "Chinese University of HongKong", "Chinese University of HongKong", "University of Sydney", "University of Sydney", "MIT", "MIT", "Queen Mary University of London", "Queen Mary University of London", "University of Adelaide", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Chinese University of HongKong", "Max Planck Institute for Informatics", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Universiti Malaya", "Universiti Kebangsaan Malaysia", "Monash University", "University of Cambridge", "Chinese University of HongKong", "Universiti Malaya", "Nanyang Technological University", "Nanyang Technological University", "Fudan University", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Chinese University of HongKong", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "University of Bath", "Nanyang Technological University", "Nanyang Technological University", "Chinese University of HongKong", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Johns Hopkins University"], "year": [2017, 2018, 2016, 2015, 2017, 2018, 2016, 2014, 2016, 2016, 2014, 2015, 2021, 2017, 2018, 2014, 2016, 2018, 2015, 2019, 2018, 2018, 2015, 2019, 2015, 2015, 2016, 2016, 2017, 2015, 2014, 2017, 2016, 2018, 2016, 2015, 2015, 2016, 2015, 2016, 2018, 2019, 2017, 2015, 2017, 2018, 2022, 2019, 2017, 2023, 2023, 2021, 2023, 2019, 2023, 2022, 2021, 2022, 2022, 2020, 2019, 2023, 2019, 2022, 2023, 2018, 2023, 2020, 2019, 2020, 2021, 2023, 2022, 2021, 2021, 2022, 2023, 2022, 2023, 2023, 2022, 2023, 2023, 2021, 2023, 2022, 2022, 2022, 2023, 2022, 2018, 2023, 2015, 2021, 2021, 2022, 2021, 2023, 2022, 2021, 2022, 2023, 2023, 2022, 2019, 2021, 2020, 2020, 2021, 2019, 2019, 2019, 2023, 2018, 2019, 2017, 2020, 2018, 2019, 2023, 2019, 2017, 2018, 2023, 2020, 2018, 2019, 2021, 2020, 2020, 2020, 2017, 2015, 2015, 2018, 2018, 2021, 2019, 2016, 2022, 2019, 2018, 2021, 2021, 2014, 2017, 2018, 2015, 2017, 2018, 2020, 2014, 2014, 2015, 2018, 2016, 2017, 2015, 2017, 2015, 2015, 2016, 2015, 2015, 2021, 2013, 2014, 2011, 2012, 2011, 2016, 2009, 2010, 2009, 2012, 2013, 2013, 2015, 2014, 2013, 2013, 2014, 2012, 2010, 2013, 2013, 2023, 2020, 2021, 2022, 2023, 2021, 2023, 2023, 2021, 2022, 2022, 2023, 2022, 2023, 2022, 2020, 2023, 2018, 2018, 2021, 2019, 2021, 2019, 2018, 2021, 2021, 2019, 2017, 2018, 2018, 2016, 2015, 2017, 2020, 2019, 2017, 2018, 2015, 2019, 2017, 2018, 2016, 2015, 2015, 2022, 2023, 2023, 2020, 2021, 2022, 2021, 2023, 2022, 2020, 2023, 2023, 2023, 2022, 2023, 2020, 2021, 2022, 2021, 2021, 2023, 2022, 2022, 2018, 2020, 2022, 2023, 2023, 2019, 2022, 2021, 2020, 2019, 2018, 2017, 2019, 2019, 2018, 2020, 2020, 2018, 2018, 2021, 2021, 2019, 2021, 2022, 2019, 2021, 2021, 2021, 2023, 2016, 2021, 2022, 2018, 2016, 2019, 2016, 2022, 2016, 2017, 2019, 2023, 2021, 2019, 2022, 2023, 2022, 2023, 2022, 2022, 2021, 2021, 2021, 2023, 2022, 2023, 2022, 2022, 2022, 2023, 2022, 2022, 2020, 2023, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2017, 2016, 2019, 2018, 2015, 2018, 2019, 2018, 2019, 2019, 2018, 2015, 2020, 2012, 2023, 2013, 2019, 2020, 2019, 2014, 2013, 2014, 2020, 2020, 2020, 2021, 2017, 2019, 2020, 2015, 2015, 2014, 2018, 2017, 2015, 2016, 2016, 2016, 2022, 2019, 2020, 2021, 2020, 2023, 2018, 2020, 2019, 2020, 2018, 2022, 2021, 2022, 2020, 2019, 2021, 2020, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2022, 2023, 2023, 2023, 2023, 2022, 2022, 2023, 2020, 2022, 2022, 2023, 2022, 2023, 2022, 2023, 2023, 2022, 2021, 2021, 2023, 2020, 2023, 2023, 2023, 2022, 2022, 2023, 2022, 2021, 2022, 2022, 2022, 2022, 2021, 2022, 2019, 2020, 2021, 2023, 2023, 2016, 2017, 2015, 2015, 2014, 2018, 2016, 2018, 2022, 2023, 2014, 2020, 2018, 2020, 2018, 2021, 2019, 2021, 2019, 2019, 2020, 2021, 2019, 2019, 2021, 2023, 2021, 2022, 2021, 2022, 2021, 2023, 2017, 2017, 2023, 2019, 2015, 2023, 2022, 2022, 2022, 2021, 2022, 2021, 2016, 2019, 2016, 2017, 2019, 2019, 2020, 2023, 2022, 2019, 2019, 2022, 2021, 2023, 2019, 2021, 2021, 2014, 2016, 2012, 2016, 2022, 2020, 2020, 2021, 2019, 2021, 2021, 2022, 2022, 2023, 2022, 2013, 2013, 2023, 2021, 2023, 2022, 2021, 2022, 2022, 2023, 2022, 2022, 2023, 2022, 2023, 2023, 2023, 2023, 2023, 2022, 2019, 2020, 2023, 2023, 2021, 2022, 2023, 2020, 2023, 2021, 2022, 2022, 2023, 2023, 2021, 2021, 2023, 2021, 2020, 2023, 2022, 2021, 2023, 2023, 2020, 2022, 2023, 2021, 2022, 2023, 2021, 2022, 2021, 2023, 2023, 2020, 2017, 2015, 2015, 2017, 2015, 2015, 2019, 2018, 2019, 2018, 2019, 2018, 2013, 2015, 2012, 2016, 2013, 2021, 2022, 2022, 2018, 2017, 2023, 2023, 2022, 2023, 2023, 2022, 2023, 2023, 2021, 2022, 2023, 2023, 2022, 2023, 2018, 2019, 2018, 2019, 2023, 2022, 2022, 2021, 2023, 2022, 2023, 2023, 2023, 2023, 2023, 2023, 2016, 2015, 2016, 2019, 2022, 2021, 2018, 2018, 2020, 2023, 2020, 2023, 2021, 2022, 2022, 2018, 2018, 2018, 2016, 2016, 2020, 2020, 2021, 2021, 2020, 2023, 2021, 2022, 2005, 2005, 2007, 2005, 2005, 2007, 2018, 2018, 2018, 2018, 2020, 2022, 2023, 2018, 2017, 2015, 2019, 2020, 2021, 2021, 2018, 2022, 2023, 2021, 2016, 2016, 2018, 2018, 2023, 2022, 2022, 2023, 2023, 2021, 2016, 2016, 2017, 2017, 2017, 2021, 2023, 2016, 2022, 2023, 2023, 2020, 2021, 2018, 2023, 2023, 2023, 2023, 2021, 2020, 2023, 2022, 2023, 2016, 2014, 2014, 2015, 2023, 2023, 2023, 2016, 2018, 2016, 2018, 2021, 2023, 2023, 2023, 2022, 2017, 2021, 2021, 2022, 2022, 2022, 2023, 2023, 2023, 2021, 2020, 2020, 2021, 2023, 2016, 2016, 2017, 2022, 2023, 2022, 2023, 2021, 2021, 2022, 2023, 2023, 2014, 2014, 2014, 2015, 2020, 2017, 2023, 2022, 2019, 2022, 2023, 2023, 2023, 2023, 2022, 2023, 2023, 2023, 2023, 2022, 2023, 2023, 2021, 2022, 2023, 2023, 2023, 2023, 2023, 2023, 2015, 2014, 2018, 2017, 2020, 2023, 2016, 2016, 2019, 2018, 2015, 2019, 2020], "title": ["Video Object Segmentation with Re-identification", "Deep Learning Markov Random Field for Semantic Segmentation", "Local Similarity-Aware Deep Feature Embedding", "From Facial Parts Responses to Face Detection: A Deep Learning Approach", "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade", "Faceness-Net: Face Detection through Deep Facial Part Responses", "Deep Cascaded Bi-Network for Face Hallucination", "Transferring Landmark Annotations for Cross-Dataset Face Alignment", "Learning Deep Representation for Imbalanced Classification", "Unsupervised Learning of Discriminative Attributes and Visual Representations", "Facial Landmark Detection by Deep Multi-task Learning", "Towards Arbitrary-View Face Alignment by Recommendation Trees", "Path-Restore: Learning Network Path Selection for Image Restoration", "Face Detection Through Scale-Friendly Deep Convolutional Networks", "Discriminative Sparse Neighbor Approximation for Imbalanced Learning", "Pedestrian Attribute Recognition At Far Distance", "Human Attribute Recognition by Deep Hierarchical Contexts", "LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation", "Learning Deep Representation for Face Alignment with Auxiliary Attributes", "Deep Imbalanced Learning for Face Recognition and Attribute Prediction", "Aesthetic-Driven Image Enhancement by Adversarial Learning", "Mix-and-Match Tuning for Self-supervised Semantic Segmentation", "Compression Artifacts Reduction by a Deep Convolutional Network", "Improving On-policy Learning with Statistical Reward Accumulation", "Face Alignment by Coarse-to-Fine Shape Searching", "Image Super-Resolution Using Deep Convolutional Networks", "Accelerating the Super-Resolution Convolutional Neural Network", "Joint Face Representation Adaptation and Clustering in Videos", "From Facial Expression Recognition to Interpersonal Relation Prediction", "Learning Social Relation Traits from Face Images", "Learning a Deep Convolutional Network for Image Super-Resolution", "Image Aesthetic Assessment: An Experimental Survey", "Unconstrained Face Alignment via Cascaded Compositional Learning", "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "Reading Scene Text in Deep Convolutional Sequences", "Semantic Image Segmentation via Deep Parsing Network", "A Large-Scale Car Dataset for Fine-Grained Categorization and Verification", "WIDER FACE: A Face Detection Benchmark", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "Deep Specialized Network for Illuminant Estimation", "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "Deep Network Interpolation for Continuous Imagery Effect Transition", "Learning to Disambiguate by Asking Discriminative Questions", "Deep Representation Learning with Target Coding", "Video Object Segmentation with Re-identification", "Deep Learning Markov Random Field for Semantic Segmentation", "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset", "Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild", "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade", "Panoptic Video Scene Graph Generation", "Text2Performer: Text-Driven Human Video Generation", "Deep Animation Video Interpolation in the Wild", "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds", "One-shot Face Reenactment", "Reference-based Image and Video Super-Resolution via C2-Matching", "VToonify: Controllable High-Resolution Portrait Video Style Transfer", "CARAFE++: Unified Content-Aware ReAssembly of FEatures", "Delving into Inter-Image Invariance for Unsupervised Visual Representations", "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer", "Knowledge Distillation Meets Self-Supervision", "CARAFE: Content-Aware ReAssembly of FEatures", "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars", "Hybrid Task Cascade for Instance Segmentation", "Conditional Prompt Learning for Vision-Language Models", "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation", "Mix-and-Match Tuning for Self-supervised Semantic Segmentation", "Transformer-based Visual Segmentation: A Survey", "Self-Supervised Scene De-occlusion", "Self-Supervised Learning via Conditional Motion Propagation", "Online Deep Clustering for Unsupervised Representation Learning", "Talk-to-Edit: Fine-Grained Facial Editing via Dialog", "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis", "Domain Generalization: A Survey", "Monocular 3D Reconstruction of Interacting Hands via Collision-Aware Factorized Refinements", "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation", "StyleGAN-Human: A Data-Centric Odyssey of Human Generation", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "Text2Human: Text-Driven Controllable Human Image Generation", "GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory", "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation", "StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces", "Semi-Supervised Domain Generalization with Stochastic StyleMatch", "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation", "Mind the Gap in Distilling StyleGANs", "Chasing the Tail in Monocular 3D Human Reconstruction with Prototype Memory", "Learning to Prompt for Vision-Language Models", "Deep Geometrized Cartoon Line Inbetweening", "On-device Domain Generalization", "Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition", "Bailando++: 3D Dance GPT With Choreographic Memory", "Semantic Image Segmentation via Deep Parsing Network", "Robust Reference-based Super-Resolution via C2-Matching", "Seesaw Loss for Long-Tailed Instance Segmentation", "Unsupervised Image-to-Image Translation with Generative Prior", "Unsupervised Object-Level Representation Learning from Scene Images", "Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup", "AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies", "Playing for 3D Human Recovery", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields", "Masked Frequency Modeling for Self-Supervised Visual Pre-Training", "StyleLight: HDR Panorama Generation for Lighting Estimation and Editing", "Instance-level Facial Attributes Transfer with Geometry-aware Flow", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Prime Sample Attention in Object Detection", "CARAFE++: Unified Content-Aware ReAssembly of FEatures", "Learning a Unified Classifier Incrementally via Rebalancing", "Learning to Cluster Faces on an Affinity Graph", "CARAFE: Content-Aware ReAssembly of FEatures", "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars", "PSANet: Point-wise Spatial Attention Network for Scene Parsing", "Hybrid Task Cascade for Instance Segmentation", "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks", "Self-Supervised Scene De-occlusion", "Lifelong Learning via Progressive Distillation and Retrospection", "Self-Supervised Learning via Conditional Motion Propagation", "Position-Guided Point Cloud Panoptic Segmentation Transformer", "Improving On-policy Learning with Statistical Reward Accumulation", "Be Your Own Prada: Fashion Synthesis with Structural Coherence", "Optimizing Video Object Detection via a Scale-Time Lattice", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "Side-Aware Boundary Localization for More Precise Object Detection", "Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition", "Region Proposal by Guided Anchoring", "Seesaw Loss for Long-Tailed Instance Segmentation", "Learning to Cluster Faces via Confidence and Connectivity Estimation", "Real or Not Real, That is the Question", "Feature Pyramid Grids", "Discover and Learn New Objects from Documentaries", "Compression Artifacts Reduction by a Deep Convolutional Network", "Image Super-Resolution Using Deep Convolutional Networks", "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform", "Path-Restore: Learning Network Path Selection for Image Restoration", "EDVR: Video Restoration with Enhanced Deformable Convolutional Networks", "Accelerating the Super-Resolution Convolutional Neural Network", "AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies", "Deep Network Interpolation for Continuous Imagery Effect Transition", "Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning", "Understanding Deformable Alignment in Video Super-Resolution", "BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond", "Learning a Deep Convolutional Network for Image Super-Resolution", "Video Object Segmentation with Re-identification", "Deep Learning Markov Random Field for Semantic Segmentation", "From Facial Parts Responses to Face Detection: A Deep Learning Approach", "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade", "Faceness-Net: Face Detection through Deep Facial Part Responses", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Facial Landmark Detection by Deep Multi-task Learning", "Pedestrian Attribute Recognition At Far Distance", "Learning Deep Representation for Face Alignment with Auxiliary Attributes", "Mix-and-Match Tuning for Self-supervised Semantic Segmentation", "Joint Face Representation Adaptation and Clustering in Videos", "From Facial Expression Recognition to Interpersonal Relation Prediction", "Learning Social Relation Traits from Face Images", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "Semantic Image Segmentation via Deep Parsing Network", "A Large-Scale Car Dataset for Fine-Grained Categorization and Verification", "WIDER FACE: A Face Detection Benchmark", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "Deep Representation Learning with Target Coding", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Constrained Clustering: Effective Constraint Propagation with Imperfect Oracles", "Person Re-Identification", "Security and Surveillance", "Stream-based Joint Exploration-Exploitation Active Learning", "Detecting and Discriminating Behavioural Anomalies", "Learning from Multiple Sources for Video Summarisation", "Modelling Activity Global Temporal Dependencies using Time Delayed Probabilistic Graphical Model", "Stream-based Active Unusual Event Detection", "Multi-camera Activity Correlation Analysis", "Person Re-identification: What Features Are Important?", "Person Re-Identification by Manifold Ranking", "From Semi-Supervised to Transfer Counting of Crowds", "Constrained Clustering with Imperfect Oracles", "On-the-fly Feature Importance Mining for Person Re-Identification", "Cumulative Attribute Space for Age and Crowd Density Estimation", "Crowd Counting and Profiling: Methodology and Evaluation", "The Re-identification Challenge", "Salient Motion Detection in Crowded Scenes", "Time-Delayed Correlation Analysis for Multi-Camera Activity Understanding", "POP: Person Re-Identification Post-Rank Optimisation", "Video Synopsis by Heterogeneous Multi-source Correlation", "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement", "RGB-D Salient Object Detection with Cross-Modality Modulation and Selection", "Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network", "CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment", "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera", "Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation", "Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond", "Adaptive Window Pruning for Efficient Local Motion Deblurring", "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", "Deep Fourier Up-Sampling", "Flare7K: A Phenomenological Nighttime Flare Removal Dataset", "ProPainter: Improving Propagation and Transformer for Video Inpainting", "Towards Robust Blind Face Restoration with Codebook Lookup Transformer", "Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior", "LEDNet: Joint Low-light Enhancement and Deblurring in the Dark", "Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement", "Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement", "Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform", "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "Path-Restore: Learning Network Path Selection for Image Restoration", "EDVR: Video Restoration with Enhanced Deformable Convolutional Networks", "ReconfigISP: Reconfigurable Camera Image Processing Pipeline", "Deep Network Interpolation for Continuous Imagery Effect Transition", "Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning", "Understanding Deformable Alignment in Video Super-Resolution", "BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond", "Improving On-policy Learning with Statistical Reward Accumulation", "Video Object Segmentation with Re-identification", "Deep Learning Markov Random Field for Semantic Segmentation", "Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation", "Pedestrian Color Naming via Convolutional Neural Network", "Semantic Image Segmentation via Deep Parsing Network", "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade", "Knowledge Distillation Meets Self-Supervision", "Hybrid Task Cascade for Instance Segmentation", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "Optimizing Video Object Detection via a Scale-Time Lattice", "From Facial Parts Responses to Face Detection: A Deep Learning Approach", "Region Proposal by Guided Anchoring", "Face Detection Through Scale-Friendly Deep Convolutional Networks", "Faceness-Net: Face Detection through Deep Facial Part Responses", "WIDER FACE: A Face Detection Benchmark", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "Deep Representation Learning with Target Coding", "Extract Free Dense Labels from CLIP", "Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion", "Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Unsupervised 3D Shape Completion through GAN Inversion", "Transformer with Implicit Edges for Particle-based Physics Simulation", "Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data", "Towards Multi-Layered 3D Garments Animation", "TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing", "Self-Supervised Scene De-occlusion", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "Correspondence Distillation from NeRF-based GAN", "Monocular 3D Object Reconstruction with GAN Inversion", "Interpret Vision Transformers as ConvNets with Dynamic Convolutions", "Real or Not Real, That is the Question", "Focal Frequency Loss for Image Reconstruction and Synthesis", "BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Bailando++: 3D Dance GPT With Choreographic Memory", "Talking Faces: Audio-to-Video Face Generation", "StyleGAN-Human: A Data-Centric Odyssey of Human Generation", "ReenactGAN: Learning to Reenact Faces via Boundary Transfer", "TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting", "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars", "TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation", "DeepFakes Detection: the DeeperForensics Dataset and Challenge", "Pareidolia Face Reenactment", "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection", "Disentangling Content and Style via Unsupervised Geometry Distillation", "The Devil of Face Recognition is in the Noise", "Video Object Segmentation with Re-identification", "Hybrid Task Cascade for Instance Segmentation", "Robust Multi-Modality Multi-Object Tracking", "PSANet: Point-wise Spatial Attention Network for Scene Parsing", "Side-Aware Boundary Localization for More Precise Object Detection", "TSIT: A Simple and Versatile Framework for Image-to-Image Translation", "Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform", "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "Path-Restore: Learning Network Path Selection for Image Restoration", "Robust Reference-based Super-Resolution via C2-Matching", "EDVR: Video Restoration with Enhanced Deformable Convolutional Networks", "Positional Encoding as Spatial Inductive Bias in GANs", "GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond", "Deep Network Interpolation for Continuous Imagery Effect Transition", "Understanding Deformable Alignment in Video Super-Resolution", "GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution", "BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond", "Semi-Supervised and Long-Tailed Object Detection with CascadeMatch", "Local Similarity-Aware Deep Feature Embedding", "FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation", "Unified Vision and Language Prompt Learning", "Discriminative Sparse Neighbor Approximation for Imbalanced Learning", "Human Attribute Recognition by Deep Hierarchical Contexts", "Deep Imbalanced Learning for Face Recognition and Attribute Prediction", "Learning Deep Representation for Imbalanced Classification", "Open-Vocabulary DETR with Conditional Matching", "Unsupervised Learning of Discriminative Attributes and Visual Representations", "Learning to Disambiguate by Asking Discriminative Questions", "Dense Intrinsic Appearance Flow for Human Pose Transfer", "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "Robust Reference-based Super-Resolution via C2-Matching", "EDVR: Video Restoration with Enhanced Deformable Convolutional Networks", "Investigating Tradeoffs in Real-World Video Super-Resolution", "Exploring CLIP for Assessing the Look and Feel of Images", "GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond", "ProPainter: Improving Propagation and Transformer for Video Inpainting", "Towards Robust Blind Face Restoration with Codebook Lookup Transformer", "On the Generalization of BasicVSR++ to Video Deblurring and Denoising", "Understanding Deformable Alignment in Video Super-Resolution", "GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution", "BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond", "Reference-based Image and Video Super-Resolution via C2-Matching", "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment", "StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation", "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset", "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer", "Unsupervised Image-to-Image Translation with Generative Prior", "GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation", "Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets", "DeepFakes Detection: the DeeperForensics Dataset and Challenge", "TSIT: A Simple and Versatile Framework for Image-to-Image Translation", "StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces", "CelebV-Text: A Large-Scale Facial Text-Video Dataset", "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection", "Focal Frequency Loss for Image Reconstruction and Synthesis", "TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing", "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation", "Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data", "VToonify: Controllable High-Resolution Portrait Video Style Transfer", "Quantifying Facial Age by Posterior of Age Comparisons", "Unconstrained Face Alignment via Cascaded Compositional Learning", "Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild", "ReenactGAN: Learning to Reenact Faces via Boundary Transfer", "Towards Arbitrary-View Face Alignment by Recommendation Trees", "Merge or Not? Learning to Group Faces via Imitation Learning", "TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation", "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "One-shot Face Reenactment", "Disentangling Content and Style via Unsupervised Geometry Distillation", "The Devil of Face Recognition is in the Noise", "Face Alignment by Coarse-to-Fine Shape Searching", "Inter-Region Affinity Distillation for Road Marking Segmentation", "Person Re-identification: What Features Are Important?", "Network Pruning via Resource Reallocation", "Person Re-Identification by Manifold Ranking", "Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks", "TSIT: A Simple and Versatile Framework for Image-to-Image Translation", "Learning Lightweight Lane Detection CNNs by Self Attention Distillation", "Evaluating Feature Importance for Re-identification", "POP: Person Re-Identification Post-Rank Optimisation", "On-the-fly Feature Importance Mining for Person Re-Identification", "Feature Pyramid Grids", "Prime Sample Attention in Object Detection", "Side-Aware Boundary Localization for More Precise Object Detection", "Seesaw Loss for Long-Tailed Instance Segmentation", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "Hybrid Task Cascade for Instance Segmentation", "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "Deeply Learned Attributes for Crowded Scene Understanding", "Scene-Independent Group Profiling in Crowd", "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition", "Learning Scene-Independent Group Descriptors for Crowd Understanding", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-Resolution", "Crowded Scene Understanding by Deeply Learned Volumetric Slices", "Slicing Convolutional Neural Network for Crowd Video Understanding", "Delving into Inter-Image Invariance for Unsupervised Visual Representations", "Learning to Cluster Faces on an Affinity Graph", "Learning to Cluster Faces via Confidence and Connectivity Estimation", "Unsupervised Object-Level Representation Learning from Scene Images", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Masked Frequency Modeling for Self-Supervised Visual Pre-Training", "Mix-and-Match Tuning for Self-supervised Semantic Segmentation", "Self-Supervised Scene De-occlusion", "Self-Supervised Learning via Conditional Motion Propagation", "Online Deep Clustering for Unsupervised Representation Learning", "Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition", "Dense Siamese Network for Dense Unsupervised Learning", "Seesaw Loss for Long-Tailed Instance Segmentation", "Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation", "Side-Aware Boundary Localization for More Precise Object Detection", "Robust Multi-Modality Multi-Object Tracking", "K-Net: Towards Unified Image Segmentation", "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation", "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction", "Transformer-based Visual Segmentation: A Survey", "Aligning Bag of Regions for Open-Vocabulary Object Detection", "Position-Guided Point Cloud Panoptic Segmentation Transformer", "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement", "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance", "CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond", "Adaptive Window Pruning for Efficient Local Motion Deblurring", "Investigating Tradeoffs in Real-World Video Super-Resolution", "Flare7K: A Phenomenological Nighttime Flare Removal Dataset", "ProPainter: Improving Propagation and Transformer for Video Inpainting", "Cross-Scale Internal Graph Neural Network for Image Super-Resolution", "Towards Robust Blind Face Restoration with Codebook Lookup Transformer", "On the Generalization of BasicVSR++ to Video Deblurring and Denoising", "Flexible Piecewise Curves Estimation for Photo Enhancement", "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment", "Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior", "LEDNet: Joint Low-light Enhancement and Deblurring in the Dark", "Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement", "BeautyREC: Robust, Efficient, and Content-preserving Makeup Transfer", "CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment", "Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation", "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", "Flexible Piecewise Curves Estimation for Photo Enhancement", "Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement", "Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement", "Contextual Object Detection with Multimodal Large Language Models", "Semi-Supervised and Long-Tailed Object Detection with CascadeMatch", "Domain Generalization: A Survey", "Unified Vision and Language Prompt Learning", "Panoptic Video Scene Graph Generation", "Conditional Prompt Learning for Vision-Language Models", "Semi-Supervised Domain Generalization with Stochastic StyleMatch", "Open-Vocabulary DETR with Conditional Matching", "Learning to Prompt for Vision-Language Models", "On-device Domain Generalization", "Dense Siamese Network for Dense Unsupervised Learning", "Seesaw Loss for Long-Tailed Instance Segmentation", "Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation", "Hybrid Task Cascade for Instance Segmentation", "Side-Aware Boundary Localization for More Precise Object Detection", "K-Net: Towards Unified Image Segmentation", "Transformer-based Visual Segmentation: A Survey", "Position-Guided Point Cloud Panoptic Segmentation Transformer", "Joint Face Representation Adaptation and Clustering in Videos", "From Facial Expression Recognition to Interpersonal Relation Prediction", "Learning Deep Representation for Face Alignment with Auxiliary Attributes", "Learning Social Relation Traits from Face Images", "Facial Landmark Detection by Deep Multi-task Learning", "Fusing Object Context to Detect Functional Area for Cognitive Robots", "Reading Scene Text in Deep Convolutional Sequences", "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "Domain Generalization: A Survey", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "Learning a Deep Convolutional Network for Image Super-Resolution", "Inter-Region Affinity Distillation for Road Marking Segmentation", "LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation", "LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation", "Optimizing Video Object Detection via a Scale-Time Lattice", "CARAFE++: Unified Content-Aware ReAssembly of FEatures", "Region Proposal by Guided Anchoring", "Seesaw Loss for Long-Tailed Instance Segmentation", "CARAFE: Content-Aware ReAssembly of FEatures", "Hybrid Task Cascade for Instance Segmentation", "Side-Aware Boundary Localization for More Precise Object Detection", "Texture Memory-Augmented Deep Patch-Based Image Inpainting", "Hybrid Task Cascade for Instance Segmentation", "Robust Multi-Modality Multi-Object Tracking", "Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network", "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera", "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", "Deep Fourier Up-Sampling", "ReconfigISP: Reconfigurable Camera Image Processing Pipeline", "GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond", "GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution", "Network Pruning via Resource Reallocation", "Video Object Segmentation with Re-identification", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "Improving Data Augmentation for Multi-Modality 3D Object Detection", "Robust Multi-Modality Multi-Object Tracking", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "The Nuts and Bolts of Adopting Transformer in GANs", "Investigating Tradeoffs in Real-World Video Super-Resolution", "GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond", "On the Generalization of BasicVSR++ to Video Deblurring and Denoising", "3D Human Texture Estimation from a Single Image with Transformers", "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment", "GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution", "Human Attribute Recognition by Deep Hierarchical Contexts", "Deep Imbalanced Learning for Face Recognition and Attribute Prediction", "Learning Deep Representation for Imbalanced Classification", "Learning to Disambiguate by Asking Discriminative Questions", "Dense Intrinsic Appearance Flow for Human Pose Transfer", "Hybrid Task Cascade for Instance Segmentation", "Inter-Region Affinity Distillation for Road Marking Segmentation", "Network Pruning via Resource Reallocation", "Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation", "Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks", "Learning Lightweight Lane Detection CNNs by Self Attention Distillation", "Mind the Gap in Distilling StyleGANs", "CARAFE++: Unified Content-Aware ReAssembly of FEatures", "The Nuts and Bolts of Adopting Transformer in GANs", "CARAFE: Content-Aware ReAssembly of FEatures", "Positional Encoding as Spatial Inductive Bias in GANs", "Texture Memory-Augmented Deep Patch-Based Image Inpainting", "The Re-identification Challenge", "Sketch Me That Shoe", "Stream-based Joint Exploration-Exploitation Active Learning", "A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-Resolution", "Self-Supervised Representation Learning: Introduction, Advances, and Challenges", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Self-Supervised Scene De-occlusion", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Self-Supervised Learning via Conditional Motion Propagation", "Talk-to-Edit: Fine-Grained Facial Editing via Dialog", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Conditional Prompt Learning for Vision-Language Models", "On-device Domain Generalization", "Panoptic Video Scene Graph Generation", "Learning to Prompt for Vision-Language Models", "Crowd Counting and Profiling: Methodology and Evaluation", "Cumulative Attribute Space for Age and Crowd Density Estimation", "Contextual Object Detection with Multimodal Large Language Models", "FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation", "Semi-Supervised and Long-Tailed Object Detection with CascadeMatch", "Unified Vision and Language Prompt Learning", "Seesaw Loss for Long-Tailed Instance Segmentation", "Open-Vocabulary DETR with Conditional Matching", "On-device Domain Generalization", "Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion", "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer", "Text2Human: Text-Driven Controllable Human Image Generation", "GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation", "Unsupervised Image-to-Image Translation with Generative Prior", "Text2Performer: Text-Driven Human Video Generation", "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation", "StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces", "DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields", "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation", "VToonify: Controllable High-Resolution Portrait Video Style Transfer", "Learning to Cluster Faces on an Affinity Graph", "Learning to Cluster Faces via Confidence and Connectivity Estimation", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars", "Playing for 3D Human Recovery", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds", "MessyTable: Instance Association in Multiple Camera Views", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "Playing for 3D Human Recovery", "Monocular 3D Object Reconstruction with GAN Inversion", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation", "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds", "Unsupervised 3D Shape Completion through GAN Inversion", "Positional Encoding as Spatial Inductive Bias in GANs", "The Nuts and Bolts of Adopting Transformer in GANs", "Texture Memory-Augmented Deep Patch-Based Image Inpainting", "TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting", "Correlational Image Modeling for Self-Supervised Visual Pre-Training", "Delving into Inter-Image Invariance for Unsupervised Visual Representations", "Unsupervised Object-Level Representation Learning from Scene Images", "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation", "Masked Frequency Modeling for Self-Supervised Visual Pre-Training", "Online Deep Clustering for Unsupervised Representation Learning", "StyleGAN-Human: A Data-Centric Odyssey of Human Generation", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "Robust Reference-based Super-Resolution via C2-Matching", "Text2Human: Text-Driven Controllable Human Image Generation", "Text2Performer: Text-Driven Human Video Generation", "Talk-to-Edit: Fine-Grained Facial Editing via Dialog", "Delving into Inter-Image Invariance for Unsupervised Visual Representations", "Unsupervised Object-Level Representation Learning from Scene Images", "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation", "Masked Frequency Modeling for Self-Supervised Visual Pre-Training", "Online Deep Clustering for Unsupervised Representation Learning", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild", "Merge or Not? Learning to Group Faces via Imitation Learning", "TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation", "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "Disentangling Content and Style via Unsupervised Geometry Distillation", "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "Constrained Clustering: Effective Constraint Propagation with Imperfect Oracles", "Constrained Clustering with Imperfect Oracles", "Comparing Visual Feature Coding for Learning Disjoint Camera Dependencies", "Learning from Multiple Sources for Video Summarisation", "Video Synopsis by Heterogeneous Multi-source Correlation", "Pareidolia Face Reenactment", "Talking Faces: Audio-to-Video Face Generation", "Audio-driven Dubbing for User Generated Contents via Style-aware Semi-parametric Synthesis", "Optimizing Video Object Detection via a Scale-Time Lattice", "Face Detection Through Scale-Friendly Deep Convolutional Networks", "Contextual Object Detection with Multimodal Large Language Models", "Correlational Image Modeling for Self-Supervised Visual Pre-Training", "Unified Vision and Language Prompt Learning", "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation", "Siamese DETR", "Open-Vocabulary DETR with Conditional Matching", "Masked Frequency Modeling for Self-Supervised Visual Pre-Training", "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement", "Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network", "CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment", "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera", "Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond", "Flare7K: A Phenomenological Nighttime Flare Removal Dataset", "Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement", "Lifelong Learning via Progressive Distillation and Retrospection", "Learning a Unified Classifier Incrementally via Rebalancing", "Lifelong Learning via Progressive Distillation and Retrospection", "Learning a Unified Classifier Incrementally via Rebalancing", "Bailando++: 3D Dance GPT With Choreographic Memory", "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory", "AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies", "Deep Animation Video Interpolation in the Wild", "Deep Geometrized Cartoon Line Inbetweening", "Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation", "Explore In-Context Learning for 3D Point Cloud Understanding", "Panoptic Video Scene Graph Generation", "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation", "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction", "Transformer-based Visual Segmentation: A Survey", "Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation", "Crowded Scene Understanding by Deeply Learned Volumetric Slices", "Deeply Learned Attributes for Crowded Scene Understanding", "Slicing Convolutional Neural Network for Crowd Video Understanding", "Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild", "Chasing the Tail in Monocular 3D Human Reconstruction with Prototype Memory", "Monocular 3D Reconstruction of Interacting Hands via Collision-Aware Factorized Refinements", "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition", "High-Quality Video Generation from Static Structural Annotations", "Siamese DETR", "MessyTable: Instance Association in Multiple Camera Views", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "Playing for 3D Human Recovery", "Monocular 3D Object Reconstruction with GAN Inversion", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "Non-Local Recurrent Network for Image Restoration", "Non-Local Recurrent Network for Image Restoration", "Non-Local Recurrent Network for Image Restoration", "Sketch Me That Shoe", "A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-Resolution", "MessyTable: Instance Association in Multiple Camera Views", "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "Unsupervised 3D Shape Completion through GAN Inversion", "Playing for 3D Human Recovery", "MessyTable: Instance Association in Multiple Camera Views", "DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields", "Unsupervised 3D Shape Completion through GAN Inversion", "Monocular 3D Object Reconstruction with GAN Inversion", "Development of a Pressure-based Typing Biometrics User Authentication System", "Pressure-Based Typing Biometrics User Authentication Using the Fuzzy ARTMAP Neural Network", "Keystroke Patterns Classification using the ARTMAP-FD Neural Network", "Development of a Pressure-based Typing Biometrics User Authentication System", "Pressure-Based Typing Biometrics User Authentication Using the Fuzzy ARTMAP Neural Network", "Keystroke Patterns Classification using the ARTMAP-FD Neural Network", "PSANet: Point-wise Spatial Attention Network for Scene Parsing", "PSANet: Point-wise Spatial Attention Network for Scene Parsing", "PSANet: Point-wise Spatial Attention Network for Scene Parsing", "PSANet: Point-wise Spatial Attention Network for Scene Parsing", "Knowledge Distillation Meets Self-Supervision", "Mind the Gap in Distilling StyleGANs", "Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup", "Optimizing Video Object Detection via a Scale-Time Lattice", "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks", "A Large-Scale Car Dataset for Fine-Grained Categorization and Verification", "Learning to Cluster Faces on an Affinity Graph", "Learning to Cluster Faces via Confidence and Connectivity Estimation", "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation", "Audio-Driven Emotional Video Portraits", "Non-Local Recurrent Network for Image Restoration", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds", "Unsupervised 3D Shape Completion through GAN Inversion", "Sketch Me That Shoe", "Sketch Me That Shoe", "Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning", "Fusing Object Context to Detect Functional Area for Cognitive Robots", "Interpret Vision Transformers as ConvNets with Dynamic Convolutions", "Extract Free Dense Labels from CLIP", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation", "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds", "Playing for 3D Human Recovery", "Reading Scene Text in Deep Convolutional Sequences", "Reading Scene Text in Deep Convolutional Sequences", "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks", "Be Your Own Prada: Fashion Synthesis with Structural Coherence", "Be Your Own Prada: Fashion Synthesis with Structural Coherence", "Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network", "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera", "Deep Cascaded Bi-Network for Face Hallucination", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds", "DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields", "MessyTable: Instance Association in Multiple Camera Views", "Unsupervised 3D Shape Completion through GAN Inversion", "The Devil of Face Recognition is in the Noise", "Deep Geometrized Cartoon Line Inbetweening", "Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation", "MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions", "Transformer-based Visual Segmentation: A Survey", "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", "Learning to Cluster Faces via Confidence and Connectivity Estimation", "Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior", "Flare7K: A Phenomenological Nighttime Flare Removal Dataset", "Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond", "Deep Specialized Network for Illuminant Estimation", "The Re-identification Challenge", "Person Re-Identification", "Deep Representation Learning with Target Coding", "ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting", "Exploring CLIP for Assessing the Look and Feel of Images", "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "Towards Robust Color Recovery for High-Capacity Color QR Codes", "Robust and Fast Decoding of High-Capacity Color QR Codes for Mobile Applications", "Towards Robust Color Recovery for High-Capacity Color QR Codes", "Robust and Fast Decoding of High-Capacity Color QR Codes for Mobile Applications", "Talk-to-Edit: Fine-Grained Facial Editing via Dialog", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars", "StyleGAN-Human: A Data-Centric Odyssey of Human Generation", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "Deep Animation Video Interpolation in the Wild", "Deep Animation Video Interpolation in the Wild", "Self-Supervised Representation Learning: Introduction, Advances, and Challenges", "Self-Supervised Representation Learning: Introduction, Advances, and Challenges", "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset", "CelebV-Text: A Large-Scale Facial Text-Video Dataset", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations", "Monocular 3D Reconstruction of Interacting Hands via Collision-Aware Factorized Refinements", "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network", "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera", "Pedestrian Color Naming via Convolutional Neural Network", "Towards Robust Color Recovery for High-Capacity Color QR Codes", "Video Object Segmentation with Re-identification", "StyleGAN-Human: A Data-Centric Odyssey of Human Generation", "ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting", "DifFace: Blind Face Restoration with Diffused Error Contraction", "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Text2Human: Text-Driven Controllable Human Image Generation", "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement", "Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement", "Crowd Saliency Detection via Global Similarity Structure", "Crowd Saliency Detection via Global Similarity Structure", "Crowd Saliency Detection via Global Similarity Structure", "An Empirical Study of Recent Face Alignment Methods", "Real or Not Real, That is the Question", "Development of Fine-Grained Pill Identification Algorithm using Deep Convolutional Network", "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis", "StyleLight: HDR Panorama Generation for Lighting Estimation and Editing", "Instance-level Facial Attributes Transfer with Geometry-aware Flow", "TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing", "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction", "Aligning Bag of Regions for Open-Vocabulary Object Detection", "Aligning Bag of Regions for Open-Vocabulary Object Detection", "Panoptic Video Scene Graph Generation", "AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies", "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "Panoptic Video Scene Graph Generation", "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis", "On-device Domain Generalization", "Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations", "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance", "Retrospective Class Incremental Learning", "BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis", "Learning Generative Structure Prior for Blind Text Image Super-resolution", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation", "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation", "Deeply Learned Attributes for Crowded Scene Understanding", "Scene-Independent Group Profiling in Crowd", "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition", "Learning Scene-Independent Group Descriptors for Crowd Understanding", "High-Quality Video Generation from Static Structural Annotations", "Siamese DETR", "Crowded Scene Understanding by Deeply Learned Volumetric Slices", "Slicing Convolutional Neural Network for Crowd Video Understanding", "Learning to Cluster Faces on an Affinity Graph", "Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "Robust Multi-Modality Multi-Object Tracking", "Face Detection"], "link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Q3-QASNKTMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:uoRD4RTSUPoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:eAUscmXIlQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IyxfXMz2bNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:gFrPXmx1TSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DGzKIA18-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ltc9rfRcsxEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:urP0JZOBBUsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:1xBWf43XMUgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cG0OFEevkNgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mS4qin7VKjkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IX653JsL2_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:MNNNGtAgD4EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:J4E9jCG1tHUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:uEM9VtRl8xsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:It0W0vAlS5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w0odbtu79TwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PZE8UkGerEcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:I-2NeQpV75MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:oAywNP-vUhwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6Zm5LS9gQ5UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:CCeGMaHljPEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:j8pvxH-kN2QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:rpSJNIKeXWsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cF7EPgIk0B4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:KIRwYnRZzWQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:8p8iYwVyaVcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:NnTm98qLMbgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:xa5BkEQK8BgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:vD2iS2Kej30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:ufKn5pxu7C0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:IT1MJ6E3JesC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:4oJvMfeQlr8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HKviVsUxM5wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ynsZFq2pu0MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6pF0wJmtdfAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:s1ouQE5r0WUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mVC4hKzE2FoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Q3-QASNKTMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KPAPMsW7zc0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:9CGX2owmTHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IyxfXMz2bNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vrnMIr82eJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eLRq4zTgah0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:F0CZgh39Fi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cx97FdCJQX8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fc7zyzPI2QAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:5UUbrqTvKfUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nj26e0utjpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CB6W3GmKGOEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:xckinRaLORAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WGv8Og3F3KgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:-fu4zM_6qcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ipvhVhH6zQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:URGbnSt0D2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:m03se8k-GH0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:I-2NeQpV75MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:pfJNqKtVkuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eIKNFFVQvJAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qSd0DAb9jMoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:lPDSu1ZU3VAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tL5YfqkXb3gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:uQyLbZAWguAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:MtS25d97-7AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:zGWyAL6qfKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:TNEldfgDb5MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:f13iAvnbnnYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ExNiBuTMO9IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Z7R3Ocg27JUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:43LB_KcVqeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6fxomyk5x4cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Agc8PWtS8JkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:plAW456RD7MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:3AIi9tQMIrsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qwOXE0mbtu4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:LNjCCq68lIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:MvIMIWP2nqIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vbti4gW_9XwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:-TLX1-BxFiYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KsTgnNRry18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dgXhHFWAKKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:ufKn5pxu7C0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CKf5m1HYVjMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:o0eFlWRwRSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Weau3kkTRIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:5N-NJrZHaHcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FKYJxdYMdFIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:y7JhAc_VBLgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:wuYnf3tzzDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KEtq3P1Vf8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:INdAjOZKYREC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ovGv7akYl-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HAmI6pRF5skC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9o6PfxSMcEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:JTtNqH-x4gYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nj26e0utjpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:gkldIfsazJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZYsTHYU9jrMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:-fu4zM_6qcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ipvhVhH6zQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:DUFsPKDdMi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:rzmi0EmCOGEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eIKNFFVQvJAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oFn-K-OQSCAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qSd0DAb9jMoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fMcxjvj1mj8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6Zm5LS9gQ5UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w7CBUyPWg-0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oH8HCDhqVGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WHBERAHVdrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KsTgnNRry18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:RVqaWcrwK10C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v6PuF9mNY3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:_n8fIOMweQoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6xXPb0EiZccC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:XeErXHja3Z8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:oAywNP-vUhwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:j8pvxH-kN2QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:jKT558fuBk8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mS4qin7VKjkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:k6nH7jlkaTkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:rpSJNIKeXWsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FKYJxdYMdFIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6pF0wJmtdfAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:uPCvBZYD9qUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WgvcDLhf7hwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jMZTt8odoasC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Q3-QASNKTMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:eAUscmXIlQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IyxfXMz2bNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:gFrPXmx1TSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9o6PfxSMcEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:It0W0vAlS5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:I-2NeQpV75MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cF7EPgIk0B4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:KIRwYnRZzWQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:8p8iYwVyaVcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:ufKn5pxu7C0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:IT1MJ6E3JesC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:4oJvMfeQlr8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mVC4hKzE2FoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HAmI6pRF5skC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:DrOLxFoABAwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:NZNkWSpQBv0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:x2hKVfJWtf0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:2mikiJ1VBVsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cxS_fjKIGZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZOYfmFL0FrgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:J-ba04ztB30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQ6BjWB6CiEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:lBZ7XAAYe3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:MnogvFdIBdwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dIILA_La5fwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:d3xjRt2Mi1YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:O0MA3yP7Y3UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mu2_AnMl8iYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PcT55Ow6fAIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:XJogQLJr2CkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:peNFRSk4lDYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QZWLLlSfqgYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6ScxedgR18sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:Zbx7W2Xs4QsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HevVnK7dagcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:jKT558fuBk8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mS4qin7VKjkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:k6nH7jlkaTkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:4Wrxgq2JVp0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6pF0wJmtdfAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:uPCvBZYD9qUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WgvcDLhf7hwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jMZTt8odoasC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6Zm5LS9gQ5UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Q3-QASNKTMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jq04SsiGh3QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:hEXC_dOfxuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:ufKn5pxu7C0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IyxfXMz2bNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WGv8Og3F3KgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oH8HCDhqVGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:eAUscmXIlQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:RVqaWcrwK10C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IX653JsL2_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:gFrPXmx1TSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:4oJvMfeQlr8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mVC4hKzE2FoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:g9YHGIZn7mcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Ui_nyea79ooC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0vYOBEH00j0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9o6PfxSMcEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:gL9scSG3K_gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:4WewbtJKmRkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qkm5LKljiV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:EmjvLWWcsQIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ZZwcLRaXOV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eIKNFFVQvJAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Vxkav03X4woC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0qX8s2k1IRwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:7CU1mCQUTf4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:_n8fIOMweQoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:djft3U1LymYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:bFuYayV9R1gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:h-xndbdg2koC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HAmI6pRF5skC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dgXhHFWAKKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:nbnL2fqDbzcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:f13iAvnbnnYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:FQ36aI_S1AEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:DwWRdx-KAo4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:43LB_KcVqeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ipvhVhH6zQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:wlzmIqt2EaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:GdZ7R06HQM4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:C5mqfHIFIucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:rJyh6hJnyfgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Lo8V22OuN40C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:I2jIoRS3jIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oYwriLWYh5YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:DUFsPKDdMi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WHBERAHVdrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3WNXLiBY60kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:jKT558fuBk8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mS4qin7VKjkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CKf5m1HYVjMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:k6nH7jlkaTkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Y0RG-0fxPaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zaHkCSmj5XkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6pF0wJmtdfAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WgvcDLhf7hwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WTQy_8Ay2UsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jMZTt8odoasC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6LV2YwJzdtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:uoRD4RTSUPoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PazO6pb-sMwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:AbQWx2m_oG8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:MNNNGtAgD4EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:J4E9jCG1tHUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w0odbtu79TwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:urP0JZOBBUsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:RZBefGmQYygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:1xBWf43XMUgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:s1ouQE5r0WUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:mgoTDWlsYNUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ZyBjE3zI-i8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CKf5m1HYVjMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:k6nH7jlkaTkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:dJ_BR67V0s4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:2TuIPqOg0e0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zaHkCSmj5XkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:XJogQLJr2CkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:peNFRSk4lDYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:SoGhKUJvMTQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WgvcDLhf7hwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WTQy_8Ay2UsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jMZTt8odoasC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fc7zyzPI2QAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:pYRxIbzCxX0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:s_OEDnM-dbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KPAPMsW7zc0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:xckinRaLORAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:o0eFlWRwRSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Z7R3Ocg27JUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:CmbFvBriOyMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:GdZ7R06HQM4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3WNXLiBY60kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Agc8PWtS8JkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dBzKUGQurMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:rJyh6hJnyfgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:djft3U1LymYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ZZwcLRaXOV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cAWJABFkdiUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qkm5LKljiV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:5UUbrqTvKfUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:RJNGbXJAtMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:xa5BkEQK8BgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:9CGX2owmTHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:FQ36aI_S1AEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cG0OFEevkNgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:-vzq6BoH5oUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:wlzmIqt2EaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ynsZFq2pu0MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cx97FdCJQX8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Lo8V22OuN40C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:I2jIoRS3jIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:CCeGMaHljPEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:aNch6Af-aFkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:jODAHDUrbwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:SPgmg5JLkoEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3WNXLiBY60kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DPO9WFcz7UcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:_u2aKJ9e-CoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6xXPb0EiZccC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:JTtNqH-x4gYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WHBERAHVdrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:09LM3QYkMKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:zdX0sdgBH_kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:3x-KLxxGyuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:xii_ZKWM4-0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Ul_CLA4dPeMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:n8FNryW2AHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nU66GSXDKhoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CB6W3GmKGOEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZYsTHYU9jrMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v6PuF9mNY3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Weau3kkTRIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9o6PfxSMcEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KEtq3P1Vf8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:I-2NeQpV75MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eIKNFFVQvJAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qSd0DAb9jMoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:lPDSu1ZU3VAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KsTgnNRry18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQY3Tb_h0-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3jqAvCjcdfEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WHBERAHVdrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oYwriLWYh5YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jPVjDSAV6m0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:09LM3QYkMKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Lpa4s8qvUTIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:AzjrKLz9U9oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:pfJNqKtVkuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fixghrsIJ_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fMcxjvj1mj8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cxS_fjKIGZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zTJoPluU4X4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQ6BjWB6CiEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ZyBjE3zI-i8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dIILA_La5fwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:d3xjRt2Mi1YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:dJ_BR67V0s4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PcT55Ow6fAIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:XJogQLJr2CkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:4uoR24qA-WYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:peNFRSk4lDYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:SoGhKUJvMTQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:YXPZ0dOdYS4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:pYRxIbzCxX0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QZWLLlSfqgYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6ScxedgR18sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HevVnK7dagcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:q82PccF7nXcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQ6BjWB6CiEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:MnogvFdIBdwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:O0MA3yP7Y3UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:YXPZ0dOdYS4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:Zbx7W2Xs4QsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HevVnK7dagcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FepLwMwnKBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6LV2YwJzdtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:MtS25d97-7AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:AbQWx2m_oG8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:URGbnSt0D2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:plAW456RD7MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:RZBefGmQYygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:MvIMIWP2nqIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:-TLX1-BxFiYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQY3Tb_h0-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3jqAvCjcdfEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WHBERAHVdrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jPVjDSAV6m0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:pfJNqKtVkuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fMcxjvj1mj8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cF7EPgIk0B4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:KIRwYnRZzWQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:It0W0vAlS5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:8p8iYwVyaVcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:hB2aVRuWZNwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:vD2iS2Kej30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:MtS25d97-7AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:aNch6Af-aFkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:uEM9VtRl8xsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Kv9jytqXTosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oH8HCDhqVGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nj26e0utjpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:RVqaWcrwK10C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:-fu4zM_6qcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WHBERAHVdrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tfDI-GPdlUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oYwriLWYh5YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:J-ba04ztB30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:lBZ7XAAYe3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:O0MA3yP7Y3UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mu2_AnMl8iYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:4Wrxgq2JVp0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zaHkCSmj5XkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WTQy_8Ay2UsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:jODAHDUrbwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:m44aUaJR3ikC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oYwriLWYh5YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v7LLNfYe7h0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:dJ_BR67V0s4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zaHkCSmj5XkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:SoGhKUJvMTQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:TY5xIG7f_2sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:pYRxIbzCxX0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WTQy_8Ay2UsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:J4E9jCG1tHUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w0odbtu79TwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:urP0JZOBBUsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:s1ouQE5r0WUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:mgoTDWlsYNUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:aNch6Af-aFkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:jODAHDUrbwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ZJ-noXUx9mkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:SPgmg5JLkoEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DPO9WFcz7UcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qwOXE0mbtu4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nj26e0utjpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v7LLNfYe7h0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:-fu4zM_6qcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Y0RG-0fxPaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tfDI-GPdlUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:x2hKVfJWtf0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZysSsiWj_g4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Ul_CLA4dPeMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DTjSuSUbmXsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9o6PfxSMcEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eIKNFFVQvJAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:h-xndbdg2koC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qSd0DAb9jMoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tL5YfqkXb3gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HAmI6pRF5skC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:URGbnSt0D2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:-TLX1-BxFiYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:MvIMIWP2nqIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FepLwMwnKBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PazO6pb-sMwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6LV2YwJzdtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:AbQWx2m_oG8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:RZBefGmQYygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:-TLX1-BxFiYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Ui_nyea79ooC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:xckinRaLORAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ExNiBuTMO9IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Z7R3Ocg27JUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:o0eFlWRwRSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vrnMIr82eJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6fxomyk5x4cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Agc8PWtS8JkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:wuYnf3tzzDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cAWJABFkdiUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:5UUbrqTvKfUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZYsTHYU9jrMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v6PuF9mNY3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ipvhVhH6zQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:y7JhAc_VBLgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:F0CZgh39Fi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:8rLWgkbgOXQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:y7JhAc_VBLgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0qX8s2k1IRwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:m03se8k-GH0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:F0CZgh39Fi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:gL9scSG3K_gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Y0RG-0fxPaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v7LLNfYe7h0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tfDI-GPdlUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:DwWRdx-KAo4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0klj8wIChNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CB6W3GmKGOEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Weau3kkTRIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:3AIi9tQMIrsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KEtq3P1Vf8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:lPDSu1ZU3VAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:f13iAvnbnnYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CKf5m1HYVjMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ExNiBuTMO9IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vrnMIr82eJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tL5YfqkXb3gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CB6W3GmKGOEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Weau3kkTRIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:3AIi9tQMIrsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KEtq3P1Vf8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:lPDSu1ZU3VAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:9CGX2owmTHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:-vzq6BoH5oUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:wlzmIqt2EaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ynsZFq2pu0MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Lo8V22OuN40C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:NZNkWSpQBv0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:DrOLxFoABAwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:C5mqfHIFIucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:nbnL2fqDbzcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qt-6tCTBDsQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oH8HCDhqVGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IX653JsL2_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FepLwMwnKBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0klj8wIChNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:AbQWx2m_oG8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:3AIi9tQMIrsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:TiLqlu47W2oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:RZBefGmQYygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KEtq3P1Vf8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cxS_fjKIGZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:J-ba04ztB30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQ6BjWB6CiEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:lBZ7XAAYe3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dIILA_La5fwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PcT55Ow6fAIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HevVnK7dagcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oFn-K-OQSCAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:gkldIfsazJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oFn-K-OQSCAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:gkldIfsazJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dgXhHFWAKKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:43LB_KcVqeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FKYJxdYMdFIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eLRq4zTgah0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vbti4gW_9XwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3jqAvCjcdfEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0d9pApVQ-n0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:3AIi9tQMIrsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:AzjrKLz9U9oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:pfJNqKtVkuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6gSKFiM3XosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:n8FNryW2AHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:zdX0sdgBH_kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nU66GSXDKhoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:9CGX2owmTHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:LNjCCq68lIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:zGWyAL6qfKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ynsZFq2pu0MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:3x-KLxxGyuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:xlVdBZVQT58C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:TiLqlu47W2oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:8rLWgkbgOXQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:y7JhAc_VBLgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0qX8s2k1IRwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZysSsiWj_g4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Ul_CLA4dPeMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:8rLWgkbgOXQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:09LM3QYkMKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:gL9scSG3K_gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:y7JhAc_VBLgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:8rLWgkbgOXQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:wuYnf3tzzDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:gL9scSG3K_gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0qX8s2k1IRwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:wE8AsS3ykUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:wE8AsS3ykUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:DUFsPKDdMi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:DUFsPKDdMi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:DUFsPKDdMi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:DUFsPKDdMi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WGv8Og3F3KgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qwOXE0mbtu4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:5N-NJrZHaHcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oH8HCDhqVGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:rzmi0EmCOGEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:IT1MJ6E3JesC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZYsTHYU9jrMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v6PuF9mNY3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:TNEldfgDb5MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:QYmifXMdJWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:F0CZgh39Fi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:gL9scSG3K_gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZysSsiWj_g4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZysSsiWj_g4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:uPCvBZYD9qUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:hB2aVRuWZNwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:7CU1mCQUTf4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:g9YHGIZn7mcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:m03se8k-GH0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:F0CZgh39Fi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:y7JhAc_VBLgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:vD2iS2Kej30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:vD2iS2Kej30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:rzmi0EmCOGEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w7CBUyPWg-0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w7CBUyPWg-0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:J-ba04ztB30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:lBZ7XAAYe3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DGzKIA18-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:F0CZgh39Fi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:wuYnf3tzzDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:8rLWgkbgOXQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:gL9scSG3K_gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:I2jIoRS3jIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vbti4gW_9XwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6gSKFiM3XosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:C6rTQemI8T8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:pfJNqKtVkuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:O0MA3yP7Y3UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v6PuF9mNY3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QZWLLlSfqgYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PcT55Ow6fAIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dIILA_La5fwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HKviVsUxM5wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:x2hKVfJWtf0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mVC4hKzE2FoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Qy-rCirNo-8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:2TuIPqOg0e0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ZyBjE3zI-i8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cdwqcPQS8ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:kvJssbFybhEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cdwqcPQS8ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:kvJssbFybhEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tL5YfqkXb3gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ipvhVhH6zQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:f13iAvnbnnYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eLRq4zTgah0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eLRq4zTgah0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DTjSuSUbmXsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DTjSuSUbmXsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KPAPMsW7zc0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dBzKUGQurMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0vYOBEH00j0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:zGWyAL6qfKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:09LM3QYkMKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:09LM3QYkMKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:J-ba04ztB30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:lBZ7XAAYe3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:hEXC_dOfxuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cdwqcPQS8ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:f13iAvnbnnYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Qy-rCirNo-8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:edV_OwlUe4UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ZyBjE3zI-i8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:h-xndbdg2koC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:h-xndbdg2koC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ExNiBuTMO9IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cxS_fjKIGZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HevVnK7dagcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Np1obAXpBq8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Np1obAXpBq8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Np1obAXpBq8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:W1ZWpF0a3GIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:_n8fIOMweQoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cOfwuRB03ygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:uQyLbZAWguAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:INdAjOZKYREC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ovGv7akYl-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ZZwcLRaXOV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:AzjrKLz9U9oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fixghrsIJ_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fixghrsIJ_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FKYJxdYMdFIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zTJoPluU4X4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:uQyLbZAWguAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:-TLX1-BxFiYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0vYOBEH00j0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zTJoPluU4X4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0paDrhnEFY0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:bFuYayV9R1gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:NObE97JSLecC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:s_OEDnM-dbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cAWJABFkdiUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:zdX0sdgBH_kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:3x-KLxxGyuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:xii_ZKWM4-0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:xlVdBZVQT58C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:TiLqlu47W2oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:n8FNryW2AHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nU66GSXDKhoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZYsTHYU9jrMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KsTgnNRry18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oYwriLWYh5YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:bkKuixW_xMkC"]}, "published_by_year": {"Year": ["2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023"], "# of Publications": [2, 0, 1, 0, 4, 2, 2, 6, 7, 11, 13, 19, 15, 22, 21, 23, 31, 43, 55]}, "citations_by_year": {"Year": ["2005", "2006", "2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [0, 3, 2, 2, 13, 18, 61, 66, 125, 255, 636, 1629, 2704, 4220, 6233, 8395, 11057, 13409, 13330, 235]}, "all_time_h_index": 103, "all_time_i10_index": 205, "h_index_by_publication_year": {"Publication Year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [2, 0, 1, 0, 4, 2, 2, 6, 7, 11, 12, 18, 13, 21, 19, 20, 25, 22, 9]}, "avg_citations_by_publication_year": {"Publication Year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [31.5, 0.0, 93.0, 0.0, 117.5, 126.0, 96.5, 221.16666666666666, 202.85714285714286, 793.7272727272727, 1029.2307692307693, 453.7368421052632, 134.06666666666666, 390.45454545454544, 385.42857142857144, 153.91304347826087, 91.70967741935483, 53.06976744186046, 4.509090909090909]}, "h_index_by_year": {"Year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [0, 1, 1, 1, 3, 4, 6, 7, 9, 13, 19, 30, 35, 43, 52, 66, 77, 90, 103]}, "h_index_by_years_from_publication_year": {"Publication Year": [2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 5, 6, 6, 6, 6, 7, 7, 7, 7, 7, 4, 8, 9, 10, 10, 11, 11, 11, 11, 11, 4, 8, 11, 12, 12, 12, 12, 12, 12, 6, 12, 16, 17, 18, 18, 18, 18, 5, 8, 11, 11, 13, 13, 13, 8, 16, 18, 20, 21, 21, 7, 15, 19, 19, 19, 8, 17, 20, 20, 9, 22, 25, 10, 22, 9]}, "all_time_i20_index": 188, "publications": {"Publication Year": ["2015", "2014", "2018", "2016", "2019", "2016", "2014", "2019", "2018", "2016", "2020", "2015", "2015", "2018", "2019", "2015", "2019", "2012", "2018", "2015", "2019", "2015", "2018", "2013", "2019", "2022", "2015", "2014", "2016", "2014", "2012", "2022", "2009", "2016", "2016", "2020", "2019", "2015", "2014", "2021", "2019", "2017", "2017", "2017", "2017", "2013", "2021", "2016", "2017", "2020", "2020", "2022", "2019", "2010", "2021", "2018", "2021", "2018", "2020", "2013", "2016", "2018", "2018", "2021", "2021", "2019", "2013", "2016", "2018", "2018", "2016", "2019", "2020", "2018", "2022", "2015", "2016", "2013", "2014", "2021", "2020", "2018", "2018", "2017", "2021", "2020", "2020", "2017", "2022", "2014", "2018", "2019", "2016", "2021", "2020", "2020", "2016", "2021", "2020", "2019", "2011", "2020", "2018", "2020", "2014", "2021", "2016", "2020", "2012", "2007", "2022", "2011", "2018", "2020", "2017", "2017", "2019", "2021", "2021", "2021", "2015", "2020", "2017", "2021", "2022", "2021", "2021", "2022", "2021", "2019", "2022", "2019", "2021", "2023", "2022", "2022", "2013", "2021", "2018", "2016", "2022", "2022", "2019", "2012", "2022", "2022", "2012", "2014", "2009", "2018", "2010", "2020", "2009", "2021", "2016", "2015", "2020", "2019", "2021", "2020", "2022", "2022", "2022", "2021", "2017", "2005", "2022", "2023", "2019", "2018", "2016", "2015", "2014", "2005", "2022", "2021", "2022", "2017", "2022", "2021", "2021", "2018", "2023", "2021", "2016", "2014", "2021", "2017", "2022", "2023", "2023", "2023", "2022", "2019", "2016", "2020", "2023", "2022", "2021", "2021", "2023", "2023", "2022", "2023", "2022", "2017", "2013", "2023", "2022", "2022", "2022", "2020", "2015", "2023", "2023", "2022", "2022", "2023", "2022", "2012", "2023", "2023", "2022", "2009", "2023", "2023", "2023", "2022", "2022", "2017", "2023", "2023", "2023", "2022", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "2022", "2022", "2023", "2023", "2023", "2020", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "2021", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2019", "2018"], "Title": ["Image Super-Resolution Using Deep Convolutional Networks", "Learning a Deep Convolutional Network for Image Super-Resolution", "ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks", "Accelerating the Super-Resolution Convolutional Neural Network", "MMDetection: OpenMMLab Detection Toolbox and Benchmark", "WIDER FACE: A Face Detection Benchmark", "Facial Landmark Detection by Deep Multi-task Learning", "Hybrid Task Cascade for Instance Segmentation", "PSANet: Point-wise Spatial Attention Network for Scene Parsing", "Learning Deep Representation for Imbalanced Classification", "Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement", "A Large-Scale Car Dataset for Fine-Grained Categorization and Verification", "Compression Artifacts Reduction by a Deep Convolutional Network", "Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform", "EDVR: Video Restoration with Enhanced Deformable Convolutional Networks", "Semantic Image Segmentation via Deep Parsing Network", "Learning a Unified Classifier Incrementally via Rebalancing", "Feature Mining for Localised Crowd Counting", "LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation", "DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection", "Region Proposal by Guided Anchoring", "Face Alignment by Coarse-to-Fine Shape Searching", "Non-Local Recurrent Network for Image Restoration", "Cumulative Attribute Space for Age and Crowd Density Estimation", "Learning Lightweight Lane Detection CNNs by Self Attention Distillation", "Domain Generalization: A Survey", "Learning Deep Representation for Face Alignment with Auxiliary Attributes", "Pedestrian Attribute Recognition At Far Distance", "Sketch Me That Shoe", "Person Re-Identification", "Person Re-identification: What Features Are Important?", "Conditional Prompt Learning for Vision-Language Models", "Multi-camera Activity Correlation Analysis", "Depth Map Super Resolution by Deep Multi-Scale Guidance", "Reading Scene Text in Deep Convolutional Sequences", "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection", "CARAFE: Content-Aware ReAssembly of FEatures", "Deeply Learned Attributes for Crowded Scene Understanding", "Scene-Independent Group Profiling in Crowd", "BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond", "Deep Imbalanced Learning for Face Recognition and Attribute Prediction", "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade", "Image Aesthetic Assessment: An Experimental Survey", "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks", "Be Your Own Prada: Fashion Synthesis with Structural Coherence", "Crowd Counting and Profiling: Methodology and Evaluation", "Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation", "Deep Cascaded Bi-Network for Face Hallucination", "From Facial Expression Recognition to Interpersonal Relation Prediction", "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation", "Knowledge Distillation Meets Self-Supervision", "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment", "Deep Flow Guided Video Inpainting", "Time-Delayed Correlation Analysis for Multi-Camera Activity Understanding", "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation", "The Devil of Face Recognition is in the Noise", "Low-Light Image and Video Enhancement Using Deep Learning: A Survey", "Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation", "Prime Sample Attention in Object Detection", "Person Re-Identification by Manifold Ranking", "Unconstrained Face Alignment via Cascaded Compositional Learning", "ReenactGAN: Learning to Reenact Faces via Boundary Transfer", "Faceness-Net: Face Detection through Deep Facial Part Responses", "K-Net: Towards Unified Image Segmentation", "Seesaw Loss for Long-Tailed Instance Segmentation", "Robust Multi-Modality Multi-Object Tracking", "POP: Person Re-Identification Post-Rank Optimisation", "Local Similarity-Aware Deep Feature Embedding", "Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning", "Lifelong Learning via Progressive Distillation and Retrospection", "Deep Specialized Network for Illuminant Estimation", "Dense Intrinsic Appearance Flow for Human Pose Transfer", "Online Deep Clustering for Unsupervised Representation Learning", "Deep Learning Markov Random Field for Semantic Segmentation", "Extract Free Dense Labels from CLIP", "Learning Social Relation Traits from Face Images", "Human Attribute Recognition by Deep Hierarchical Contexts", "From Semi-Supervised to Transfer Counting of Crowds", "The Re-identification Challenge", "GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution", "A Lightweight Optical Flow CNN - Revisiting Data Fidelity and Regularization", "Pose-Robust Face Recognition via Deep Residual Equivariant Mapping", "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition", "DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks", "Focal Frequency Loss for Image Reconstruction and Synthesis", "MEAD: A Large-scale Audio-visual Dataset for Emotional Talking Face Generation", "Cross-Scale Internal Graph Neural Network for Image Super-Resolution", "Face Detection Through Scale-Friendly Deep Convolutional Networks", "Self-Supervised Representation Learning: Introduction, Advances, and Challenges", "Constructing Robust Affinity Graph for Spectral Clustering", "Optimizing Video Object Detection via a Scale-Time Lattice", "Learning to Cluster Faces on an Affinity Graph", "A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-Resolution", "Audio-Driven Emotional Video Portraits", "RGB-D Salient Object Detection with Cross-Modality Modulation and Selection", "Side-Aware Boundary Localization for More Precise Object Detection", "Unsupervised Learning of Discriminative Attributes and Visual Representations", "Understanding Deformable Alignment in Video Super-Resolution", "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation", "Detecting and Discriminating Behavioural Anomalies", "Self-Supervised Scene De-occlusion", "Aesthetic-Driven Image Enhancement by Adversarial Learning", "Inter-Region Affinity Distillation for Road Marking Segmentation", "On-the-fly Feature Importance Mining for Person Re-Identification", "Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs", "Slicing Convolutional Neural Network for Crowd Video Understanding", "TSIT: A Simple and Versatile Framework for Image-to-Image Translation", "Stream-based Joint Exploration-Exploitation Active Learning", "Keystroke Patterns Classification using the ARTMAP-FD Neural Network", "Everybody's Talkin': Let Me Talk As You Want", "Security and Surveillance", "Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition", "Learning to Cluster Faces via Confidence and Connectivity Estimation", "Quantifying Facial Age by Posterior of Age Comparisons", "Video Object Segmentation with Re-identification", "Deep Network Interpolation for Continuous Imagery Effect Transition", "Unsupervised 3D Shape Completion through GAN Inversion", "Path-Restore: Learning Network Path Selection for Image Restoration", "Positional Encoding as Spatial Inductive Bias in GANs", "Deep Representation Learning with Target Coding", "LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation", "Learning Scene-Independent Group Descriptors for Crowd Understanding", "FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation", "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory", "Deceive D: Adaptive Pseudo Augmentation for GAN Training with Limited Data", "Deep Animation Video Interpolation in the Wild", "StyleGAN-Human: A Data-Centric Odyssey of Human Generation", "Talk-to-Edit: Fine-Grained Facial Editing via Dialog", "Delving Deep into Hybrid Annotations for 3D Human Recovery in the Wild", "Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation", "Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks", "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis", "Improving Data Augmentation for Multi-Modality 3D Object Detection", "Open-Vocabulary DETR with Conditional Matching", "Text2Human: Text-Driven Controllable Human Image Generation", "Video Synopsis by Heterogeneous Multi-source Correlation", "Unsupervised Object-Level Representation Learning from Scene Images", "Robust and Fast Decoding of High-Capacity Color QR Codes for Mobile Applications", "Joint Face Representation Adaptation and Clustering in Videos", "Unified Vision and Language Prompt Learning", "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer", "One-shot Face Reenactment", "Incremental Activity Modelling in Multiple Disjoint Cameras", "Video K-Net: A Simple, Strong, and Unified Baseline for Video Segmentation", "Investigating Tradeoffs in Real-World Video Super-Resolution", "Salient Motion Detection in Crowded Scenes", "Crowd Saliency Detection via Global Similarity Structure", "Modelling Activity Global Temporal Dependencies using Time Delayed Probabilistic Graphical Model", "Mix-and-Match Tuning for Self-supervised Semantic Segmentation", "Stream-based Active Unusual Event Detection", "TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting", "Modelling Multi-object Activity by Gaussian Processes", "Robust Reference-based Super-Resolution via C2-Matching", "Crowded Scene Understanding by Deeply Learned Volumetric Slices", "An Empirical Study of Recent Face Alignment Methods", "Feature Pyramid Grids", "Self-Supervised Learning via Conditional Motion Propagation", "Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network", "Real or Not Real, That is the Question", "Towards Robust Blind Face Restoration with Codebook Lookup Transformer", "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "Delving into Inter-Image Invariance for Unsupervised Visual Representations", "Texture Memory-Augmented Deep Patch-Based Image Inpainting", "Development of Fine-Grained Pill Identification Algorithm using Deep Convolutional Network", "Pressure-Based Typing Biometrics User Authentication Using the Fuzzy ARTMAP Neural Network", "LEDNet: Joint Low-light Enhancement and Deblurring in the Dark", "Exploring CLIP for Assessing the Look and Feel of Images", "Instance-level Facial Attributes Transfer with Geometry-aware Flow", "Discriminative Sparse Neighbor Approximation for Imbalanced Learning", "Learning from Multiple Sources for Video Summarisation", "Constrained Clustering with Imperfect Oracles", "Transferring Landmark Annotations for Cross-Dataset Face Alignment", "Development of a Pressure-based Typing Biometrics User Authentication System", "TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing", "CARAFE++: Unified Content-Aware ReAssembly of FEatures", "CelebV-HQ: A Large-Scale Video Facial Attributes Dataset", "Learning to Disambiguate by Asking Discriminative Questions", "Unsupervised Image-to-Image Translation with Generative Prior", "3D Human Texture Estimation from a Single Image with Transformers", "Monocular 3D Reconstruction of Interacting Hands via Collision-Aware Factorized Refinements", "Merge or Not? Learning to Group Faces via Imitation Learning", "Transformer-based Visual Segmentation: A Survey", "Playing for 3D Human Recovery", "Pedestrian Color Naming via Convolutional Neural Network", "Evaluating Feature Importance for Re-identification", "Semi-Supervised Domain Generalization with Stochastic StyleMatch", "Discover and Learn New Objects from Documentaries", "Deep Fourier Up-Sampling", "Masked Frequency Modeling for Self-Supervised Visual Pre-Training", "Computation-Efficient Knowledge Distillation via Uncertainty-Aware Mixup", "Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement", "VToonify: Controllable High-Resolution Portrait Video Style Transfer", "Disentangling Content and Style via Unsupervised Geometry Distillation", "Towards Robust Color Recovery for High-Capacity Color QR Codes", "MessyTable: Instance Association in Multiple Camera Views", "The Nuts and Bolts of Adopting Transformer in GANs", "Flare7K: A Phenomenological Nighttime Flare Removal Dataset", "ReconfigISP: Reconfigurable Camera Image Processing Pipeline", "Pareidolia Face Reenactment", "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation", "Flexible Piecewise Curves Estimation for Photo Enhancement", "Chasing the Tail in Monocular 3D Human Reconstruction with Prototype Memory", "Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation", "Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets", "Deep Learning for Scene Independent Crowd Analysis", "Constrained Clustering: Effective Constraint Propagation with Imperfect Oracles", "Aligning Bag of Regions for Open-Vocabulary Object Detection", "GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond", "On the Generalization of BasicVSR++ to Video Deblurring and Denoising", "StyleLight: HDR Panorama Generation for Lighting Estimation and Editing", "High-Quality Video Generation from Static Structural Annotations", "Towards Arbitrary-View Face Alignment by Recommendation Trees", "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "Network Pruning via Resource Reallocation", "Monocular 3D Object Reconstruction with GAN Inversion", "MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks", "Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation", "DifFace: Blind Face Restoration with Diffused Error Contraction", "Comparing Visual Feature Coding for Learning Disjoint Camera Dependencies", "Contextual Object Detection with Multimodal Large Language Models", "Panoptic Video Scene Graph Generation", "Dense Siamese Network for Dense Unsupervised Learning", "Authenticating the Identity of Computer Users with Typing Biometrics and the Fuzzy Min-Max Neural Network", "SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis", "Text2Performer: Text-Driven Human Video Generation", "CelebV-Text: A Large-Scale Facial Text-Video Dataset", "On-device Domain Generalization", "DeepFakes Detection: the DeeperForensics Dataset and Challenge", "Deep Learning Face Attributes for Face Detection and Alignment", "Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations", "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera", "Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior", "Mind the Gap in Distilling StyleGANs", "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "Semi-Supervised and Long-Tailed Object Detection with CascadeMatch", "MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions", "DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering", "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement", "Siamese DETR", "Learning Generative Structure Prior for Blind Text Image Super-resolution", "Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion", "Audio-driven Dubbing for User Generated Contents via Style-aware Semi-parametric Synthesis", "CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment", "AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies", "Talking Faces: Audio-to-Video Face Generation", "MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation", "Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond", "Reference-based Image and Video Super-Resolution via C2-Matching", "Face Detection", "Position-Guided Point Cloud Panoptic Segmentation Transformer", "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars", "Explore In-Context Learning for 3D Point Cloud Understanding", "StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation", "GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation", "BeautyREC: Robust, Efficient, and Content-preserving Makeup Transfer", "Correlational Image Modeling for Self-Supervised Visual Pre-Training", "Transformer with Implicit Edges for Particle-based Physics Simulation", "BRACE: The Breakdancing Competition Dataset for Dance Motion Synthesis", "Retrospective Class Incremental Learning", "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction", "Bailando++: 3D Dance GPT With Choreographic Memory", "Interpret Vision Transformers as ConvNets with Dynamic Convolutions", "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds", "Adaptive Window Pruning for Efficient Local Motion Deblurring", "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation", "Deep Geometrized Cartoon Line Inbetweening", "ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting", "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance", "DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields", "ProPainter: Improving Propagation and Transformer for Video Inpainting", "Correspondence Distillation from NeRF-based GAN", "Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation", "Towards Multi-Layered 3D Garments Animation", "StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces", "Improving On-policy Learning with Statistical Reward Accumulation", "Fusing Object Context to Detect Functional Area for Cognitive Robots"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:j8pvxH-kN2QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:koF6b02d8EEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:rpSJNIKeXWsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:wUCFpcnEedwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:4oJvMfeQlr8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:dMpQl7XwOw4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:DUFsPKDdMi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:urP0JZOBBUsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:Zbx7W2Xs4QsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:IT1MJ6E3JesC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:oAywNP-vUhwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:jKT558fuBk8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:k6nH7jlkaTkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:ufKn5pxu7C0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:gkldIfsazJcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:0EnyYjriUFMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&citation_for_view=559LF80AAAAJ:uEM9VtRl8xsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HqhvjgTjE9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:RVqaWcrwK10C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:CCeGMaHljPEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:nroGzMJTTpEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:KlAtU1dfN6UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DPO9WFcz7UcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:MtS25d97-7AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:It0W0vAlS5QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:R3hNpaxXUhUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZysSsiWj_g4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:mVmsd5A6BfQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:5nxA0vEk-isC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:URGbnSt0D2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VjBpw8Hezy4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:vD2iS2Kej30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:rJyh6hJnyfgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:-fu4zM_6qcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:zdX0sdgBH_kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:L8Ckcad2t8MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jMZTt8odoasC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w0odbtu79TwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IyxfXMz2bNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:NnTm98qLMbgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:rzmi0EmCOGEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:w7CBUyPWg-0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:4TOpqqG69KYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:MnogvFdIBdwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DGzKIA18-3YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:KIRwYnRZzWQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9o6PfxSMcEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WGv8Og3F3KgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:pYRxIbzCxX0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:JdL-Xu2nR38C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:TNEldfgDb5MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:I2jIoRS3jIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:O0MA3yP7Y3UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jq04SsiGh3QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:JTtNqH-x4gYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Zph67rFs4hoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:xa5BkEQK8BgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:FQ36aI_S1AEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:gFrPXmx1TSsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:jPVjDSAV6m0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:VBDT71xRUdcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oYwriLWYh5YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:M3ejUd6NZC8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:uoRD4RTSUPoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:uPCvBZYD9qUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oFn-K-OQSCAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:HKviVsUxM5wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:mgoTDWlsYNUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:lPDSu1ZU3VAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Q3-QASNKTMEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:g9YHGIZn7mcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:8p8iYwVyaVcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:J4E9jCG1tHUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:x2hKVfJWtf0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WTQy_8Ay2UsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ON9TkA72AHEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ynsZFq2pu0MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:3x-KLxxGyuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:cww_0JKUTDwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:djft3U1LymYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:CC3C2HR4nz8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:4uoR24qA-WYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:IX653JsL2_EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:DTjSuSUbmXsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:oH8HCDhqVGsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZYsTHYU9jrMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:Ul_CLA4dPeMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:QYmifXMdJWgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:ZOYfmFL0FrgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WHBERAHVdrEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:1xBWf43XMUgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=20&pagesize=80&citation_for_view=559LF80AAAAJ:WgvcDLhf7hwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:09LM3QYkMKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:wlzmIqt2EaEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Tyk-4Ss8FVUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eIKNFFVQvJAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PZE8UkGerEcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:aNch6Af-aFkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:QIV2ME_5wuYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HAmI6pRF5skC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nU66GSXDKhoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3WNXLiBY60kC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:UebtZRa9Y70C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:u-x6o8ySG0sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Q_E8KsG3g9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eQOLeE2rZwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KsTgnNRry18C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v6PuF9mNY3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:RJNGbXJAtMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ei5r6KrKXVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6pF0wJmtdfAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:gL9scSG3K_gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mS4qin7VKjkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Y0RG-0fxPaAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mVC4hKzE2FoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Kv9jytqXTosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:xii_ZKWM4-0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PazO6pb-sMwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:43LB_KcVqeAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qkm5LKljiV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:eLRq4zTgah0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:f13iAvnbnnYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tL5YfqkXb3gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:9CGX2owmTHMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ZJ-noXUx9mkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:SPgmg5JLkoEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:h-xndbdg2koC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:m44aUaJR3ikC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:RZBefGmQYygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ExNiBuTMO9IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:aqlVkmm33-oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Weau3kkTRIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:kvJssbFybhEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cF7EPgIk0B4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:AbQWx2m_oG8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:xckinRaLORAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cx97FdCJQX8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Se3iqnhoufwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:3jqAvCjcdfEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:dJ_BR67V0s4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:2mikiJ1VBVsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Np1obAXpBq8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:I-2NeQpV75MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:DwWRdx-KAo4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CKf5m1HYVjMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:n8FNryW2AHIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:W1ZWpF0a3GIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6xXPb0EiZccC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:qSd0DAb9jMoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:J-ba04ztB30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:_n8fIOMweQoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:peNFRSk4lDYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:S_fw-_riRmcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:CB6W3GmKGOEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:tfDI-GPdlUQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cOfwuRB03ygC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:roLk4NBRz8UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:6ScxedgR18sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:2TuIPqOg0e0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ovGv7akYl-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:MNNNGtAgD4EC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:DrOLxFoABAwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:NZNkWSpQBv0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Ltc9rfRcsxEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:wE8AsS3ykUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:ZZwcLRaXOV4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:nj26e0utjpAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KPAPMsW7zc0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:s1ouQE5r0WUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:o0eFlWRwRSUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:TY5xIG7f_2sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:zGWyAL6qfKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:-vzq6BoH5oUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:pfJNqKtVkuoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:y7JhAc_VBLgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:hEXC_dOfxuUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:_u2aKJ9e-CoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:plAW456RD7MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:XeErXHja3Z8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:mu2_AnMl8iYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:KEtq3P1Vf8oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:5N-NJrZHaHcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:HevVnK7dagcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:5UUbrqTvKfUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:Lo8V22OuN40C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:cdwqcPQS8ssC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:8rLWgkbgOXQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:v7LLNfYe7h0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=100&pagesize=100&citation_for_view=559LF80AAAAJ:PcT55Ow6fAIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:4Wrxgq2JVp0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:C5mqfHIFIucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6fxomyk5x4cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:YXPZ0dOdYS4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:LNjCCq68lIgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Lpa4s8qvUTIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:CmbFvBriOyMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:TeJ9juy8vcMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qxL8FJ1GzNcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fixghrsIJ_wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zaHkCSmj5XkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:SoGhKUJvMTQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:INdAjOZKYREC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:xlVdBZVQT58C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cG0OFEevkNgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ZyBjE3zI-i8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:jODAHDUrbwUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0qX8s2k1IRwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:rCzfLUpcSPoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6gSKFiM3XosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:edV_OwlUe4UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:8k81kl-MbHgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FepLwMwnKBMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ocbgtyEEUOwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQY3Tb_h0-cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qQc65DSaYXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:uQyLbZAWguAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vrnMIr82eJkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dBzKUGQurMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:-TLX1-BxFiYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:GdZ7R06HQM4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:8gBurD7jEYQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0vYOBEH00j0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:lBZ7XAAYe3oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QZWLLlSfqgYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qwOXE0mbtu4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:QhqSGEHatosC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6LV2YwJzdtgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:C6rTQemI8T8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6YFk0eKgftsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cxS_fjKIGZMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:TiLqlu47W2oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:NObE97JSLecC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Ui_nyea79ooC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:qt-6tCTBDsQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:PQ6BjWB6CiEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:FKYJxdYMdFIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:nbnL2fqDbzcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:3AIi9tQMIrsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dIILA_La5fwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fc7zyzPI2QAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:bkKuixW_xMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:fMcxjvj1mj8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:ipvhVhH6zQ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0d9pApVQ-n0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:s_OEDnM-dbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Z7R3Ocg27JUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:q82PccF7nXcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0klj8wIChNAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:4WewbtJKmRkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:bFuYayV9R1gC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:0paDrhnEFY0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:AzjrKLz9U9oC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:dgXhHFWAKKUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:7CU1mCQUTf4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:F0CZgh39Fi0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:d3xjRt2Mi1YC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:m03se8k-GH0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:vbti4gW_9XwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Qy-rCirNo-8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:zTJoPluU4X4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:wuYnf3tzzDUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:XJogQLJr2CkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Vxkav03X4woC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:cAWJABFkdiUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:EmjvLWWcsQIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:Agc8PWtS8JkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:6Zm5LS9gQ5UC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=559LF80AAAAJ&cstart=200&pagesize=100&citation_for_view=559LF80AAAAJ:hB2aVRuWZNwC"], "Topic": ["Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Federated Learning", "Others", "Others", "Computer Vision", "Others", "Others", "Federated Learning", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Others", "Others", "Computer Vision", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Others", "Federated Learning", "Others", "Federated Learning", "Others", "Others", "Others", "Federated Learning", "Others", "Others", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Artificial Intelligenc", "Computer Scienc", "Federated Learning", "Others", "Others", "Others", "Federated Learning", "Federated Learning", "Others", "Others", "Federated Learning", "Computer Scienc", "Federated Learning", "Federated Learning", "Federated Learning", "Others", "Others", "Federated Learning", "Federated Learning", "Computer Vision", "Federated Learning", "Others", "Others", "Federated Learning", "Artificial Intelligenc", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Others", "Federated Learning", "Federated Learning", "Federated Learning", "Others", "Federated Learning", "Federated Learning", "Federated Learning", "Artificial Intelligenc", "Federated Learning", "Others", "Computer Scienc", "Federated Learning", "Federated Learning", "Artificial Intelligenc", "Federated Learning", "Federated Learning", "Federated Learning", "Computer Scienc", "Computer Scienc", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Others", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Emerging Technologies in Computing", "Federated Learning", "Others", "Federated Learning", "Artificial Intelligenc", "Federated Learning", "Federated Learning", "Federated Learning", "Federated Learning", "Computer Scienc", "Federated Learning", "Federated Learning", "Federated Learning", "Computer Vision", "Federated Learning", "Federated Learning", "Computer Scienc", "Federated Learning", "Others", "Others"], "# of Citations": [8382, 5434, 3323, 3178, 2319, 1844, 1623, 1180, 1013, 997, 972, 921, 892, 856, 849, 795, 812, 734, 711, 686, 632, 615, 584, 540, 526, 505, 466, 453, 441, 432, 402, 391, 379, 344, 338, 321, 317, 312, 294, 298, 299, 294, 285, 285, 281, 268, 271, 264, 256, 247, 227, 224, 214, 209, 212, 210, 211, 210, 205, 202, 199, 199, 186, 187, 187, 188, 184, 183, 180, 178, 176, 177, 176, 172, 168, 170, 170, 164, 165, 166, 166, 162, 156, 150, 143, 139, 138, 135, 136, 136, 131, 131, 126, 124, 122, 118, 111, 112, 111, 110, 110, 109, 106, 102, 102, 97, 93, 93, 91, 93, 85, 84, 85, 86, 83, 82, 81, 78, 73, 74, 73, 72, 70, 69, 65, 65, 65, 62, 62, 64, 63, 61, 57, 56, 54, 52, 52, 52, 53, 50, 51, 47, 49, 47, 45, 47, 47, 46, 45, 45, 43, 43, 42, 41, 42, 41, 40, 40, 39, 39, 37, 35, 32, 34, 32, 33, 31, 31, 31, 30, 29, 30, 28, 30, 29, 28, 27, 27, 26, 25, 23, 23, 21, 21, 22, 22, 19, 21, 20, 19, 17, 17, 17, 17, 17, 15, 13, 13, 13, 13, 12, 12, 12, 10, 11, 11, 11, 9, 10, 9, 9, 9, 8, 7, 8, 8, 8, 7, 7, 7, 6, 6, 6, 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 4, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show\u00a0\u2026", "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.", "The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN\u2013network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github. com/xinntao/ESRGAN.", "As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt\u00a0\u2026", "We present MMDetection, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. The toolbox started from a codebase of MMDet team who won the detection track of COCO Challenge 2018. It gradually evolves into a unified platform that covers many popular detection methods and contemporary modules. It not only includes training and inference codes, but also provides weights for more than 200 network models. We believe this toolbox is by far the most complete detection toolbox. In this paper, we introduce the various features of this toolbox. In addition, we also conduct a benchmarking study on different methods, components, and their hyper-parameters. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors. Code and models are available at https://github.com/open-mmlab/mmdetection. The project is under active development and we will keep this document updated.", "Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.", "Facial landmark detection has long been impeded by the problems of occlusion and pose variation. Instead of treating the detection task as a single and independent problem, we investigate the possibility of improving detection robustness through multi-task learning. Specifically, we wish to optimize facial landmark detection together with heterogeneous but subtly correlated tasks, e.g. head pose estimation and facial attribute inference. This is non-trivial since different tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, with task-wise early stopping to facilitate learning convergence. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to\u00a0\u2026", "Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects:(1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing;(2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4% and 1.5% improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at https://github. com/open-mmlab/mmdetection.", "We notice information flow in convolutional neural networks is restricted inside local neighborhood regions due to the physical design of convolutional filters, which limits the overall understanding of complex scenes. In this paper, we propose the point-wise spatial attention network (PSANet) to relax the local neighborhood constraint. Each position on the feature map is connected to all the other ones through a self-adaptively learned attention mask. Moreover, information propagation in bi-direction for scene parsing is enabled. Information at other positions can be collected to help the prediction of the current position and vice versa, information at the current position can be distributed to assist the prediction of other ones. Our proposed approach achieves top performance on various competitive scene parsing datasets, including ADE20K, PASCAL VOC 2012 and Cityscapes, demonstrating its effectiveness and generality.", "Data in vision domain often exhibit highly-skewed class distribution, ie, most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional neural network (CNN) typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain both inter-cluster and inter-class margins. This tighter constraint effectively reduces the class imbalance inherent in the local data neighborhood. We show that the margins can be easily deployed in standard deep learning framework through quintuplet instance sampling and the associated triple-header hinge loss. The representation learned by our approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high-and low-level vision classification tasks that exhibit imbalanced class distribution.", "The paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network. Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image. The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, ie, it does not require any paired or unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Our method is efficient as image enhancement can be achieved by an intuitive and simple nonlinear curve mapping. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively. Furthermore, the potential benefits of our Zero-DCE to face detection in the dark are discussed.", "This paper aims to highlight vision related tasks centered around\" car\", which has been largely neglected by vision community in comparison to other objects. We show that there are still many interesting car-related problems and applications, which are not yet well explored and researched. To facilitate future car-related research, in this paper we present our on-going effort in collecting a large-scale dataset,\" CompCars\", that covers not only different car views, but also their different internal and external parts, and rich attributes. Importantly, the dataset is constructed with a cross-modality nature, containing a surveillancenature set and a web-nature set. We further demonstrate a few important applications exploiting the dataset, namely car model classification, car model verification, and attribute prediction. We also discuss specific challenges of the car-related problems and other potential applications that worth further investigations. The latest dataset can be downloaded at http://mmlab. ie. cuhk. edu. hk/datasets/comp_cars/index. html", "Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar\" easy to hard\" idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use cases (ie Twitter).", "Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet.", "Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects:(1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable convolutions, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at https://github. com/xinntao/EDVR.", "This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%.", "Conventionally, deep neural networks are trained offline, relying on a large dataset prepared in advance. This paradigm is often challenged in real-world applications, eg online services that involve continuous streams of incoming data. Recently, incremental learning receives increasing attention, and is considered as a promising solution to the practical challenges mentioned above. However, it has been observed that incremental learning is subject to a fundamental difficulty--catastrophic forgetting, namely adapting a model to new data often results in severe performance degradation on previous tasks or classes. Our study reveals that the imbalance between previous and new data is a crucial cause to this problem. In this work, we develop a new framework for incrementally learning a unified classifier, eg a classifier that treats both old and new classes uniformly. Specifically, we incorporate three components, cosine normalization, less-forget constraint, and inter-class separation, to mitigate the adverse effects of the imbalance. Experiments show that the proposed method can effectively rebalance the training process, thus obtaining superior performance compared to the existing methods. On CIFAR-100 and ImageNet, our method can reduce the classification errors by more than 6% and 13% respectively, under the incremental setting of 10 phases.", "This paper presents a multi-output regression model for crowd counting in public scenes. Existing counting by regression methods either learn a single model for global counting, or train a large number of separate regressors for localised density estimation. In contrast, our single regression model based approach is able to estimate people count in spatially localised regions and is more scalable without the need for training a large number of regressors proportional to the number of local regions. In particular, the proposed model automatically learns the functional mapping between interdependent low-level features and multi-dimensional structured outputs. The model is able to discover the inherent importance of different features for people counting at different spatial locations. Extensive evaluations on an existing crowd analysis benchmark dataset and a new more challenging dataset demonstrate the effectiveness of our approach.", "FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks:(1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network.(2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution.(3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github. com/twhui/LiteFlowNet.", "In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection diagram has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN, which is the state-of-the-art, from  to  on the ILSVRC2014 detection dataset. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provide a global view for people to understand the deep learning object detection pipeline.", "Region anchors are the cornerstone of modern object detection techniques. State-of-the-art detectors mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the spatial domain with a predefined set of scales and aspect ratios. In this paper, we revisit this foundational stage. Our study shows that it can be done much more effectively and efficiently. Specifically, we present an alternative scheme, named Guided Anchoring, which leverages semantic features to guide the anchoring. The proposed method jointly predicts the locations where the center of objects of interest are likely to exist as well as the scales and aspect ratios at different locations. On top of predicted anchor shapes, we mitigate the feature inconsistency with a feature adaption module. We also study the use of high-quality proposals to improve detection performance. The anchoring scheme can be seamlessly integrated into proposal methods and detectors. With Guided Anchoring, we achieve 9.1% higher recall on MS COCO with 90% fewer anchors than the RPN baseline. We also adopt Guided Anchoring in Fast R-CNN, Faster R-CNN and RetinaNet, respectively improving the detection mAP by 2.2%, 2.7% and 1.2%. Code is available at https://github. com/open-mmlab/mmdetection.", "We present a novel face alignment framework based on coarse-to-fine shape searching. Unlike the conventional cascaded regression approaches that start with an initial shape and refine the shape in a cascaded manner, our approach begins with a coarse search over a shape space that contains diverse shapes, and employs the coarse solution to constrain subsequent finer search of shapes. The unique stage-by-stage progressive and adaptive search i) prevents the final solution from being trapped in local optima due to poor initialisation, a common problem encountered by cascaded regression approaches; and ii) improves the robustness in coping with large pose variations. The framework demonstrates real-time performance and state-of-theart results on various benchmarks including the challenging 300-W dataset.", "Many classic methods have shown non-local self-similarity in natural images to be an effective prior for image restoration. However, it remains unclear and challenging to make use of this intrinsic property via deep networks. In this paper, we propose a non-local recurrent network (NLRN) as the first attempt to incorporate non-local operations into a recurrent neural network (RNN) for image restoration. The main contributions of this work are:(1) Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood.(2) We fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states. This new design boosts robustness against inaccurate correlation estimation due to severely degraded images.(3) We show that it is essential to maintain a confined neighborhood for computing deep feature correlation given degraded images. This is in contrast to existing practice that deploys the whole image. Extensive experiments on both image denoising and super-resolution tasks are conducted. Thanks to the recurrent non-local operations and correlation propagation, the proposed NLRN achieves superior results to state-of-the-art methods with many fewer parameters.", "A number of computer vision problems such as human age estimation, crowd density estimation and body/face pose (view angle) estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalarvalued output. Such a learning problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated. Encouraged by the recent success in using attributes for solving classification problems with sparse training data, this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available. More precisely, low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation (a label) that captures how the scalar output value (eg age, people count) changes continuously and cumulatively. Extensive experiments show that our cumulative attribute framework gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models, especially when the labelled training data is sparse with imbalanced sampling.", "Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, eg, severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, ie, Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of'free'supervision for further representation learning through performing top-down and layer-wise attention distillation within the net-work itself. SAD can be easily incorporated in any feed-forward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet-18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks.", "Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d. assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past\u00a0\u2026", "In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and\u00a0\u2026", "The capability of recognizing pedestrian attributes, such as gender and clothing style, at far distance, is of practical interest in far-view surveillance scenarios where face and body close-shots are hardly available. We make two contributions in this paper. First, we release a new pedestrian attribute dataset, which is by far the largest and most diverse of its kind. We show that the large-scale dataset facilitates the learning of robust attribute detectors with good generalization performance. Second, we present the benchmark performance by SVM-based method and propose an alternative approach that exploits context of neighboring pedestrian images for improved attribute inference.", "We investigate the problem of fine-grained sketch-based image retrieval (SBIR), where free-hand human sketches are used as queries to perform instance-level retrieval of images. This is an extremely challenging task because (i) visual comparisons not only need to be fine-grained but also executed cross-domain,(ii) free-hand (finger) sketches are highly abstract, making fine-grained matching harder, and most importantly (iii) annotated cross-domain sketch-photo datasets required for training are scarce, challenging many state-of-the-art machine learning techniques. In this paper, for the first time, we address all these challenges, providing a step towards the capabilities that would underpin a commercial sketch-based image retrieval application. We introduce a new database of 1,432 sketch-photo pairs from two categories with 32,000 fine-grained triplet ranking annotations. We then develop a deep triplet-ranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data. Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine-grained cross-domain ranking tasks.", "A fundamental task for a distributed multi-camera system is to associate people across camera views at different locations and times. In a crowded and uncontrolled environment observed by cameras from a distance, person re-identification by biometrics such as face and gait is infeasible due to insufficient image details and arbitrary viewing conditions. Visual appearance features, extracted mainly from clothing, are intrinsically weak for matching people. For instance, most people in public spaces wear dark clothes in winter. A person\u2019s appearance can also change significantly between different camera views if large changes occur in view angle, lighting, background clutter and occlusion. This results in different people appearing more alike than that of the same person across different camera views. In this chapter, we describe a method for learning the optimal matching distance criterion, regardless feature\u00a0\u2026", "State-of-the-art person re-identification methods seek robust person matching through combining various feature types. Often, these features are implicitly assigned with a single vector of global weights, which are assumed to be universally good for all individuals, independent to their different appearances. In this study, we show that certain features play more important role than others under different circumstances. Consequently, we propose a novel unsupervised approach for learning a bottom-up feature importance, so features extracted from different individuals are weighted adaptively driven by their unique and inherent appearance attributes. Extensive experiments on two public datasets demonstrate that attribute-sensitive feature importance facilitates more accurate person matching when it is fused together with global weights obtained using existing methods.", "With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning---a recent trend in NLP---to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github. com/KaiyangZhou/CoOp.", "We propose a novel approach for modelling correlations between activities in a busy public space captured by multiple non-overlapping and uncalibrated cameras. In our approach, each camera view is automatically decomposed into semantic regions, across which different spatio-temporal activity patterns are observed. A novel Cross Canonical Correlation Analysis (xCCA) framework is formulated to detect and quantify temporal and causal relationships between regional activities within and across camera views. The approach accomplishes three tasks: (1) estimate the spatial and temporal topology of the camera network; (2) facilitate more robust and accurate person re-identification; (3) perform global activity modelling and video temporal segmentation by linking visual evidence collected across camera views. Our approach differs from the state of the art in that it does not rely on either intra or inter camera\u00a0\u2026", "Depth boundaries often lose sharpness when upsampling from low-resolution (LR) depth maps especially at large upscaling factors. We present a new method to address the problem of depth map super resolution in which a high-resolution (HR) depth map is inferred from a LR depth map and an additional HR intensity image of the same scene. We propose a Multi-Scale Guided convolutional network (MSG-Net) for depth map super resolution. MSG-Net complements LR depth features with HR intensity features using a multi-scale fusion strategy. Such a multi-scale guidance allows the network to better adapt for upsampling of both fine- and large-scale structures. Specifically, the rich hierarchical HR intensity features at different levels progressively resolve ambiguity in depth map upsampling. Moreover, we employ a high-frequency domain training method to not only reduce training time but also facilitate\u00a0\u2026", "We develop a Deep-Text Recurrent Network (DTRN) that regards scene text reading as a sequence labelling problem. We leverage recent advances of deep convolutional neural networks to generate an ordered highlevel sequence from a whole word image, avoiding the difficult character segmentation problem. Then a deep recurrent model, building on long short-term memory (LSTM), is developed to robustly recognize the generated CNN sequences, departing from most existing approaches recognising each character independently. Our model has a number of appealing properties in comparison to existing scene text recognition methods:(i) It can recognise highly ambiguous words by leveraging meaningful context information, allowing it to work reliably without either pre-or post-processing;(ii) the deep CNN feature is robust to various image distortions;(iii) it retains the explicit order information in word image, which is essential to discriminate word strings;(iv) the model does not depend on pre-defined dictionary, and it can process unknown words and arbitrary strings. It achieves impressive results on several benchmarks, advancing the-state-of-the-art substantially.", "We present our on-going effort of constructing a large-scale benchmark for face forgery detection. The first version of this benchmark, DeeperForensics-1.0, represents the largest face forgery detection dataset by far, with 60, 000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. Extensive real-world perturbations are applied to obtain a more challenging benchmark of larger scale and higher diversity. All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed end-to-end face swapping framework. The quality of generated videos outperforms those in existing datasets, validated by user studies. The benchmark features a hidden test set, which contains manipulated videos achieving high deceptive scores in human evaluations. We further contribute a comprehensive study that evaluates five representative detection baselines and make a thorough analysis of different settings.", "Feature upsampling is a key operation in a number of modern convolutional network architectures, eg feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties:(1) Large field of view. Unlike previous works (eg bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field.(2) Content-aware handling. Instead of using a fixed kernel for all samples (eg deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly.(3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2% AP, 1.3% AP, 1.8% mIoU, 1.1 dB respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. Code and models are available at https://github. com/open-mmlab/mmdetection.", "Crowded scene understanding is a fundamental problem in computer vision. In this study, we develop a multi-task deep model to jointly learn and combine appearance and motion features for crowd understanding. We propose crowd motion channels as the input of the deep model and the channel design is inspired by generic properties of crowd systems. To well demonstrate our deep model, we construct a new large-scale WWW Crowd dataset with 10000 videos from 8257 crowded scenes, and build an attribute set with 94 attributes on WWW. We further measure user study performance on WWW and compare this with the proposed deep models. Extensive experiments show that our deep models display significant performance improvements in cross-scene attribute recognition compared to strong crowd-related feature-based baselines, and the deeply learned features behave an superior performance in multi-task learning.", "Groups are the primary entities that make up a crowd. Understanding group-level dynamics and properties is thus scientifically important and practically useful in a wide range of applications, especially for crowd understanding. In this study we show that fundamental group-level properties, such as intra-group stability and inter-group conflict, can be systematically quantified by visual descriptors. This is made possible through learning a novel Collective Transition prior, which leads to a robust approach for group segregation in public spaces. From the prior, we further devise a rich set of group property visual descriptors. These descriptors are scene-independent, and can be effectively applied to public-scene with variety of crowd densities and distributions. Extensive experiments on hundreds of public scene video clips demonstrate that such property descriptors are not only useful but also necessary for group state analysis and crowd scene understanding.", "Video super-resolution (VSR) approaches tend to have more components than the image counterparts as they need to exploit the additional temporal dimension. Complex designs are not uncommon. In this study, we wish to untangle the knots and reconsider some most essential components for VSR guided by four basic functionalities, ie, Propagation, Alignment, Aggregation, and Upsampling. By reusing some existing components added with minimal redesigns, we show a succinct pipeline, BasicVSR, that achieves appealing improvements in terms of speed and restoration quality in comparison to many state-of-the-art algorithms. We conduct systematic analysis to explain how such gain can be obtained and discuss the pitfalls. We further show the extensibility of BasicVSR by presenting an information-refill mechanism and a coupled propagation scheme to facilitate information aggregation. The BasicVSR and its extension, IconVSR, can serve as strong baselines for future VSR approaches.", "Data for face analysis often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary deep learning methods typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain inter-cluster margins both within and between classes. This tight constraint effectively reduces the class imbalance inherent in the local data neighborhood, thus carving much more balanced class boundaries locally. We show that it is easy to deploy angular margins between the cluster\u00a0\u2026", "We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and'difficulty-aware'learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.", "This article reviews recent computer vision techniques used in the assessment of image aesthetic quality. Image aesthetic assessment aims at computationally distinguishing high-quality from low-quality photos based on photographic rules, typically in the form of binary classification or quality scoring. A variety of approaches has been proposed in the literature to try to solve this challenging problem. In this article, we summarize these approaches based on visual feature types (hand-crafted features and deep features) and evaluation criteria (data set characteristics and evaluation metrics). The main contributions and novelties of the reviewed approaches are highlighted and discussed. In addition, following the emergence of deep-learning techniques, we systematically evaluate recent deep-learning settings that are useful for developing a robust deep model for aesthetic scoring.", "A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks is met with a diminishing return and increased training difficulty; on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.", "We present a novel and effective approach for generating new clothing on a wearer through generative adversarial learning. Given an input image of a person and a sentence describing a different outfit, our model\" redresses\" the person as desired, while at the same time keeping the wearer and her/his pose unchanged. Generating new outfits with precise regions conforming to a language description while retaining wearer's body structure is a new challenging task. Existing generative adversarial networks are not ideal in ensuring global coherence of structure given both the input photograph and language description as conditions. We address this challenge by decomposing the complex generative process into two conditional stages. In the first stage, we generate a plausible semantic segmentation map that obeys the wearer's pose as a latent spatial arrangement. An effective spatial constraint is formulated to guide the generation of this semantic segmentation map. In the second stage, a generative model with a newly proposed compositional mapping layer is used to render the final image with precise regions and textures conditioned on this map. We extended the DeepFashion dataset [8] by collecting sentence descriptions for 79K images. We demonstrate the effectiveness of our approach through both quantitative and qualitative evaluations. A user study is also conducted.", "Video imagery based crowd analysis for population profiling and density estimation in public spaces can be a highly effective tool for establishing global situational awareness. Different strategies such as counting by detection and counting by clustering have been proposed, and more recently counting by regression has also gained considerable interest due to its feasibility in handling relatively more crowded environments. However, the scenarios studied by existing regression-based techniques are rather diverse in terms of both evaluation data and experimental settings. It can be difficult to compare them in order to draw general conclusions on their effectiveness. In addition, contributions of individual components in the processing pipeline such as feature extraction and perspective normalization remain unclear and less well studied. This study describes and compares the state-of-the-art methods for\u00a0\u2026", "This paper presents a novel method, Zero-Reference Deep Curve Estimation (Zero-DCE), which formulates light enhancement as a task of image-specific curve estimation with a deep network. Our method trains a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order curves for dynamic range adjustment of a given image. The curve estimation is specially designed, considering pixel value range, monotonicity, and differentiability. Zero-DCE is appealing in its relaxed assumption on reference images, i.e., it does not require any paired or even unpaired data during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and drive the learning of the network. Despite its simplicity, we show that it generalizes well to diverse lighting conditions. Our method is efficient as image enhancement can be achieved by an\u00a0\u2026", "We present a novel framework for hallucinating faces of unconstrained poses and with very low resolution (face size as small as 5pxIOD). In contrast to existing studies that mostly ignore or assume pre-aligned face spatial configuration (e.g. facial landmarks localization or dense correspondence field), we alternatingly optimize two complementary tasks, namely face hallucination and dense correspondence field estimation, in a unified framework. In addition, we propose a new gated deep bi-network that contains two functionality-specialized branches to recover different levels of texture details. Extensive experiments demonstrate that such formulation allows exceptional hallucination quality on in-the-wild low-res faces with significant pose and illumination variations.", "Interpersonal relation defines the association, e.g., warm, friendliness, and dominance, between two or more people. We investigate if such fine-grained and high-level relation traits can be characterized and quantified from face images in the wild. We address this challenging problem by first studying a deep network architecture for robust recognition of facial expressions. Unlike existing models that typically learn from facial expression labels alone, we devise an effective multitask network that is capable of learning from rich auxiliary attributes such as gender, age, and head pose, beyond just facial expression data. While conventional supervised training requires datasets with complete labels (e.g., all samples must be labeled with gender, age, and expression), we show that this requirement can be relaxed via a novel attribute propagation method. The approach further allows us to leverage the inherent\u00a0\u2026", "Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig. 1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible restoration and manipulation are made possible through relaxing the assumption of existing GAN inversion methods, which tend to fix the generator\u00a0\u2026", "Knowledge distillation, which involves extracting the \u201cdark knowledge\u201d from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting \u201cricher dark knowledge\u201d from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the\u00a0\u2026", "A recurrent structure is a popular framework choice for the task of video super-resolution. The state-of-the-art method BasicVSR adopts bidirectional propagation with feature alignment to effectively exploit information from the entire input video. In this study, we redesign BasicVSR by proposing second-order grid propagation and flow-guided deformable alignment. We show that by empowering the recurrent framework with enhanced propagation and alignment, one can exploit spatiotemporal information across misaligned video frames more effectively. The new components lead to an improved performance under a similar computational constraint. In particular, our model BasicVSR++ surpasses BasicVSR by a significant 0.82 dB in PSNR with similar number of parameters. BasicVSR++ is generalizable to other video restoration tasks, and obtains three champions and one first runner-up in NTIRE 2021 video restoration challenge.", "Video inpainting, which aims at filling in missing regions in a video, remains challenging due to the difficulty of preserving the precise spatial and temporal coherence of video contents. In this work we propose a novel flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, we consider the video inpainting as a pixel propagation problem. We first synthesize a spatially and temporally coherent optical flow field across video frames using a newly designed Deep Flow Completion network, then use the synthesized flow fields to guide the propagation of pixels to fill up the missing regions in the video. Specifically, the Deep Flow Competion network follows a coarse-to-fine refinement strategy to complete the flow fields, while their quality is further improved by hard flow example mining. Following the guide of the completed flow fields, the missing video regions can be filled up precisely. Our method is evaluated on DAVIS and YouTubeVOS datasets qualitatively and quantitatively, achieving the state-of-the-art performance in terms of inpainting quality and speed.", "We propose a novel approach to understanding activities from their partial observations monitored through multiple non-overlapping cameras separated by unknown time gaps. In our approach, each camera view is first decomposed automatically into regions based on the correlation of object dynamics across different spatial locations in all camera views. A new Cross Canonical Correlation Analysis (xCCA) is then formulated to discover and quantify the time delayed correlations of regional activities observed within and across multiple camera views in a single common reference space. We show that learning the time delayed activity correlations offers important contextual information for (i) spatial and temporal topology inference of a camera network; (ii) robust person re-identification and (iii) global activity interpretation and video temporal segmentation. Crucially, in contrast to conventional methods, our\u00a0\u2026", "While accurate lip synchronization has been achieved for arbitrary-subject audio-driven talking face generation, the problem of how to efficiently drive the head pose remains. Previous methods rely on pre-estimated structural information such as landmarks and 3D parameters, aiming to generate personalized rhythmic movements. However, the inaccuracy of such estimated information under extreme conditions would lead to degradation problems. In this paper, we propose a clean yet effective framework to generate pose-controllable talking faces. We operate on non-aligned raw face images, using only a single photo as an identity reference. The key is to modularize audio-visual representations by devising an implicit low-dimension pose code. Substantially, both speech content and head pose information lie in a joint non-identity embedding space. While speech content information can be defined by learning the intrinsic synchronization between audio-visual modalities, we identify that a pose code will be complementarily learned in a modulated convolution-based reconstruction framework. Extensive experiments show that our method generates accurately lip-synced talking faces whose poses are controllable by other videos. Moreover, our model has multiple advanced capabilities including extreme view robustness and talking face frontalization.", null, "Low-light image enhancement (LLIE) aims at improving the perception or interpretability of an image captured in an environment with poor illumination. Recent advances in this area are dominated by deep learning-based solutions, where many learning strategies, network structures, loss functions, training data, etc. have been employed. In this paper, we provide a comprehensive survey to cover various aspects ranging from algorithm taxonomy to unsolved open issues. To examine the generalization of existing methods, we propose a low-light image and video dataset, in which the images and videos are taken by different mobile phones\u2019 cameras under diverse illumination conditions. Besides, for the first time, we provide a unified online platform that covers many popular LLIE methods, of which the results can be produced through a user-friendly web interface. In addition to qualitative and quantitative evaluation\u00a0\u2026", "The problem of video object segmentation can become extremely challenging when multiple instances co-exist. While each instance may exhibit large scale and pose variations, the problem is compounded when instances occlude each other causing failures in tracking. In this study, we formulate a deep recurrent network that is capable of segmenting and tracking objects in video simultaneously by their temporal continuity, yet able to re-identify them when they re-appear after a prolonged occlusion. We combine both temporal propagation and re-identification functionalities into a single framework that can be trained end-to-end. In particular, we present a re-identification module with template expansion to retrieve missing objects despite their large appearance changes. In addition, we contribute a new attention-based recurrent mask propagation approach that is robust to distractors not belonging to the target segment. Our approach achieves a new state-of-the-art global mean (Region Jaccard and Boundary F measure) of 68.2 on the challenging DAVIS 2017 benchmark (test-dev set), outperforming the winning solution which achieves a global mean of 66.1 on the same partition.", "It is a common paradigm in object detection frameworks to treat all samples equally and target at maximizing the performance on average. In this work, we revisit this paradigm through a careful study on how different samples contribute to the overall performance measured in terms of mAP. Our study suggests that the samples in each mini-batch are neither independent nor equally important, and therefore a better classifier on average does not necessarily result in higher mAP. Motivated by this study, we propose the notion of Prime Samples, those that play a key role in driving the detection performance. We further develop a simple yet effective sampling and learning strategy called PrIme Sample Attention (PISA) that directs the focus of the training process towards such samples. Our experiments demonstrate that it is often more effective to focus on prime samples than hard samples when training a detector. Particularly, on the MSCOCO dataset, PISA outperforms the random sampling baseline and hard mining schemes, eg OHEM and Focal Loss, consistently by around 2% on both single-stage and two-stage detectors, even with a strong backbone ResNeXt-101. Code is available at: https://github. com/open-mmlab/mmdetection.", "Existing person re-identification methods conventionally rely on labelled pairwise data to learn a task-specific distance metric for ranking. The value of unlabelled gallery instances is generally overlooked. In this study, we show that it is possible to propagate the query information along the unlabelled data manifold in an unsupervised way to obtain robust ranking results. In addition, we demonstrate that the performance of existing supervised metric learning methods can be significantly boosted once integrated into the proposed manifold ranking-based framework. Extensive evaluation is conducted on three benchmark datasets.", "We present a practical approach to address the problem of unconstrained face alignment for a single image. In our unconstrained problem, we need to deal with large shape and appearance variations under extreme head poses and rich shape deformation. To equip cascaded regressors with the capability to handle global shape variation and irregular appearance-shape relation in the unconstrained scenario, we partition the optimisation space into multiple domains of homogeneous descent, and predict a shape as a composition of estimations from multiple domain-specific regressors. With a specially formulated learning objective and a novel tree splitting function, our approach is capable of estimating a robust and meaningful composition. In addition to achieving state-of-the-art accuracy over existing approaches, our framework is also an efficient solution (350 FPS), thanks to the on-the-fly domain exclusion mechanism and the capability of leveraging the fast pixel feature.", "We present a novel learning-based framework for face reenactment. The proposed method, known as ReenactGAN, is capable of transferring facial movements and expressions from an arbitrary person\u2019s monocular video input to a target person\u2019s video. Instead of performing a direct transfer in the pixel space, which could result in structural artifacts, we first map the source face onto a boundary latent space. A transformer is subsequently used to adapt the source face\u2019s boundary to the target\u2019s boundary. Finally, a target-specific decoder is used to generate the reenacted target face. Thanks to the effective and reliable boundary-based transfer, our method can perform photo-realistic face reenactment. In addition, ReenactGAN is appealing in that the whole reenactment process is purely feed-forward, and thus the reenactment process can run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model are publicly available on our project page.", "We propose a deep convolutional neural network (CNN) for face detection leveraging on facial attributes based supervision. We observe a phenomenon that part detectors emerge within CNN trained to classify attributes from uncropped face images, without any explicit part supervision. The observation motivates a new method for finding faces through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is data-driven, and carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variations. Our method achieves promising performance on popular benchmarks including FDDB, PASCAL Faces, AFW, and WIDER FACE.", "Semantic, instance, and panoptic segmentations have been addressed using different and specialized frameworks despite their underlying connections. This paper presents a unified, simple, and effective framework for these essentially similar tasks. The framework, named K-Net, segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. To remedy the difficulties of distinguishing various instances, we propose a kernel update strategy that enables each kernel dynamic and conditional on its meaningful group in the input image. K-Net can be trained in an end-to-end manner with bipartite matching, and its training and inference are naturally NMS-free and box-free. Without bells and whistles, K-Net surpasses all previous published state-of-the-art single-model results of panoptic segmentation on MS COCO test-dev split and semantic segmentation on ADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with 60%-90% faster inference speeds. Code and models will be released at https://github. com/ZwwWayne/K-Net/.", "Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail. Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative samples on tail classes lead to a biased learning process for classifiers. Consequently, objects of tail categories are more likely to be misclassified as backgrounds or head categories. To tackle this problem, we propose Seesaw Loss to dynamically re-balance gradients of positive and negative samples for each category, with two complementary factors, ie, mitigation factor and compensation factor. The mitigation factor reduces punishments to tail categories wrt the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified instances to avoid false positives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains significant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset without bells and whistles. Code is available at https://github. com/open-mmlab/mmdetection.", "Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (eg, center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (ie, sensors) is capable of performing its role independently to preserve reliability, and could further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github. com/ZwwWayne/mmMOT.", "Owing to visual ambiguities and disparities, person reidentification methods inevitably produce suboptimal ranklist, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likelycandidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank OPtimisation (POP) method, which allows a user to quickly refine their search by either\" one-shot\" or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user's searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the stateof-the-art distance metric learning based ranking models, even with just\" one shot\" feedback optimisation, by as much as over 30% performance improvement for rank 1 reidentification on the VIPeR and i-LIDS datasets.", "Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.", "We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain.", "Lifelong learning aims at adapting a learned model to new tasks while retaining the knowledge gained earlier. A key challenge for lifelong learning is how to strike a balance between the preservation on old tasks and the adaptation to a new one within a given model. Approaches that combine both objectives in training have been explored in previous works. Yet the performance still suffers from considerable degradation in a long sequence of tasks. In this work, we propose a novel approach to lifelong learning, which tries to seek a better balance between preservation and adaptation via two techniques: Distillation and Retrospection. Specifically, the target model adapts to the new task by knowledge distillation from an intermediate expert, while the previous knowledge is more effectively preserved by caching a small subset of data for old tasks. The combination of Distillation and Retrospection leads to a more gentle learning curve for the target model, and extensive experiments demonstrate that our approach can bring consistent improvements on both old and new tasks.", "Illuminant estimation to achieve color constancy is an ill-posed problem. Searching the large hypothesis space for an accurate illuminant estimation is hard due to the ambiguities of unknown reflections and local patch appearances. In this work, we propose a novel Deep Specialized Network (DS-Net) that is adaptive to diverse local regions for estimating robust local illuminants. This is achieved through a new convolutional network architecture with two interacting sub-networks, i.e.\u00a0an hypotheses network (HypNet) and a selection network (SelNet). In particular, HypNet generates multiple illuminant hypotheses that inherently capture different modes of illuminants with its unique two-branch structure. SelNet then adaptively picks for confident estimations from these plausible hypotheses. Extensive experiments on the two largest color constancy benchmark datasets show that the proposed \u2018hypothesis\u00a0\u2026", "We present a novel approach for the task of human pose transfer, which aims at synthesizing a new image of a person from an input image of that person and a target pose. We address the issues of limited correspondences identified between keypoints only and invisible pixels due to self-occlusion. Unlike existing methods, we propose to estimate dense and intrinsic 3D appearance flow to better guide the transfer of pixels between poses. In particular, we wish to generate the 3D flow from just the reference and target poses. Training a network for this purpose is non-trivial, especially when the annotations for 3D appearance flow are scarce by nature. We address this problem through a flow synthesis stage. This is achieved by fitting a 3D model to the given pose pair and project them back to the 2D plane to compute the dense appearance flow for training. The synthesized ground-truths are then used to train a feedforward network for efficient mapping from the input and target skeleton poses to the 3D appearance flow. With the appearance flow, we perform feature warping on the input image and generate a photorealistic image of the target pose. Extensive results on DeepFashion and Market-1501 datasets demonstrate the effectiveness of our approach over existing methods. Our code is available at http://mmlab. ie. cuhk. edu. hk/projects/pose-transfer", "Joint clustering and feature learning methods have shown remarkable performance in unsupervised representation learning. However, the training schedule alternating between feature clustering and network parameters update leads to unstable learning of visual representations. To overcome this challenge, we propose Online Deep Clustering (ODC) that performs clustering and network update simultaneously rather than alternatingly. Our key insight is that the cluster centroids should evolve steadily in keeping the classifier stably updated. Specifically, we design and maintain two dynamic memory modules, ie, samples memory to store samples' labels and features, and centroids memory for centroids evolution. We break down the abrupt global clustering into steady memory update and batch-wise label re-assignment. The process is integrated into network update iterations. In this way, labels and the network evolve shoulder-to-shoulder rather than alternatingly. Extensive experiments demonstrate that ODC stabilizes the training process and boosts the performance effectively.", "Semantic segmentation tasks can be well modeled by Markov Random Field (MRF). This paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN to model unary terms and additional layers are devised to approximate the mean field (MF) algorithm for pairwise terms. It has several appealing properties. First, different from the recent works that required many iterations of MF during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many\u00a0\u2026", "Contrastive Language-Image Pre-training (CLIP) has made a remarkable breakthrough in open-vocabulary zero-shot image recognition. Many recent studies leverage the pre-trained CLIP models for image-level classification and manipulation. In this paper, we wish examine the intrinsic potential of CLIP for pixel-level dense prediction, specifically in semantic segmentation. To this end, with minimal modification, we show that MaskCLIP yields compelling segmentation results on open concepts across various datasets in the absence of annotations and fine-tuning. By adding pseudo labeling and self-training, MaskCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods by large margins, e.g., mIoUs of unseen classes on PASCAL VOC/PASCAL Context/COCO Stuff are improved from 35.6/20.7/30.3 to 86.1/66.7/54.7. We also test the robustness of MaskCLIP under input corruption and\u00a0\u2026", "Social relation defines the association, eg, warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos.", "We present an approach for recognizing human attributes in unconstrained settings. We train a Convolutional Neural Network (CNN) to select the most attribute-descriptive human parts from all poselet detections, and combine them with the whole body as a pose-normalized deep representation. We further improve by using deep hierarchical contexts ranging from human-centric level to scene level. Human-centric context captures human relations, which we compute from the nearest neighbor parts of other people on a pyramid of CNN feature maps. The matched parts are then average pooled and they act as a similarity regularization. To utilize the scene context, we re-score human-centric predictions by the global scene classification score jointly learned in our CNN, yielding final scene-aware predictions. To facilitate our study, a large-scale WIDER Attribute dataset(Dataset URL:                      http://mmlab\u00a0\u2026", "Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives:(1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively.(2) Rather than learning from only labelled data, the abundant unlabelled data are exploited.(3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.", "For making sense of the vast quantity of visual data generated by the rapid expansion of large-scale distributed multi-camera systems, automated person re-identification is essential. However, it poses a significant challenge to computer vision systems. Fundamentally, person re-identification requires to solve two difficult problems of \u2018finding needles in haystacks\u2019 and \u2018connecting the dots\u2019 by identifying instances and associating the whereabouts of targeted people travelling across large distributed space\u2013time locations in often crowded environments. This capability would enable the discovery of, and reasoning about, individual-specific long-term structured activities and behaviours. Whilst solving the person re-identification problem is inherently challenging, it also promises enormous potential for a wide range of practical applications, ranging from security and surveillance to retail and health care. As a result\u00a0\u2026", "We show that pre-trained Generative Adversarial Networks (GANs), eg, StyleGAN, can be used as a latent bank to improve the restoration quality of large-factor image super-resolution (SR). While most existing SR approaches attempt to generate realistic textures through learning with adversarial loss, our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass to generate the upscaled image. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Switching the bank allows the method to deal with images from diverse categories, eg, cat, building, human face, and car. Images upscaled by GLEAN shows clear improvements in terms of fidelity and texture faithfulness in comparison to existing methods.", "Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2 [1] , the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet [2] but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with\u00a0\u2026", "Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced-there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead.", "Recognizing visual relationships< subject-predicate-object> among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions:(i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features.(ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our final Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the effectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@ 1 11.42% from 8.16%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.", "In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN    , which was the state-of-the-art, from    to    percent on the ILSVRC2014\u00a0\u2026", "Image reconstruction and synthesis have witnessed remarkable progress thanks to the development of generative models. Nonetheless, gaps could still exist between the real and generated images, especially in the frequency domain. In this study, we show that narrowing gaps in the frequency domain can ameliorate image reconstruction and synthesis quality further. We propose a novel focal frequency loss, which allows a model to adaptively focus on frequency components that are hard to synthesize by down-weighting the easy ones. This objective function is complementary to existing spatial losses, offering great impedance against the loss of important frequency information due to the inherent bias of neural networks. We demonstrate the versatility and effectiveness of focal frequency loss to improve popular models, such as VAE, pix2pix, and SPADE, in both perceptual quality and quantitative performance. We further show its potential on StyleGAN2.", "The synthesis of natural emotional reactions is an essential criterion in vivid talking-face video generation. This criterion is nevertheless seldom taken into consideration in previous works due to the absence of a large-scale, high-quality emotional audio-visual dataset. To address this issue, we build the Multi-view Emotional Audio-visual Dataset (MEAD), a talking-face video corpus featuring 60 actors and actresses talking with eight different emotions at three different intensity levels. High-quality audio-visual clips are captured at seven different view angles in a strictly-controlled environment. Together with the dataset, we release an emotional talking-face generation baseline that enables the manipulation of both emotion and its intensity. Our dataset could benefit a number of different research fields including conditional generation, cross-modal understanding and expression recognition. Code, model and data are\u00a0\u2026", "Non-local self-similarity in natural images has been well studied as an effective prior in image restoration. However, for single image super-resolution (SISR), most existing deep non-local methods (eg, non-local neural networks) only exploit similar patches within the same scale of the low-resolution (LR) input image. Consequently, the restoration is limited to using the same-scale information while neglecting potential high-resolution (HR) cues from other scales. In this paper, we explore the cross-scale patch recurrence property of a natural image, ie, similar patches tend to recur many times across different scales. This is achieved using a novel cross-scale internal graph neural network (IGNN). Specifically, we dynamically construct a cross-scale graph by searching k-nearest neighboring patches in the downsampled LR image for each query patch in the LR image. We then obtain the corresponding k HR neighboring patches in the LR image and aggregate them adaptively in accordance to the edge label of the constructed graph. In this way, the HR information can be passed from k HR neighboring patches to the LR query patch to help it recover more detailed textures. Besides, these internal image-specific LR/HR exemplars are also significant complements to the external information learned from the training dataset. Extensive experiments demonstrate the effectiveness of IGNN against the state-of-the-art SISR methods including existing non-local networks on standard benchmarks.", "In this paper, we share our experience in designing a convolutional network-based face detector that could handle faces of an extremely wide range of scales. We show that faces with different scales can be modeled through a specialized set of deep convolutional networks with different structures. These detectors can be seamlessly integrated into a single unified network that can be trained end-to-end. In contrast to existing deep models that are designed for wide scale range, our network does not require an image pyramid input and the model is of modest complexity. Our network, dubbed ScaleFace, achieves promising performance on WIDER FACE and FDDB datasets with practical runtime speed. Specifically, our method achieves 76.4 average precision on the challenging WIDER FACE dataset and 96% recall rate on the FDDB dataset with 7 frames per second (fps) for 900 * 1300 input image.", "Self-supervised representation learning (SSRL) methods aim to provide powerful, deep feature learning without the requirement of large annotated data sets, thus alleviating the annotation bottleneck\u2014one of the main barriers to the practical deployment of deep learning today. These techniques have advanced rapidly in recent years, with their efficacy approaching and sometimes surpassing fully supervised pretraining alternatives across a variety of data modalities, including image, video, sound, text, and graphs. This article introduces this vibrant area, including key concepts, the four main families of approaches and associated state-of-the-art techniques, and how self-supervised methods are applied to diverse modalities of data. We further discuss practical considerations including workflows, representation transferability, and computational cost. Finally, we survey major open challenges in the field, that provide\u00a0\u2026", "Spectral clustering requires robust and meaningful affinity graphs as input in order to form clusters with desired structures that can well support human intuition. To construct such affinity graphs is non-trivial due to the ambiguity and uncertainty inherent in the raw data. In contrast to most existing clustering methods that typically employ all available features to construct affinity matrices with the Euclidean distance, which is often not an accurate representation of the underlying data structures, we propose a novel unsupervised approach to generating more robust affinity graphs via identifying and exploiting discriminative features for improving spectral clustering. Specifically, our model is capable of capturing and combining subtle similarity information distributed over discriminative feature subspaces for more accurately revealing the latent data distribution and thereby leading to improved data clustering, especially with heterogeneous data sources. We demonstrate the efficacy of the proposed approach on challenging image and video datasets.", "High-performance object detection relies on expensive convolutional networks to compute features, often leading to significant challenges in applications, eg those that re-quire detecting objects from video streams in real time. The key to this problem is to trade accuracy for efficiency in an effective way, ie reducing the computing cost while maintaining competitive performance. To seek a good balance, previous efforts usually focus on optimizing the model architectures. This paper explores an alternative approach, that is, to reallocate the computation over a scale-time space. The basic idea is to perform expensive detection sparsely and propagate the results across both scales and time with substantially cheaper networks, by exploiting the strong correlations among them. Specifically, we present a unified framework that integrates detection, temporal propagation, and across-scale refinement on a Scale-Time Lattice. On this framework, one can explore various strategies to balance performance and cost. Taking advantage of this flexibility, we further develop an adaptive scheme with the detector invoked on demand and thus obtain improved tradeoff. On ImageNet VID dataset, the proposed method can achieve a competitive mAP 79.6% at 20 fps, or 79.0% at 62 fps as a performance/speed tradeoff.", "Face recognition sees remarkable progress in recent years, and its performance has reached a very high level. Taking it to a next level requires substantially larger data, which would involve prohibitive annotation cost. Hence, exploiting unlabeled data becomes an appealing alternative. Recent works have shown that clustering unlabeled faces is a promising approach, often leading to notable performance gains. Yet, how to effectively cluster, especially on a large-scale (ie million-level or above) dataset, remains an open question. A key challenge lies in the complex variations of cluster patterns, which make it difficult for conventional clustering methods to meet the needed accuracy. This work explores a novel approach, namely, learning to cluster instead of relying on hand-crafted criteria. Specifically, we propose a framework based on graph convolutional network, which combines a detection and a segmentation module to pinpoint face clusters. Experiments show that our method yields significantly more accurate face clusters, which, as a result, also lead to further performance gain in face recognition.", "Heterogeneous face recognition (HFR) refers to matching face imagery across different domains. It has received much interest from the research community as a result of its profound implications in law enforcement. A wide variety of new invariant features, cross-modality matching models and heterogeneous datasets are being established in recent years. This survey provides a comprehensive review of established techniques and recent developments in HFR. Moreover, we offer a detailed account of datasets and benchmarks commonly used for evaluation. We finish by assessing the state of the field and discussing promising directions for future research.", "Despite previous success in generating audio-driven talking heads, most of the previous studies focus on the correlation between speech content and the mouth shape. Facial emotion, which is one of the most important features on natural human faces, is always neglected in their methods. In this work, we present Emotional Video Portraits (EVP), a system for synthesizing high-quality video portraits with vivid emotional dynamics driven by audios. Specifically, we propose the Cross-Reconstructed Emotion Disentanglement technique to decompose speech into two decoupled spaces, ie, a duration-independent emotion space and a duration dependent content space. With the disentangled features, dynamic 2D emotional facial landmarks can be deduced. Then we propose the Target-Adaptive Face Synthesis technique to generate the final high-quality video portraits, by bridging the gap between the deduced landmarks and the natural head poses of target videos. Extensive experiments demonstrate the effectiveness of our method both qualitatively and quantitatively.", "We present an effective method to progressively integrate and refine the cross-modality complementarities for RGB-D salient object detection (SOD). The proposed network mainly solves two challenging issues: 1) how to effectively integrate the complementary information from RGB image and its corresponding depth map, and 2) how to adaptively select more saliency-related features. First, we propose a cross-modality feature modulation (cmFM) module to enhance feature representations by taking the depth features as prior, which models the complementary relations of RGB-D data. Second, we propose an adaptive feature selection (AFS) module to select saliency-related features and suppress the inferior ones. The AFS module exploits multi-modality spatial feature fusion with the self-modality and cross-modality interdependencies of channel features are considered. Third, we employ a saliency\u00a0\u2026", "Current object detection frameworks mainly rely on bounding box regression to localize objects. Despite the remarkable progress in recent years, the precision of bounding box regression remains unsatisfactory, hence limiting performance in object detection. We observe that precise localization requires careful placement of each side of the bounding box. However, the mainstream approach, which focuses on predicting centers and sizes, is not the most effective way to accomplish this task, especially when there exists displacements with large variance between the anchors and the targets. In this paper, we propose an alternative approach, named as  Side-Aware Boundary Localization (SABL), where each side of the bounding box is respectively localized with a dedicated network branch. To tackle the difficulty of precise localization in the presence of displacements with large variance, we further propose\u00a0\u2026", "Attributes offer useful mid-level features to interpret visual data. While most attribute learning methods are supervised by costly human-generated labels, we introduce a simple yet powerful unsupervised approach to learn and predict visual attributes directly from data. Given a large unlabeled image collection as input, we train deep Convolutional Neural Networks (CNNs) to output a set of discriminative, binary attributes often with semantic meanings. Specifically, we first train a CNN coupled with unsupervised discriminative clustering, and then use the cluster membership as a soft supervision to discover shared attributes from the clusters while maximizing their separability. The learned attributes are shown to be capable of encoding rich imagery properties from both natural images and contour patches. The visual representations learned in this way are also transferrable to other tasks such as object detection. We show other convincing results on the related tasks of image retrieval and classification, and contour detection.", "Deformable convolution, originally proposed for the adaptation to geometric variations of objects, has recently shown compelling performance in aligning multiple frames and is increasingly adopted for video super-resolution. Despite its remarkable performance, its underlying mechanism for alignment remains unclear. In this study, we carefully investigate the relation between deformable alignment and the classic flow-based alignment. We show that deformable convolution can be decomposed into a combination of spatial warping and convolution. This decomposition reveals the commonality of deformable alignment and flow-based alignment in formulation, but with a key difference in their offset diversity. We further demonstrate through experiments that the increased diversity in deformable alignment yields better-aligned features, and hence significantly improves the quality of video super-resolution output. Based on our observations, we propose an offset-fidelity loss that guides the offset learning with optical flow. Experiments show that our loss successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Aside from the contributions to deformable alignment, our formulation inspires a more flexible approach to introduce offset diversity to flow-based alignment, improving its performance.", "Neural Architecture Search (NAS) achieves significant progress in many computer vision tasks. While many methods are proposed to improve the efficiency of NAS, the search progress is still laborious because training and evaluating plausible architectures over large search space is time-consuming. Assessing network candidates under a proxy (ie, computationally reduced setting) thus becomes inevitable. In this paper, we observe that most existing proxies exhibit different behaviors in maintaining the rank consistency among network candidates. In particular, some proxies can be more reliable-the rank of candidates does not differ much comparing their reduced setting performance and final performance. In this paper, we systematically investigate some widely adopted reduction factors and report our observations. Inspired by these observations, we present a reliable proxy and further formulate a hierarchical proxy strategy that spends more computations on candidate networks that are potentially more accurate, while discards unpromising ones in early stage with a fast proxy. This leads to an economical evolutionary-based NAS (EcoNAS), which achieves an impressive 400xsearch time reduction in comparison to the evolutionary-based state of the art [19](8 vs 3150 GPU days). Some new proxies led by our observations can also be applied to accelerate other NAS methods while still able to discover good candidate networks with performance matching those found by previous proxy strategies. Codes and models will be released to facilitate future research.", "Unsupervised image-to-image translation aims at learning a mapping between two visual domains. However, learning a translation across large geometry variations al-ways ends up with failure. In this work, we present a novel disentangle-and-translate framework to tackle the complex objects image-to-image translation task. Instead of learning the mapping on the image space directly, we disentangle image space into a Cartesian product of the appearance and the geometry latent spaces. Specifically, we first in-troduce a geometry prior loss and a conditional VAE loss to encourage the network to learn independent but com-plementary representations. The translation is then built on appearance and geometry space separately. Extensive experiments demonstrate the superior performance of our method to other state-of-the-art approaches, especially in the challenging near-rigid and non-rigid objects translation tasks. In addition, by taking different exemplars as the ap-pearance references, our method also supports multimodal translation. Project page: https://wywu. github. io/projects/TGaGa/TGaGa. html", "This paper aims to address the problem of anomaly detection and discrimination in complex behaviours, where anomalies are subtle and difficult to detect owing to the complex temporal dynamics and correlations among multiple objects\u2019 behaviours. Specifically, we decompose a complex behaviour pattern according to its temporal characteristics or spatial-temporal visual contexts. The decomposed behaviour is then modelled using a cascade of Dynamic Bayesian Networks (CasDBNs). In contrast to existing standalone models, the proposed behaviour decomposition and cascade modelling offers distinct advantage in simplicity for complex behaviour modelling. Importantly, the decomposition and cascade structure map naturally to the structure of complex behaviour, allowing for a more effective detection of subtle anomalies in surveillance videos. Comparative experiments using both indoor and outdoor data are\u00a0\u2026", "Natural scene understanding is a challenging task, particularly when encountering images of multiple objects that are partially occluded. This obstacle is given rise by varying object ordering and positioning. Existing scene understanding paradigms are able to parse only the visible parts, resulting in incomplete and unstructured scene interpretation. In this paper, we investigate the problem of scene de-occlusion, which aims to recover the underlying occlusion ordering and complete the invisible parts of occluded objects. We make the first attempt to address the problem through a novel and unified framework that recovers hidden scene structures without ordering and amodal annotations as supervisions. This is achieved via Partial Completion Network (PCNet)-mask (M) and-content (C), that learn to recover fractions of object masks and contents, respectively, in a self-supervised manner. Based on PCNet-M and PCNet-C, we devise a novel inference scheme to accomplish scene de-occlusion, via progressive ordering recovery, amodal completion and content completion. Extensive experiments on real-world scenes demonstrate the superior performance of our approach to other alternatives. Remarkably, our approach that is trained in a self-supervised manner achieves comparable results to fully-supervised methods. The proposed scene de-occlusion framework benefits many applications, including high-quality and controllable image manipulation and scene recomposition (see Fig. 1), as well as the conversion of existing modal mask annotations to amodal mask annotations.", "We introduce EnhanceGAN, an adversarial learning based model that performs automatic image enhancement. Traditional image enhancement frameworks typically involve training models in a fully-supervised manner, which require expensive annotations in the form of aligned image pairs. In contrast to these approaches, our proposed EnhanceGAN only requires weak supervision (binary labels on image aesthetic quality) and is able to learn enhancement operators for the task of aesthetic-based image enhancement. In particular, we show the effectiveness of a piecewise color enhancement module trained with weak supervision, and extend the proposed EnhanceGAN framework to learning a deep filtering-based aesthetic enhancer. The full differentiability of our image enhancement operators enables the training of EnhanceGAN in an end-to-end manner. We further demonstrate the capability of EnhanceGAN\u00a0\u2026", "We study the problem of distilling knowledge from a large deep teacher network to a much smaller student network for the task of road marking segmentation. In this work, we explore a novel knowledge distillation (KD) approach that can transfer'knowledge'on scene structure more effectively from a teacher to a student model. Our method is known as Inter-Region Affinity KD (IntRA-KD). It decomposes a given road scene image into different regions and represents each region as a node in a graph. An inter-region affinity graph is then formed by establishing pairwise relationships between nodes based on their similarity in feature distribution. To learn structural knowledge from the teacher network, the student is required to match the graph generated by the teacher. The proposed method shows promising results on three large-scale road marking segmentation benchmarks, ie, ApolloScape, CULane and LLAMAS, by taking various lightweight models as students and ResNet-101 as the teacher. IntRA-KD consistently brings higher performance gains on all lightweight models, compared to previous distillation methods. Our code is available at https://github. com/cardwing/Codes-for-IntRA-KD.", "State-of-the-art person re-identification methods seek robust person matching through combining various feature types. Often, these features are implicitly assigned with generic weights, which are assumed to be universally and equally good for all individuals, independent of people's different appearances. In this study, we show that certain features play more important role than others under different viewing conditions. To explore this characteristic, we propose a novel unsupervised approach to bottom-up feature importance mining on-the-fly specific to each re-identification probe target image, so features extracted from different individuals are weighted adaptively driven by their salient and inherent appearance attributes. Extensive experiments on three public datasets give insights on how feature importance can vary depending on both the viewing condition and specific person's appearance, and demonstrate\u00a0\u2026", "Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code is available at https://github.com/XingangPan/GAN2Shape.", "Learning and capturing both appearance and dynamic representations are pivotal for crowd video understanding. Convolutional Neural Networks (CNNs) have shown its remarkable potential in learning appearance representations from images. However, the learning of dynamic representation, and how it can be effectively combined with appearance features for video analysis, remains an open problem. In this study, we propose a novel spatio-temporal CNN, named Slicing CNN (S-CNN), based on the decomposition of 3D feature maps into 2D spatio-and 2D temporal-slices representations. The decomposition brings unique advantages:(1) the model is capable of capturing dynamics of different semantic units such as groups and objects,(2) it learns separated appearance and dynamic representations while keeping proper interactions between them, and (3) it exploits the selectiveness of spatial filters to discard irrelevant background clutter for crowd understanding. We demonstrate the effectiveness of the proposed S-CNN model on the WWW crowd video dataset for attribute recognition and observe significant performance improvements to the state-of-the-art methods (62.55% from 51.84%[21]).", "We introduce a simple and versatile framework for image-to-image translation. We unearth the importance of normalization layers, and provide a carefully designed two-stream generative model with newly proposed feature transformations in a coarse-to-fine fashion. This allows multi-scale semantic structure information and style representation to be effectively captured and fused by the network, permitting our method to scale to various tasks in both unsupervised and supervised settings. No additional constraints (e.g., cycle consistency) are needed, contributing to a very clean and simple method. Multi-modal image synthesis with arbitrary style control is made possible. A systematic study compares the proposed method with several state-of-the-art task-specific baselines, verifying its effectiveness in both perceptual quality and quantitative evaluations. GitHub:                  https://github.com/EndlessSora/TSIT\u00a0\u2026", "Learning from streams of evolving and unbounded data is an important problem, for example in visual surveillance or internet scale data. For such large and evolving real-world data, exhaustive supervision is impractical, particularly so when the full space of classes is not known in advance therefore joint class discovery (exploration) and boundary learning (exploitation) becomes critical. Active learning has shown promise in jointly optimising exploration-exploitation with minimal human supervision. However, existing active learning methods either rely on heuristic multi-criteria weighting or are limited to batch processing. In this paper, we present a new unified framework for joint exploration-exploitation active learning in streams without any heuristic weighting. Extensive evaluation on classification of various image and surveillance video datasets demonstrates the superiority of our framework over existing methods.", "This paper presents the development of a keystroke dynamics-based user authentication system using the ARTMAP-FD neural network. The effectiveness of ARTMAP- FD in classifying keystroke patterns is analyzed and compared against a number of widely used machine learning systems. The results show that ARTMAP-FD performs well against many of its counterparts in keystroke patterns classification. Apart from that, instead of using the conventional typing timing characteristics, the applicability of typing pressure to ascertaining user's identity is investigated. The experimental results show that combining both latency and pressure patterns can improve the Equal Error Rate (ERR) of the system. Keywords: Keystroke dynamics, typing biometrics, novelty detection, ARTMAP FD, Fuzzy ARTMAP", "We present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photo-realistic video. This method is unique because it is highly dynamic. It does not assume a person-specific rendering network yet capable of translating one source audio into one random chosen video output within a set of speech videos. Instead of learning a highly heterogeneous and nonlinear mapping from audio to the video directly, we first factorize each target video frame into orthogonal parameter spaces,  i.e. , expression, geometry, and pose, via monocular 3D face reconstruction. Next, a recurrent network is introduced to translate source audio into expression parameters that are primarily related to the audio content. The audio-translated expression parameters are then used to synthesize a photo-realistic human subject in each video frame, with the movement of the mouth regions precisely\u00a0\u2026", "Human eyes are highly efficient devices for scanning through a large quantity of low-level visual sensory data and delivering selective information to one\u2019s brain for high-level semantic interpretation and gaining situational awareness. Over the last few decades, the computer vision community has endeavoured to bring about similar perceptual capabilities to artificial visual sensors. Substantial efforts have been made towards understanding static images of individual objects and the corresponding processes in the human visual system. This endeavour is intensified further by the need for understanding a massive quantity of video data, with the aim to comprehend multiple entities not only within a single image but also over time across multiple video frames for understanding their spatio-temporal relations. A significant application of video analysis and understanding is intelligent surveillance, which aims to\u00a0\u2026", "Face recognition has witnessed great progresses in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environment and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationship by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two well-designed modules, the\" committee\" and the\" mediator\", which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all the labels are employed.", "Face clustering is an essential tool for exploiting the unlabeled face data, and has a wide range of applications including face annotation and retrieval. Recent works show that supervised clustering can result in noticeable performance gain. However, they usually involve heuristic steps and require numerous overlapped subgraphs, severely restricting their accuracy and efficiency. In this paper, we propose a fully learnable clustering framework without requiring a large number of overlapped subgraphs. Instead, we transform the clustering problem into two sub-problems. Specifically, two graph convolutional networks, named GCN-V and GCN-E, are designed to estimate the confidence of vertices and the connectivity of edges, respectively. With the vertex confidence and edge connectivity, we can naturally organize more relevant vertices on the affinity graph and group them into clusters. Experiments on two large-scale benchmarks show that our method significantly improves clustering accuracy and thus performance of the recognition models trained on top, yet it is an order of magnitude more efficient than existing supervised methods.", "We introduce a novel approach for annotating large quantity of in-the-wild facial images with high-quality posterior age distribution as labels. Each posterior provides a probability distribution of estimated ages for a face. Our approach is motivated by observations that it is easier to distinguish who is the older of two people than to determine the person's actual age. Given a reference database with samples of known ages and a dataset to label, we can transfer reliable annotations from the former to the latter via human-in-the-loop comparisons. We show an effective way to transform such comparisons to posterior via fully-connected and SoftMax layers, so as to permit end-to-end training in a deep network. Thanks to the efficient and effective annotation approach, we collect a new large-scale facial age dataset, dubbed `MegaAge', which consists of 41,941 images. Data can be downloaded from our project page mmlab.ie.cuhk.edu.hk/projects/MegaAge and github.com/zyx2012/Age_estimation_BMVC2017. With the dataset, we train a network that jointly performs ordinal hyperplane classification and posterior distribution learning. Our approach achieves state-of-the-art results on popular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge.", "Conventional video segmentation methods often rely on temporal continuity to propagate masks. Such an assumption suffers from issues like drifting and inability to handle large displacement. To overcome these issues, we formulate an effective mechanism to prevent the target from being lost via adaptive object re-identification. Specifically, our Video Object Segmentation with Re-identification (VS-ReID) model includes a mask propagation module and a ReID module. The former module produces an initial probability map by flow warping while the latter module retrieves missing instances by adaptive matching. With these two modules iteratively applied, our VS-ReID records a global mean (Region Jaccard and Boundary F measure) of 0.699, the best performance in 2017 DAVIS Challenge.", "Deep convolutional neural network has demonstrated its capability of learning a deterministic mapping for the desired imagery effect. However, the large variety of user flavors motivates the possibility of continuous transition among different output effects. Unlike existing methods that require a specific design to achieve one particular transition (eg, style transfer), we propose a simple yet universal approach to attain a smooth control of diverse imagery effects in many low-level vision tasks, including image restoration, image-to-image translation, and style transfer. Specifically, our method, namely Deep Network Interpolation (DNI), applies linear interpolation in the parameter space of two or more correlated networks. A smooth control of imagery effects can be achieved by tweaking the interpolation coefficients. In addition to DNI and its broad applications, we also investigate the mechanism of network interpolation from the perspective of learned filters.", "Most 3D shape completion approaches rely heavily on partial-complete shape pairs and learn in a fully supervised manner. Despite their impressive performances on in-domain data, when generalizing to partial shapes in other forms or real-world partial scans, they often obtain unsatisfactory results due to domain gaps. In contrast to previous fully supervised approaches, in this paper we present ShapeInversion, which introduces Generative Adversarial Network (GAN) inversion to shape completion for the first time. ShapeInversion uses a GAN pre-trained on complete shapes by searching for a latent code that gives a complete shape that best reconstructs the given partial input. In this way, ShapeInversion no longer needs paired training data, and is capable of incorporating the rich prior captured in a well-trained generative model. On the ShapeNet benchmark, the proposed ShapeInversion outperforms the SOTA unsupervised method, and is comparable with supervised methods that are learned using paired data. It also demonstrates remarkable generalization ability, giving robust results for real-world scans and partial inputs of various forms and incompleteness levels. Importantly, ShapeInversion naturally enables a series of additional abilities thanks to the involvement of a pre-trained GAN, such as producing multiple valid complete shapes for an ambiguous partial input, as well as shape manipulation and interpolation.", "Very deep Convolutional Neural Networks (CNNs) have greatly improved the performance on various image restoration tasks. However, this comes at a price of increasing computational burden, hence limiting their practical usages. We observe that some corrupted image regions are inherently easier to restore than others since the distortion and content vary within an image. To leverage this, we propose Path-Restore, a multi-path CNN with a pathfinder that can dynamically select an appropriate route for each image region. We train the pathfinder using reinforcement learning with a difficulty-regulated reward. This reward is related to the performance, complexity and \u201cthe difficulty of restoring a region\u201d. A policy mask is further investigated to jointly process all the image regions. We conduct experiments on denoising and mixed restoration tasks. The results show that our method achieves comparable or superior\u00a0\u2026", "SinGAN shows impressive capability in learning internal patch distribution despite its limited effective receptive field. We are interested in knowing how such a translation-invariant convolutional generator could capture the global structure with just a spatially iid input. In this work, taking SinGAN and StyleGAN2 as examples, we show that such capability, to a large extent, is brought by the implicit positional encoding when using zero padding in the generators. Such positional encoding is indispensable for generating images with high fidelity. The same phenomenon is observed in other generative architectures such as DCGAN and PGGAN. We further show that zero padding leads to an unbalanced spatial bias with a vague relation between locations. To offer a better spatial inductive bias, we investigate alternative positional encodings and analyze their effects. Based on a more flexible positional encoding explicitly, we propose a new multi-scale training strategy and demonstrate its effectiveness in the state-of-the-art unconditional generator StyleGAN2. Besides, the explicit spatial inductive bias substantially improve SinGAN for more versatile image manipulation.", "We consider the problem of learning deep representation when target labels are available. In this paper, we show that there exists intrinsic relationship between target coding and feature representation learning in deep networks. Specifically, we found that distributed binary acode with error correcting capability is more capable of encouraging discriminative features, in comparison tothe 1-of-K coding that is typically used in supervised deep learning. This new finding reveals additional benefit of using error-correcting code for deep model learning, apart from its well-known error correcting property. Extensive experiments are conducted on popular visual benchmark datasets.", "Deep learning approaches have achieved great success in addressing the problem of optical flow estimation. The keys to success lie in the use of cost volume and coarse-to-fine flow inference. However, the matching problem becomes ill-posed when partially occluded or homogeneous regions exist in images. This causes a cost volume to contain outliers and affects the flow decoding from it. Besides, the coarse-to-fine flow inference demands an accurate flow initialization. Ambiguous correspondence yields erroneous flow fields and affects the flow inferences in subsequent levels. In this paper, we introduce LiteFlowNet3, a deep network consisting of two specialized modules, to address the above challenges. (1) We ameliorate the issue of outliers in the cost volume by amending each cost vector through an adaptive modulation prior to the flow decoding. (2) We further improve the flow accuracy by\u00a0\u2026", "Groups are the primary entities that make up a crowd. Understanding group-level dynamics and properties is thus scientifically important and practically useful in a wide range of applications, especially for crowd understanding. In this paper, we show that fundamental group-level properties, such as intra-group stability and inter-group conflict, can be systematically quantified by visual descriptors. This is made possible through learning a novel collective transition prior, which leads to a robust approach for group segregation in public spaces. From the former, we further devise a rich set of group-property visual descriptors. These descriptors are scene-independent and can be effectively applied to public scenes with a variety of crowd densities and distributions. Extensive experiments on hundreds of public scene video clips demonstrate that such property descriptors are complementary to each other, scene\u00a0\u2026", "Recent methods for long-tailed instance segmentation still struggle on rare object classes with few training data. We propose a simple yet effective method, Feature Augmentation and Sampling Adaptation (FASA), that addresses the data scarcity issue by augmenting the feature space especially for rare classes. Both the Feature Augmentation (FA) and feature sampling components are adaptive to the actual training status--FA is informed by the feature mean and variance of observed real samples from past iterations, and we sample the generated virtual features in a loss-adapted manner to avoid over-fitting. FASA does not require any elaborate loss design, and removes the need for inter-class transfer learning that often involves large cost and manually-defined head/tail class groups. We show FASA is a fast, generic method that can be easily plugged into standard or long-tailed segmentation frameworks, with consistent performance gains and little added cost. FASA is also applicable to other tasks like long-tailed classification with state-of-the-art performance.", "Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner.", "Generative adversarial networks (GANs) typically require ample data for training in order to synthesize high-fidelity images. Recent studies have shown that training GANs with limited data remains formidable due to discriminator overfitting, the underlying cause that impedes the generator's convergence. This paper introduces a novel strategy called Adaptive Pseudo Augmentation (APA) to encourage healthy competition between the generator and the discriminator. As an alternative method to existing approaches that rely on standard data augmentations or model regularization, APA alleviates overfitting by employing the generator itself to augment the real data distribution with generated images, which deceives the discriminator adaptively. Extensive experiments demonstrate the effectiveness of APA in improving synthesis quality in the low-data regime. We provide a theoretical analysis to examine the convergence and rationality of our new training strategy. APA is simple and effective. It can be added seamlessly to powerful contemporary GANs, such as StyleGAN2, with negligible computational cost. Code: https://github. com/EndlessSora/DeceiveD.", "In the animation industry, cartoon videos are usually produced at low frame rate since hand drawing of such frames is costly and time-consuming. Therefore, it is desirable to develop computational models that can automatically interpolate the in-between animation frames. However, existing video interpolation methods fail to produce satisfying results on animation data. Compared to natural videos, animation videos possess two unique characteristics that make frame interpolation difficult: 1) cartoons comprise lines and smooth color pieces. The smooth areas lack textures and make it difficult to estimate accurate motions on animation videos. 2) cartoons express stories via exaggeration. Some of the motions are non-linear and extremely large. In this work, we formally define and study the animation video interpolation problem for the first time. To address the aforementioned challenges, we propose an effective framework, AnimeInterp, with two dedicated modules in a coarse-to-fine manner. Specifically, 1) Segment-Guided Matching resolves the\" lack of textures\" challenge by exploiting global matching among color pieces that are piece-wise coherent. 2) Recurrent Flow Refinement resolves the\" non-linear and extremely large motion\" challenge by recurrent predictions using a transformer-like architecture. To facilitate comprehensive training and evaluations, we build a large-scale animation triplet dataset, ATD-12K, which comprises 12,000 triplets with rich annotations. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art interpolation methods for animation videos. Notably, AnimeInterp shows favorable\u00a0\u2026", "Unconditional human image generation is an important task in vision and graphics, enabling various applications in the creative industry. Existing studies in this field mainly focus on \u201cnetwork engineering\u201d such as designing new components and objective functions. This work takes a data-centric perspective and investigates multiple critical aspects in \u201cdata engineering\u201d, which we believe would complement the current practice. To facilitate a comprehensive study, we collect and annotate a large-scale human image dataset with over 230K samples capturing diverse poses and textures. Equipped with this large dataset, we rigorously investigate three essential factors in data engineering for StyleGAN-based human generation, namely data size, data distribution, and data alignment. Extensive experiments reveal several valuable observations w.r.t. these aspects: 1) Large-scale data, more than 40K images, are\u00a0\u2026", "Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (eg, editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual\"\" semantic field\"\" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field. We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants.", "Though much progress has been achieved in single-image 3D human recovery, estimating 3D model for in-the-wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that consumes enormous amount of resources and manpower. To tackle this problem, previous methods adopt a hybrid training strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the efficacy of each annotation not thoroughly investigated. In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations. Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D annotations are not fully available. Through extensive experiments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less competent in guiding 3D human recovery. 2) Dense Correspondence such as DensePose is effective. When there are no paired in-the-wild 3D annotations available, the model exploiting dense correspondence can achieve 92% of the performance compared to a model trained with paired 3D data. We show that incorporating dense correspondence into in-the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research.", "This article addresses the problem of distilling knowledge from a large teacher model to a slim student network for LiDAR semantic segmentation. Directly employing previous distillation approaches yields inferior results due to the intrinsic challenges of point cloud, ie, sparsity, randomness and varying density. To tackle the aforementioned problems, we propose the Point-to-Voxel Knowledge Distillation (PVD), which transfers the hidden knowledge from both point level and voxel level. Specifically, we first leverage both the pointwise and voxelwise output distillation to complement the sparse supervision signals. Then, to better exploit the structural information, we divide the whole point cloud into several supervoxels and design a difficultyaware sampling strategy to more frequently sample supervoxels containing less frequent classes and faraway objects. On these supervoxels, we propose inter-point and intervoxel affinity distillation, where the similarity information between points and voxels can help the student model better capture the structural information of the surrounding environment. We conduct extensive experiments on two popular LiDAR segmentation benchmarks, ie, nuScenes [3] and SemanticKITTI [1]. On both benchmarks, our PVD consistently outperforms previous distillation approaches by a large margin on three representative backbones, ie, Cylinder3D [27, 28], SPVNAS [20] and MinkowskiNet [5]. Notably, on the challenging nuScenes and SemanticKITTI datasets, our method can achieve roughly 75% MACs reduction and 2x speedup on the competitive Cylinder3D model and rank 1st on the SemanticKITTI leaderboard among\u00a0\u2026", "The training of many existing end-to-end steering angle prediction models heavily relies on steering angles as the supervisory signal. Without learning from much richer contexts, these methods are susceptible to the presence of sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes. In this paper, we considerably improve the accuracy and robustness of predictions through heterogeneous auxiliary networks feature mimicking, a new and effective training method that provides us with much richer contextual signals apart from steering direction. Specifically, we train our steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, eg, image segmentation or optical flow estimation. As opposed to multi-task learning, our method does not require expensive annotations of related tasks on the target set. This is made possible by applying contemporary off-the-shelf networks on the target set and mimicking their features in different layers after transformation. The auxiliary networks are discarded after training without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma. ai, outperforming the previous best by a large margin of 12.8% and 52.1% 1, respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.", "The advancement of generative radiance fields has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code is available at https://github. com/XingangPan/ShadeGAN.", "It is counter-intuitive that multi-modality methods based on point cloud and images perform only marginally better or sometimes worse than approaches that solely use point cloud. This paper investigates the reason behind this phenomenon. Due to the fact that multi-modality data augmentation must maintain consistency between point cloud and images, recent methods in this field typically use relatively insufficient data augmentation. This shortage makes their performance under expectation. Therefore, we contribute a pipeline, named transformation flow, to bridge the gap between single and multi-modality data augmentation with transformation reversing and replaying. In addition, considering occlusions, a point in different modalities may be occupied by different objects, making augmentations such as cut and paste non-trivial for multi-modality detection. We further present Multi-mOdality Cut and pAste (MoCa), which simultaneously considers occlusion and physical plausibility to maintain the multi-modality consistency. Without using ensemble of detectors, our multi-modality detector achieves new state-of-the-art performance on nuScenes dataset and competitive performance on KITTI 3D benchmark. Our method also wins the best PKL award in the 3rd nuScenes detection challenge. Code and models will be released at https://github.com/open-mmlab/mmdetection3d.", "Open-vocabulary object detection, which is concerned with the problem of detecting novel objects guided by natural language, has gained increasing attention from the community. Ideally, we would like to extend an open-vocabulary detector such that it can produce bounding box predictions based on user inputs in form of either natural language or exemplar image. This offers great flexibility and user experience for human-computer interaction. To this end, we propose a novel open-vocabulary detector based on DETR\u2014hence the name OV-DETR\u2014which, once trained, can detect any object given its class name or an exemplar image. The biggest challenge of turning DETR into an open-vocabulary detector is that it is impossible to calculate the classification cost matrix of novel classes without access to their labeled images. To overcome this challenge, we formulate the learning objective as a binary matching one\u00a0\u2026", "Generating high-quality and diverse human images is an important yet challenging task in vision and graphics. However, existing generative models often fall short under the high diversity of clothing shapes and textures. Furthermore, the generation process is even desired to be intuitively controllable for layman users. In this work, we present a text-driven controllable framework, Text2Human, for a high-quality and diverse human generation. We synthesize full-body human images starting from a given human pose with two dedicated steps. 1) With some texts describing the shapes of clothes, the given human pose is first translated to a human parsing map. 2) The final human image is then generated by providing the system with more attributes about the textures of clothes. Specifically, to model the diversity of clothing textures, we build a hierarchical texture-aware codebook that stores multi-scale neural\u00a0\u2026", "Generating coherent synopsis for surveillance video stream remains a formidable challenge due to the ambiguity and uncertainty inherent to visual observations. In contrast to existing video synopsis approaches that rely on visual cues alone, we propose a novel multi-source synopsis framework capable of correlating visual data and independent non-visual auxiliary information to better describe and summarise subtle physical events in complex scenes. Specifically, our unsupervised framework is capable of seamlessly uncovering latent correlations among heterogeneous types of data sources, despite the non-trivial heteroscedasticity and dimensionality discrepancy problems. Additionally, the proposed model is robust to partial or missing non-visual information. We demonstrate the effectiveness of our framework on two crowded public surveillance datasets.", "Contrastive self-supervised learning has largely narrowed the gap to supervised pre-training on ImageNet. However, its success highly relies on the object-centric priors of ImageNet, ie, different augmented views of the same image correspond to the same object. Such a heavily curated constraint becomes immediately infeasible when pre-trained on more complex scene images with many objects. To overcome this limitation, we introduce Object-level Representation Learning (ORL), a new self-supervised learning framework towards scene images. Our key insight is to leverage image-level self-supervised pre-training as the prior to discover object-level semantic correspondence, thus realizing object-level representation learning from scene images. Extensive experiments on COCO show that ORL significantly improves the performance of self-supervised learning on scene images, even surpassing supervised ImageNet pre-training on several downstream tasks. Furthermore, ORL improves the downstream performance when more unlabeled scene images are available, demonstrating its great potential of harnessing unlabeled data in the wild. We hope our approach can motivate future research on more general-purpose unsupervised representation learning from scene data.", "The use of color in QR codes brings extra data capacity, but also inflicts tremendous challenges on the decoding process due to chromatic distortion\u2014cross-channel color interference and illumination variation. Particularly, we further discover a new type of chromatic distortion in high-density color QR codes\u2014cross-module color interference\u2014caused by the high density, which also makes the geometric distortion correction more challenging. To address these problems, we propose two approaches, LSVM-CMI and QDA-CMI, which jointly model these different types of chromatic distortion. Extended from SVM and QDA, respectively, both LSVM-CMI and QDA-CMI optimize over a particular objective function and learn a color classifier. Furthermore, a robust geometric transformation method and several pipeline refinements are proposed to boost the decoding performance for mobile applications. We put forth and\u00a0\u2026", "Clustering faces in movies or videos is extremely challenging since characters\u2019 appearance can vary drastically under different scenes. In addition, the various cinematic styles make it difficult to learn a universal face representation for all videos. Unlike previous methods that assume fixed handcrafted features for face clustering, in this work, we formulate a joint face representation adaptation and clustering approach in a deep learning framework. The proposed method allows face representation to gradually adapt from an external source domain\u00a0to a target video domain. The adaptation of deep representation is achieved without any strong supervision but through iteratively discovered weak pairwise identity constraints derived from potentially noisy face clustering result. Experiments on three benchmark video datasets demonstrate that our approach generates character clusters with high purity compared\u00a0\u2026", "Prompt tuning, a parameter- and data-efficient transfer learning paradigm that tunes only a small number of parameters in a model's input space, has become a trend in the vision community since the emergence of large vision-language models like CLIP. We present a systematic study on two representative prompt tuning methods, namely text prompt tuning and visual prompt tuning. A major finding is that none of the unimodal prompt tuning methods performs consistently well: text prompt tuning fails on data with high intra-class visual variances while visual prompt tuning cannot handle low inter-class variances. To combine the best from both worlds, we propose a simple approach called Unified Prompt Tuning (UPT), which essentially learns a tiny neural network to jointly optimize prompts across different modalities. Extensive experiments on over 11 vision datasets show that UPT achieves a better trade-off than the unimodal counterparts on few-shot learning benchmarks, as well as on domain generalization benchmarks. Code and models will be released to facilitate future research.", "Recent studies on StyleGAN show high performance on artistic portrait generation by transfer learning with limited data. In this paper, we explore more challenging exemplar-based high-resolution portrait style transfer by introducing a novel DualStyleGAN with flexible control of dual styles of the original face domain and the extended artistic portrait domain. Different from StyleGAN, DualStyleGAN provides a natural way of style transfer by characterizing the content and style of a portrait with an intrinsic style path and a new extrinsic style path, respectively. The delicately designed extrinsic style path enables our model to modulate both the color and complex structural styles hierarchically to precisely pastiche the style example. Furthermore, a novel progressive fine-tuning scheme is introduced to smoothly transform the generative space of the model to the target domain, even with the above modifications on the network architecture. Experiments demonstrate the superiority of DualStyleGAN over state-of-the-art methods in high-quality portrait style transfer and flexible style control.", "To enable realistic shape (e.g. pose and expression) transfer, existing face reenactment methods rely on a set of target faces for learning subject-specific traits. However, in real-world scenario end-users often only have one target face at hand, rendering existing methods inapplicable. In this work, we bridge this gap by proposing a novel one-shot face reenactment learning framework. Our key insight is that the one-shot learner should be able to disentangle and compose appearance and shape information for effective modeling. Specifically, the target face appearance and the source face shape are first projected into latent spaces with their corresponding encoders. Then these two latent spaces are associated by learning a shared decoder that aggregates multi-level features to produce the final reenactment results. To further improve the synthesizing quality on mustache and hair regions, we additionally propose FusionNet which combines the strengths of our learned decoder and the traditional warping method. Extensive experiments show that our one-shot face reenactment system achieves superior transfer fidelity as well as identity preserving capability than alternatives. More remarkably, our approach trained with only one target image per subject achieves competitive results to those using a set of target images, demonstrating the practical merit of this work. Code, models and an additional set of reenacted faces have been publicly released at the project page.", "Activity modeling and unusual event detection in a network of cameras is challenging, particularly when the camera views are not overlapped. We show that it is possible to detect unusual events in multiple disjoint cameras as context-incoherent patterns through incremental learning of time delayed dependencies between distributed local activities observed within and across camera views. Specifically, we model multicamera activities using a Time Delayed Probabilistic Graphical Model (TD-PGM) with different nodes representing activities in different decomposed regions from different views and the directed links between nodes encoding their time delayed dependencies. To deal with visual context changes, we formulate a novel incremental learning method for modeling time delayed dependencies that change over time. We validate the effectiveness of the proposed approach using a synthetic data set and\u00a0\u2026", "This paper presents Video K-Net, a simple, strong, and unified framework for fully end-to-end video panoptic segmentation. The method is built upon K-Net, a method that unifies image segmentation via a group of learnable kernels. We observe that these learnable kernels from K-Net, which encode object appearances and contexts, can naturally associate identical instances across video frames. Motivated by this observation, Video K-Net learns to simultaneously segment and track\" things\" and\" stuff\" in a video with simple kernel-based appearance modeling and cross-temporal kernel interaction. Despite the simplicity, it achieves state-of-the-art video panoptic segmentation results on Citscapes-VPS and KITTI-STEP without bells and whistles. In particular on KITTI-STEP, the simple method can boost almost 12% relative improvements over previous methods. We also validate its generalization on video semantic segmentation, where we boost various baselines by 2% on the VSPW dataset. Moreover, we extend K-Net into clip-level video framework for video instance segmentation where we obtain 40.5% for ResNet50 backbone and 51.5% mAP for Swin-base on YouTube-2019 validation set. We hope this simple yet effective method can serve as a new flexible baseline in video segmentation. Both code and models are released at\\href https://github. com/lxtGH/Video-K-Net.", "The diversity and complexity of degradations in real-world video super-resolution (VSR) pose non-trivial challenges in inference and training. First, while long-term propagation leads to improved performance in cases of mild degradations, severe in-the-wild degradations could be exaggerated through propagation, impairing output quality. To balance the tradeoff between detail synthesis and artifact suppression, we found an image pre-cleaning stage indispensable to reduce noises and artifacts prior to propagation. Equipped with a carefully designed cleaning module, our RealBasicVSR outperforms existing methods in both quality and efficiency. Second, real-world VSR models are often trained with diverse degradations to improve generalizability, requiring increased batch size to produce a stable gradient. Inevitably, the increased computational burden results in various problems, including 1) speed-performance tradeoff and 2) batch-length tradeoff. To alleviate the first tradeoff, we propose a stochastic degradation scheme that reduces up to 40% of training time without sacrificing performance. We then analyze different training settings and suggest that employing longer sequences rather than larger batches during training allows more effective uses of temporal information, leading to more stable performance during inference. To facilitate fair comparisons, we propose the new VideoLQ dataset, which contains a large variety of real-world low-quality video sequences containing rich textures and patterns. Our dataset can serve as a common ground for benchmarking. Code, models, and the dataset will be made publicly available.", "To reduce cognitive overload in CCTV monitoring, it is critical to have an automated way to focus the attention of operators on interesting events taking place in crowded public scenes. We present a global motion saliency detection method based on spectral analysis, which aims to discover and localise interesting regions, of which the flows are salient in relation to the dominant crowd flows. The method is fast and does not rely on prior knowledge specific to a scene and any training videos. We demonstrate its potential on public scene videos, with applications in salient action detection, counter flow detection, and unstable crowd flow detection.", "It is common for CCTV operators to overlook interesting events taking place within the crowd due to large number of people in the crowded scene (i.e. marathon, rally). Thus, there is a dire need to automate the detection of salient crowd regions acquiring immediate attention for a more effective and proactive surveillance. This paper proposes a novel framework to identify and localize salient regions in a crowd scene, by transforming low-level features extracted from crowd motion field into a global similarity structure. The global similarity structure representation allows the discovery of the intrinsic manifold of the motion dynamics, which could not be captured by the low-level representation. Ranking is then performed on the global similarity structure to identify a set of extrem a. The proposed approach is unsupervised so learning stage is eliminated. Experimental results on public datasets demonstrates the\u00a0\u2026", "We present a novel approach for detecting global behaviour anomalies in multiple disjoint cameras by learning time delayed dependencies between activities cross camera views. Specifically, we propose to model multi-camera activities using a Time Delayed Probabilistic Graphical Model (TD-PGM) with different nodes representing activities in different semantically decomposed regions from different camera views, and the directed links between nodes encoding causal relationships between the activities. A novel two-stage structure learning algorithm is formulated to learn globally optimised time-delayed dependencies. A new cumulative abnormality score is also introduced to replace the conventional log-likelihood score for gaining significantly more robust and reliable real-time anomaly detection. The effectiveness of the proposed approach is validated using a camera network installed at a busy underground\u00a0\u2026", "Deep convolutional networks for semantic image segmentation typically require large-scale labeled data, eg, ImageNet and MS COCO, for network pre-training. To reduce annotation efforts, self-supervised semantic segmentation is recently proposed to pre-train a network without any human-provided labels. The key of this new form of learning is to design a proxy task (eg, image colorization), from which a discriminative loss can be formulated on unlabeled data. Many proxy tasks, however, lack the critical supervision signals that could induce discriminative representation for the target image segmentation task. Thus self-supervision\u2019s performance is still far from that of supervised pre-training. In this study, we overcome this limitation by incorporating a\" mix-and-match\"(M&M) tuning stage in the self-supervision pipeline. The proposed approach is readily pluggable to many self-supervision methods and does not use more annotated samples than the original process. Yet, it is capable of boosting the performance of target image segmentation task to surpass fully-supervised pre-trained counterpart. The improvement is made possible by better harnessing the limited pixel-wise annotations in the target dataset. Specifically, we first introduce the\" mix\" stage, which sparsely samples and mixes patches from the target set to reflect rich and diverse local patch statistics of target images. A \u2018match\u2019stage then forms a class-wise connected graph, which can be used to derive a strong triplet-based discriminative loss for finetuning the network. Our paradigm follows the standard practice in existing self-supervised studies and no extra data or label is required\u00a0\u2026", "We present a new active learning approach to incorporate human feedback for on-line unusual event detection. In contrast to most existing unsupervised methods that perform passive mining for unusual events, our approach automatically requests supervision for critical points to resolve ambiguities of interest, leading to more robust and accurate detection on subtle unusual events. The active learning strategy is formulated as a stream-based solution, i.e. it makes decision on-the-fly on whether to query for labels. It adaptively combines multiple active learning criteria to achieve (i) quick discovery of unknown event classes and (ii) refinement of classification boundary. Experimental results on busy public space videos show that with minimal human supervision, our approach outperforms existing supervised and unsupervised learning strategies in identifying unusual events. In addition, better performance is\u00a0\u2026", "We present a lightweight video motion retargeting approach TransMoMo that is capable of transferring motion of a person in a source video realistically to another video of a target person. Without using any paired data for supervision, the proposed method can be trained in an unsupervised manner by exploiting invariance properties of three orthogonal factors of variation including motion, structure, and view-angle. Specifically, with loss functions carefully derived based on invariance, we train an auto-encoder to disentangle the latent representations of such factors given the source and target video clips. This allows us to selectively transfer motion extracted from the source video seamlessly to the target video in spite of structural and view-angle disparities between the source and the target. The relaxed assumption of paired data allows our method to be trained on a vast amount of videos needless of manual annotation of source-target pairing, leading to improved robustness against large structural variations and extreme motion in videos. We demonstrate the effectiveness of our method over the state-of-the-art methods. Code, model and data are publicly available on our project page (https://yzhq97. github. io/transmomo).", "We present a new approach for activity modelling and anomaly detection based on non-parametric Gaussian Process (GP) models. Specifically, GP regression models are formulated to learn non-linear relationships between multi-object activity patterns observed from semantically decomposed regions in complex scenes. Predictive distributions are inferred from the regression models to compare with the actual observations for real-time anomaly detection. The use of a flexible, non-parametric model alleviates the difficult problem of selecting appropriate model complexity encountered in parametric models such as Dynamic Bayesian Networks (DBNs). Crucially, our GP models need fewer parameters; they are thus less likely to overfit given sparse data. In addition, our approach is robust to the inevitable noise in activity representation as noise is modelled explicitly in the GP models. Experimental results on a public traffic scene show that our models outperform DBNs in terms of anomaly sensitivity, noise robustness, and flexibility in modelling complex activity.", "Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (eg scale and rotation) and the resolution gap (eg HR and LR). To tackle these challenges, we propose C^ 2-Matching in this work, which produces explicit robust matching crossing transformation and resolution. 1) For the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) For the resolution gap, we adopt a teacher-student correlation distillation, which distills knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation module to address the potential misalignment issue. In addition, to faithfully evaluate the performance of Ref-SR under a realistic setting, we contribute the Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario. Extensive experiments demonstrate that our proposed C^ 2-Matching significantly outperforms current state-of-the-art methods by over 1dB on the standard CUFED5 benchmark. Notably, it also shows great generalizability on WR-SR dataset as well as robustness across large scale and rotation\u00a0\u2026", "Crowd video analysis is one of the hallmark tasks of crowded scene understanding. While we observe a tremendous progress in image-based tasks with the rise of convolutional neural networks (CNNs), performance on video analysis has not (yet) attained the same level of success. In this paper, we introduce intuitive but effective temporal-aware crowd motion channels by uniformly slicing the video volume from different dimensions. Multiple CNN structures with different data-fusion strategies and weight-sharing schemes are proposed to learn the connectivity both spatially and temporally from these motion channels. To well demonstrate our deep model, we construct a new large-scale Who do What at someWhere crowd data set with 10 000 videos from 8257 crowded scenes, and build an attribute set with 94 attributes. Extensive experiments on crowd video attribute prediction demonstrate the effectiveness of our\u00a0\u2026", "The problem of face alignment has been intensively studied in the past years. A large number of novel methods have been proposed and reported very good performance on benchmark dataset such as 300W. However, the differences in the experimental setting and evaluation metric, missing details in the description of the methods make it hard to reproduce the results reported and evaluate the relative merits. For instance, most recent face alignment methods are built on top of face detection but from different face detectors. In this paper, we carry out a rigorous evaluation of these methods by making the following contributions: 1) we proposes a new evaluation metric for face alignment on a set of images, i.e., area under error distribution curve within a threshold, AUC, given the fact that the traditional evaluation measure (mean error) is very sensitive to big alignment error. 2) we extend the 300W database with more practical face detections to make fair comparison possible. 3) we carry out face alignment sensitivity analysis w.r.t. face detection, on both synthetic and real data, using both off-the-shelf and re-retrained models. 4) we study factors that are particularly important to achieve good performance and provide suggestions for practical applications. Most of the conclusions drawn from our comparative analysis cannot be inferred from the original publications.", "Feature pyramid networks have been widely adopted in the object detection literature to improve feature representations for better handling of variations in scale. In this paper, we present Feature Pyramid Grids (FPG), a deep multi-pathway feature pyramid, that represents the feature scale-space as a regular grid of parallel bottom-up pathways which are fused by multi-directional lateral connections. FPG can improve single-pathway feature pyramid networks by significantly increasing its performance at similar computation cost, highlighting importance of deep pyramid representations. In addition to its general and uniform structure, over complicated structures that have been found with neural architecture search, it also compares favorably against such approaches without relying on search. We hope that FPG with its uniform and effective nature can serve as a strong component for future work in object recognition.", "Intelligent agent naturally learns from motion. Various self-supervised algorithms have leveraged the motion cues to learn effective visual representations. The hurdle here is that motion is both ambiguous and complex, rendering previous works either suffer from degraded learning efficacy, or resort to strong assumptions on object motions. In this work, we design a new learning-from-motion paradigm to bridge these gaps. Instead of explicitly modeling the motion probabilities, we design the pretext task as a conditional motion propagation problem. Given an input image and several sparse flow guidance on it, our framework seeks to recover the full-image motion. Compared to other alternatives, our framework has several appealing properties:(1) Using sparse flow guidance during training resolves the inherent motion ambiguity, and thus easing feature learning.(2) Solving the pretext task of conditional motion propagation encourages the emergence of kinematically-sound representations that poss greater expressive power. Extensive experiments demonstrate that our framework learns structural and coherent features; and achieves state-of-the-art self-supervision performance on several downstream tasks including semantic segmentation, instance segmentation and human parsing. Furthermore, our framework is successfully extended to several useful applications such as semi-automatic pixel-level annotation.", "Recent development of Under-Display Camera (UDC) systems provides a true bezel-less and notch-free viewing experience on smartphones (and TV, laptops, tablets), while allowing images to be captured from the selfie camera embedded underneath. In a typical UDC system, the microstructure of the semi-transparent organic light-emitting diode (OLED) pixel array attenuates and diffracts the incident light on the camera, resulting in significant image quality degradation. Oftentimes, noise, flare, haze, and blur can be observed in UDC images. In this work, we aim to analyze and tackle the aforementioned degradation problems. We define a physics-based image formation model to better understand the degradation. In addition, we utilize one of the world's first commodity UDC smartphone prototypes to measure the real-world Point Spread Function (PSF) of the UDC system, and provide a model-based data synthesis pipeline to generate realistically degraded images. We specially design a new domain knowledge-enabled Dynamic Skip Connection Network (DISCNet) to restore the UDC images. We demonstrate the effectiveness of our method through extensive experiments on both synthetic and real UDC data. Our physics-based image formation model and proposed DISCNet can provide foundations for further exploration in UDC image restoration, and even for general diffraction artifact removal in a broader sense.", "While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. Compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate realistic images at 1024*1024 resolution when trained from scratch.", "Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to 1) improve the mapping from degraded inputs to desired outputs, or 2) complement high-quality details lost in the inputs. In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting\\textit {blind face restoration} as a\\textit {code prediction} task, while providing rich visual atoms for generating high-quality faces. Under this paradigm, we propose a Transformer-based prediction network, named\\textit {CodeFormer}, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded. To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling,\\textit {CodeFormer} outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation. Extensive experimental results on synthetic and real-world datasets verify the effectiveness of our method.", "4D human sensing and modeling are fundamental tasks in vision and graphics with numerous applications. With the advances of new sensors and algorithms, there is an increasing demand for more versatile datasets. In this work, we contribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human subjects, 400k sequences and 60M frames. HuMMan has several appealing properties: 1) multi-modal data and annotations including color images, point clouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile device is included in the sensor suite; 3) a set of 500 actions, designed to cover fundamental movements; 4) multiple tasks such as action recognition, pose estimation, parametric human recovery, and textured mesh reconstruction are supported and evaluated. Extensive experiments on HuMMan voice the need for further study on challenges such as fine-grained action\u00a0\u2026", "Contrastive learning has recently shown immense potential in unsupervised visual representation learning. Existing studies in this track mainly focus on intra-image invariance learning. The learning typically uses rich intra-image transformations to construct positive pairs and then maximizes agreement using a contrastive loss. The merits of inter-image invariance, conversely, remain much less explored. One major obstacle to exploit inter-image invariance is that it is unclear how to reliably construct inter-image positive pairs, and further derive effective supervision from them since no pair annotations are available. In this work, we present a comprehensive empirical study to better understand the role of inter-image invariance learning from three main constituting components: pseudo-label maintenance, sampling strategy, and decision boundary design. To facilitate the study, we introduce a unified and generic\u00a0\u2026", "Patch-based methods and deep networks have been employed to tackle image inpainting problem, with their own strengths and weaknesses. Patch-based methods are capable of restoring a missing region with high-quality texture through searching nearest neighbor patches from the unmasked regions. However, these methods bring problematic contents when recovering large missing regions. Deep networks, on the other hand, show promising results in completing large regions. Nonetheless, the results often lack faithful and sharp details that resemble the surrounding area. By bringing together the best of both paradigms, we propose a new deep inpainting framework where texture generation is guided by a texture memory of patch samples extracted from unmasked regions. The framework has a novel design that allows texture memory retrieval to be trained end-to-end with the deep inpainting network. In\u00a0\u2026", "ObjectiveOral pills, including tablets and capsules, are one of the most popular pharmaceutical dosage forms available. Compared to other dosage forms, such as liquid and injections, oral pills are very stable and are easy to be administered. However, it is not uncommon for pills to be misidentified, be it within the healthcare institutes or after the pills were dispensed to the patients. Our objective is to develop groundwork for automatic pill identification and verification using Deep Convolutional Network (DCN) that surpasses the existing methods.Materials and methodsA DCN model was developed using pill images captured with mobile phones under unconstraint environments. The performance of the DCN model was compared to two baseline methods of hand-crafted features.ResultsThe DCN model outperforms the baseline methods. The mean accuracy rate of DCN at Top-1 return was 95.35%, whereas the\u00a0\u2026", "In spite of the popularity of password-based authentication method, there are many inherent flaws with this approach. To minimize the risk of intrusion, typing biometrics can be used to complement and strengthen this popular authentication method. In this paper we investigate how keystroke pressure is used to strengthen the security of traditional password-based authentication system, and compare its performance with that of the conventional timingbased keystroke technique. The paper also investigates the use of combined keystroke pressure and latency for the verification process. The performances of several classification methods in user authentication, namely Multilayer Perceptron (MLP), Logistic Regression (LR), Fuzzy ARTMAP (FAM) neural networks, and a statistical approach, are studied and compared. Although keystroke latency gives better results than keystroke pressure, a combination of both techniques yields the best performance, with the False Acceptance Rate (FAR) of 0.87% and the False Rejection Rate (FRR) of 4.4%.", "Night photography typically suffers from both low light and blurring issues due to the dim environment and the common use of long exposure. While existing light enhancement and deblurring methods could deal with each problem individually, a cascade of such methods cannot work harmoniously to cope well with joint degradation of visibility and sharpness. Training an end-to-end network is also infeasible as no paired data is available to characterize the coexistence of low light and blurs. We address the problem by introducing a novel data synthesis pipeline that models realistic low-light blurring degradations, especially for blurs in saturated regions, e.g., light streaks, that often appear in the night images. With the pipeline, we present the first large-scale dataset for joint low-light enhancement and deblurring. The dataset, LOL-Blur, contains 12,000 low-blur/normal-sharp pairs with diverse darkness and blurs in\u00a0\u2026", "Measuring the perception of visual content is a long-standing problem in computer vision. Many mathematical models have been developed to evaluate the look or quality of an image. Despite the effectiveness of such tools in quantifying degradations such as noise and blurriness levels, such quantification is loosely coupled with human language. When it comes to more abstract perception about the feel of visual content, existing methods can only rely on supervised models that are explicitly trained with labeled data collected via laborious user study. In this paper, we go beyond the conventional paradigms by exploring the rich visual language prior encapsulated in Contrastive Language-Image Pre-training (CLIP) models for assessing both the quality perception (look) and abstract perception (feel) of images without explicit task-specific training. In particular, we discuss effective prompt designs and show an effective prompt pairing strategy to harness the prior. We also provide extensive experiments on controlled datasets and Image Quality Assessment (IQA) benchmarks. Our results show that CLIP captures meaningful priors that generalize well to different perceptual assessments.", "We address the problem of instance-level facial attribute transfer without paired training data, eg, faithfully transferring the exact mustache from a source face to a target face. This is a more challenging task than the conventional semantic-level attribute transfer, which only preserves the generic attribute style instead of instance-level traits. We propose the use of geometry-aware flow, which serves as a wellsuited representation for modeling the transformation between instance-level facial attributes. Specifically, we leverage the facial landmarks as the geometric guidance to learn the differentiable flows automatically, despite of the large pose gap existed. Geometry-aware flow is able to warp the source face attribute into the target face context and generate a warp-and-blend result. To compensate for the potential appearance gap between source and target faces, we propose a hallucination sub-network that produces an appearance residual to further refine the warp-and-blend result. Finally, a cycle-consistency framework consisting of both attribute transfer module and attribute removal module is designed, so that abundant unpaired face images can be used as training data. Extensive evaluations validate the capability of our approach in transferring instance-level facial attributes faithfully across large pose and appearance gaps. Thanks to the flow representation, our approach can readily be applied to generate realistic details on high-resolution images 1.", "Data imbalance is common in many vision tasks where one or more classes are rare. Without addressing this issue, conventional methods tend to be biased toward the majority class with poor predictive accuracy for the minority class. These methods further deteriorate on small, imbalanced data that have a large degree of class overlap. In this paper, we propose a novel discriminative sparse neighbor approximation (DSNA) method to ameliorate the effect of class-imbalance during prediction. Specifically, given a test sample, we first traverse it through a cost-sensitive decision forest to collect a good subset of training examples in its local neighborhood. Then, we generate from this subset several class-discriminating but overlapping clusters and model each as an affine subspace. From these subspaces, the proposed DSNA iteratively seeks an optimal approximation of the test sample and outputs an unbiased\u00a0\u2026", "Many visual surveillance tasks, e.g. video summarisation, is conventionally accomplished through analysing imagery-based features. Relying solely on visual cues for public surveillance video understanding is unreliable, since visual observations obtained from public space CCTV video data are often not sufficiently trustworthy and events of interest can be subtle. We believe that non-visual data sources such as weather reports and traffic sensory signals can be exploited to complement visual data for video content analysis and summarisation. In this paper, we present a novel unsupervised framework to learn jointly from both visual and independently-drawn non-visual data sources for discovering meaningful latent structure of surveillance video data. In particular, we investigate ways to cope with discrepant dimension and representation whilst associating these heterogeneous data sources, and derive\u00a0\u2026", "While clustering is usually an unsupervised operation, there are circumstances where we have access to prior belief that pairs of samples should (or should not) be assigned with the same cluster. Constrained clustering aims to exploit this prior belief as constraint (or weak supervision) to influence the cluster formation so as to obtain a data structure more closely resembling human perception. Two important issues remain open: 1) how to exploit sparse constraints effectively and 2) how to handle ill-conditioned/noisy constraints generated by imperfect oracles. In this paper, we present a novel pairwise similarity measure framework to address the above issues. Specifically, in contrast to existing constrained clustering approaches that blindly rely on all features for constraint propagation, our approach searches for neighborhoods driven by discriminative feature selection for more effective constraint diffusion. Crucially\u00a0\u2026", "Dataset bias is a well known problem in object recognition domain. This issue, nonetheless, is rarely explored in face alignment research. In this study, we show that dataset plays an integral part of face alignment performance. Specifically, owing to face alignment dataset bias, training on one database and testing on another or unseen domain would lead to poor performance. Creating an unbiased dataset through combining various existing databases, however, is non-trivial as one has to exhaustively re-label the landmarks for standardisation. In this work, we propose a simple and yet effective method to bridge the disparate annotation spaces between databases, making datasets fusion possible. We show extensive results on combining various popular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset and unseen data alignment.", "In this paper, the development of a pressure-based typing biometrics user authentication system is presented. Studies have shown that identity of a user can be recognized based on his/her keystroke timing pattern [1-2]. However, instead of using conventional typing timing characteristics, the work presented in this paper investigates the applicability of using a different approach to ascertain the identity of a user\u2013the user\u2019s typing force. In the following sections, the hardware architecture, including all the constituent components and the software development will be elaborated.", "Recent advances like StyleGAN have promoted the growth of controllable facial editing. To address its core challenge of attribute decoupling in a single latent space, attempts have been made to adopt dual-space GAN for better disentanglement of style and content representations. Nonetheless, these methods are still incompetent to obtain plausible editing results with high controllability, especially for complicated attributes. In this study, we highlight the importance of interaction in a dual-space GAN for more controllable editing. We propose TransEditor, a novel Transformer-based framework to enhance such interaction. Besides, we develop a new dual-space editing and inversion strategy to provide additional editing flexibility. Extensive experiments demonstrate the superiority of the proposed framework in image quality and editing capability, suggesting the effectiveness of TransEditor for highly controllable facial editing. Code and models are publicly available at https://github. com/BillyXYB/TransEditor.", "Feature reassembly, i.e. feature downsampling and upsampling, is a key operation in a number of modern convolutional network architectures, e.g., residual networks and feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose unified Content-Aware ReAssembly of FEatures (CARAFE++), a universal, lightweight, and highly effective operator to fulfill this goal. CARAFE++ has several appealing properties: (1) Unlike conventional methods such as pooling and interpolation that only exploit sub-pixel neighborhood, CARAFE++ aggregates contextual information within a large receptive field. (2) Instead of using a fixed kernel for all samples (e.g. convolution and deconvolution), CARAFE++ generates adaptive kernels on-the-fly to enable instance-specific content-aware handling. (3) CARAFE++ introduces little computational\u00a0\u2026", "Large-scale datasets have played indispensable roles in the recent success of face generation/editing and significantly facilitated the advances of emerging research fields. However, the academic community still lacks a video dataset with diverse facial attribute annotations, which is crucial for the research on face-related videos. In this work, we propose a large-scale, high-quality, and diverse video dataset with rich facial attribute annotations, named the High-Quality Celebrity Video Dataset (CelebV-HQ). CelebV-HQ contains 35,\u00a0666 video clips with the resolution of  at least, involving 15,\u00a0653 identities. All clips are labeled manually with 83 facial attributes, covering appearance, action, and emotion. We conduct a comprehensive analysis in terms of age, ethnicity, brightness stability, motion smoothness, head pose diversity, and data quality to demonstrate the diversity and temporal coherence of CelebV-HQ\u00a0\u2026", "The ability to ask questions is a powerful tool to gather information in order to learn about the world and resolve ambiguities. In this paper, we explore a novel problem of generating discriminative questions to help disambiguate visual instances. Our work can be seen as a complement and new extension to the rich research studies on image captioning and question answering. We introduce the first large-scale dataset with over 10,000 carefully annotated images-question tuples to facilitate benchmarking. In particular, each tuple consists of a pair of images and 4.6 discriminative questions (as positive samples) and 5.9 non-discriminative questions (as negative samples) on average. In addition, we present an effective method for visual discriminative question generation. The method can be trained in a weakly supervised manner without discriminative images-question tuples but just existing visual question answering datasets. Promising results are shown against representative baselines through quantitative evaluations and user studies.", "Unsupervised image-to-image translation aims to learn the translation between two visual domains without paired data. Despite the recent progress in image translation models, it remains challenging to build mappings between complex domains with drastic visual discrepancies. In this work, we present a novel framework, Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), to improve the overall quality and applicability of the translation algorithm. Our key insight is to leverage the generative prior from pre-trained class-conditional GANs (eg, BigGAN) to learn rich content correspondences across various domains. We propose a novel coarse-to-fine scheme: we first distill the generative prior to capture a robust coarse-level content representation that can link objects at an abstract semantic level, based on which fine-level content features are adaptively learned for more accurate multi-level content correspondences. Extensive experiments demonstrate the superiority of our versatile framework over state-of-the-art methods in robust, high-quality and diversified translations, even for challenging and distant domains.", "We propose a Transformer-based framework for 3D human texture estimation from a single image. The proposed Transformer is able to effectively exploit the global information of the input image, overcoming the limitations of existing methods that are solely based on convolutional neural networks. In addition, we also propose a mask-fusion strategy to combine the advantages of the RGB-based and texture-flow-based models. We further introduce a part-style loss to help reconstruct high-fidelity colors without introducing unpleasant artifacts. Extensive experiments demonstrate the effectiveness of the proposed method against state-of-the-art 3D human texture estimation approaches both quantitatively and qualitatively.", "3D interacting hand reconstruction is essential to facilitate human-machine interaction and human behaviors understanding. Previous works in this field either rely on auxiliary inputs such as depth images or they can only handle a single hand if monocular single RGB images are used. Single-hand methods tend to generate collided hand meshes, when applied to closely interacting hands, since they cannot model the interactions between two hands explicitly. In this paper, we make the first attempt to reconstruct 3D interacting hands from monocular single RGB images. Our method can generate 3D hand meshes with both precise 3D poses and minimal collisions. This is made possible via a two-stage framework. Specifically, the first stage adopts a convolutional neural network to generate coarse predictions that tolerate collisions but encourage pose-accurate hand meshes. The second stage progressively\u00a0\u2026", "Face grouping remains a challenging problem despite the remarkable capability of deep learning approaches in learning face representation. In particular, grouping results can still be egregious given profile faces and a large number of uninteresting faces and noisy detections. Often, a user needs to correct the erroneous grouping manually. In this study, we formulate a novel face grouping framework that learns clustering strategy from ground-truth simulated behavior. This is achieved through imitation learning (aka apprenticeship learning or learning by watching) via inverse reinforcement learning (IRL). In contrast to existing clustering approaches that group instances by similarity, our framework makes sequential decision to dynamically decide when to merge two face instances/groups driven by short-and long-term rewards. Extensive experiments on three benchmark datasets show that our framework outperforms unsupervised and supervised baselines.", "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmenation-With-Transformer. We\u00a0\u2026", "Image- and video-based 3D human recovery (i.e., pose and shape estimation) have achieved substantial progress. However, due to the prohibitive cost of motion capture, existing datasets are often limited in scale and diversity. In this work, we obtain massive human sequences by playing the video game with automatically annotated 3D ground truths. Specifically, we contribute GTA-Human, a large-scale 3D human dataset generated with the GTA-V game engine, featuring a highly diverse set of subjects, actions, and scenarios. More importantly, we study the use of game-playing data and obtain five major insights. First, game-playing data is surprisingly effective. A simple frame-based baseline trained on GTA-Human outperforms more sophisticated methods by a large margin. For video-based methods, GTA-Human is even on par with the in-domain training set. Second, we discover that synthetic data provides critical complements to the real data that is typically collected indoor. Our investigation into domain gap provides explanations for our data mixture strategies that are simple yet useful. Third, the scale of the dataset matters. The performance boost is closely related to the additional data available. A systematic study reveals the model sensitivity to data density from multiple key aspects. Fourth, the effectiveness of GTA-Human is also attributed to the rich collection of strong supervision labels (SMPL parameters), which are otherwise expensive to acquire in real datasets. Fifth, the benefits of synthetic data extend to larger models such as deeper convolutional neural networks (CNNs) and Transformers, for which a significant impact is also\u00a0\u2026", "Color serves as an important cue for many computer vision tasks. Nevertheless, obtaining accurate color description from images is non-trivial due to varying illumination conditions, view angles, and surface reflectance. This is especially true for the challenging problem of pedestrian description in public spaces. We made two contributions in this study: (1) We contribute a large-scale pedestrian color naming dataset with 14,213 hand-labeled images. (2) We address the problem of assigning consistent color name to regions of single object\u2019s surface. We propose an end-to-end, pixel-to-pixel convolutional neural network (CNN) for pedestrian color naming. We demonstrate that our Pedestrian Color Naming CNN (PCN-CNN) is superior over existing approaches in providing consistent color names on real-world pedestrian images. In addition, we show the effectiveness of color descriptor extracted from PCN\u00a0\u2026", "Person re-identification methods seek robust person matching through combining feature types. Often, these features are assigned implicitly with a single vector of global weights, which are assumed to be universally and equally good for matching all individuals, independent of their different appearances. In this study, we present a comprehensive comparison and evaluation of up-to-date imagery features for person re-identification. We show that certain features play more important roles than others for different people. To that end, we introduce an unsupervised approach to learning a bottom-up measurement of feature importance. This is achieved through first automatically grouping individuals with similar appearance characteristics into different prototypes/clusters. Different features extracted from different individuals are then automatically weighted adaptively driven by their inherent appearance\u00a0\u2026", "Ideally, visual learning algorithms should be generalizable, for dealing with any unseen domain shift when deployed in a new target environment; and data-efficient, for reducing development costs by using as little labels as possible. To this end, we study semi-supervised domain generalization (SSDG), which aims to learn a domain-generalizable model using multi-source, partially-labeled training data. We design two benchmarks that cover state-of-the-art methods developed in two related fields, i.e., domain generalization (DG) and semi-supervised learning (SSL). We find that the DG methods, which by design are unable to handle unlabeled data, perform poorly with limited labels in SSDG; the SSL methods, especially FixMatch, obtain much better results but are still far away from the basic vanilla model trained using full labels. We propose StyleMatch, a simple approach that extends FixMatch with a couple of\u00a0\u2026", "Despite the remarkable progress in recent years, detecting objects in a new context remains a challenging task. Detectors learned from a public dataset can only work with a fixed list of categories, while training from scratch usually requires a large amount of training data with detailed annotations. This work aims to explore a novel approach--learning object detectors from documentary films in a weakly supervised manner. This is inspired by the observation that documentaries often provide dedicated exposition of certain object categories, where visual presentations are aligned with subtitles. We believe that object detectors can be learned from such a rich source of information. Towards this goal, we develop a joint probabilistic framework, where individual pieces of information, including video frames and subtitles, are brought together via both visual and linguistic links. On top of this formulation, we further derive a weakly supervised learning algorithm, where object model learning and training set mining are unified in an optimization procedure. Experimental results on a real world dataset demonstrate that this is an effective approach to learning new object detectors.", "Existing convolutional neural networks widely adopt spatial down-/up-sampling for multi-scale modeling. However, spatial up-sampling operators (\\emph{e.g.}, interpolation, transposed convolution, and un-pooling) heavily depend on local pixel attention, incapably exploring the global dependency. In contrast, the Fourier domain obeys the nature of global modeling according to the spectral convolution theorem. Unlike the spatial domain that performs up-sampling with the property of local similarity, up-sampling in the Fourier domain is more challenging as it does not follow such a local property. In this study, we propose a theoretically sound Deep Fourier Up-Sampling (FourierUp) to solve these issues. We revisit the relationships between spatial and Fourier domains and reveal the transform rules on the features of different resolutions in the Fourier domain, which provide key insights for FourierUp's designs. FourierUp as a generic operator consists of three key components: 2D discrete Fourier transform, Fourier dimension increase rules, and 2D inverse Fourier transform, which can be directly integrated with existing networks. Extensive experiments across multiple computer vision tasks, including object detection, image segmentation, image de-raining, image dehazing, and guided image super-resolution, demonstrate the consistent performance gains obtained by introducing our FourierUp.", "We present Masked Frequency Modeling (MFM), a unified frequency-domain-based approach for self-supervised pre-training of visual models. Instead of randomly inserting mask tokens to the input embeddings in the spatial domain, in this paper, we shift the perspective to the frequency domain. Specifically, MFM first masks out a portion of frequency components of the input image and then predicts the missing frequencies on the frequency spectrum. Our key insight is that predicting masked components in the frequency domain is more ideal to reveal underlying image patterns rather than predicting masked patches in the spatial domain, due to the heavy spatial redundancy. Our findings suggest that with the right configuration of mask-and-predict strategy, both the structural information within high-frequency components and the low-level statistics among low-frequency counterparts are useful in learning good representations. For the first time, MFM demonstrates that, for both ViT and CNN, a simple non-Siamese framework can learn meaningful representations even using none of the following: (i) extra data, (ii) extra model, (iii) mask token. Experimental results on ImageNet and several robustness benchmarks show the competitive performance and advanced robustness of MFM compared with recent masked image modeling approaches. Furthermore, we also comprehensively investigate the effectiveness of classical image restoration tasks for representation learning from a unified frequency perspective and reveal their intriguing relations with our MFM approach. Project page: https://www.mmlab-ntu.com/project/mfm/index.html.", "Knowledge distillation (KD) has emerged as an essential technique not only for model compression, but also other learning tasks such as continual learning. Given the richer application spectrum and potential online usage of KD, knowledge distillation efficiency becomes a pivotal component. In this work, we study this little-explored but important topic. Unlike previous works that focus solely on the accuracy of student network, we attempt to achieve a harder goal \u2013 to obtain a performance comparable to conventional KD with a lower computation cost during the transfer. To this end, we present UNcertainty-aware mIXup (UNIX), an effective approach that can reduce transfer cost by 20% to 30% and yet maintain comparable or achieve even better student performance than conventional KD. This is made possible via effective uncertainty sampling and a novel adaptive mixup approach that select informative samples\u00a0\u2026", "Ultra-High-Definition (UHD) photo has gradually become the standard configuration in advanced imaging devices. The new standard unveils many issues in existing approaches for low-light image enhancement (LLIE), especially in dealing with the intricate issue of joint luminance enhancement and noise removal while remaining efficient. Unlike existing methods that address the problem in the spatial domain, we propose a new solution, UHDFour, that embeds Fourier transform into a cascaded network. Our approach is motivated by a few unique characteristics in the Fourier domain: 1) most luminance information concentrates on amplitudes while noise is closely related to phases, and 2) a high-resolution image and its low-resolution version share similar amplitude patterns.Through embedding Fourier into our network, the amplitude and phase of a low-light image are separately processed to avoid amplifying noise when enhancing luminance. Besides, UHDFour is scalable to UHD images by implementing amplitude and phase enhancement under the low-resolution regime and then adjusting the high-resolution scale with few computations. We also contribute the first real UHD LLIE dataset, \\textbf{UHD-LL}, that contains 2,150 low-noise/normal-clear 4K image pairs with diverse darkness and noise levels captured in different scenarios. With this dataset, we systematically analyze the performance of existing LLIE methods for processing UHD images and demonstrate the advantage of our solution. We believe our new framework, coupled with the dataset, would push the frontier of LLIE towards UHD. The code and dataset are available at https\u00a0\u2026", "Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to\u00a0\u2026", "It is challenging to disentangle an object into two orthogonal spaces of content and style since each can influence the visual observation differently and unpredictably. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Autoencoder framework. For the structural content branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This constraint encourages the branch to distill geometry information. Another branch learns the complementary style information. The two branches form an effective framework that can disentangle object's content-style representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesized and real-world data. We are able to generate photo-realistic images with 256*256 resolution that are clearly disentangled in content and style.", "Color brings extra data capacity for QR codes, but it also brings tremendous challenges to the decoding because of color interference and illumination variation, especially for high-density QR codes. In this paper, we put forth a framework for high-capacity QR codes, HiQ, which optimizes the decoding algorithm for high-density QR codes to achieve robust and fast decoding on mobile devices, and adopts a learning-based approach for color recovery. Moreover, we propose a robust geometric transformation algorithm to correct the geometric distortion. We also provide a challenging color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR code samples captured by different smartphones under different lighting conditions. Experimental results show that HiQ outperforms the baseline [1] by 286% in decoding success rate and 60% in bit error rate.", "We present an interesting and challenging dataset that features a large number of scenes with messy tables captured from multiple camera views. Each scene in this dataset is highly complex, containing multiple object instances that could be identical, stacked and occluded by other instances. The key challenge is to associate all instances given the RGB image of all views. The seemingly simple task surprisingly fails many popular methods or heuristics that we assume good performance in object association. The dataset challenges existing methods in mining subtle appearance differences, reasoning based on contexts, and fusing appearance with geometric cues for establishing an association. We report interesting findings with some popular baselines, and discuss how this dataset could help inspire new problems and catalyse more robust formulations to tackle real-world instance association problems\u00a0\u2026", "Transformer becomes prevalent in computer vision, especially for high-level vision tasks. However, adopting Transformer in the generative adversarial network (GAN) framework is still an open yet challenging problem. In this paper, we conduct a comprehensive empirical study to investigate the properties of Transformer in GAN for high-fidelity image synthesis. Our analysis highlights and reaffirms the importance of feature locality in image generation, although the merits of the locality are well known in the classification task. Perhaps more interestingly, we find the residual connections in self-attention layers harmful for learning Transformer-based discriminators and conditional generators. We carefully examine the influence and propose effective ways to mitigate the negative impacts. Our study leads to a new alternative design of Transformers in GAN, a convolutional neural network (CNN)-free generator termed as STrans-G, which achieves competitive results in both unconditional and conditional image generations. The Transformer-based discriminator, STrans-D, also significantly reduces its gap against the CNN-based discriminators.", "Artificial lights commonly leave strong lens flare artifacts on images captured at night. Nighttime flare not only affects the visual quality but also degrades the performance of vision algorithms. Existing flare removal methods mainly focus on removing daytime flares and fail in nighttime. Nighttime flare removal is challenging because of the unique luminance and spectrum of artificial lights and the diverse patterns and image degradation of the flares captured at night. The scarcity of nighttime flare removal datasets limits the research on this crucial task. In this paper, we introduce, Flare7K, the first nighttime flare removal dataset, which is generated based on the observation and statistics of real-world nighttime lens flares. It offers 5,000 scattering and 2,000 reflective flare images, consisting of 25 types of scattering flares and 10 types of reflective flares. The 7,000 flare patterns can be randomly added to flare-free images, forming the flare-corrupted and flare-free image pairs. With the paired data, we can train deep models to restore flare-corrupted images taken in the real world effectively. Apart from abundant flare patterns, we also provide rich annotations, including the labeling of light source, glare with shimmer, reflective flare, and streak, which are commonly absent from existing datasets. Hence, our dataset can facilitate new work in nighttime flare removal and more fine-grained analysis of flare patterns. Extensive experiments show that our dataset adds diversity to existing flare datasets and pushes the frontier of nighttime flare removal.", "Image Signal Processor (ISP) is a crucial component in digital cameras that transforms sensor signals into images for us to perceive and understand. Existing ISP designs always adopt a fixed architecture, eg, several sequential modules connected in a rigid order. Such a fixed ISP architecture may be suboptimal for real-world applications, where camera sensors, scenes and tasks are diverse. In this study, we propose a novel Reconfigurable ISP (ReconfigISP) whose architecture and parameters can be automatically tailored to specific data and tasks. In particular, we implement several ISP modules, and enable backpropagation for each module by training a differentiable proxy, hence allowing us to leverage the popular differentiable neural architecture search and effectively search for the optimal ISP architecture. A proxy tuning mechanism is adopted to maintain the accuracy of proxy networks in all cases. Extensive experiments conducted on image restoration and object detection, with different sensors, light conditions and efficiency constraints, validate the effectiveness of ReconfigISP. Only hundreds of parameters need tuning for every task.", "We present a new application direction named Pareidolia Face Reenactment, which is defined as animating a static illusory face to move in tandem with a human face in the video. For the large differences between pareidolia face reenactment and traditional human face reenactment, two main challenges are introduced, ie, shape variance and texture variance. In this work, we propose a novel Parametric Unsupervised Reenactment Algorithm to tackle these two challenges. Specifically, we propose to decompose the reenactment into three catenate processes: shape modeling, motion transfer and texture synthesis. With the decomposition, we introduce three crucial components, ie, Parametric Shape Modeling, Expansionary Motion Transfer and Unsupervised Texture Synthesizer, to overcome the problems brought by the remarkably variances on pareidolia faces. Extensive experiments show the superior performance of our method both qualitatively and quantitatively. Code, model and data are available on our project page.", "Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos.", "This paper presents a new method, called FlexiCurve, for photo enhancement. Unlike most existing methods that perform image-to-image mapping, which requires expensive pixel-wise reconstruction, FlexiCurve takes an input image and estimates global curves to adjust the image. The adjustment curves are specially designed for performing piecewise mapping, taking nonlinear adjustment and differentiability into account. To cope with challenging and diverse illumination properties in real-world images, FlexiCurve is formulated as a multi-task framework to produce diverse estimations and the associated confidence maps. These estimations are adaptively fused to improve local enhancements of different regions. Thanks to the image-to-curve formulation, for an image with a size of 512*512*3, FlexiCurve only needs a lightweight network (150K trainable parameters) and it has a fast inference speed (83FPS on a single NVIDIA 2080Ti GPU). The proposed method improves efficiency without compromising the enhancement quality and losing details in the original image. The method is also appealing as it is not limited to paired training data, thus it can flexibly learn rich enhancement styles from unpaired data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on photo enhancement quantitively and qualitatively.", "Deep neural networks have achieved remarkable progress in single-image 3D human reconstruction. However, existing methods still fall short in predicting rare poses. The reason is that most of the current models perform regression based on a single human prototype, which is similar to common poses while far from the rare poses. In this work, we 1) identify and analyze this learning obstacle and 2) propose a prototype memory-augmented network, PM-Net, that effectively improves performances of predicting rare poses. The core of our framework is a memory module that learns and stores a set of 3D human prototypes capturing local distributions for either common poses or rare poses. With this formulation, the regression starts from a better initialization, which is relatively easier to converge. Extensive experiments on several widely employed datasets demonstrate the proposed framework\u2019s effectiveness\u00a0\u2026", "Video segmentation aims to segment and track every pixel in diverse scenarios accurately. In this paper, we present Tube-Link, a versatile framework that addresses multiple core tasks of video segmentation with a unified architecture. Our framework is a near-online approach that takes a short subclip as input and outputs the corresponding spatial-temporal tube masks. To enhance the modeling of cross-tube relationships, we propose an effective way to perform tube-level linking via attention along the queries. In addition, we introduce temporal contrastive learning to instance-wise discriminative features for tube-level association. Our approach offers flexibility and efficiency for both short and long video inputs, as the length of each subclip can be varied according to the needs of datasets or scenarios. Tube-Link outperforms existing specialized architectures by a significant margin on five video segmentation datasets. Specifically, it achieves almost 13% relative improvements on VIPSeg and 4% improvements on KITTI-STEP over the strong baseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019 and 2021, Tube-Link boosts IDOL by 3% and 4%, respectively. Code is available at https://github. com/lxtGH/Tube-Link.", "This paper performs comprehensive analysis on datasets for occlusion-aware face segmentation, a task that is crucial for many downstream applications. The collection and annotation of such datasets are time-consuming and labor-intensive. Although some efforts have been made in synthetic data generation, the naturalistic aspect of data remains less explored. In our study, we propose two occlusion generation techniques, Naturalistic Occlusion Generation (NatOcc), for producing high-quality naturalistic synthetic occluded faces; and Random Occlusion Generation (RandOcc), a more general synthetic occluded data generation method. We empirically show the effectiveness and robustness of both methods, even for unseen occlusions. To facilitate model evaluation, we present two high-resolution real-world occluded face datasets with fine-grained annotations, RealOcc and RealOcc-Wild, featuring both careful alignment preprocessing and an in-the-wild setting for robustness test. We further conduct a comprehensive analysis on a newly introduced segmentation benchmark, offering insights for future exploration.", "In large cities with high population densities, the assembly of large crowds in public events and public areas increases risks to public safety and transportation, which have become major concerns to the community. Although much effort has been made on crowd scene understanding, many of existing works are scene-specific, i.e., models learned from a particular scene cannot be well applied to other scenes. It limits the application of these technologies, since extra training samples have to be collected from a new scene. This book chapter will introduce scene-independent crowd analysis with deep learning. Once generic deep models are learned from large-scale training sets, they can be applied to various crowd scenes without being trained again. The topics cover crowd density estimation, crowd counting, and crowd attribute recognition. Deep learning is driven by large-scale training. Several large-scale\u00a0\u2026", "While spectral clustering is usually an unsupervised operation, there are circumstances in which we have prior belief that pairs of samples should (or should not) be assigned with the same cluster. Constrained spectral clustering aims to exploit this prior belief as constraint (or weak supervision) to influence the cluster formation so as to obtain a structure more closely resembling human perception. Two important issues remain open: (1) how to propagate sparse constraints effectively, (2) how to handle ill-conditioned/noisy constraints generated by imperfect oracles. In this paper we present a unified framework to address the above issues. Specifically, in contrast to existing constrained spectral clustering approaches that blindly rely on all features for constructing the spectral, our approach searches for neighbours driven by discriminative feature selection for more effective constraint diffusion. Crucially, we formulate a\u00a0\u2026", "Pre-trained vision-language models (VLMs) learn to align vision and language representations on large-scale datasets, where each image-text pair usually contains a bag of semantic concepts. However, existing open-vocabulary object detectors only align region embeddings individually with the corresponding features extracted from the VLMs. Such a design leaves the compositional structure of semantic concepts in a scene under-exploited, although the structure may be implicitly learned by the VLMs. In this work, we propose to align the embedding of bag of regions beyond individual regions. The proposed method groups contextually interrelated regions as a bag. The embeddings of regions in a bag are treated as embeddings of words in a sentence, and they are sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which is learned to be aligned to the corresponding features extracted by a frozen VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the previous best results by 4.6 box AP 50 and 2.8 mask AP on novel categories of open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are available at https://github. com/wusize/ovdet.", "We show that pre-trained Generative Adversarial Networks (GANs) such as StyleGAN and BigGAN can be used as a latent bank to improve the performance of image super-resolution. While most existing perceptual-oriented approaches attempt to generate realistic outputs through learning with adversarial loss, our method,  G enerative  L at E nt b AN k (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass for restoration. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Employing priors from different generative models allows GLEAN to be applied to diverse categories (e.g., human faces, cats, buildings, and cars). We\u00a0\u2026", "The exploitation of long-term information has been a long-standing problem in video restoration. The recent BasicVSR and BasicVSR++ have shown remarkable performance in video super-resolution through long-term propagation and effective alignment. Their success has led to a question of whether they can be transferred to different video restoration tasks. In this work, we extend BasicVSR++ to a generic framework for video restoration tasks. In tasks where inputs and outputs possess identical spatial size, the input resolution is reduced by strided convolutions to maintain efficiency. With only minimal changes from BasicVSR++, the proposed framework achieves compelling performance with great efficiency in various video restoration tasks including video deblurring and denoising. Notably, BasicVSR++ achieves comparable performance to Transformer-based approaches with up to 79% of parameter reduction and 44x speedup. The promising results demonstrate the importance of propagation and alignment in video restoration tasks beyond just video super-resolution. Code and models are available at https://github.com/ckkelvinchan/BasicVSR_PlusPlus.", "We present a new lighting estimation and editing framework to generate high-dynamic-range (HDR) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-HDR lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and HDR panorama synthesis into a unified framework. The LDR and HDR panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal\u00a0\u2026", "This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle\u00a0\u2026", "Learning to simultaneously handle face alignment of arbitrary views, e.g. frontal and profile views, appears to be more challenging than we thought. The difficulties lay in i) accommodating the complex appearance-shape relations exhibited in different views, and ii) encompassing the varying landmark point sets due to self-occlusion and different landmark protocols. Most existing studies approach this problem via training multiple viewpoint-specific models, and conduct head pose estimation for model selection. This solution is intuitive but the performance is highly susceptible to inaccurate head pose estimation. In this study, we address this shortcoming through learning an Ensemble of Model Recommendation Trees (EMRT), which is capable of selecting optimal model configuration without prior head pose estimation. The unified framework seamlessly handles different viewpoints and landmark protocols, and it is trained by optimising directly on landmark locations, thus yielding superior results on arbitrary-view face alignment. This is the first study that performs face alignment on the full AFLWdataset with faces of different views including profile view. State-of-the-art performances are also reported on MultiPIE and AFW datasets containing both frontaland profile-view faces.", "We present a novel approach to leverage prior knowledge encapsulated in pre-trained text-to-image diffusion models for blind super-resolution (SR). Specifically, by employing our time-aware encoder, we can achieve promising restoration results without altering the pre-trained synthesis model, thereby preserving the generative prior and minimizing training cost. To remedy the loss of fidelity caused by the inherent stochasticity of diffusion models, we introduce a controllable feature wrapping module that allows users to balance quality and fidelity by simply adjusting a scalar value during the inference process. Moreover, we develop a progressive aggregation sampling strategy to overcome the fixed-size constraints of pre-trained diffusion models, enabling adaptation to resolutions of any size. A comprehensive evaluation of our method using both synthetic and real-world benchmarks demonstrates its superiority over current state-of-the-art approaches.", "Channel pruning is broadly recognized as an effective approach to obtain a small compact model through eliminating unimportant channels from a large cumbersome network. Contemporary methods typically perform iterative pruning procedure from the original over-parameterized model, which is both tedious and expensive especially when the pruning is aggressive. In this paper, we propose a simple yet effective channel pruning technique, termed network Pruning via rEsource rEalLocation (PEEL), to quickly produce a desired slim model with negligible cost. Specifically, PEEL first constructs a predefined backbone and then conducts resource reallocation on it to shift parameters from less informative layers to more important layers in one round, thus amplifying the positive effect of these informative layers. To demonstrate the effectiveness of PEEL , we perform extensive experiments on ImageNet with ResNet\u00a0\u2026", "Recovering a textured 3D mesh from a monocular image is highly challenging, particularly for in-the-wild objects that lack 3D ground truths. In this work, we present MeshInversion, a novel framework to improve the reconstruction by exploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh synthesis. Reconstruction is achieved by searching for a latent space in the 3D GAN that best resembles the target mesh in accordance with the single view observation. Since the pre-trained GAN encapsulates rich 3D semantics in terms of mesh geometry and texture, searching within the GAN manifold thus naturally regularizes the realness and fidelity of the reconstruction. Importantly, such regularization is directly applied in the 3D space, providing crucial guidance of mesh parts that are unobserved in the 2D space. Experiments on standard benchmarks show that our framework obtains faithful 3D\u00a0\u2026", "We present a novel framework that brings the 3D motion retargeting task from controlled environments to in-the-wild scenarios. In particular, our method is capable of retargeting body motion from a character in a 2D monocular video to a 3D character without using any motion capture system or 3D reconstruction procedure. It is designed to leverage massive online videos for unsupervised training, needless of 3D annotations or motion-body pairing information. The proposed method is built upon two novel canonicalization operations, structure canonicalization and view canonicalization. Trained with the canonicalization operations and the derived regularizations, our method learns to factorize a skeleton sequence into three independent semantic subspaces, ie, motion, structure, and view angle. The disentangled representation enables motion retargeting from 2D to 3D with high precision. Our method achieves superior performance on motion transfer benchmarks with large body variations and challenging actions. Notably, the canonicalized skeleton sequence could serve as a disentangled and interpretable representation of human motion that benefits action analysis and motion retrieval.", "In this work, we focus on instance-level open vocabulary segmentation, intending to expand a segmenter for instance-wise novel categories without mask annotations. We investigate a simple yet effective framework with the help of image captions, focusing on exploiting thousands of object nouns in captions to discover instances of novel classes. Rather than adopting pretrained caption models or using massive caption datasets with complex pipelines, we propose an end-to-end solution from two aspects: caption grounding and caption generation. In particular, we devise a joint Caption Grounding and Generation (CGG) framework based on a Mask Transformer baseline. The framework has a novel grounding loss that performs explicit and implicit multi-modal feature alignments. We further design a lightweight caption generation head to allow for additional caption supervision. We find that grounding and generation complement each other, significantly enhancing the segmentation performance for novel categories. We conduct extensive experiments on the COCO dataset with two settings: Open Vocabulary Instance Segmentation (OVIS) and Open Set Panoptic Segmentation (OSPS). The results demonstrate the superiority of our CGG framework over previous OVIS methods, achieving a large improvement of 6.8% mAP on novel classes without extra caption data. Our method also achieves over 15% PQ improvements for novel classes on the OSPS benchmark under various settings.", "While deep learning-based methods for blind face restoration have achieved unprecedented success, they still suffer from two major limitations. First, most of them deteriorate when facing complex degradations out of their training data. Second, these methods require multiple constraints, e.g., fidelity, perceptual, and adversarial losses, which require laborious hyper-parameter tuning to stabilize and balance their influences. In this work, we propose a novel method named DifFace that is capable of coping with unseen and complex degradations more gracefully without complicated loss designs. The key of our method is to establish a posterior distribution from the observed low-quality (LQ) image to its high-quality (HQ) counterpart. In particular, we design a transition distribution from the LQ image to the intermediate state of a pre-trained diffusion model and then gradually transmit from this intermediate state to the HQ target by recursively applying a pre-trained diffusion model. The transition distribution only relies on a restoration backbone that is trained with  loss on some synthetic data, which favorably avoids the cumbersome training process in existing methods. Moreover, the transition distribution can contract the error of the restoration backbone and thus makes our method more robust to unknown degradations. Comprehensive experiments show that DifFace is superior to current state-of-the-art methods, especially in cases with severe degradations. Our code and model are available at https://github.com/zsyOAOA/DifFace.", "Problem: This work systematically investigates the effectiveness of various visual feature coding schemes for facilitating the learning of timedelayed dependencies among disjoint multi-camera views. Related work: Quite a few studies [3, 4, 6] have been proposed to model inter-camera dependency across non-overlapping camera views. Learning time-delayed correlations among disjoint cameras in crowded public scenarios is a non-trivial task:(1) the time gaps between camera views are unknown therefore activities in two related views may occur at arbitrary time delays with high uncertainty;(2) the features are inevitably noisy, ambiguous, and may vary drastically across views owning to illumination condition, camera angles, and changes in object pose. Most state-ofthe-art methods typically hand pick a few features tailored to the target environment, with the hope that those chosen features contain robust and\u00a0\u2026", "Recent Multimodal Large Language Models (MLLMs) are remarkable in vision-language tasks, such as image captioning and question answering, but lack the essential perception ability, i.e., object detection. In this work, we address this limitation by introducing a novel research problem of contextual object detection -- understanding visible objects within different human-AI interactive contexts. Three representative scenarios are investigated, including the language cloze test, visual captioning, and question answering. Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction. Our ContextDET involves three key submodels: (i) a visual encoder for extracting visual representations, (ii) a pre-trained LLM for multimodal context decoding, and (iii) a visual decoder for predicting bounding boxes given contextual object words. The new generate-then-detect framework enables us to detect object words within human vocabulary. Extensive experiments show the advantages of ContextDET on our proposed CODE benchmark, open-vocabulary detection, and referring image segmentation. Github: https://github.com/yuhangzang/ContextDET.", "Towards building comprehensive real-world visual perception systems, we propose and study a new problem called panoptic scene graph generation (PVSG). PVSG is related to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects localized with bounding boxes in videos. However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causes VidSGG systems to miss key details that are crucial for comprehensive video understanding. In contrast, PVSG requires nodes in scene graphs to be grounded by more precise, pixel-level segmentation masks, which facilitate holistic scene understanding. To advance research in this new area, we contribute a high-quality PVSG dataset, which consists of 400 videos (289 third-person+ 111 egocentric videos) with totally 150K frames labeled with panoptic segmentation masks as well as fine, temporal scene graphs. We also provide a variety of baseline methods and share useful design practices for future work.", "This paper presents Dense Siamese Network (DenseSiam), a simple unsupervised learning framework for dense prediction tasks. It learns visual representations by maximizing the similarity between two views of one image with two types of consistency, i.e., pixel consistency and region consistency. Concretely, DenseSiam first maximizes the pixel level spatial consistency according to the exact location correspondence in the overlapped area. It also extracts a batch of region embeddings that correspond to some sub-regions in the overlapped area to be contrasted for region consistency. In contrast to previous methods that require negative pixel pairs, momentum encoders or heuristic masks, DenseSiam benefits from the simple Siamese network and optimizes the consistency of different granularities. It also proves that the simple location correspondence and interacted region embeddings are effective enough to\u00a0\u2026", null, "Neural Radiance Field (NeRF) significantly degrades when only a limited number of views are available. To complement the lack of 3D information, depth-based models, such as DSNeRF and MonoSDF, explicitly assume the availability of accurate depth maps of multiple views. They linearly scale the accurate depth maps as supervision to guide the predicted depth of few-shot NeRFs. However, accurate depth maps are difficult and expensive to capture due to wide-range depth distances in the wild. In this work, we present a new Sparse-view NeRF (SparseNeRF) framework that exploits depth priors from real-world inaccurate observations. The inaccurate depth observations are either from pre-trained depth models or coarse depth maps of consumer-level depth sensors. Since coarse depth maps are not strictly scaled to the ground-truth depth maps, we propose a simple yet effective constraint, a local depth ranking method, on NeRFs such that the expected depth ranking of the NeRF is consistent with that of the coarse depth maps in local patches. To preserve the spatial continuity of the estimated depth of NeRF, we further propose a spatial continuity constraint to encourage the consistency of the expected depth continuity of NeRF with coarse depth maps. Surprisingly, with simple depth ranking constraints, SparseNeRF outperforms all state-of-the-art few-shot NeRF methods (including depth-based models) on standard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBD that contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13 Pro. Extensive experiments on NVS-RGBD dataset also validate the\u00a0\u2026", "Text-driven content creation has evolved to be a transformative technique that revolutionizes creativity. Here we study the task of text-driven human video generation, where a video sequence is synthesized from texts describing the appearance and motions of a target performer. Compared to general text-driven video generation, human-centric video generation requires maintaining the appearance of synthesized human while performing complex motions. In this work, we present Text2Performer to generate vivid human videos with articulated motions from texts. Text2Performer has two novel designs: 1) decomposed human representation and 2) diffusion-based motion sampler. First, we decompose the VQVAE latent space into human appearance and pose representation in an unsupervised manner by utilizing the nature of human videos. In this way, the appearance is well maintained along the generated frames. Then, we propose continuous VQ-diffuser to sample a sequence of pose embeddings. Unlike existing VQ-based methods that operate in the discrete space, continuous VQ-diffuser directly outputs the continuous pose embeddings for better motion modeling. Finally, motion-aware masking strategy is designed to mask the pose embeddings spatial-temporally to enhance the temporal coherence. Moreover, to facilitate the task of text-driven human video generation, we contribute a Fashion-Text2Video dataset with manually annotated action labels and text descriptions. Extensive experiments demonstrate that Text2Performer generates high-quality human videos (up to 512x256 resolution) with diverse appearances and flexible motions.", "Text-driven generation models are flourishing in video generation and editing. However, face-centric text-to-video generation remains a challenge due to the lack of a suitable dataset containing high-quality videos and highly relevant texts. This paper presents CelebV-Text, a large-scale, diverse, and high-quality dataset of facial text-video pairs, to facilitate research on facial text-to-video generation tasks. CelebV-Text comprises 70,000 in-the-wild face video clips with diverse visual content, each paired with 20 texts generated using the proposed semi-automatic text generation strategy. The provided texts are of high quality, describing both static and dynamic attributes precisely. The superiority of CelebV-Text over other datasets is demonstrated via comprehensive statistical analysis of the videos, texts, and text-video relevance. The effectiveness and potential of CelebV-Text are further shown through extensive self-evaluation. A benchmark is constructed with representative methods to standardize the evaluation of the facial text-to-video generation task. All data and models are publicly available.", "We present a systematic study of domain generalization (DG) for tiny neural networks, a problem that is critical to on-device machine learning applications but has been overlooked in the literature where research has been focused on large models only. Tiny neural networks have much fewer parameters and lower complexity, and thus should not be trained the same way as their large counterparts for DG applications. We find that knowledge distillation is a strong candidate for solving the problem: it outperforms state-of-the-art DG methods that were developed using large models with a large margin. Moreover, we observe that the teacher-student performance gap on test data with domain shift is bigger than that on in-distribution data. To improve DG for tiny neural networks without increasing the deployment cost, we propose a simple idea called out-of-distribution knowledge distillation (OKD), which aims to teach the student how the teacher handles (synthetic) out-of-distribution data and is proved to be a promising framework for solving the problem. We also contribute a scalable method of creating DG datasets, called DOmain Shift in COntext (DOSCO), which can be applied to broad data at scale without much human effort. Code and models are released at \\url{https://github.com/KaiyangZhou/on-device-dg}.", "Recent years have witnessed exciting progress in automatic face swapping and editing. Many techniques have been proposed, facilitating the rapid development of creative content creation. The emergence and easy accessibility of such techniques, however, also cause potential unprecedented ethical and moral issues. To this end, academia and industry proposed several effective forgery detection methods. Nonetheless, challenges could still exist.(1) Current face manipulation advances can produce high-fidelity fake videos, rendering forgery detection challenging.(2) The generalization capability of most existing detection models is poor, particularly in real-world scenarios where the media sources and distortions are unknown. The primary difficulty in overcoming these challenges is the lack of amenable datasets for real-world face forgery detection. Most existing datasets are either of a small number, of low quality, or overly artificial. Meanwhile, the large distribution gap between training data and actual test videos also leads to weak generalization ability. In this chapter, we present our on-going effort of constructing DeeperForensics-1.0, a large-scale forgery detection dataset, to address the challenges above. We discuss approaches to ensure the quality and diversity of the dataset. Besides, we describe the observations we obtained from organizing DeeperForensics Challenge 2020, a real-world face forgery detection competition based on DeeperForensics-1.0. Specifically, we summarize the winning solutions and provide some discussions on potential research directions.", "Describable face attributes are labels that can be given to a face image to describe its characteristics. Examples of face attributes include gender, age, ethnicity, face shape, and nose size. Predicting face attributes in the wild is challenging due to complex face variations. This chapter aims to provide an in-depth presentation of recent progress and the current state-of-the-art approaches to solving some of the fundamental challenges in face attribute recognition, particularly from the angle of deep learning. We highlight effective techniques for training deep convolutional networks for predicting face attributes in the wild, and addressing the problem of imbalanced distribution of attributes. In addition, we discuss the use of face attributes as rich contexts to facilitate accurate face detection and face alignment in return. The chapter ends by posing an open question for the face attribute recognition challenge arising from\u00a0\u2026", "Digital human motion synthesis is a vibrant research field with applications in movies, AR/VR, and video games. Whereas methods were proposed to generate natural and realistic human motions, most only focus on modeling humans and largely ignore object movements. Generating task-oriented human-object interaction motions in simulation is challenging. For different intents of using the objects, humans conduct various motions, which requires the human first to approach the objects and then make them move consistently with the human instead of staying still. Also, to deploy in downstream applications, the synthesized motions are desired to be flexible in length, providing options to personalize the predicted motions for various purposes. To this end, we propose TOHO: Task-Oriented Human-Object Interactions Generation with Implicit Neural Representations, which generates full human-object interaction motions to conduct specific tasks, given only the task type, the object, and a starting human status. TOHO generates human-object motions in three steps: 1) it first estimates the keyframe poses of conducting a task given the task type and object information; 2) then, it infills the keyframes and generates continuous motions; 3) finally, it applies a compact closed-form object motion estimation to generate the object motion. Our method generates continuous motions that are parameterized only by the temporal coordinate, which allows for upsampling or downsampling of the sequence to arbitrary frames and adjusting the motion speeds by designing the temporal coordinate vector. We demonstrate the effectiveness of our method, both\u00a0\u2026", "Due to the difficulty in collecting large-scale and perfectly aligned paired training data for Under-Display Camera (UDC) image restoration, previous methods resort to monitor-based image systems or simulation-based methods, sacrificing the realness of the data and introducing domain gaps. In this work, we revisit the classic stereo setup for training data collection--capturing two images of the same scene with one UDC and one standard camera. The key idea is to\" copy\" details from a high-quality reference image and\" paste\" them on the UDC image. While being able to generate real training pairs, this setting is susceptible to spatial misalignment due to perspective and depth of field changes. The problem is further compounded by the large domain discrepancy between the UDC and normal images, which is unique to UDC restoration. In this paper, we mitigate the non-trivial domain discrepancy and spatial misalignment through a novel Transformer-based framework that generates well-aligned yet high-quality target data for the corresponding UDC input. This is made possible through two carefully designed components, namely, the Domain Alignment Module (DAM) and Geometric Alignment Module (GAM), which encourage robust and accurate discovery of correspondence between the UDC and normal views. Extensive experiments show that high-quality and well-aligned pseudo UDC training pairs are beneficial for training a robust restoration network. Code and the dataset are available at https://github. com/jnjaby/AlignFormer.", "Reflective flare is a phenomenon that occurs when light reflects inside lenses, causing bright spots or a\" ghosting effect\" in photos, which can impact their quality. Eliminating reflective flare is highly desirable but challenging. Many existing methods rely on manually designed features to detect these bright spots, but they often fail to identify reflective flares created by various types of light and may even mistakenly remove the light sources in scenarios with multiple light sources. To address these challenges, we propose an optical center symmetry prior, which suggests that the reflective flare and light source are always symmetrical around the lens's optical center. This prior helps to locate the reflective flare's proposal region more accurately and can be applied to most smartphone cameras. Building on this prior, we create the first reflective flare removal dataset called BracketFlare, which contains diverse and realistic reflective flare patterns. We use continuous bracketing to capture the reflective flare pattern in the underexposed image and combine it with a normally exposed image to synthesize a pair of flare-corrupted and flare-free images. With the dataset, neural networks can be trained to remove the reflective flares effectively. Extensive experiments demonstrate the effectiveness of our method on both synthetic and real-world datasets.", "StyleGAN family is one of the most popular Generative Adversarial Networks (GANs) for unconditional generation. Despite its impressive performance, its high demand on storage and computation impedes their deployment on resource-constrained devices. This paper provides a comprehensive study of distilling from the popular StyleGAN-like architecture. Our key insight is that the main challenge of StyleGAN distillation lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. Standard knowledge distillation losses typically fail under this heterogeneous distillation scenario. We conduct thorough analysis about the reasons and effects of this discrepancy issue, and identify that the mapping network plays a vital role in determining semantic information of generated images. Based on this finding, we propose a novel initialization strategy for the\u00a0\u2026", "This work aims to learn a high-quality text-to-video (T2V) generative model by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a highly desirable yet challenging task to simultaneously a) accomplish the synthesis of visually realistic and temporally coherent videos while b) preserving the strong creative generation nature of the pre-trained T2I model. To this end, we propose LaVie, an integrated video generation framework that operates on cascaded video latent diffusion models, comprising a base T2V model, a temporal interpolation model, and a video super-resolution model. Our key insights are two-fold: 1) We reveal that the incorporation of simple temporal self-attentions, coupled with rotary positional encoding, adequately captures the temporal correlations inherent in video data. 2) Additionally, we validate that the process of joint image-video fine-tuning plays a pivotal role in producing high-quality and creative outcomes. To enhance the performance of LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M, consisting of 25 million text-video pairs that prioritize quality, diversity, and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves state-of-the-art performance both quantitatively and qualitatively. Furthermore, we showcase the versatility of pre-trained LaVie models in various long video generation and personalized video synthesis applications.", "This paper focuses on long-tailed object detection in the semi-supervised learning setting, which poses realistic challenges, but has rarely been studied in the literature. We propose a novel pseudo-labeling-based detector called CascadeMatch. Our detector features a cascade network architecture, which has multi-stage detection heads with progressive confidence thresholds. To avoid manually tuning the thresholds, we design a new adaptive pseudo-label mining mechanism to automatically identify suitable values from data . To mitigate confirmation bias, where a model is negatively reinforced by incorrect pseudo-labels produced by itself, each detection head is trained by the ensemble pseudo-labels of all detection heads. Experiments on two long-tailed datasets, i.e., LVIS and COCO-LT, demonstrate that CascadeMatch surpasses existing state-of-the-art semi-supervised approaches\u2014across a wide range of detection\u00a0\u2026", "This paper strives for motion expressions guided video segmentation, which focuses on segmenting objects in video content based on a sentence describing the motion of the objects. Existing referring video object datasets typically focus on salient objects and use language expressions that contain excessive static attributes that could potentially enable the target object to be identified in a single frame. These datasets downplay the importance of motion in video content for language-guided video object segmentation. To investigate the feasibility of using motion expressions to ground and segment objects in videos, we propose a large-scale dataset called MeViS, which contains numerous motion expressions to indicate target objects in complex environments. We benchmarked 5 existing referring video object segmentation (RVOS) methods and conducted a comprehensive comparison on the MeViS dataset. The results show that current RVOS methods cannot effectively address motion expression-guided video segmentation. We further analyze the challenges and propose a baseline approach for the proposed MeViS dataset. The goal of our benchmark is to provide a platform that enables the development of effective language-guided video segmentation algorithms that leverage motion expressions as a primary cue for object segmentation in complex video scenes. The proposed MeViS dataset has been released at https://henghuiding. github. io/MeViS.", "Realistic human-centric rendering plays a key role in both computer vision and computer graphics. Rapid progress has been made in the algorithm aspect over the years, yet existing human-centric rendering datasets and benchmarks are rather impoverished in terms of diversity (eg, outfit's fabric/material, body's interaction with objects, and motion sequences), which are crucial for rendering effect. Researchers are usually constrained to explore and evaluate a small set of rendering problems on current datasets, while real-world applications require methods to be robust across different scenarios. In this work, we present DNA-Rendering, a large-scale, high-fidelity repository of human performance data for neural actor rendering. DNA-Rendering presents several appealing attributes. First, our dataset contains over 1500 human subjects, 5000 motion sequences, and 67.5 M frames' data volume. Upon the massive collections, we provide human subjects with grand categories of pose actions, body shapes, clothing, accessories, hairdos, and object intersection, which ranges the geometry and appearance variances from everyday life to professional occasions. Second, we provide rich assets for each subject-2D/3D human body keypoints, foreground masks, SMPLX models, cloth/accessory materials, multi-view images, and videos. These assets boost the current method's accuracy on downstream rendering tasks. Third, we construct a professional multi-view system to capture data, which contains 60 synchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern camera calibration steps, ensuring high-quality resources for task\u00a0\u2026", "We propose a novel unsupervised backlit image enhancement method, abbreviated as CLIP-LIT, by exploring the potential of Contrastive Language-Image Pre-Training (CLIP) for pixel-level image enhancement. We show that the open-world CLIP prior not only aids in distinguishing between backlit and well-lit images, but also in perceiving heterogeneous regions with different luminance, facilitating the optimization of the enhancement network. Unlike high-level and image manipulation tasks, directly applying CLIP to enhancement tasks is non-trivial, owing to the difficulty in finding accurate prompts. To solve this issue, we devise a prompt learning framework that first learns an initial prompt pair by constraining the text-image similarity between the prompt (negative/positive sample) and the corresponding image (backlit image/well-lit image) in the CLIP latent space. Then, we train the enhancement network based on the text-image similarity between the enhanced result and the initial prompt pair. To further improve the accuracy of the initial prompt pair, we iteratively fine-tune the prompt learning framework to reduce the distribution gaps between the backlit images, enhanced results, and well-lit images via rank learning, boosting the enhancement performance. Our method alternates between updating the prompt learning framework and enhancement network until visually pleasing results are achieved. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in terms of visual quality and generalization ability, without requiring any paired data.", "Recent self-supervised methods are mainly designed for representation learning with the base model, e.g., ResNets or ViTs. They cannot be easily transferred to DETR, with task-specific Transformer modules. In this work, we present Siamese DETR, a Siamese self-supervised pretraining approach for the Transformer architecture in DETR. We consider learning view-invariant and detection-oriented representations simultaneously through two complementary tasks, i.e., localization and discrimination, in a novel multi-view learning framework. Two self-supervised pretext tasks are designed: (i) Multi-View Region Detection aims at learning to localize regions-of-interest between augmented views of the input, and (ii) Multi-View Semantic Discrimination attempts to improve object-level discrimination for each region. The proposed Siamese DETR achieves state-of-the-art transfer performance on COCO and PASCAL VOC detection using different DETR variants in all setups. Code is available at https://github.com/Zx55/SiameseDETR.", "Blind text image super-resolution (SR) is challenging as one needs to cope with diverse font styles and unknown degradation. To address the problem, existing methods perform character recognition in parallel to regularize the SR task, either through a loss constraint or intermediate feature condition. Nonetheless, the high-level prior could still fail when encountering severe degradation. The problem is further compounded given characters of complex structures, eg, Chinese characters that combine multiple pictographic or ideographic symbols into a single character. In this work, we present a novel prior that focuses more on the character structure. In particular, we learn to encapsulate rich and diverse structures in a StyleGAN and exploit such generative structure priors for restoration. To restrict the generative space of StyleGAN so that it obeys the structure of characters yet remains flexible in handling different font styles, we store the discrete features for each character in a codebook. The code subsequently drives the StyleGAN to generate high-resolution structural details to aid text SR. Compared to priors based on character recognition, the proposed structure prior exerts stronger character-specific guidance to restore faithful and precise strokes of a designated character. Extensive experiments on synthetic and real datasets demonstrate the compelling performance of the proposed generative structure prior in facilitating robust text SR. Our code is available at https://github. com/csxmli2016/MARCONet.", "StyleGAN has achieved great progress in 2D face reconstruction and semantic editing via image inversion and latent editing. While studies over extending 2D StyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion framework is still missing, limiting the applications of 3D face reconstruction and semantic editing. In this paper, we study the challenging problem of 3D GAN inversion where a latent code is predicted given a single face image to faithfully recover its 3D shapes and detailed textures. The problem is ill-posed: innumerable compositions of shape and texture could be rendered to the current image. Furthermore, with the limited capacity of a global latent code, 2D inversion methods cannot preserve faithful shape and texture at the same time when applied to 3D models. To solve this problem, we devise an effective self-training scheme to constrain the learning of inversion. The learning is done efficiently without any real-world 2D-3D training pairs but proxy samples generated from a 3D GAN. In addition, apart from a global latent code that captures the coarse shape and texture information, we augment the generation network with a local branch, where pixel-aligned features are added to faithfully reconstruct face details. We further consider a new pipeline to perform 3D view-consistent editing. Extensive experiments show that our method outperforms state-of-the-art inversion methods in both shape and texture reconstruction quality.", "Existing automated dubbing methods are usually designed for Professionally Generated Content (PGC) production, which requires massive training data and training time to learn a person-specific audio-video mapping. In this paper, we investigate an audio-driven dubbing method that is more feasible for User Generated Content (UGC) production. There are two unique challenges to design a method for UGC: 1) the appearances of speakers are diverse and arbitrary as the method needs to generalize across users; 2) the available video data of one speaker are very limited. In order to tackle the above challenges, we first introduce a new Style Translation Network to integrate the speaking style of the target and the speaking content of the source via a cross-modal AdaIN module. It enables our model to quickly adapt to a new speaker. Then, we further develop a semi-parametric video renderer, which takes full\u00a0\u2026", "We present Curve Distillation, CuDi, for efficient and controllable exposure adjustment without the requirement of paired or unpaired data during training. Our method inherits the zero-reference learning and curve-based framework from an effective low-light image enhancement method, Zero-DCE, with further speed up in its inference speed, reduction in its model size, and extension to controllable exposure adjustment. The improved inference speed and lightweight model are achieved through novel curve distillation that approximates the time-consuming iterative operation in the conventional curve-based framework by high-order curve's tangent line. The controllable exposure adjustment is made possible with a new self-supervised spatial exposure control loss that constrains the exposure levels of different spatial regions of the output to be close to the brightness distribution of an exposure map serving as an input condition. Different from most existing methods that can only correct either underexposed or overexposed photos, our approach corrects both underexposed and overexposed photos with a single model. Notably, our approach can additionally adjust the exposure levels of a photo globally or locally with the guidance of an input condition exposure map, which can be pre-defined or manually set in the inference stage. Through extensive experiments, we show that our method is appealing for its fast, robust, and flexible performance, outperforming state-of-the-art methods in real scenes. Project page: https://li-chongyi.github.io/CuDi_files/.", "Visual correspondence of 2D animation is the core of many applications and deserves careful study. Existing correspondence datasets for 2D cartoon suffer from simple frame composition and monotonic movements, making them insufficient to simulate real animations. In this work, we present a new 2D animation visual correspondence dataset, AnimeRun, by converting open source 3D movies to full scenes in 2D style, including simultaneous moving background and interactions of multiple subjects. Statistics show that our proposed dataset not only resembles real anime more in image composition, but also possesses richer and more complex motion patterns compared to existing datasets. With this dataset, we establish a comprehensive benchmark by evaluating several existing optical flow and segment matching methods, and analyze shortcomings of these methods on animation data. Data are available at https://lisiyao21. github. io/projects/AnimeRun.", "Talking face generation aims at synthesizing coherent and realistic face sequences given an input speech. The task enjoys a wide spectrum of downstream applications, such as teleconferencing, movie dubbing, and virtual assistant. The emergence of deep learning and cross-modality research has led to many interesting works that address talking face generation. Despite great research efforts in talking face generation, the problem remains challenging due to the need for fine-grained control of face components and the generalization to arbitrary sentences. In this chapter, we first discuss the definition and underlying challenges of the problem. Then, we present an overview of recent progress in talking face generation. In addition, we introduce some widely used datasets and performance metrics. Finally, we discuss open questions, potential future directions, and ethical considerations in this task.", "We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchmarks demonstrate that MosaicFusion can significantly improve the performance of existing instance segmentation models, especially for rare and novel categories. Code will be released at https://github.com/Jiahao000/MosaicFusion.", "Artificial lights commonly leave strong lens flare artifacts on the images captured at night, degrading both the visual quality and performance of vision algorithms. Existing flare removal approaches mainly focus on removing daytime flares and fail in nighttime cases. Nighttime flare removal is challenging due to the unique luminance and spectrum of artificial lights, as well as the diverse patterns and image degradation of the flares. The scarcity of the nighttime flare removal dataset constraints the research on this crucial task. In this paper, we introduce Flare7K++, the first comprehensive nighttime flare removal dataset, consisting of 962 real-captured flare images (Flare-R) and 7,000 synthetic flares (Flare7K). Compared to Flare7K, Flare7K++ is particularly effective in eliminating complicated degradation around the light source, which is intractable by using synthetic flares alone. Besides, the previous flare removal pipeline relies on the manual threshold and blur kernel settings to extract light sources, which may fail when the light sources are tiny or not overexposed. To address this issue, we additionally provide the annotations of light sources in Flare7K++ and propose a new end-to-end pipeline to preserve the light source while removing lens flares. Our dataset and pipeline offer a valuable foundation and benchmark for future investigations into nighttime flare removal studies. Extensive experiments demonstrate that Flare7K++ supplements the diversity of existing flare datasets and pushes the frontier of nighttime flare removal towards real-world scenarios.", "Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image or video by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap ( e.g.  scale and rotation) and the resolution gap ( e.g.  HR and LR). To tackle these challenges, we propose   -Matching in this work, which performs explicit robust matching crossing transformation and resolution. 1) To bridge the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) To address the\u00a0\u2026", "BackgroundWhile face detection (localizing faces in a given image) is regarded as the starting point for any face analysis task, face alignment is an essential intermediary step for solving tasks such as expression recognition, face recognition, face modeling, etc. In recent years, facial landmark extraction, subsequently used for aligning the face images, has received significant interest from the research community. Methods developed using convolution neural networks have been able to localize facial landmarks with great precision even in unconstrained settings, which in turn have set new benchmarks in face recognition, identification, and emotion recognition tasks. A brief description of the evolution of facial landmark localization methods is presented in section \u201c Theory and Applications.\u201d A large number of approaches have been proposed to tackle the problem of face alignment of 2D images with varying degrees\u00a0\u2026", "DEtection TRansformer (DETR) started a trend that uses a group of learnable queries for unified visual perception. This work begins by applying this appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple yet effective baseline. Although the naive adaptation obtains fair results, the instance segmentation performance is noticeably inferior to previous works. By diving into the details, we observe that instances in the sparse point clouds are relatively small to the whole scene and often have similar geometry but lack distinctive appearance for segmentation, which are rare in the image domain. Considering instances in 3D are more featured by their positional information, we emphasize their roles during the modeling and design a robust Mixed-parameterized Positional Embedding (MPE) to guide the segmentation process. It is embedded into backbone features and later guides the mask prediction and query update processes iteratively, leading to Position-Aware Segmentation (PA-Seg) and Masked Focal Attention (MFA). All these designs impel the queries to attend to specific regions and identify various instances. The method, named Position-guided Point cloud Panoptic segmentation transFormer (P3Former), outperforms previous state-of-the-art methods by 3.4% and 1.2% PQ on SemanticKITTI and nuScenes benchmark, respectively. The source code and models are available at https://github.com/SmartBot-PJLab/P3Former .", "Synthesizing high-fidelity head avatars is a central problem for computer vision and graphics. While head avatar synthesis algorithms have advanced rapidly, the best ones still face great obstacles in real-world scenarios. One of the vital causes is inadequate datasets -- 1) current public datasets can only support researchers to explore high-fidelity head avatars in one or two task directions; 2) these datasets usually contain digital head assets with limited data volume, and narrow distribution over different attributes. In this paper, we present RenderMe-360, a comprehensive 4D human head dataset to drive advance in head avatar research. It contains massive data assets, with 243+ million complete head frames, and over 800k video sequences from 500 different identities captured by synchronized multi-view cameras at 30 FPS. It is a large-scale digital library for head avatars with three key attributes: 1) High Fidelity: all subjects are captured by 60 synchronized, high-resolution 2K cameras in 360 degrees. 2) High Diversity: The collected subjects vary from different ages, eras, ethnicities, and cultures, providing abundant materials with distinctive styles in appearance and geometry. Moreover, each subject is asked to perform various motions, such as expressions and head rotations, which further extend the richness of assets. 3) Rich Annotations: we provide annotations with different granularities: cameras' parameters, matting, scan, 2D/3D facial landmarks, FLAME fitting, and text description. Based on the dataset, we build a comprehensive benchmark for head avatar research, with 16 state-of-the-art methods performed on five main tasks: novel\u00a0\u2026", "With the rise of large-scale models trained on broad data, in-context learning has become a new learning paradigm that has demonstrated significant potential in natural language processing and computer vision tasks. Meanwhile, in-context learning is still largely unexplored in the 3D point cloud domain. Although masked modeling has been successfully applied for in-context learning in 2D vision, directly extending it to 3D point clouds remains a formidable challenge. In the case of point clouds, the tokens themselves are the point cloud positions (coordinates) that are masked during inference. Moreover, position embedding in previous works may inadvertently introduce information leakage. To address these challenges, we introduce a novel framework, named Point-In-Context, designed especially for in-context learning in 3D point clouds, where both inputs and outputs are modeled as coordinates for each task. Additionally, we propose the Joint Sampling module, carefully designed to work in tandem with the general point sampling operator, effectively resolving the aforementioned technical issues. We conduct extensive experiments to validate the versatility and adaptability of our proposed methods in handling a wide range of tasks. Furthermore, with a more effective prompt selection strategy, our framework surpasses the results of individually trained models.", "Unconditional video generation is a challenging task that involves synthesizing high-quality videos that are both coherent and of extended duration. To address this challenge, researchers have used pretrained StyleGAN image generators for high-quality frame synthesis and focused on motion generator design. The motion generator is trained in an autoregressive manner using heavy 3D convolutional discriminators to ensure motion coherence during video generation. In this paper, we introduce a novel motion generator design that uses a learning-based inversion network for GAN. The encoder in our method captures rich and smooth priors from encoding images to latents, and given the latent of an initially generated frame as guidance, our method can generate smooth future latent by modulating the inversion encoder temporally. Our method enjoys the advantage of sparse training and naturally constrains the generation space of our motion generator with the inversion network guided by the initial frame, eliminating the need for heavy discriminators. Moreover, our method supports style transfer with simple fine-tuning when the encoder is paired with a pretrained StyleGAN generator. Extensive experiments conducted on various benchmarks demonstrate the superiority of our method in generating long and high-resolution videos with decent single-frame quality and temporal consistency. Code is available at https://github. com/johannwyh/StyleInV.", "Recent advances in deep learning have witnessed many successful unsupervised image-to-image translation models that learn correspondences between two visual domains without paired data. However, it is still a great challenge to build robust mappings between various domains especially for those with drastic visual discrepancies. In this paper, we introduce a novel versatile framework, Generative Prior-guided UNsupervised Image-to-image Translation ( GP-UNIT ), that improves the quality, applicability and controllability of the existing translation models. The key idea of GP-UNIT is to distill the generative prior from pre-trained class-conditional GANs to build coarse-level cross-domain correspondences, and to apply the learned prior to adversarial translations to excavate fine-level correspondences. With the learned multi-level content correspondences, GP-UNIT is able to perform valid translations between\u00a0\u2026", "In this work, we propose a Robust, Efficient, and Component-specific makeup transfer method (abbreviated as BeautyREC). A unique departure from prior methods that leverage global attention, simply concatenate features, or implicitly manipulate features in latent space, we propose a component-specific correspondence to directly transfer the makeup style of a reference image to the corresponding components (e.g., skin, lips, eyes) of a source image, making elaborate and accurate local makeup transfer. As an auxiliary, the long-range visual dependencies of Transformer are introduced for effective global makeup transfer. Instead of the commonly used cycle structure that is complex and unstable, we employ a content consistency loss coupled with a content encoder to implement efficient single-path makeup transfer. The key insights of this study are modeling component-specific correspondence for local makeup transfer, capturing long-range dependencies for global makeup transfer, and enabling efficient makeup transfer via a single-path structure. We also contribute BeautyFace, a makeup transfer dataset to supplement existing datasets. This dataset contains 3,000 faces, covering more diverse makeup styles, face poses, and races. Each face has annotated parsing map. Extensive experiments demonstrate the effectiveness of our method against state-of-the-art methods. Besides, our method is appealing as it is with only 1M parameters, outperforming the state-of-the-art methods (BeautyGAN: 8.43M, PSGAN: 12.62M, SCGAN: 15.30M, CPM: 9.24M, SSAT: 10.48M).", "We introduce Correlational Image Modeling (CIM), a novel but surprisingly effective approach to self-supervised visual pre-training. Our CIM performs a simple pretext task: we randomly crop image regions (exemplar) from an input image (context) and predict correlation maps between the exemplars and the context. Three key designs enable correlational image modeling as a nontrivial and meaningful self-supervisory task. First, to generate useful exemplar-context pairs, we consider cropping image regions with various scales, shapes, rotations, and transformations. Second, we employ a bootstrap learning framework that involves online and target networks. During pre-training, the former takes exemplars as inputs while the latter converts the context. Third, we model the output correlation maps via a simple cross-attention block, within which the context serves as queries and the exemplars offer values and keys. We show that CIM performs on par or better than the current state of the art on self-supervised and transfer benchmarks.", "Particle-based systems provide a flexible and unified way  to simulate physics systems with complex dynamics. Most existing data-driven simulators for particle-based systems adopt graph neural networks (GNNs) as their network backbones, as particles and their interactions can be naturally represented by graph nodes and graph edges. However, while particle-based systems usually contain hundreds even thousands of particles, the explicit modeling of particle interactions as graph edges inevitably leads to a significant computational overhead, due to the increased number of particle interactions. Consequently, in this paper we propose a novel Transformer-based method, dubbed as Transformer with Implicit Edges (TIE), to capture the rich semantics of particle interactions in an edge-free manner. The core idea of TIE\u00a0is to decentralize the computation involving pair-wise particle interactions into per-particle\u00a0\u2026", "Generative models for audio-conditioned dance motion synthesis map music features to dance movements. Models are trained to associate motion patterns to audio patterns, usually without an explicit knowledge of the human body. This approach relies on a few assumptions: strong music-dance correlation, controlled motion data and relatively simple poses and movements. These characteristics are found in all existing datasets for dance motion synthesis, and indeed recent methods can achieve good results. We introduce a new dataset aiming to challenge these common assumptions, compiling a set of dynamic dance sequences displaying complex human poses. We focus on breakdancing which features acrobatic moves and tangled postures. We source our data from the Red Bull BC One competition videos. Estimating human keypoints from these videos is difficult due to the complexity of the dance, as well\u00a0\u2026", "Existing works study the Class Incremental learning (CIL) problem with the assumption that the data for previous classes are absent, or only a small subset of samples (known as exemplars) are accessible. Differently, we propose a new and practical setting called retrospective CIL, where all the previous data are accessible, but with bounded training budgets for old data replay. Since only a small subset of old samples can be replayed, it brings a new research problem, i.e., dynamically sampling old data along the incremental training process. As incremental learning particularly suffers from catastrophic forgetting, we propose to use the forgettability of the old samples as the sampling priorities to favour the forgotten samples during the dynamic sampling process. To achieve this, we introduce a forgetting rate metric with graph- based propagation to estimate the sample forgettability. The proposed method brings\u00a0\u2026", "Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in zero-shot image classification. However, when transferring the vision-language alignment of CLIP from global image representation to local region representation for the openvocabulary dense prediction tasks, CLIP ViTs suffer from the domain shift from full images to local image regions. In this paper, we embark on an in-depth analysis of the region-language alignment in CLIP models, which is essential for downstream open-vocabulary dense prediction tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the image-level recognition ability of CLIP ViT to local image regions without needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by aligning a region representation extracted from its dense feature map with the image-level representation of the corresponding image crop. With the enhanced CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary object detection, semantic segmentation, and panoptic segmentation across various benchmarks. Models and code will be available at https://github. com/wusize/CLIPSelf.", "Our proposed music-to-dance framework,  Bailando ++, addresses the challenges of driving 3D characters to dance in a way that follows the constraints of choreography norms and maintains temporal coherency with different music genres.  Bailando ++ consists of two components: a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequences, and an actor-critic Generative Pre-trained Transformer (GPT) that composes these units into a fluent dance coherent to the music. In particular, to synchronize the diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a novel beat-align reward function. Additionally, we consider learning human dance poses in the rotation domain to avoid body distortions incompatible with human morphology, and introduce a musical contextual encoding to allow the motion GPT to\u00a0\u2026", "There has been a debate about the superiority between vision Transformers and ConvNets, serving as the backbone of computer vision models. Although they are usually considered as two completely different architectures, in this paper, we interpret vision Transformers as ConvNets with dynamic convolutions, which enables us to characterize existing Transformers and dynamic ConvNets in a unified framework and compare their design choices side by side. In addition, our interpretation can also guide the network design as researchers now can consider vision Transformers from the design space of ConvNets and vice versa. We demonstrate such potential through two specific studies. First, we inspect the role of softmax in vision Transformers as the activation function and find it can be replaced by commonly used ConvNets modules, such as ReLU and Layer Normalization, which results in a faster convergence rate and better performance. Second, following the design of depth-wise convolution, we create a corresponding depth-wise vision Transformer that is more efficient with comparable performance. The potential of the proposed unified interpretation is not limited to the given examples and we hope it can inspire the community and give rise to more advanced network architectures.", "Human pose and shape estimation (HPS) has attracted increasing attention in recent years. While most existing studies focus on HPS from 2D images or videos with inherent depth ambiguity, there are surging need to investigate HPS from 3D point clouds as depth sensors have been frequently employed in commercial devices. However, real-world sensory 3D points are usually noisy and incomplete, and also human bodies could have different poses of high diversity. To tackle these challenges, we propose a principled framework, PointHPS, for accurate 3D HPS from point clouds captured in real-world settings, which iteratively refines point features through a cascaded architecture. Specifically, each stage of PointHPS performs a series of downsampling and upsampling operations to extract and collate both local and global cues, which are further enhanced by two novel modules: 1) Cross-stage Feature Fusion (CFF) for multi-scale feature propagation that allows information to flow effectively through the stages, and 2) Intermediate Feature Enhancement (IFE) for body-aware feature aggregation that improves feature quality after each stage. To facilitate a comprehensive study under various scenarios, we conduct our experiments on two large-scale benchmarks, comprising i) a dataset that features diverse subjects and actions captured by real commercial sensors in a laboratory environment, and ii) controlled synthetic data generated with realistic considerations such as clothed humans in crowded outdoor scenes. Extensive experiments demonstrate that PointHPS, with its powerful point feature extraction and processing scheme, outperforms\u00a0\u2026", "Local motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images and wasting unnecessary computation on sharp pixels, especially for high-resolution images. This paper aims to adaptively and efficiently restore high-resolution locally blurred images. We propose a local motion deblurring vision Transformer (LMD-ViT) built on adaptive window pruning Transformer blocks (AdaWPT). To focus deblurring on local regions and reduce computation, AdaWPT prunes unnecessary windows, only allowing the active windows to be involved in the deblurring processes. The pruning operation relies on the blurriness confidence predicted by a confidence predictor that is trained end-to-end using a reconstruction loss with Gumbel-Softmax re-parameterization and a pruning loss guided by annotated blur masks. Our method removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements (+0.24dB) compared to state-of-the-art methods. In addition, our approach substantially reduces FLOPs by 66% and achieves more than a twofold increase in inference speed compared to Transformer-based deblurring methods. We will make our code and annotated blur masks publicly available.", "Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on confined training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. 1) For the data scaling, we perform a systematic investigation on 32 EHPS datasets, encompassing a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. 2) For the model scaling, we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into specialist models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF (62.3 mm PVE without finetuning).", "We aim to address a significant but understudied problem in the anime industry, namely the inbetweening of cartoon line drawings. Inbetweening involves generating intermediate frames between two black-and-white line drawings and is a time-consuming and expensive process that can benefit from automation. However, existing frame interpolation methods that rely on matching and warping whole raster images are unsuitable for line inbetweening and often produce blurring artifacts that damage the intricate line structures. To preserve the precision and detail of the line drawings, we propose a new approach, called AnimeInbet, which geometrizes raster line drawings into graphs of endpoints and reframes the inbetweening task as a graph fusion problem with vertex repositioning. Our method can effectively capture the sparsity and unique structure of line drawings while preserving the details during inbetweening. This is made possible through our novel modules, ie, vertex encoding, a vertex correspondence Transformer, an effective mechanism for vertex repositioning and a visibility predictor. To train our method, we introduce MixamoLine240, a new dataset of line drawings with ground truth vectorization and matching labels. Our experiments demonstrate that AnimeInbet synthesizes high-quality, clean, and complete intermediate line drawings, outperforming existing methods quantitatively and qualitatively, especially in cases with large motions.", "Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Our method constructs a Markov chain that transfers between the high-resolution image and the low-resolution image by shifting the residual between them, substantially improving the transition efficiency. Additionally, an elaborate noise schedule is developed to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at least comparable performance to current state-of-the-art methods on both synthetic and real-world datasets, even only with 15 sampling steps. Our code and model are available at https://github.com/zsyOAOA/ResShift.", "Exploiting pre-trained diffusion models for restoration has recently become a favored alternative to the traditional task-specific training approach. Previous works have achieved noteworthy success by limiting the solution space using explicit degradation models. However, these methods often fall short when faced with complex degradations as they generally cannot be precisely modeled. In this paper, we propose PGDiff by introducing partial guidance, a fresh perspective that is more adaptable to real-world degradations compared to existing works. Rather than specifically defining the degradation process, our approach models the desired properties, such as image structure and color statistics of high-quality images, and applies this guidance during the reverse diffusion process. These properties are readily available and make no assumptions about the degradation process. When combined with a diffusion prior, this partial guidance can deliver appealing results across a range of restoration tasks. Additionally, PGDiff can be extended to handle composite tasks by consolidating multiple high-quality image properties, achieved by integrating the guidance from respective tasks. Experimental results demonstrate that our method not only outperforms existing diffusion-prior-based approaches but also competes favorably with task-specific models.", "In this paper, we address the challenging problem of 3D toonification, which involves transferring the style of an artistic domain onto a target 3D face with stylized geometry and texture. Although fine-tuning a pre-trained 3D GAN on the artistic domain can produce reasonable performance, this strategy has limitations in the 3D domain. In particular, fine-tuning can deteriorate the original GAN latent space, which affects subsequent semantic editing, and requires independent optimization and storage for each new style, limiting flexibility and efficient deployment. To overcome these challenges, we propose DeformToon3D, an effective toonification framework tailored for hierarchical 3D GAN. Our approach decomposes 3D toonification into subproblems of geometry and texture stylization to better preserve the original latent space. Specifically, we devise a novel StyleField that predicts conditional 3D deformation to align a real-space NeRF to the style space for geometry stylization. Thanks to the StyleField formulation, which already handles geometry stylization well, texture stylization can be achieved conveniently via adaptive style mixing that injects information of the artistic domain into the decoder of the pre-trained 3D GAN. Due to the unique design, our method enables flexible style degree control and shape-texture-specific style swap. Furthermore, we achieve efficient training without any real-world 2D-3D training pairs but proxy samples synthesized from off-the-shelf 2D toonification models.", "Flow-based propagation and spatiotemporal Transformer are two mainstream mechanisms in video inpainting (VI). Despite the effectiveness of these components, they still suffer from some limitations that affect their performance. Previous propagation-based approaches are performed separately either in the image or feature domain. Global image propagation isolated from learning may cause spatial misalignment due to inaccurate optical flow. Moreover, memory or computational constraints limit the temporal range of feature propagation and video Transformer, preventing exploration of correspondence information from distant frames. To address these issues, we propose an improved framework, called ProPainter, which involves enhanced ProPagation and an efficient Transformer. Specifically, we introduce dual-domain propagation that combines the advantages of image and feature warping, exploiting global correspondences reliably. We also propose a mask-guided sparse video Transformer, which achieves high efficiency by discarding unnecessary and redundant tokens. With these components, ProPainter outperforms prior arts by a large margin of 1.46 dB in PSNR while maintaining appealing efficiency.", "The neural radiance field (NeRF) has shown promising results in preserving the fine details of objects and scenes. However, unlike explicit shape representations e.g., mesh, it remains an open problem to build dense correspondences across different NeRFs of the same category, which is essential in many downstream tasks. The main difficulties of this problem lie in the implicit nature of NeRF and the lack of ground-truth correspondence annotations. In this paper, we show it is possible to bypass these challenges by leveraging the rich semantics and structural priors encapsulated in a pre-trained NeRF-based GAN. Specifically, we exploit such priors from three aspects, namely (1) a dual deformation field that takes latent codes as global structural indicators, (2) a learning objective that regards generator features as geometric-aware local descriptors, and (3) a source of infinite object-specific NeRF samples. Our\u00a0\u2026", "Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.", "Mimicking realistic dynamics in 3D garment animations is a challenging task due to the complex nature of multi-layered garments and the variety of outer forces involved. Existing approaches mostly focus on single-layered garments driven by only human bodies and struggle to handle general scenarios. In this paper, we propose a novel data-driven method, called LayersNet, to model garment-level animations as particle-wise interactions in a micro physics system. We improve simulation efficiency by representing garments as patch-level particles in a two-level structural hierarchy. Moreover, we introduce a novel Rotation Equivalent Transformation that leverages the rotation invariance and additivity of physics systems to better model outer forces. To verify the effectiveness of our approach and bridge the gap between experimental environments and real-world scenarios, we introduce a new challenging dataset, D-LAYERS, containing 700K frames of dynamics of 4,900 different combinations of multi-layered garments driven by both human bodies and randomly sampled wind. Our experiments show that LayersNet achieves superior performance both quantitatively and qualitatively. We will make the dataset and code publicly available at https://mmlab-ntu.github.io/project/layersnet/index.html .", "Recent advances in face manipulation using StyleGAN have produced impressive results. However, StyleGAN is inherently limited to cropped aligned faces at a fixed image resolution it is pre-trained on. In this paper, we propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. To enable real face inversion and manipulation, we introduce a corresponding encoder that provides the first-layer feature of the extended StyleGAN in addition to the latent style code. We validate the effectiveness of our method using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including facial attribute editing, super-resolution, sketch/mask-to-face translation, and face toonification.", "Deep reinforcement learning has obtained significant breakthroughs in recent years. Most methods in deep-RL achieve good results via the maximization of the reward signal provided by the environment, typically in the form of discounted cumulative returns. Such reward signals represent the immediate feedback of a particular action performed by an agent. However, tasks with sparse reward signals are still challenging to on-policy methods. In this paper, we introduce an effective characterization of past reward statistics (which can be seen as long-term feedback signals) to supplement this immediate reward feedback. In particular, value functions are learned with multi-critics supervision, enabling complex value functions to be more easily approximated in on-policy learning, even when the reward signals are sparse. We also introduce a novel exploration mechanism called \"hot-wiring\" that can give a boost to seemingly trapped agents. We demonstrate the effectiveness of our advantage actor multi-critic (A2MC) method across the discrete domains in Atari games as well as continuous domains in the MuJoCo environments. A video demo is provided at https://youtu.be/zBmpf3Yz8tc.", "A cognitive robot usually needs to perform multiple tasks in practice and needs to locate the desired area for each task. Since deep learning has achieved substantial progress in image recognition, to solve this area detection problem, it is straightforward to label a functional area (affordance) image dataset and apply a well-trained deep-model-based classifier on all the potential image regions. However, annotating the functional area is time consuming and the requirement of large amount of training data limits the application scope. We observe that the functional area are usually related to the surrounding object context. In this work, we propose to use the existing object detection dataset and employ the object context as effective prior to improve the performance without additional annotated data. In particular, we formulate a two-stream network that fuses the object-related and functionality-related feature for\u00a0\u2026"]}}