{"full_name": "Luu Anh Tuan", "designation": "Assistant Professor, School of Computer Science and Engineering", "email": "anhtuan.luu@ntu.edu.sg", "image_path": "./profile_img/luu_anh_tuan.jpg", "biography": "Luu Anh Tuan is an \bAssistant Professor in the School of Computer Science and Engineering (SCSE) at Nanyang Technological University, Singapore. He received his MSc degree from National University of Singapore in 2011 and Ph.D. degree from Nanyang Technological University in 2017. He was a Post-doctoral Research Fellow at Massachusetts Institute of Technology (MIT) from 2018 to 2020 under the supervision of Professor Regina Bazillay. Prior to that, he was a Research Scientist at Institute for Infocomm Research (I2R), A*STAR from 2016 to 2018.Tuan's research interests lie in the intersection of Artificial Intelligence, Deep Learning, and Natural Language Processing. He has published over 40 papers on top-tier conferences and journals including NeurIPS, ICLR, ACL, EMNLP, KDD, WWW, TACL, AAAI, etc. Tuan also served as the Senior Area Chair of EMNLP 2020, Area Chair of ACL 2021, Senior Program Committee of IJCAI 2020-2021, and Program Committee member of NeuIPS, ICLR, ICML, AAAI, etc. and standing reviewer of Transaction of ACL (TACL), Computational Linguistics (CL), Transactions on Knowledge and Data Engineering (TKDE), ACM Computing Surveys. He is a recipient of the Ministry of Trade and Industry (MTI) Singapore Innovation Award in 2013.", "grants": "AI-Supported Collaborative Activity DesignPreparing for the Era of AI-powered Synthetic Advertising: Theoretical Development, Practical and Policy RecommendationsProject STORYSemi Supervised Learning for Knowledge Graph ConstructionTowards Robustness of Neural Networks Against Natural Language AttacksTowards Semantic-Aware Multimodal and Multilingual Deep Learning Systems for E-Commerce ApplicationsTRUSTWORTHY AI CENTRE NTU (TAICeN)TRUSTWORTHY AI CENTRE NTU (TAICeN) (NTU)TRUSTWORTHY AI CENTRE NTU (TAICeN) (NUS)TRUSTWORTHY AI CENTRE NTU (TAICeN) (SMU)", "google_scholar": "https://scholar.google.com/citations?user=d6ixOGYAAAAJ&hl=en&oi=ao", "orcid": null, "github": "https://tuanluu.github.io/", "scopus": null, "web_of_science": null, "dr_ntu": "https://dr.ntu.edu.sg/cris/rp/rp01296", "other_websites": [], "interests": ["AI", "NLP", "Pretrained Language Model", "Robustness & TrustworthyInformation Extraction & Semantics"], "bachelor_degree": null, "masters": "National University of Singapore", "phd": "Nanyang Technological University", "collaboration_network": {"target": ["Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Yi Tay", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Hui Siu Cheung", "Vijay Prakash Dwivedi", "Vijay Prakash Dwivedi", "Vijay Prakash Dwivedi", "Vijay Prakash Dwivedi", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "See-Kiong Ng", "Yoshua Bengio", "Yoshua Bengio", "Xavier Bresson", "Xavier Bresson", "Jung-jae Kim", "Jung-jae Kim", "Jung-jae Kim", "Jung-jae Kim", "Jung-jae Kim", "Xinshuai Dong", "Xinshuai Dong", "Xinshuai Dong", "Xinshuai Dong", "Xinshuai Dong", "Xinshuai Dong", "Shuohang Wang", "Shuohang Wang", "Shuohang Wang", "Jin Song Dong", "Jin Song Dong", "Jin Song Dong", "Jin Song Dong", "Liu Yang", "Liu Yang", "Liu Yang", "Liu Yang", "Darsh J Shah", "Sun Jun"], "target_id": ["VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "VBclY_cAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "d4ZYx6gAAAAJ", "8MS7iC0AAAAJ", "8MS7iC0AAAAJ", "8MS7iC0AAAAJ", "8MS7iC0AAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "_wsommYAAAAJ", "kukA0LcAAAAJ", "kukA0LcAAAAJ", "9pSK04MAAAAJ", "9pSK04MAAAAJ", "iMKgkrQAAAAJ", "iMKgkrQAAAAJ", "iMKgkrQAAAAJ", "iMKgkrQAAAAJ", "iMKgkrQAAAAJ", "A7JyL1sAAAAJ", "A7JyL1sAAAAJ", "A7JyL1sAAAAJ", "A7JyL1sAAAAJ", "A7JyL1sAAAAJ", "A7JyL1sAAAAJ", "mN-IO6wAAAAJ", "mN-IO6wAAAAJ", "mN-IO6wAAAAJ", "tuLa1AsAAAAJ", "tuLa1AsAAAAJ", "tuLa1AsAAAAJ", "tuLa1AsAAAAJ", "_Pvgwd0AAAAJ", "_Pvgwd0AAAAJ", "_Pvgwd0AAAAJ", "_Pvgwd0AAAAJ", "XscFiX8AAAAJ", "DVsEyn0AAAAJ"], "type": ["Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "NTU", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside SCSE", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "Outside NTU", "NTU", "NTU", "NTU", "NTU", "Outside NTU", "Outside NTU"], "location": ["Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Google", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "University of Montreal", "University of Montreal", "National University of Singapore", "National University of Singapore", "Unknown", "Unknown", "Unknown", "Unknown", "Unknown", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Carnegie Mellon University", "Microsoft", "Microsoft", "Microsoft", "National University of Singapore", "National University of Singapore", "National University of Singapore", "National University of Singapore", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "Nanyang Technological University", "MIT", "Singapore Management University"], "year": [2018, 2017, 2018, 2018, 2017, 2018, 2019, 2019, 2018, 2019, 2018, 2018, 2017, 2017, 2016, 2018, 2019, 2020, 2017, 2018, 2018, 2018, 2019, 2017, 2018, 2018, 2017, 2018, 2017, 2018, 2017, 2016, 2018, 2023, 2018, 2017, 2018, 2019, 2019, 2018, 2018, 2018, 2017, 2017, 2016, 2018, 2018, 2019, 2017, 2021, 2018, 2018, 2018, 2019, 2018, 2018, 2018, 2017, 2018, 2018, 2022, 2022, 2022, 2023, 2013, 2016, 2016, 2015, 2014, 2023, 2022, 2022, 2023, 2023, 2023, 2022, 2023, 2022, 2023, 2015, 2012, 2014, 2012, 2014, 2023, 2022, 2021, 2023, 2022, 2023, 2019, 2019, 2019, "unknown", "unknown", 2012, "unknown", "unknown", "unknown", 2012, "unknown", 2020, 2012], "title": ["Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering", "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis", "Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking", "Multi-cast attention networks", "SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring", "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs", "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives", "Compositional De-Attention Networks", "Multi-granular sequence encoding via dilated compositional units for reading comprehension", "Holistic multi-modal memory network for movie question answering", "Multi-Pointer Co-Attention Networks for Recommendation", "Densely connected attention propagation for reading comprehension", "Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs", "Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture", "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network", "CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation", "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks", "Would you rather? a new benchmark for learning machine alignment with cultural values and social preferences", "Non-Parametric Estimation of Multiple Embeddings for Link Prediction on Dynamic Knowledge Graphs", "Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains.", "Recurrently controlled recurrent networks", "Co-stack residual affinity networks with multi-level attention refinement for matching text sequences", "Holographic Factorization Machines for Recommendation", "Random Semantic Tensor Ensemble for Scalable Knowledge Graph Link Prediction", "Reasoning with Sarcasm by Reading In-between", "Attentive gated lexicon reader with contrastive contextual co-attention for sentiment classification", "Dyadic Memory Networks for Aspect-based Sentiment Analysis", "Compare, compress and propagate: Enhancing neural architectures with alignment factorization for natural language inference", "A Syntactic Parse-Key Tree-Based Approach for English Grammar Question Retrieval", "Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering", "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis", "Utilizing Temporal Information for Taxonomy Construction", "Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking", "A contrastive learning framework for Event Detection via semantic type prototype representation modelling", "Multi-cast attention networks", "SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring", "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs", "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives", "Compositional De-Attention Networks", "Multi-granular sequence encoding via dilated compositional units for reading comprehension", "Multi-Pointer Co-Attention Networks for Recommendation", "Densely connected attention propagation for reading comprehension", "Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs", "Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture", "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network", "CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation", "Multi-range reasoning for machine comprehension", "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks", "Non-Parametric Estimation of Multiple Embeddings for Link Prediction on Dynamic Knowledge Graphs", "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with  Parameters", "Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains.", "Recurrently controlled recurrent networks", "Co-stack residual affinity networks with multi-level attention refinement for matching text sequences", "Holographic Factorization Machines for Recommendation", "Reasoning with Sarcasm by Reading In-between", "Syntactic based approach for grammar question retrieval", "Attentive gated lexicon reader with contrastive contextual co-attention for sentiment classification", "Dyadic Memory Networks for Aspect-based Sentiment Analysis", "Personalized question recommendation for English grammar learning", "Compare, compress and propagate: Enhancing neural architectures with alignment factorization for natural language inference", "Long Range Graph Benchmark", "Graph Neural Networks with Learnable Structural and Positional Representations", "Recipe for a General, Powerful, Scalable Graph Transformer", "Benchmarking graph neural networks", "Gene ontology concept recognition using crossproducts and statistical methods", "Utilizing Temporal Information for Taxonomy Construction", "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network", "Incorporating Trustiness and Collective Synonym/Contrastive Evidence into Taxonomy Construction", "Taxonomy Construction Using Syntactic Contextual Evidence", "Topic-Aware Causal Intervention for Counterfactual Detection", "Vision-and-Language Pretraining", "Multi-Sample Contrastive Neural Topic Model as Multi-Task Learning", "Vision-and-Language Pretraining: Methods, Applications, and Future Challenges", "Enhancing Large Language Model Induced Task-Oriented Dialogue Systems Through Look-Forward Motivated Goals", "Using Punctuation as an Adversarial Attack on Deep Learning-Based NLP Systems: An Empirical Study", "Graph Neural Networks with Learnable Structural and Positional Representations", "Benchmarking graph neural networks", "Graph Neural Networks with Learnable Structural and Positional Representations", "Benchmarking graph neural networks", "Incorporating Trustiness and Collective Synonym/Contrastive Evidence into Taxonomy Construction", "Hybrid pattern matching for complex ontology term recognition", "Taxonomy Construction Using Syntactic Contextual Evidence", "Automatic suggestion for pubmed query reformulation", "Overview of the gene ontology task at BioCreative IV", "Effective Neural Topic Modeling with Embedding Clustering Regularization", "Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning", "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?", "Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction", "Certified Robustness Against Natural Language Attacks by Causal Intervention", "InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling", "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks", "Compositional De-Attention Networks", "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives", "A Framework for Automatic Verifying Privacy Properties of Security Protocols", "Technical report: SeVe implementation", "SeVe: automatic tool for verification of security protocols", "Automated Verifying Anonymity and Privacy Properties of Security Protocols", "A Framework for Automatic Verifying Privacy Properties of Security Protocols", "Technical report: SeVe implementation", "SeVe: automatic tool for verification of security protocols", "Automated Verifying Anonymity and Privacy Properties of Security Protocols", "Capturing Greater Context for Question Generation", "SeVe: automatic tool for verification of security protocols"], "link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:hMod-77fHWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:k_IJM867U9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:70eg2SAEIzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:2P1L_qKh6hAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:RHpTSmoSYBkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:M05iB0D1s5AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:vV6vV6tmYwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:e5wmG9Sq2KIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:hMod-77fHWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:k_IJM867U9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:7T2F9Uy0os0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:70eg2SAEIzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:2P1L_qKh6hAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:RHpTSmoSYBkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:pyW8ca7W8N0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:M05iB0D1s5AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:4OULZ7Gr8RgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:vV6vV6tmYwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:bnK-pcrLprsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:5ugPr518TE4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:J-pR_7NvFogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:-_dYPAW6P2MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:ZfRJV9d4-WMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:Fu2w8maKXqMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:kzcrU_BdoSEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:35r97b3x0nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:J-pR_7NvFogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:J-pR_7NvFogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:JoZmwDi-zQgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u9iWguZQMMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:SdhP9T11ey4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:5awf1xo2G04C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:qjMakFHDy7sC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u5HHmVD_uO8C"]}, "published_by_year": {"Year": ["2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Publications": [1, 0, 0, 2, 0, 3, 1, 2, 1, 2, 11, 17, 6, 3, 5, 13, 23, 4]}, "citations_by_year": {"Year": ["2007", "2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "unknown"], "# of Citations": [0, 0, 0, 1, 3, 6, 13, 15, 27, 38, 48, 252, 469, 573, 655, 730, 1025, 510]}, "all_time_h_index": 32, "all_time_i10_index": 51, "h_index_by_publication_year": {"Publication Year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [1, 0, 0, 2, 0, 2, 1, 2, 1, 2, 9, 15, 6, 3, 5, 6, 4]}, "avg_citations_by_publication_year": {"Publication Year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "Avg Citations per Publication": [1.0, 0.0, 0.0, 31.0, 0.0, 7.333333333333333, 7.0, 57.0, 14.0, 48.0, 74.81818181818181, 85.58823529411765, 54.166666666666664, 35.666666666666664, 45.8, 26.53846153846154, 11.08695652173913]}, "h_index_by_year": {"Year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023], "h-index": [0, 0, 0, 1, 1, 1, 2, 2, 4, 4, 6, 12, 18, 21, 24, 28, 32]}, "h_index_by_years_from_publication_year": {"Publication Year": [2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2017, 2017, 2017, 2017, 2017, 2017, 2017, 2018, 2018, 2018, 2018, 2018, 2018, 2019, 2019, 2019, 2019, 2019, 2020, 2020, 2020, 2020, 2021, 2021, 2021, 2022, 2022, 2023], "Year": [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2018, 2019, 2020, 2021, 2022, 2023, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2021, 2022, 2023, 2022, 2023, 2023], "h-index": [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 7, 8, 9, 9, 9, 9, 6, 10, 13, 14, 15, 15, 3, 4, 6, 6, 6, 1, 3, 3, 3, 2, 5, 5, 4, 6, 4]}, "all_time_i20_index": 43, "publications": {"Publication Year": ["2023", "2018", "2018", "2017", "2017", "2018", "2018", "2018", "2017", "2022", "2022", "2019", "2017", "2017", "2016", "2021", "2019", "2018", "2019", "2021", "2018", "2014", "2010", "2020", "2014", "2018", "2022", "2018", "2018", "2017", "2020", "2018", "2018", "2017", "2021", "2021", "2017", "2021", "2017", "2019", "2018", "2018", "2022", "2019", "2018", "2019", "2020", "2015", "2012", "2023", "2023", "2022", "2018", "2018", "2022", "2013", "2012", "2022", "2023", "2023", "2010", "2023", "2022", "2022", "2023", "2016", "2023", "2023", "2012", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "2017", "2017", "2007", "Unknown", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2023", "2022", "2022", "Unknown", "Unknown", "Unknown"], "Title": ["Benchmarking graph neural networks", "Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking", "Multi-Pointer Co-Attention Networks for Recommendation", "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based Sentiment Analysis", "Non-Parametric Estimation of Multiple Embeddings for Link Prediction on Dynamic Knowledge Graphs", "Compare, compress and propagate: Enhancing neural architectures with alignment factorization for natural language inference", "Reasoning with Sarcasm by Reading In-between", "Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering", "Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture", "Graph Neural Networks with Learnable Structural and Positional Representations", "Recipe for a General, Powerful, Scalable Graph Transformer", "NURBS-based postbuckling analysis of functionally graded carbon nanotube-reinforced composite shells", "SkipFlow: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring", "Dyadic Memory Networks for Aspect-based Sentiment Analysis", "Learning Term Embeddings for Taxonomic Relation Identification Using Dynamic Weighting Neural Network", "Towards Robustness Against Natural Language Word Substitutions", "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives", "Multi-cast attention networks", "Lightweight and Efficient Neural Natural Language Processing with Quaternion Networks", "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with  Parameters", "Densely connected attention propagation for reading comprehension", "Taxonomy Construction Using Syntactic Contextual Evidence", "Modeling and verification of safety critical systems: A case study on pacemaker", "Capturing Greater Context for Question Generation", "Overview of the gene ontology task at BioCreative IV", "Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains.", "Long Range Graph Benchmark", "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs", "Multi-range reasoning for machine comprehension", "Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs", "Deep learning to estimate RECIST in patients with NSCLC treated with PD-1 blockade", "Co-stack residual affinity networks with multi-level attention refinement for matching text sequences", "CoupleNet: Paying Attention to Couples with Coupled Attention for Relationship Recommendation", "Enabling Efficient Question Answer Retrieval via Hyperbolic Neural Networks", "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?", "Contrastive Learning for Neural Topic Model", "Random Semantic Tensor Ensemble for Scalable Knowledge Graph Link Prediction", "Enriching and Controlling Global Semantics for Text Summarization", "Truly multi-modal youtube-8m video classification with video, audio, and text", "Compositional De-Attention Networks", "Recurrently controlled recurrent networks", "Personalized question recommendation for English grammar learning", "Improving neural cross-lingual abstractive summarization via employing optimal transport distance for knowledge distillation", "Holographic Factorization Machines for Recommendation", "Attentive gated lexicon reader with contrastive contextual co-attention for sentiment classification", "Holistic multi-modal memory network for movie question answering", "Would you rather? a new benchmark for learning machine alignment with cultural values and social preferences", "Incorporating Trustiness and Collective Synonym/Contrastive Evidence into Taxonomy Construction", "SeVe: automatic tool for verification of security protocols", "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models", "Certified Robustness Against Natural Language Attacks by Causal Intervention", "Multi-granular sequence encoding via dilated compositional units for reading comprehension", "Syntactic based approach for grammar question retrieval", "Is Discourse Role Important for Emotion Recognition in Conversation?", "Gene ontology concept recognition using crossproducts and statistical methods", "Automatic suggestion for pubmed query reformulation", "Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning", "A brief survey on recent advances in coreference resolution", "Fact-Checking Complex Claims with Program-Guided Reasoning", "Modeling and verifying security protocols using PAT approach", "Effective Neural Topic Modeling with Embedding Clustering Regularization", "Adaptive Contrastive Learning on Multimodal Transformer for Review Helpfulness Predictions", "Textual Manifold-based Defense Against Natural Language Adversarial Examples", "InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling", "Utilizing Temporal Information for Taxonomy Construction", "Using Punctuation as an Adversarial Attack on Deep Learning-Based NLP Systems: An Empirical Study", "Towards Interpretable Federated Learning", "Hybrid pattern matching for complex ontology term recognition", "A novel, cognitively inspired, unified graph-based multi-task framework for information extraction", "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System", "A Survey on Neural Topic Models: Methods, Applications, and Challenges", "Zero-Shot Text Classification via Self-Supervised Tuning", "Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation", "Vision-and-Language Pretraining", "Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition", "Automatic taxonomy construction from textual documents", "A Syntactic Parse-Key Tree-Based Approach for English Grammar Question Retrieval", "Axiom-oriented Reasoning to Deal with Inconsistency Between Ontology and Knowledge Base", "Learning facial expression and body gesture visual information for video emotion recognition", "A contrastive learning framework for Event Detection via semantic type prototype representation modelling", "Rethinking Negative Pairs in Code Search", "Topic-Aware Causal Intervention for Counterfactual Detection", "Knowledge Bases and Language Models: Complementing Forces", "Enhancing Large Language Model Induced Task-Oriented Dialogue Systems Through Look-Forward Motivated Goals", "Towards the TopMost: A Topic Modeling System Toolkit", "Vision-and-Language Pretraining: Methods, Applications, and Future Challenges", "Multi-Scale Receptive Field Graph Model for Emotion Recognition in Conversations", "Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction", "How to train your draGAN: A task oriented solution to imbalanced classification", "Multi-Sample Contrastive Neural Topic Model as Multi-Task Learning", "A Framework for Automatic Verifying Privacy Properties of Security Protocols", "Automated Verifying Anonymity and Privacy Properties of Security Protocols", "Technical report: SeVe implementation"], "Link": ["https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:J-pR_7NvFogC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:M3NEmzRMIkIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:blknAaTinKkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:k_IJM867U9cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:TQgYirikUcIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:maZDTaKrznsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:GnPB-g6toBAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:hMod-77fHWUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:RHpTSmoSYBkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:uWQEDVKXjbEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:5ugPr518TE4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:V3AGJWp-ZtQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:TFP_iSt0sucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:iH-uZ7U-co4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:-f6ydRqryjwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:f2IySw72cVMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:HoB7MX3m0LUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:J_g5lzvAfSwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:ZHo1McVdvXMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&citation_for_view=d6ixOGYAAAAJ:pyW8ca7W8N0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:2P1L_qKh6hAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:IWHjjKOFINEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:d1gkVwhDpl0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u_35RYKgDlwC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:ZeXyd9-uunAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:yD5IFk8b50cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:bnK-pcrLprsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:isC4tDSrTZIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:YFjsv_pBGBYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:r0BpntZqJG4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:D03iK_w7-QYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:lSLTfruPkqcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:O3NaXMp0MMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:j3f4tGmQtD8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u9iWguZQMMsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:OU6Ihb5iCvQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:mB3voiENLucC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:_xSYboBqXhAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:JV2RwH3_ST0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:zA6iFVUQeVQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:M05iB0D1s5AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:bEWYMUwI8FkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:dTyEYWd-f8wC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:g5m5HwL7SMYC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:vV6vV6tmYwMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:ldfaerwXgEUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:cFHS6HbyZ2cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:qUcmZB5y_30C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:u5HHmVD_uO8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:tKAzc9rXhukC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:eMMeJKvmdy0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:wbdj-CoPYUoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:70eg2SAEIzsC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:4OULZ7Gr8RgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:XiVPGOgt02cC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:hC7cP41nSMkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:Wp0gIr-vW9MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:WA5NYHcadZ8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:LjlpjdlvIbIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:WqliGbK-hY8C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:9yKSN-GCB0IC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:JoZmwDi-zQgC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:HE397vMXCloC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:t6usbXjVLHcC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:5awf1xo2G04C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:hFOr9nPyWt4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:eq2jaN3J8jMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:tkaPQYYpVKoC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:_kc_bZDykSQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:NJ774b8OgUMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:W5xh706n7nkC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:_Ybze24A_UAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:9vf0nzSNQJEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:uLbwQdceFCQC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:ZfRJV9d4-WMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:olpn-zPbct0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:fPk4N6BV_jEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:e5wmG9Sq2KIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:dhFuZR0502QC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:tzM49s52ZIMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:7T2F9Uy0os0C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:fEOibwPWpKIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:-_dYPAW6P2MC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:_Re3VWB3Y0AC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:35r97b3x0nAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:evX43VCCuoAC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:kzcrU_BdoSEC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:Y5dfb0dijaUC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:SdhP9T11ey4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:_B80troHkn4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:Fu2w8maKXqMC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:IjCSPb-OGe4C", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:UeHWp8X0CEIC", "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=d6ixOGYAAAAJ&cstart=20&pagesize=80&citation_for_view=d6ixOGYAAAAJ:qjMakFHDy7sC"], "Topic": ["Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Others", "Artificial Intelligenc", "Others", "Others", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Artificial Intelligenc", "Federated Learning", "Others", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Others", "Others", "Others", "Artificial Intelligenc", "Software Engineering", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Artificial Intelligenc", "Others", "Others", "Others"], "# of Citations": [713, 307, 303, 179, 176, 156, 140, 131, 129, 126, 124, 120, 120, 96, 93, 84, 83, 78, 66, 66, 60, 60, 58, 57, 54, 48, 46, 45, 40, 40, 37, 37, 31, 30, 28, 26, 27, 26, 26, 22, 22, 21, 19, 19, 20, 15, 13, 14, 13, 10, 10, 9, 9, 8, 7, 7, 7, 6, 4, 5, 5, 4, 4, 4, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "Description": ["In the last few years, graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. This emerging field has witnessed an extensive growth of promising techniques that have been applied with success to computer science, mathematics, biology, physics and chemistry. But for any successful field to become mainstream and reliable, benchmarks must be developed to quantify progress. This led us in March 2020 to release a benchmark framework that i) comprises of a diverse collection of mathematical and real-world graphs, ii) enables fair model comparison with the same parameter budget to identify key architectures, iii) has an open-source, easy-to-use and reproducible code infrastructure, and iv) is flexible for researchers to experiment with new theoretical ideas. As of December 2022, the GitHub repository 1 has reached 2,000 stars and 380 forks, which demonstrates the utility of the proposed open-source framework through the wide usage by the GNN community. In this paper, we present an updated version of our benchmark with a concise presentation of the aforementioned framework characteristics, an additional medium-sized molecular dataset AQSOL, similar to the popular ZINC, but with a real-world measured chemical target, and discuss how this framework can be leveraged to explore new GNN designs and insights. As a proof of value of our benchmark, we study the case of graph positional encoding (PE) in GNNs, which was introduced with this benchmark and has since spurred interest of exploring more powerful PE for Transformers and GNNs in a robust experimental setting.", "This paper proposes a new neural architecture for collaborative ranking with implicit feedback. Our model, LRML (Latent Relational Metric Learning) is a novel metric learning approach for recommendation. More specifically, instead of simple push-pull mechanisms between user and item pairs, we propose to learn latent relations that describe each user item interaction. This helps to alleviate the potential geometric inflexibility of existing metric learning approaches. This enables not only better performance but also a greater extent of modeling capability, allowing our model to scale to a larger number of interactions. In order to do so, we employ a augmented memory module and learn to attend over these memory blocks to construct latent relations. The memory-based attention module is controlled by the user-item interaction, making the learned relation vector specific to each user-item pair. Hence, this can be\u00a0\u2026", "Many recent state-of-the-art recommender systems such as D-ATT, TransNet and DeepCoNN exploit reviews for representation learning. This paper proposes a new neural architecture for recommendation with reviews. Our model operates on a multi-hierarchical paradigm and is based on the intuition that not all reviews are created equal, i.e., only a selected few are important. The importance, however, should be dynamically inferred depending on the current target. To this end, we propose a review-by-review pointer-based learning scheme that extracts important reviews from user and item reviews and subsequently matches them in a word-by-word fashion. This enables not only the most informative reviews to be utilized for prediction but also a deeper word-level interaction. Our pointer-based method operates with a gumbel-softmax based pointer mechanism that enables the incorporation of discrete vectors\u00a0\u2026", "Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a given document with respect to a given aspect entity. While neural network architectures have been successful in predicting the overall polarity of sentences, aspect-specific sentiment analysis still remains as an open problem. In this paper, we propose a novel method for integrating aspect information into the neural model. More specifically, we incorporate aspect information into the neural model by modeling word-aspect relationships. Our novel model, Aspect Fusion LSTM (AF-LSTM) learns to attend based on associative relationships between sentence words and aspect which allows our model to adaptively focus on the correct words given an aspect term. This ameliorates the flaws of other state-of-the-art models that utilize naive concatenations to model word-aspect similarity. Instead, our model adopts circular convolution and circular correlation to model the similarity between aspect and words and elegantly incorporates this within a differentiable neural attention framework. Finally, our model is end-to-end differentiable and highly related to convolution-correlation (holographic like) memories. Our proposed neural model achieves state-of-the-art performance on benchmark datasets, outperforming ATAE-LSTM by 4%-5% on average across multiple datasets.", "Knowledge graphs play a significant role in many intelligent systems such as semantic search and recommendation systems. Recent works in this area of knowledge graph embeddings such as TransE, TransH and TransR have shown extremely competitive and promising results in relational learning. In this paper, we propose a novel extension of the translational embedding model to solve three main problems of the current models. Firstly, translational models are highly sensitive to hyperparameters such as margin and learning rate. Secondly, the translation principle only allows one spot in vector space for each golden triplet. Thus, congestion of entities and relations in vector space may reduce precision. Lastly, the current models are not able to handle dynamic data especially the introduction of new unseen entities/relations or removal of triplets. In this paper, we propose Parallel Universe TransE (puTransE), an adaptable and robust adaptation of the translational model. Our approach non-parametrically estimates the energy score of a triplet from multiple embedding spaces of structurally and semantically aware triplet selection. Our proposed approach is simple, robust and parallelizable. Our experimental results show that our proposed approach outperforms TransE and many other embedding methods for link prediction on knowledge graphs on both public benchmark dataset and a real world dynamic dataset.", "This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a  times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.", "Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of modeling contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.", "The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, HyperQA, is a parameter efficient neural network that outperforms other parameter intensive models such as\u00a0\u2026", "We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory (LSTM) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the benefits of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the LSTM are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive\u00a0\u2026", "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes.", "We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being ,  or . The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the first architecture with a complexity linear in the number of nodes and edges  by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients:(i) positional/structural encoding,(ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework  that supports multiple types of encodings and that provides efficiency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical benefits gained by the modularity and the combination of different strategies.", "An investigation into the postbuckling and geometrically nonlinear behaviors of functionally graded carbon nanotube-reinforced composite (FG-CNTRC) shells is carried out in this study. The discrete nonlinear equation system is established based on non-uniform rational B-Spline (NURBS) basis functions and the first-order shear deformation shell theory (FSDT). The nonlinearity of shells is formed in the Total Lagrangian approach considering the von Karman assumption. The incremental solutions are obtained by using a modified Riks method. In the present formulation, the rule of mixture is used to estimate the effective material properties of FG-CNTRC shells. Effects of CNTs distribution, volume fraction and CNTs orientation on the postbuckling behavior of FG-CNTRC shells are particularly investigated. Exact geometries of shells are modeled by using NURBS interpolation. Several verifications are given to show\u00a0\u2026", "Deep learning has demonstrated tremendous potential for Automatic Text Scoring (ATS) tasks. In this paper, we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features. Our new method proposes a new SkipFlow mechanism that models relationships between snapshots of the hidden representations of a long short-term memory (LSTM) network as it reads. Subsequently, the semantic relationships between multiple snapshots are used as auxiliary features for prediction. This has two main benefits. Firstly, essays are typically long sequences and therefore the memorization capability of the LSTM network may be insufficient. Implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients. The parameters of the SkipFlow mechanism also acts as an auxiliary memory. Secondly, modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence. In our model, we call this neural coherence features. Overall, we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion. Our approach demonstrates state-of-the-art performance on the benchmark ASAP dataset, outperforming not only feature engineering baselines but also other deep learning models.", "This paper proposes Dyadic Memory Networks (DyMemNN), a novel extension of end-to-end memory networks (memNN) for aspect-based sentiment analysis (ABSA). Originally designed for question answering tasks, memNN operates via a memory selection operation in which relevant memory pieces are adaptively selected based on the input query. In the problem of ABSA, this is analogous to aspects and documents in which the relationship between each word in the document is compared with the aspect vector. In the standard memory networks, simple dot products or feed forward neural networks are used to model the relationship between aspect and words which lacks representation learning capability. As such, our dyadic memory networks ameliorates this weakness by enabling rich dyadic interactions between aspect and word embeddings by integrating either parameterized neural tensor compositions\u00a0\u2026", "Taxonomic relation identification aims to recognize the \u2018is-a\u2019relation between two terms. Previous works on identifying taxonomic relations are mostly based on statistical and linguistic approaches, but the accuracy of these approaches is far from satisfactory. In this paper, we propose a novel supervised learning approach for identifying taxonomic relations using term embeddings. For this purpose, we first design a dynamic weighting neural network to learn term embeddings based on not only the hypernym and hyponym terms, but also the contextual information between them. We then apply such embeddings as features to identify taxonomic relations using a supervised method. The experimental results show that our proposed approach significantly outperforms other state-of-the-art methods by 9% to 13% in terms of accuracy for both general and specific domain datasets.", "Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either -ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel \\textit{Adversarial Sparse Convex Combination} (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on the ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, \\emph{i.e.}, sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.", "This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by  relative improvement on BLEU-4 and  relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components.", "Attention is typically used to select informative sub-phrases that are used for prediction. This paper investigates the novel use of attention as a form of feature augmentation, i.e, casted attention. We propose Multi-Cast Attention Networks (MCAN), a new attention mechanism and general model architecture for a potpourri of ranking tasks in the conversational modeling and question answering domains. Our approach performs a series of soft attention operations, each time casting a scalar feature upon the inner word embeddings. The key idea is to provide a real-valued hint (feature) to a subsequent encoder layer and is targeted at improving the representation learning process. There are several advantages to this design, e.g., it allows an arbitrary number of attention mechanisms to be casted, allowing for multiple attention types (e.g., co-attention, intra-attention) and attention variants (e.g., alignment-pooling, max\u00a0\u2026", "Many state-of-the-art neural models for NLP are heavily parameterized and thus memory inefficient. This paper proposes a series of lightweight and memory efficient neural architectures for a potpourri of natural language processing (NLP) tasks. To this end, our models exploit computation using Quaternion algebra and hypercomplex spaces, enabling not only expressive inter-component interactions but also significantly () reduced parameter size due to lesser degrees of freedom in the Hamilton product. We propose Quaternion variants of models, giving rise to new architectures such as the Quaternion attention Model and Quaternion Transformer. Extensive experiments on a battery of NLP tasks demonstrates the utility of proposed Quaternion-inspired models, enabling up to  reduction in parameter size without significant loss in performance.", "Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, \"fully-connected layers with Quaternions\" (4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of Quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary nD hypercomplex space, providing more architectural flexibility using arbitrarily  learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and Transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach.", "We propose DecaProp (Densely Connected Attention Propagation), a new densely connected neural architecture for reading comprehension (RC). There are two distinct characteristics of our model. Firstly, our model densely connects all pairwise layers of the network, modeling relationships between passage and query across all hierarchical levels. Secondly, the dense connectors in our network are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging connections throughout the network. We conduct extensive experiments on four challenging RC benchmarks. Our proposed approach achieves state-of-the-art results on all four, outperforming existing baselines by up to 2.6% to 14.2% in absolute F1 score.", "Taxonomies are the backbone of many structured, semantic knowledge resources. Recent works for extracting taxonomic relations from text focused on collecting lexical-syntactic patterns to extract the taxonomic relations by matching the patterns to text. These approaches, however, often show low coverage due to the lack of contextual analysis across sentences. To address this issue, we propose a novel approach that collectively utilizes contextual information of terms in syntactic structures such that if the set of contexts of a term includes most of contexts of another term, a subsumption relation between the two terms is inferred. We apply this method to the task of taxonomy construction from scratch, where we introduce another novel graph-based algorithm for taxonomic structure induction. Our experiment results show that the proposed method is well complementary with previous methods of linguistic pattern matching and significantly improves recall and thus F-measure.", "The pacemaker challenge proposed by Software Quality Research Laboratory is looking for formal methods to produce precise and reliable systems. Safety critical systems like pacemaker need to guarantee important properties (like deadlock-free, safety, etc.), which concern human lives. Formal methods have been applied in designing safety critical systems with verified desirable properties. In this paper, we propose a formal model of pacemaker, modeling its behaviors and its communication with the external environment, using a real-time formalism. Critical properties, such as deadlock freeness and heart rate limits are then verified using the model checker PAT(Process Analysis Toolkit). This work yields a verified formal model of pacemaker systems, which can serve as specification for real pacemaker implementations.", "Automatic question generation can benefit many applications ranging from dialogue systems to reading comprehension. While questions are often asked with respect to long documents, there are many challenges with modeling such long documents. Many existing techniques generate questions by effectively looking at one sentence at a time, leading to questions that are easy and not reflective of the human process of question generation. Our goal is to incorporate interactions across multiple sentences to generate realistic questions for long documents. In order to link a broad document context to the target answer, we represent the relevant context via a multi-stage attention mechanism, which forms the foundation of a sequence to sequence model. We outperform state-of-the-art methods on question generation on three question-answering datasets-SQuAD, MS MARCO and NewsQA. 1", "Gene Ontology (GO) annotation is a common task among model organism databases (MODs) for capturing gene function data from journal articles. It is a time-consuming and labor-intensive task, and is thus often considered as one of the bottlenecks in literature curation. There is a growing need for semiautomated or fully automated GO curation techniques that will help database curators to rapidly and accurately identify gene function information in full-length articles. Despite multiple attempts in the past, few studies have proven to be useful with regard to assisting real-world GO curation. The shortage of sentence-level training data and opportunities for interaction between text-mining developers and GO curators has limited the advances in algorithm development and corresponding use in practical circumstances. To this end, we organized a text-mining challenge task for literature-based GO annotation in\u00a0\u2026", "Co-Attentions are highly effective attention mechanisms for text matching applications. Co-Attention enables the learning of pairwise attentions, ie, learning to attend based on computing word-level affinity scores between two documents. However, text matching problems can exist in either symmetrical or asymmetrical domains. For example, paraphrase identification is a symmetrical task while question-answer matching and entailment classification are considered asymmetrical domains. In this paper, we argue that Co-Attention models in asymmetrical domains require different treatment as opposed to symmetrical domains, ie, a concept of word-level directionality should be incorporated while learning word-level similarity scores. Hence, the standard inner product in real space commonly adopted in co-attention is not suitable. This paper leverages attractive properties of the complex vector space and proposes a co-attention mechanism based on the complex-valued inner product (Hermitian products). Unlike the real dot product, the dot product in complex space is asymmetric because the first item is conjugated. Aside from modeling and encoding directionality, our proposed approach also enhances the representation learning process. Extensive experiments on five text matching benchmark datasets demonstrate the effectiveness of our approach.", "Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: , , ,  and  that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP GNNs and Graph Transformer architectures that are intended to capture LRI.", "Temporal gates play a significant role in modern recurrent-based neural encoders, enabling fine-grained control over recursive compositional operations over time. In recurrent models such as the long short-term memory (LSTM), temporal gates control the amount of information retained or discarded over time, not only playing an important role in influencing the learned representations but also serving as a protection against vanishing gradients. This paper explores the idea of learning temporal gates for sequence pairs (question and answer), jointly influencing the learned representations in a pairwise manner. In our approach, temporal gates are learned via 1D convolutional layers and then subsequently cross applied across question and answer for joint learning. Empirically, we show that this conceptually simple sharing of temporal gates can lead to competitive performance across multiple benchmarks. Intuitively, what our network achieves can be interpreted as learning representations of question and answer pairs that are aware of what each other is remembering or forgetting, ie, pairwise temporal gating. Via extensive experiments, we show that our proposed model achieves state-of-the-art performance on two community-based QA datasets and competitive performance on one factoid-based QA dataset.", "We propose MRU (Multi-Range Reasoning Units), a new fast compositional encoder for machine comprehension (MC). Our proposed MRU encoders are characterized by multi-ranged gating, executing a series of parameterized contract-and-expand layers for learning gating vectors that benefit from long and short-term dependencies. The aims of our approach are as follows: (1) learning representations that are concurrently aware of long and short-term context, (2) modeling relationships between intra-document blocks and (3) fast and efficient sequence encoding. We show that our proposed encoder demonstrates promising results both as a standalone encoder and as well as a complementary building block. We conduct extensive experiments on three challenging MC datasets, namely RACE, SearchQA and NarrativeQA, achieving highly competitive performance on all. On the RACE benchmark, our model outperforms DFN (Dynamic Fusion Networks) by 1.5%-6% without using any recurrent or convolution layers. Similarly, we achieve competitive performance relative to AMANDA on the SearchQA benchmark and BiDAF on the NarrativeQA benchmark without using any LSTM/GRU layers. Finally, incorporating MRU encoders with standard BiLSTM architectures further improves performance, achieving state-of-the-art results.", "Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete attributes for each entity. Intuitively, these attributes such as height, price or population count are able to richly characterize entities in knowledge graphs. This additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting. Specifically, we train a neural network for triplet prediction along with a separate network for attribute value regression. Via multi-task learning, we are\u00a0\u2026", null, "Learning a matching function between two text sequences is a long standing problem in NLP research. This task enables many potential applications such as question answering and paraphrase identification. This paper proposes Co-Stack Residual Affinity Networks (CSRAN), a new and universal neural architecture for this problem. CSRAN is a deep architecture, involving stacked (multi-layered) recurrent encoders. Stacked/Deep architectures are traditionally difficult to train, due to the inherent weaknesses such as difficulty with feature propagation and vanishing gradients. CSRAN incorporates two novel components to take advantage of the stacked architecture. Firstly, it introduces a new bidirectional alignment mechanism that learns affinity weights by fusing sequence pairs across stacked hierarchies. Secondly, it leverages a multi-level attention refinement component between stacked recurrent layers. The key intuition is that, by leveraging information across all network hierarchies, we can not only improve gradient flow but also improve overall performance. We conduct extensive experiments on six well-studied text sequence matching datasets, achieving state-of-the-art performance on all.", "Dating and romantic relationships not only play a huge role in our personal lives but also collectively influence and shape society. Today, many romantic partnerships originate from the Internet, signifying the importance of technology and the web in modern dating. In this paper, we present a text-based computational approach for estimating the relationship compatibility of two users on social media. Unlike many previous works that propose reciprocal recommender systems for online dating websites, we devise a distant supervision heuristic to obtain real world couples from social platforms such as Twitter. Our approach, the CoupleNet is an end-to-end deep learning basedestimator that analyzes the social profiles of two users and subsequently performs a similarity match between the users. Intuitively, our approach performs both user profiling and match-making within a unified end-to-end framework. CoupleNet utilizes hierarchical recurrent neural models for learning representations of user profiles and subsequently coupled attention mechanisms to fuse information aggregated from two users. To the best of our knowledge, our approach is the first data-driven deep learning approach for our novel relationship recommendation problem. We benchmarkour CoupleNet against several machine learning and deep learning baselines. Experimental results show that our approach outperformsall approaches significantly in terms of precision. Qualitative analysis shows that our model is capable of also producing explainable results to users.", null, "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, eg, word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.", "Recent empirical studies show that adversarial topic models (ATM) can successfully capture semantic patterns of the document by differentiating a document with another dissimilar sample. However, utilizing that discriminative-generative architecture has two important drawbacks:(1) the architecture does not relate similar documents, which has the same document-word distribution of salient words;(2) it restricts the ability to integrate external information, such as sentiments of the document, which has been shown to benefit the training of neural topic model. To address those issues, we revisit the adversarial topic architecture in the view point of mathematical analysis, propose a novel approach to re-formulate discriminative goal as an optimization problem, and design a novel sampling method which facilitates the integration of external variables. The reformulation encourages the model to incorporate the relations among similar samples and enforces the constraint on the similarity among dissimilar ones; while the sampling method, which is based on the internal input and reconstructed output, helps inform the model of salient words contributing to the main topic. Experimental results show that our framework outperforms other state-of-the-art neural topic models in three common benchmark datasets that belong to various domains, vocabulary sizes, and document lengths in terms of topic coherence.", "Link prediction on knowledge graphs is useful in numerous application areas such as semantic search, question answering, entity disambiguation, enterprise decision support, recommender systems and so on. While many of these applications require a reasonably quick response and may operate on data that is constantly changing, existing methods often lack speed and adaptability to cope with these requirements. This is aggravated by the fact that knowledge graphs are often extremely large and may easily contain millions of entities rendering many of these methods impractical. In this paper, we address the weaknesses of current methods by proposing Random Semantic Tensor Ensemble (RSTE), a scalable ensemble-enabled framework based on tensor factorization. Our proposed approach samples a knowledge graph tensor in its graph representation and performs link prediction via ensembles of tensor\u00a0\u2026", "Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the summarization model. In addition, to avoid the overwhelming effect of global semantics on contextualized representation, we introduce a mechanism to control the amount of global semantics supplied to the text generation module. Our method outperforms state-of-the-art summarization models on five common text summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and PubMed.", "The YouTube-8M video classification challenge requires teams to classify 0.7 million videos into one or more of 4,716 classes. In this Kaggle competition, we placed in the top 3% out of 650 participants using released video and audio features. Beyond that, we extend the original competition by including text information in the classification, making this a truly multi-modal approach with vision, audio and text. The newly introduced text data is termed as YouTube-8M-Text. We present a classification framework for the joint use of text, visual and audio features, and conduct an extensive set of experiments to quantify the benefit that this additional mode brings. The inclusion of text yields state-of-the-art results, e.g. 86.7% GAP on the YouTube-8M-Text validation dataset.", "Attentional models are distinctly characterized by their ability to learn relative importance, ie, assigning a different weight to input values. This paper proposes a new quasi-attention that is compositional in nature, ie, learning whether to\\textit {add},\\textit {subtract} or\\textit {nullify} a certain vector when learning representations. This is strongly contrasted with vanilla attention, which simply re-weights input tokens. Our proposed\\textit {Compositional De-Attention}(CoDA) is fundamentally built upon the intuition of both similarity and dissimilarity (negative affinity) when computing affinity scores, benefiting from a greater extent of expressiveness. We evaluate CoDA on six NLP tasks, ie open domain question answering, retrieval/ranking, natural language inference, machine translation, sentiment analysis and text2code generation. We obtain promising experimental results, achieving state-of-the-art performance on several tasks/datasets.", "Recurrent neural networks (RNNs) such as long short-term memory and gated recurrent units are pivotal building blocks across a broad spectrum of sequence modeling problems. This paper proposes a recurrently controlled recurrent network (RCRN) for expressive and powerful sequence encoding. More concretely, the key idea behind our approach is to learn the recurrent gating functions using recurrent networks. Our architecture is split into two components-a controller cell and a listener cell whereby the recurrent controller actively influences the compositionality of the listener cell. We conduct extensive experiments on a myriad of tasks in the NLP domain such as sentiment analysis (SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment classification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading comprehension (NarrativeQA). Across all 26 datasets, our results demonstrate that RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs, suggesting that our controller architecture might be a suitable replacement for the widely adopted stacked architecture. Additionally, RCRN achieves state-of-the-art results on several well-established datasets.", "Learning English grammar is a very challenging task for many students especially for nonnative English speakers. To learn English well, it is important to understand the concepts of the English grammar with lots of practise on exercise questions. Previous recommendation systems for learning English mainly focused on recommending reading materials and vocabulary. Different from reading material and vocabulary recommendations, grammar question recommendation should recommend questions that have similar grammatical structure and usage to the question of interest. The content similarity calculation methods used in existing recommendation methods cannot represent the similarity between grammar questions effectively. In this paper, we propose a content\u2010based approach for personalized grammar question recommendation, which recommends similar grammatical structure and usage questions for\u00a0\u2026", "Current state-of-the-art cross-lingual summarization models employ multi-task learning paradigm, which works on a shared vocabulary module and relies on the self-attention mechanism to attend among tokens in two languages. However, correlation learned by self-attention is often loose and implicit, inefficient in capturing crucial cross-lingual representations between languages. The matter worsens when performing on languages with separate morphological or structural features, making the cross-lingual alignment more challenging, resulting in the performance drop. To overcome this problem, we propose a novel Knowledge-Distillation-based framework for Cross-Lingual Summarization, seeking to explicitly construct cross-lingual correlation by distilling the knowledge of the monolingual summarization teacher into the cross-lingual summarization student. Since the representations of the teacher and the student lie on two different vector spaces, we further propose a Knowledge Distillation loss using Sinkhorn Divergence, an Optimal-Transport distance, to estimate the discrepancy between those teacher and student representations. Due to the intuitively geometric nature of Sinkhorn Divergence, the student model can productively learn to align its produced cross-lingual hidden states with monolingual hidden states, hence leading to a strong correlation between distant languages. Experiments on cross-lingual summarization datasets in pairs of distant languages demonstrate that our method outperforms state-of-the-art models under both high and low-resourced settings.", "Factorization Machines (FMs) are a class of popular algorithms that have been widely adopted for collaborative filtering and recommendation tasks. FMs are characterized by its usage of the inner product of factorized parameters to model pairwise feature interactions, making it highly expressive and powerful. This paper proposes Holographic Factorization Machines (HFM), a new novel method of enhancing the representation capability of FMs without increasing its parameter size. Our approach replaces the inner product in FMs with holographic reduced representations (HRRs), which are theoretically motivated by associative retrieval and compressed outer products. Empirically, we found that this leads to consistent improvements over vanilla FMs by up to 4% improvement in terms of mean squared error, with improvements larger at smaller parameterization. Additionally, we propose a neural adaptation of HFM which enhances its capability to handle nonlinear structures. We conduct extensive experiments on nine publicly available datasets for collaborative filtering with explicit feedback. HFM achieves state-of-theart performance on all nine, outperforming strong competitors such as Attentional Factorization Machines (AFM) and Neural Matrix Factorization (NeuMF).", "This paper proposes a new neural architecture that exploits readily available sentiment lexicon resources. The key idea is that that incorporating a word-level prior can aid in the representation learning process, eventually improving model performance. To this end, our model employs two distinctly unique components, ie,(1) we introduce a lexicon-driven contextual attention mechanism to imbue lexicon words with long-range contextual information and (2), we introduce a contrastive co-attention mechanism that models contrasting polarities between all positive and negative words in a sentence. Via extensive experiments, we show that our approach outperforms many other neural baselines on sentiment classification tasks on multiple benchmark datasets.", "Answering questions using multi-modal context is a challenging problem, as it requires a deep integration of diverse data sources. Existing approaches only consider a subset of all possible interactions among data sources during one attention hop. In this paper, we present a holistic multi-modal memory network (HMMN) framework that fully considers interactions between different input sources (multi-modal context and question) at each hop. In addition, to hone in on relevant information, our framework takes answer choices into consideration during the context retrieval stage. Our HMMN framework effectively integrates information from the multi-modal context, question, and answer choices, enabling more informative context to be retrieved for question answering. Experimental results on the Movie QA and TVQA datasets validate the effectiveness of our HMMN framework. Extensive ablation studies show the\u00a0\u2026", "Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding. Concretely, we present a new task and corpus for learning alignments between machine and human preferences. Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations. Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform. We benchmark several state-of-the-art neural models, along with BERT and friends on this task. Our experimental results show that current state-of-the-art NLP models still leave much room for improvement.", "Taxonomy plays an important role in many applications by organizing domain knowledge into a hierarchy of is-a relations between terms. Previous works on the taxonomic relation identification from text corpora lack in two aspects: 1) They do not consider the trustiness of individual source texts, which is important to filter out incorrect relations from unreliable sources. 2) They also do not consider collective evidence from synonyms and contrastive terms, where synonyms may provide additional supports to taxonomic relations, while contrastive terms may contradict them. In this paper, we present a method of taxonomic relation identification that incorporates the trustiness of source texts measured with such techniques as PageR-ank and knowledge-based trust, and the collective evidence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning. The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure.", "Security protocols play more and more important roles with wide use in many applications nowadays. Currently, there are many tools for specifying and verifying security protocols such as Casper/FDR, ProVerif, or AVISPA. In these tools, the intruder\u2019s ability, which either needs to be specified explicitly or set by default, is not flexible in some circumstances. Moreover, whereas most of the existing tools focus on secrecy and authentication properties, few supports privacy properties like anonymity, receipt freeness, and coercion resistance, which are crucial in many applications such as in electronic voting systems or anonymous online transactions. In this paper, we introduce a framework for specifying security protocols in the labeled transition system (LTS) semantics model, which embeds the knowledge of the participants and parameterizes the ability of an attacker. Using this model, we give the formal\u00a0\u2026", "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.", "The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers. All data and code used in our models are publically available\\footnote{\\url{https://github.com/shuaizhao95/Prompt_attack}}.", "Deep learning models have achieved great success in many fields, yet they are vulnerable to adversarial examples. This paper follows a causal perspective to look into the adversarial vulnerability and proposes Causal Intervention by Semantic Smoothing (CISS), a novel framework towards robustness against natural language attacks. Instead of merely fitting observational data, CISS learns causal effects p (y| do (x)) by smoothing in the latent semantic space to make robust predictions, which scales to deep architectures and avoids tedious construction of noise customized for specific attacks. CISS is provably robust against word substitution attacks, as well as empirically robust even when perturbations are strengthened by unknown attack algorithms. For example, on YELP, CISS surpasses the runner-up by 6.8% in terms of certified robustness against word substitutions, and achieves 80.7% empirical robustness when syntactic attacks are integrated.", "Sequence encoders are crucial components in many neural architectures for learning to read and comprehend. This paper presents a new compositional encoder for reading comprehension (RC). Our proposed encoder is not only aimed at being fast but also expressive. Specifically, the key novelty behind our encoder is that it explicitly models across multiple granularities using a new dilated composition mechanism. In our approach, gating functions are learned by modeling relationships and reasoning over multi-granular sequence information, enabling compositional learning that is aware of both long and short term information. We conduct experiments on three RC datasets, showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block. Empirical results show that simple Bi-Attentive architectures augmented with our proposed encoder not only achieves state-of-the-art/highly competitive results but is also considerably faster than other published works.", "With the popularity of online educational platforms, English learners can learn and practice no matter where they are and what they do. English grammar is one of the important components in learning English. To learn English grammar effectively, it requires students to practice questions containing focused grammar knowledge. In this paper, we study a novel problem of retrieving English grammar questions with similar grammatical focus. Since the grammatical focus similarity is different from textual similarity or sentence syntactic similarity, existing approaches cannot be applied directly to our problem. To address this problem, we propose a syntactic based approach for English grammar question retrieval which can retrieve related grammar questions with similar grammatical focus effectively. In the proposed syntactic based approach, we first propose a new syntactic tree, namely parse-key tree, to capture English\u00a0\u2026", "A conversation is a sequence of utterances, where each utterance plays a specific discourse role while expressing a particular emotion. This paper proposes a novel method to exploit latent discourse role information of an utterance to determine the emotion it conveys in a conversation. Specifically, we use a variant of the Variational-Autoencoder (VAE) to model the context-aware latent discourse roles of each utterance in an unsupervised way. The latent discourse role representation further equips the utterance representation with a salient clue for more accurate emotion recognition. Our experiments show that our proposed method beats the best-reported performances on three public Emotion Recognition in Conversation datasets. This proves that the discourse role information of an utterance plays an important role in the emotion recognition task, which no previous work has studied.", "In biomedical domain, the Gene Ontology (GO) has evolved as the standard for providing a controlled and structured vocabulary of terms describing attributes of genes and gene products, but it is still challenging to automatically locate evidence to GO terms in text. The majority of previous approaches to the automatic recognition of GO terms follow the bag-of-words model for GO term representation. Is this paper, we introduce a novel approach using the GO crossproducts as the GO term representation to recognize GO terms in text and show that it is complementary to a state-of-the-art method based on bag-of-words model.", "Query reformulation is an interactive process of revising user queries according to the query results. To assist biomedical researchers in this process, we present novel methods for automatically generating query reformulation suggestions. While previous work on query reformulation focused on addition of words to user queries, our method can deal with three types of query reformulation (i.e., addition, removal and replacement). The accuracy of the method for the addition type is ten times better than PubMed\u2019s \u201cAlso try\u201d, while the execution time is short enough for practical use.", "To overcome the data sparsity issue in short text topic modeling, existing methods commonly rely on data augmentation or the data characteristic of short texts to introduce more word co-occurrence information. However, most of them do not make full use of the augmented data or the data characteristic: they insufficiently learn the relations among samples in data, leading to dissimilar topic distributions of semantically similar text pairs. To better address data sparsity, in this paper we propose a novel short text topic modeling framework, Topic-Semantic Contrastive Topic Model (TSCTM). To sufficiently model the relations among samples, we employ a new contrastive learning method with efficient positive and negative sampling strategies based on topic semantics. This contrastive learning method refines the representations, enriches the learning signals, and thus mitigates the sparsity issue. Extensive experimental results show that our TSCTM outperforms state-of-the-art baselines regardless of the data augmentation availability, producing high-quality topics and topic distributions.", "The task of resolving repeated objects in natural languages is known as coreference resolution, and it is an important part of modern natural language processing. It is classified into two categories depending on the resolved objects, namely entity coreference resolution and event coreference resolution. Predicting coreference connections and identifying mentions/triggers are the major challenges in coreference resolution, because these implicit relationships are particularly difficult in natural language understanding in downstream tasks. Coreference resolution techniques have experienced considerable advances in recent years, encouraging us to review this task in the following aspects: current employed evaluation metrics, datasets, and methods. We investigate 10 widely used metrics, 18 datasets and 4 main technical trends in this survey. We believe that this work is a comprehensive roadmap for\u00a0\u2026", "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.", "Security protocols play more and more important role nowadays, ranging from banking to electronic commerce systems. They are designed to provide properties such as authentication, key exchanges, key distribution, non-repudiation, proof of origin, integrity, confidentiality and anonymity, for users who wish to exchange messages over a medium over which they have little control. These properties are often difficult to characterize formally (or even informally). The protocols themselves often contain a great deal of combinatorial complexity, making their verification extremely difficult and prone to error. To overcome these obstacles, many different approaches are proposed such as using theorem provers or ranking systems. However, they are lack of automation, sufficiency in demand or time verification. In this paper, we will propose an approach using Real Time System (RTS) and an model checker PAT to deal with\u00a0\u2026", "Topic models have been prevalent for decades with various applications. However, existing topic models commonly suffer from the notorious topic collapsing: discovered topics semantically collapse towards each other, leading to highly repetitive topics, insufficient topic discovery, and damaged model interpretability. In this paper, we propose a new neural topic model, Embedding Clustering Regularization Topic Model (ECRTM). Besides the existing reconstruction error, we propose a novel Embedding Clustering Regularization (ECR), which forces each topic embedding to be the center of a separately aggregated word embedding cluster in the semantic space. This enables each produced topic to contain distinct word semantics, which alleviates topic collapsing. Regularized by ECR, our ECRTM generates diverse and coherent topics together with high-quality topic distributions of documents. Extensive experiments on benchmark datasets demonstrate that ECRTM effectively addresses the topic collapsing issue and consistently surpasses state-of-the-art baselines in terms of topic quality, topic distributions of documents, and downstream classification tasks.", "Modern Review Helpfulness Prediction systems are dependent upon multiple modalities, typically texts and images. Unfortunately, those contemporary approaches pay scarce attention to polish representations of cross-modal relations and tend to suffer from inferior optimization. This might cause harm to model's predictions in numerous cases. To overcome the aforementioned issues, we propose Multimodal Contrastive Learning for Multimodal Review Helpfulness Prediction (MRHP) problem, concentrating on mutual information between input modalities to explicitly elaborate cross-modal relations. In addition, we introduce Adaptive Weighting scheme for our contrastive learning approach in order to increase flexibility in optimization. Lastly, we propose Multimodal Interaction module to address the unalignment nature of multimodal data, thereby assisting the model in producing more reasonable multimodal representations. Experimental results show that our method outperforms prior baselines and achieves state-of-the-art results on two publicly available benchmark datasets for MRHP problem.", "Recent studies on adversarial images have shown that they tend to leave the underlying low-dimensional data manifold, making them significantly more challenging for current models to make correct predictions. This so-called off-manifold conjecture has inspired a novel line of defenses against adversarial attacks on images. In this study, we find a similar phenomenon occurs in the contextualized embedding space induced by pretrained language models, in which adversarial texts tend to have their embeddings diverge from the manifold of natural ones. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that projects text embeddings onto an approximated embedding manifold before classification. It reduces the complexity of potential adversarial examples, which ultimately enhances the robustness of the protected model. Through extensive experiments, our method consistently and significantly outperforms previous defenses under various attack settings without trading off clean accuracy. To the best of our knowledge, this is the first NLP defense that leverages the manifold structure against adversarial attacks. Our code is available at \\url{https://github.com/dangne/tmd}.", "Cross-lingual topic models have been prevalent for cross-lingual text analysis by revealing aligned latent topics. However, most existing methods suffer from producing repetitive topics that hinder further analysis and performance decline caused by low-coverage dictionaries. In this paper, we propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM). Instead of the direct alignment in previous work, we propose a topic alignment with mutual information method. This works as a regularization to properly align topics and prevent degenerate topic representations of words, which mitigates the repetitive topic issue. To address the low-coverage dictionary issue, we further propose a cross-lingual vocabulary linking method that finds more linked cross-lingual words for topic alignment beyond the translations of a given dictionary. Extensive experiments on English, Chinese, and Japanese datasets demonstrate that our method outperforms state-of-the-art baselines, producing more coherent, diverse, and well-aligned topics and showing better transferability for cross-lingual classification tasks.", "Taxonomies play an important role in many applications by organizing domain                     knowledge into a hierarchy of \u2018is-a\u2019 relations                     between terms. Previous work on automatic construction of taxonomies from text                     documents either ignored temporal information or used fixed time periods to                     discretize the time series of documents. In this paper, we propose a time-aware                     method to automatically construct and effectively maintain a taxonomy from a                     given series of documents preclustered for a domain of interest. The method                     extracts temporal information from the documents and uses a timestamp                     contribution function to score the temporal relevance of the evidence from                     source texts when identifying the taxonomic relations for constructing the                     taxonomy. Experimental results show that our proposed method\u00a0\u2026", "This work empirically investigates punctuation insertions as adversarial attacks on NLP systems. Data from experiments on three tasks, five datasets, and six models with four attacks show that punctuation insertions, when limited to a few symbols (apostrophes and hyphens), are a superior attack vector compared to character insertions due to 1) a lower after-attack accuracy () than alphabetical character insertions; 2) higher semantic similarity between the resulting and original texts; and 3) a resulting text that is easier and faster to read as assessed with the Test of Word Reading Efficiency (TOWRE)). The tests also indicate that 4) grammar checking does not mitigate punctuation insertions and 5) punctuation insertions outperform word-level attacks in settings with a limited number of word synonyms and queries to the victim\u2019s model. Our findings indicate that inserting a few punctuation types that result in easy-to-read samples is a general attack mechanism. In light of this threat, we assess the impact of punctuation insertions, potential mitigations, the mitigation\u2019s tradeoffs, punctuation insertion\u2019s worst-case scenarios and summarize our findings in a qualitative casual map, so that developers can design safer, more secure systems.", "Federated learning (FL) enables multiple data owners to build machine learning models collaboratively without exposing their private local data. In order for FL to achieve widespread adoption, it is important to balance the need for performance, privacy-preservation and interpretability, especially in mission critical applications such as finance and healthcare. Thus, interpretable federated learning (IFL) has become an emerging topic of research attracting significant interest from the academia and the industry alike. Its interdisciplinary nature can be challenging for new researchers to pick up. In this paper, we bridge this gap by providing (to the best of our knowledge) the first survey on IFL. We propose a unique IFL taxonomy which covers relevant works enabling FL models to explain the prediction results, support model debugging, and provide insights into the contributions made by individual data owners or data samples, which in turn, is crucial for allocating rewards fairly to motivate active and reliable participation in FL. We conduct comprehensive analysis of the representative IFL approaches, the commonly adopted performance evaluation metrics, and promising directions towards building versatile IFL techniques.", "Ontology term recognition is a key task of ontology-based text mining. Previous approaches of statistical analysis and syntactic pattern matching have such limitations that they do not consider relations between words and that their handcrafted patterns are expensive and show low coverage, respectively. These limitations are critical especially when dealing with long and complex ontology terms. We propose a hybrid approach that combines the two approaches sequentially: It first uses syntactic pattern matching and, when its results are partial due to lack of required patterns, then completes them with supplementary evidence from a statistical method. Additionally, we present a novel method that automatically learns syntactic patterns from an annotated corpus. We tested the proposed approach for the tasks of recognizing Gene Ontology (GO) terms in text and also of associating the GO terms with proteins. When\u00a0\u2026", "Information extraction (IE) transforms unstructured textual sources into structured knowledge, closely resembling human reasoning. IE involves several subtasks, such as named entity recognition (NER), relation extraction (RE), and coreference resolution (CR). The early IE models tend to treat each subtask as a separate task or to apply a sequential pipeline approach, which can lead to cascading errors and obfuscation of the inherent relationship between tasks. Recent research has shown that it is advantageous to incorporate the interdependence of subtasks and optimize performance through joint learning. However, they do not properly model the interaction between tasks, either by modeling the subtasks sequentially or by using shared input data. Inspired by human reasoning, a graph-based multitask IE framework is presented that facilitates the interaction between several IE tasks capable of capturing both\u00a0\u2026", "Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user's requirements. Through empirical experiments on two TOD benchmarks, we validate the effectiveness of our method. The results demonstrate that our approach outperforms previous state-of-the-art (SOTA) results.", "Topic models aim to discover latent topics and infer what topics a document contains in an unsupervised fashion. They have been prevalent for decades with wide applications like text analysis. Recently the rise of neural networks has facilitated a new research field---Neural Topic Models (NTMs). Different from conventional models, NTMs directly optimize parameters without model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a thorough survey on neural topic models carefully and widely. In detail, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. Then we discuss developed popular applications based on NTMs. We finally highlight the challenges of NTMs to motivate future research.", "Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less sensitive to the prompt design. Our code and pre-trained models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning .", "Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for joint semi-supervised entity and relation extraction, which captures the global structure information between individual tasks and exploits interactions within unlabeled data. Specifically, we construct a unified span-based heterogeneous graph from entity and relation candidates and propagate class labels based on confidence scores. We then employ a propagation learning scheme to leverage the affinities between labelled and unlabeled samples. Experiments on benchmark datasets show that our framework outperforms the state-of-the-art semi-supervised approaches on NER and RE tasks. We show that the joint semi-supervised learning of the two tasks benefits from their codependency and validates the importance of utilizing the shared information between unlabeled data.", "With the burgeoning amount of data of image-text pairs and diversity of Vision-and-Language (V\\&L) tasks, scholars have introduced an abundance of deep learning models in this research domain. Furthermore, in recent years, transfer learning has also shown tremendous success in Computer Vision for tasks such as Image Classification, Object Detection, etc., and in Natural Language Processing for Question Answering, Machine Translation, etc. Inheriting the spirit of Transfer Learning, research works in V\\&L have devised multiple pretraining techniques on large-scale datasets in order to enhance the performance of downstream tasks. The aim of this article is to provide a comprehensive revision of contemporary V\\&L pretraining models. In particular, we categorize and delineate pretraining approaches, along with the summary of state-of-the-art vision-and-language pretrained models. Moreover, a list of training datasets and downstream tasks is supplied to further polish the perspective into V\\&L pretraining. Lastly, we decided to take a further step to discuss numerous directions for future research.", "Speech emotion recognition has made significant progress in recent years, in which feature representation learning has been paid more attention, but discriminative emotional features extraction has remained unresolved. In this paper, we propose MDSCM-a Multi-attention based Depthwise Separable Convolutional Model for speech emotional feature extraction that can reduce the feature redundancy through separating spatialwise convolution and channel-wise convolution. MDSCM also enhances the feature discriminability by the multi-attention module that focuses on learning features with more emotional information. In addition, we propose an Audio-Visual Domain Adaptation Learning paradigm (AVDAL) to learn an audiovisual emotion-identity space. A shared audio-visual representation encoder is built to transfer the emotional knowledge learned from the visual domain to complement and enhance the\u00a0\u2026", "The explosion of unstructured text data makes it difficult to find information for our interests. To provide access to information effectively, it is important to organize the unstructured data in a structured and meaningful manner. Taxonomies, which serve as the backbone for structured knowledge, are useful for many NLP applications such as question answering and document clustering by organizing domain knowledge into a hierarchy of \u2018is-a\u2019 relations between terms. Currently, there have been an increasing number of public hand-crafted taxonomies available such as WordNet and Freebase. However, it will be more effective to use taxonomies that are created specifically for the domain of interest in practice rather than re-using existing taxonomies created for other tasks or domains. As such, we often face the challenge of creating a brand new taxonomy for a specific domain from scratch.  In this thesis, we propose an effective framework for automatic domain-specific taxonomy construction from textual documents, which consists of three steps, namely domain term extraction, taxonomic relation identification and taxonomy induction. Domain term extraction aims to extract the relevant domain terms from a given text collection of specific domain. Taxonomic relation identification aims to identify the taxonomic relations (i.e. \u2018is-a\u2019 relations) among domain terms. Taxonomy induction aims to construct the taxonomy structure from the identified taxonomic relations. We use the big data approach which combines linguistics, statistical and deep learning methods to address the challenges in these steps. The main contributions of our research are summarized\u00a0\u2026", "Grammar question retrieval aims to find relevant grammar questions that have similar grammatical structure and usage as the input question query. Previous work on text and sentence retrieval which is mainly based on statistical analysis approach and syntactic analysis approach is not effective in finding relevant grammar questions with similar grammatical focus. In this paper, we propose a syntactic parse-key tree based approach for English grammar question retrieval which can find relevant grammar questions with similar grammatical focus effectively. In particular, we propose a syntactic parse-key tree to capture the grammatical focus of grammar questions according to the blank or answer position of the questions. Then we propose a novel method to compute the parse-key tree similarity between the parse-trees of the question query and the database questions for question retrieval. The performance\u00a0\u2026", "When deployed in practical applications, Ontologies and KBs often suffer various kinds of inconsistency, which limit the applications performances significantly. In this paper, we propose a framework to reason inconsistency between Ontology and KB and refine the inconsistency accordingly. To make our framework efficient, we only focus on reasoning a part responsible for the inconsistency, rather than the whole structures of Ontology and KB. Moreover, to improve the execution speed of algorithms employed in the framework, we also discuss an axiom-oriented strategy to reason on a reduced space of formula to be inferred in Ontology and KB.", "Recent research has shown that facial expressions and body gestures are two significant implications in identifying human emotions. However, these studies mainly focus on contextual information of adjacent frames, and rarely explore the spatio-temporal relationships between distant or global frames. In this paper, we revisit the facial expression and body gesture emotion recognition problems, and propose to improve the performance of video emotion recognition by extracting the spatio-temporal features via further encoding temporal information. Specifically, for facial expression, we propose a super image-based spatio-temporal convolutional model (SISTCM) and a two-stream LSTM model to capture the local spatio-temporal features and learn global temporal cues of emotion changes. For body gestures, a novel representation method and an attention-based channel-wise convolutional model (ACCM) are\u00a0\u2026", "The diversity of natural language expressions for describing events poses a challenge for the task of Event Detection (ED) with machine learning methods. To detect and classify event mentions, ED models essentially need to construct a semantic linkage between representations of the mentions and a set of target types. Unfortunately, most existing models use meaningless homogeneous one-hot vectors to represent the event type classes in ED, ignoring the fact that the event type labels also consist of meaningful words and can provide important clues for type representation learning. In this paper, we propose a Contrastive Semantic Prototype Representation Learning Framework for Event Detection (SemPRE), which exploits the pre-defined event type label words to inject the semantic information of the types and guide event detection. Specifically, we utilize pre-trained BERT to fuse text and event type into a joint\u00a0\u2026", "Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of negative pairs and show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE. Theoretically, we analyze the effects of Soft-InfoNCE on controlling the distribution of learnt code representations and on deducing a more precise mutual information estimation. We furthermore discuss the superiority of proposed loss functions with other design alternatives. Extensive experiments demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods under state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. Source code is available at \\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.", "Counterfactual statements, which describe events that did not or cannot take place, are beneficial to numerous NLP applications. Hence, we consider the problem of counterfactual detection (CFD) and seek to enhance the CFD models. Previous models are reliant on clue phrases to predict counterfactuality, so they suffer from significant performance drop when clue phrase hints do not exist during testing. Moreover, these models\u2019 prediction also biases towards non-counterfactual over the counterfactual class. To address these issues, we propose to integrate neural topic model into the CFD model to capture the global semantics of the input statement. We continue to causally intervene the hidden representations of the CFD model to balance the effect of the class labels. Extensive experiments show that our approach outperforms previous state-of-the-art CFD and bias-resolving methods in both the CFD and other bias-sensitive tasks.", "Large language models (LLMs), as a particular instance of generative artificial intelligence, have revolutionized natural language processing. In this invited paper, we argue that LLMs are complementary to structured data repositories such as databases or knowledge bases, which use symbolic knowledge representations. Hence, the two ways of knowledge representation will likely continue to co-exist, at least in the near future. We discuss ways that have been explored to make the two approaches work together, and point out opportunities and challenges for their symbiosis.", "Recently, the development of large language models (LLMs) has been significantly enhanced the question answering and dialogue generation, and makes them become increasingly popular in current practical scenarios. While unlike the general dialogue system which emphasizes the semantic performance, the task-oriented dialogue (ToD) systems aim to achieve the dialogue goal efficiently and successfully in multiple turns. Unfortunately, existing LLM-induced ToD systems lack the direct reward toward the final goal and do not take account of the dialogue proactivity that can strengthen the dialogue efficiency. To fill these gaps, we introduce the ProToD (Proactively Goal-Driven LLM-Induced ToD) approach, which anticipates the future dialogue actions and incorporates the goal-oriented reward signal to enhance ToD systems. Additionally, we present a novel evaluation method that assesses ToD systems based on goal-driven dialogue simulations. This method allows us to gauge user satisfaction, system efficiency and successful rate while overcoming the limitations of current Information and Success metrics. Empirical experiments conducted on the MultiWoZ 2.1 dataset demonstrate that our model can achieve superior performance using only 10% of the data compared to previous end-to-end fully supervised models. This improvement is accompanied by enhanced user satisfaction and efficiency.", "Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.", "With the burgeoning amount of data of image-text pairs and diversity of Vision-and-Language (V&L) tasks, scholars have introduced an abundance of deep learning models in this research domain. Furthermore, in recent years, transfer learning has also shown tremendous success in Computer Vision for tasks such as Image Classification, Object Detection, etc., and in Natural Language Processing for Question Answering, Machine Translation, etc. Inheriting the spirit of Transfer Learning, research works in V&L have devised multiple pretraining techniques on large-scale datasets in order to enhance the performance of downstream tasks. The aim of this article is to provide a comprehensive revision of contemporary V&L pretraining models. In particular, we categorize and delineate pretrain-ing approaches, along with the summary of state-of-the-art vision-and-language pretrained models. Moreover, a list of training datasets and downstream tasks is supplied to further polish the perspective into V&L pretraining. Lastly, we decided to take a further step to discuss numerous directions for future research.", "Emotion recognition in conversations (ERC) has gained more attention, where contextual information modeling and multimodal fusion have been the focus and challenges in recent years. In this paper, we proposed a Multi-Scale Receptive Field Graph model (MSRFG) to tackle the challenges of ERC. Specifically, MSRFG constructs multi-scale perception graphs and learns contextual information via parallel multi-scale receptive field paths. To compensate for the deficiency of temporal information learning by the graph network, MSRFG injects temporal dependencies into the graph network to model the temporal relationships between utterances. Moreover, to achieve the effective fusion of multimodal information, MSRFG converges the multi-scale features of each modality separately and performs the learning of attention weights after the integration of converged features. We carried out experiments on IEMOCAP\u00a0\u2026", "Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score predictor and pairwise loss as the training objective. However, FCNNs have been shown to perform inefficient splitting for review features, making the model difficult to clearly differentiate helpful from unhelpful reviews. Furthermore, pairwise objective, which works on review pairs, may not completely capture the MRHP goal to produce the ranking for the entire review list, and possibly induces low generalization during testing. To address these issues, we propose a listwise attention network that clearly captures the MRHP ranking context and a listwise optimization objective that enhances model generalization. We further propose gradient-boosted decision tree as the score predictor to efficaciously partition product reviews' representations. Extensive experiments demonstrate that our method achieves state-of-the-art results and polished generalization performance on two large-scale MRHP benchmark datasets.", "The long-standing challenge of building effective classification models for small and imbalanced datasets has seen little improvement since the creation of the Synthetic Minority Over-sampling Technique (SMOTE) over 20 years ago. Though GAN based models seem promising, there has been a lack of purpose built architectures for solving the aforementioned problem, as most previous studies focus on applying already existing models. This paper proposes a unique, performance-oriented, data-generating strategy that utilizes a new architecture, coined draGAN, to generate both minority and majority samples. The samples are generated with the objective of optimizing the classification model's performance, rather than similarity to the real data. We benchmark our approach against state-of-the-art methods from the SMOTE family and competitive GAN based approaches on 94 tabular datasets with varying degrees of imbalance and linearity. Empirically we show the superiority of draGAN, but also highlight some of its shortcomings. All code is available on: https://github.com/LeonGuertler/draGAN.", "Recent representation learning approaches to polish global semantics of neural topic models optimize the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the discriminative objective that contrasts instance pairings. However, contrastive learning on the individual level might capture noisy mutual information that is irrelevant to the topic modeling task. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive term which attempts to generalize representations among inputs. To address the issues, we firstly hypothesize that useful features should be shared among multiple input samples. For that reason, we propose a novel set-based contrastive learning method for neural topic models to employ the concept of multi-sample representation learning. Secondly, because the solution of the linear combination approach might not satisfy all objectives when they compete, we explicitly cast contrastive topic modeling as gradient-based multi-objective optimization, with the goal of achieving a Pareto stationary solution. Extensive experiments demonstrate that our framework consistently produces higher-performing neural topic models in terms of topic coherence and downstream performance.", "Security protocols play more and more important role with widely use in many applications nowadays. They are designed to provide security properties for users who wish to exchange messages over unsecured medium. There are many tools were designed to specify and verify security protocols such as Casper/FDR, ProVerif or AVISPA. While most of the existing tools focus on secrecy and authentication properties. few supports properties like anonymity and privacy, which are crucial in many protocols such as in electronic voting systems or anonymous online transactions. Moreover, to the best of our knowledge, there is still not have a fully automatic tool using formal methods to verify these two properties. In this paper, we introduce a framework for specifying security protocols in the Labeled Transition System (LTS) semantics model and give the formal definition for three existing types of anonymity and privacy properties using this model. We also come up with the verification algorithms for verification and implement all the ideas in a module SeVe inside PAT model checker.", "Security protocols play more and more important role with widely use in many applications nowadays. They are designed to provide security properties for users who wish to exchange messages over unsecured medium. There are many tools were designed to specify and verify security protocols such as Casper/FDR, ProVerif or AVISPA. While most of the existing tools focus on secrecy and authentication properties. few supports properties like anonymity and privacy, which are crucial in many protocols such as in electronic voting systems or anonymous online transactions. Moreover, to the best of our knowledge, there is still not have a fully automatic tool using formal methods to verify these two properties. In this paper, we introduce a framework for specifying security protocols in the Labeled Transition System (LTS) semantics model and give the formal definition for three existing types of anonymity and privacy properties using this model. We also come up with the verification algorithms for verification and implement all the ideas in a module SeVe inside PAT model checker.", "There are many languages to describe the security protocols so far; however, most of them are complicated to use or only suitable for one specific studies but not for all cases. In this section, we will investigate three well-known security languages which are commonly used: Casper, ProVerif and HLPSL. For easy comparison, I give an example description of Needham Schroeder public key protocol in three languages as well as our security language SeVe."]}}