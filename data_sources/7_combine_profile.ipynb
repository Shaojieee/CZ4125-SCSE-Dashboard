{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from utils import get_h_index\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dr_ntu_dir='./raw_dr_ntu'\n",
    "raw_faculty_db = 'scse_profile'\n",
    "\n",
    "process_faculty_db_dir ='./processed'\n",
    "process_faculty_db = 'scse_profile'\n",
    "process_co_author_db = 'google_scholar_co_author'\n",
    "\n",
    "raw_google_scholar_dir = './raw_google_scholar'\n",
    "raw_google_search_dir = './google_search'\n",
    "\n",
    "process_publications_dir = './processed_google_scholar_publications'\n",
    "\n",
    "research_interest_dir = './research_interest'\n",
    "\n",
    "education_output_dir = './dr_ntu_education'\n",
    "\n",
    "profile_dir = './profile'\n",
    "os.makedirs(profile_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculties = pd.read_csv(os.path.join(process_faculty_db_dir, process_faculty_db+'.csv'))\n",
    "google_scholar_faculties = faculties[faculties['google_scholar'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR NTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_ntu_information = [\n",
    "    'designation',\n",
    "    'biography', 'orcid', 'other_websites', 'image_path'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 4015.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm(faculties.iterrows()):\n",
    "    name, id = row['full_name'], row['dr_ntu_id']\n",
    "\n",
    "    profile = {'full_name': name, 'email': row['email'], 'google_scholar': row['google_scholar'], 'dr_ntu': row['dr_ntu']}\n",
    "    with open(f'{raw_dr_ntu_dir}/{id}.json', 'r') as f:\n",
    "        dr_ntu = json.load(f)\n",
    "\n",
    "    profile.update({k:v for k, v in dr_ntu.items() if k in dr_ntu_information})\n",
    "\n",
    "    with open(f\"{profile_dir}/{id}.json\", 'w') as f:\n",
    "        json.dump(profile, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 5604.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm(faculties.iterrows()):\n",
    "    name, id = row['full_name'], row['dr_ntu_id']\n",
    "\n",
    "    education_file = f\"{education_output_dir}/{id}.json\"\n",
    "    if os.path.exists(education_file):\n",
    "        with open(education_file, 'r') as f:\n",
    "            education = json.load(f)\n",
    "        \n",
    "        with open(f\"{profile_dir}/{id}.json\", 'r') as f:\n",
    "            profile = json.load(f)\n",
    "        profile.update(education)\n",
    "        \n",
    "        with open(f\"{profile_dir}/{id}.json\", 'w') as f:\n",
    "            json.dump(profile, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [00:00, 232.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, row in tqdm(google_scholar_faculties.iterrows()):\n",
    "    name, google_scholar_id, dr_ntu_id = row['full_name'], row['google_scholar_id'], row['dr_ntu_id']\n",
    "\n",
    "    research_interest_file = f\"{research_interest_dir}/{google_scholar_id}.json\"\n",
    "    publication_file = f\"{process_publications_dir}/{google_scholar_id}.json\"\n",
    "    if os.path.exists(publication_file):\n",
    "        with open(publication_file, 'r') as f:\n",
    "            pubs = json.load(f)\n",
    "        pubs_df = pd.DataFrame(pubs)\n",
    "    if os.path.exists(research_interest_file):\n",
    "        with open(research_interest_file, 'r') as f:\n",
    "            research_interests = json.load(f)\n",
    "        research_interests_df = pd.DataFrame(research_interests)\n",
    "        pubs_df = pubs_df.merge(research_interests_df, how='left', on='link')\n",
    "        pubs_df['topic'] = pubs_df['topic'].fillna('Others')\n",
    "\n",
    "    pubs_df['publication_year'] = pubs_df['publication_year'].fillna(0).astype(int).replace(0, 'Unknown').astype(str)\n",
    "    pubs_df['total_citations'] = pubs_df['citations_by_year'].apply(lambda x: sum(x['num_citations']))    \n",
    "\n",
    "    publications = {\n",
    "        'Publication Year': pubs_df['publication_year'].to_list(), \n",
    "        'Title': pubs_df['title'].to_list(), \n",
    "        'Link': pubs_df['link'].to_list(), \n",
    "        'Topic': pubs_df['topic'].to_list() if os.path.exists(research_interest_file) else ['Others']*len(pubs_df),\n",
    "        '# of Citations': pubs_df['total_citations'].to_list(), \n",
    "        'Description': pubs_df['description'].to_list()\n",
    "    }\n",
    "    # publications['Publication Year'].append(str(pub['publication_year']) if pub['publication_year'] is not None else 'Unknown')\n",
    "    # publications['Title'].append(pub['title'])\n",
    "    # publications['Description'].append(pub['description'])\n",
    "    # publications['Link'].append(pub['link'])\n",
    "    # publications['Topic'].append(pub['final_topic'] if pub['final_topic'] is not None else 'Others')\n",
    "    # publications['# of Citations'].append(sum(pub['citations_by_year']['num_citations']))\n",
    "    \n",
    "    with open(f'{profile_dir}/{dr_ntu_id}.json', 'r') as f:\n",
    "        profile = json.load(f)\n",
    "        profile['publications'] = publications\n",
    "    \n",
    "    with open(f'{profile_dir}/{dr_ntu_id}.json', 'w') as f:\n",
    "        json.dump(profile, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [00:00, 55121.26it/s]\n"
     ]
    }
   ],
   "source": [
    "scse_google_scholar = {}\n",
    "for i, row in tqdm(google_scholar_faculties.iterrows()):\n",
    "    name, google_scholar_id, dr_ntu_id = row['full_name'], row['google_scholar_id'], row['dr_ntu_id']\n",
    "    scse_google_scholar[google_scholar_id] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:02, 36.77it/s]\n"
     ]
    }
   ],
   "source": [
    "google_scholar_org_id = {'3012140508424117850': 'Nanyang Technological University'}\n",
    "\n",
    "for i, row in tqdm(faculties.iterrows()):\n",
    "    name, google_scholar_id, dr_ntu_id = row['full_name'], row['google_scholar_id'], row['dr_ntu_id']\n",
    "\n",
    "    google_scholar_profile_file = f\"{raw_google_scholar_dir}/{google_scholar_id}.json\"\n",
    "    if os.path.exists(google_scholar_profile_file):\n",
    "        with open(google_scholar_profile_file, 'r') as f:\n",
    "            google_scholar_profile = json.load(f)\n",
    "    \n",
    "    titles = set([x['title'].lower() for x in google_scholar_profile['publications']])\n",
    "    co_authors = google_scholar_profile['co_authors']\n",
    "\n",
    "    network = {'target':[], 'target_id':[], 'type':[], 'location':[], 'year': [], 'title': [], 'link': []}\n",
    "    for co_author in co_authors:\n",
    "        co_author_id = co_author['link'].split('user=')[1].split('&')[0]\n",
    "        co_author_name = scse_google_scholar[co_author_id] if co_author_id in scse_google_scholar else co_author['name']\n",
    "        co_author_file = f\"{raw_google_scholar_dir}/{co_author_id}.json\"\n",
    "\n",
    "        with open(co_author_file, 'r') as f:\n",
    "            co_author_profile = json.load(f)\n",
    "            \n",
    "        if co_author_profile['publications'] is not None: \n",
    "            co_author_titles = set([x['title'].lower()for x in co_author_profile['publications']])\n",
    "        else:\n",
    "            co_author_titles = set()\n",
    "\n",
    "        # Checking no. of collaborations\n",
    "        same_titles = len(titles.intersection(co_author_titles))\n",
    "        if same_titles==0:\n",
    "            continue\n",
    "\n",
    "        # Getting affiliation details\n",
    "        affiliates = co_author_profile['affiliates']\n",
    "        if affiliates is None or len(affiliates)==0:\n",
    "            type='Unknown'\n",
    "        else:\n",
    "            type = 'Outside NTU'\n",
    "        \n",
    "        if co_author_id in scse_google_scholar:\n",
    "            type = 'NTU'\n",
    "            location = google_scholar_org_id['3012140508424117850']\n",
    "        elif len(affiliates)>0:\n",
    "            org = affiliates[0]['link'].split('org=')[1].split('&')[0]\n",
    "            # Storing org name to get standardised organisation name\n",
    "            google_scholar_org_id[org] = google_scholar_org_id.get(org, affiliates[0]['name'])\n",
    "            type = 'Outside NTU'\n",
    "            location = google_scholar_org_id[org]\n",
    "            # Checking for multiple affiliations if there is a NTU affiliation\n",
    "            for aff in affiliates:\n",
    "                if 'org=3012140508424117850' in aff['link'].lower():\n",
    "                    type = 'Outside SCSE'\n",
    "                    location = google_scholar_org_id['3012140508424117850']\n",
    "        else:\n",
    "            type = 'Unknown'\n",
    "            location = 'Unknown'\n",
    "        \n",
    "\n",
    "        min_year = float('inf')\n",
    "        # Get year of publication\n",
    "        for title in titles.intersection(co_author_titles):\n",
    "            for pub in google_scholar_profile['publications']:\n",
    "                if pub['title'].lower()==title:\n",
    "                    if not math.isnan(pub['year']):\n",
    "                        publication_year = int(pub['year'])\n",
    "                        min_year = min(min_year, publication_year)\n",
    "                    else:\n",
    "                        publication_year = 'Unknown'\n",
    "                        \n",
    "                    network['target'].append(co_author_name); network['target_id'].append(co_author_id)\n",
    "                    network['type'].append(type); network['location'].append(location)\n",
    "                    network['year'].append(publication_year)\n",
    "                    network['title'].append(pub['title']); network['link'].append(pub['title_link'])\n",
    "                    break\n",
    "\n",
    "    with open(f'{profile_dir}/{dr_ntu_id}.json', 'r') as f:\n",
    "        profile = json.load(f)\n",
    "        profile['collaboration_network'] = network\n",
    "    \n",
    "    with open(f'{profile_dir}/{dr_ntu_id}.json', 'w') as f:\n",
    "        json.dump(profile, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No. of publications by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 300.77it/s]\n"
     ]
    }
   ],
   "source": [
    "cur_year = datetime.datetime.now().year\n",
    "\n",
    "for i, row in tqdm(faculties.iterrows()):\n",
    "    name, google_scholar_id, dr_ntu_id = row['full_name'], row['google_scholar_id'], row['dr_ntu_id']\n",
    "\n",
    "    publication_file = f\"{process_publications_dir}/{google_scholar_id}.json\"\n",
    "\n",
    "    if os.path.exists(publication_file):\n",
    "        with open(publication_file, 'r') as f:\n",
    "            pubs = json.load(f)\n",
    "\n",
    "        pubs_df = pd.DataFrame(pubs)\n",
    "\n",
    "        pubs_df['publication_year'] = pubs_df['publication_year'].fillna(0).astype(int).replace(0, 'unknown').astype(str)\n",
    "        min_year = pubs_df['publication_year'].min()\n",
    "        publication_by_year = pubs_df.groupby(by=['publication_year'])['title'].count().reset_index()\n",
    "        publication_by_year = publication_by_year.set_index('publication_year')\n",
    "        publication_by_year = publication_by_year.to_dict('index')\n",
    "\n",
    "        final = {'Year': [], '# of Publications': []}\n",
    "        if min_year!='unknown':\n",
    "            # For years in between with no citations\n",
    "            for year in range(int(min_year), cur_year+1):\n",
    "                final['Year'].append(str(year))\n",
    "                final['# of Publications'].append(publication_by_year.get(str(year), {'title':0})['title'])\n",
    "        \n",
    "        if 'unknown' in publication_by_year:\n",
    "            final['Year'].append('unknown')\n",
    "            final['# of Publications'].append(publication_by_year['unknown']['title'])\n",
    "            \n",
    "        \n",
    "        with open(f'{profile_dir}/{dr_ntu_id}.json', 'r') as f:\n",
    "            profile = json.load(f)\n",
    "            profile['published_by_year'] = final\n",
    "        \n",
    "        with open(f'{profile_dir}/{dr_ntu_id}.json', 'w') as f:\n",
    "            json.dump(profile, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No. of citations by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 304.62it/s]\n"
     ]
    }
   ],
   "source": [
    "cur_year = datetime.datetime.now().year\n",
    "\n",
    "for i, row in tqdm(faculties.iterrows()):\n",
    "    name, google_scholar_id, dr_ntu_id = row['full_name'], row['google_scholar_id'], row['dr_ntu_id']\n",
    "\n",
    "    publication_file = f\"{process_publications_dir}/{google_scholar_id}.json\"\n",
    "\n",
    "    if os.path.exists(publication_file):\n",
    "        with open(publication_file, 'r') as f:\n",
    "            pubs = json.load(f)\n",
    "    \n",
    "        total_citations_by_year = {}\n",
    "        min_year = float('inf')\n",
    "        for details in pubs:\n",
    "            citations_by_year = details['citations_by_year']\n",
    "            for i in range(len(citations_by_year['year'])):\n",
    "                year = citations_by_year['year'][i]\n",
    "                citations = citations_by_year['num_citations'][i]\n",
    "                # Has unknown\n",
    "                total_citations_by_year[str(year)] = total_citations_by_year.get(str(year), 0) + citations\n",
    "                if year!='unknown':\n",
    "                    min_year = min(min_year, year)\n",
    "\n",
    "        # For years in between with no citations\n",
    "        final = {'Year': [], '# of Citations': []}\n",
    "        for year in range(min_year, cur_year+1):\n",
    "            final['Year'].append(str(year))\n",
    "            final['# of Citations'].append(total_citations_by_year.get(str(year), 0))\n",
    "        \n",
    "        if 'unknown' in total_citations_by_year:\n",
    "            final['Year'].append('unknown')\n",
    "            final['# of Citations'].append(total_citations_by_year['unknown'])\n",
    "            \n",
    "\n",
    "        with open(f'{profile_dir}/{dr_ntu_id}.json', 'r') as f:\n",
    "            profile = json.load(f)\n",
    "            profile['citations_by_year'] = final\n",
    "        \n",
    "        with open(f'{profile_dir}/{dr_ntu_id}.json', 'w') as f:\n",
    "            json.dump(profile, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h-index & i10-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:00, 216.19it/s]\n"
     ]
    }
   ],
   "source": [
    "cur_year = datetime.datetime.now().year\n",
    "\n",
    "for i, row in tqdm(faculties.iterrows()):\n",
    "    name, google_scholar_id, dr_ntu_id = row['full_name'], row['google_scholar_id'], row['dr_ntu_id']\n",
    "\n",
    "    publication_file = f\"{process_publications_dir}/{google_scholar_id}.json\"\n",
    "\n",
    "    if os.path.exists(publication_file):\n",
    "        with open(publication_file, 'r') as f:\n",
    "            pubs = json.load(f)\n",
    "    \n",
    "        citations_list = []\n",
    "        by_year = {}\n",
    "        by_publication_year = {}\n",
    "        by_year_since_published = {}\n",
    "\n",
    "\n",
    "        min_year = float('inf')\n",
    "        for details in pubs:\n",
    "            if details['publication_year'] is not None:\n",
    "                citations_by_year = details['citations_by_year']\n",
    "                publication_year = details['publication_year']\n",
    "                min_year = min(publication_year, min_year)\n",
    "\n",
    "                cur_year_since_published = by_year_since_published.get(publication_year, {})\n",
    "                total_citations = 0\n",
    "                sorted_year_index = sorted([x for x in range(len(citations_by_year['year']))], key=lambda x: str(citations_by_year['year'][x]))\n",
    "                \n",
    "                for i in sorted_year_index:\n",
    "                    year = citations_by_year['year'][i]\n",
    "                    citations = citations_by_year['num_citations'][i]\n",
    "                    # No unknowns\n",
    "                    if year!='unknown':\n",
    "                        total_citations += citations\n",
    "                        by_year[year] = by_year.get(year, []) + [total_citations]\n",
    "                        cur_year_since_published[year] = cur_year_since_published.get(year, []) + [total_citations]\n",
    "                \n",
    "                citations_list.append(total_citations)\n",
    "                by_publication_year[publication_year] = by_publication_year.get(publication_year, []) + [total_citations]\n",
    "                by_year_since_published[publication_year] = cur_year_since_published    \n",
    "\n",
    "\n",
    "        h_index_by_year_df = {'Year': [], 'h-index': []}\n",
    "        h_index_by_publication_year_df = {'Publication Year': [], 'h-index': []}\n",
    "        avg_citations_by_publication_year_df = {'Publication Year': [], 'Avg Citations per Publication': []}\n",
    "        h_index_by_year_since_published_df = {'Publication Year': [], 'Year': [], 'h-index': []}\n",
    "        for year in range(min_year, cur_year+1):\n",
    "            h_index_by_year_df['Year'].append(year)\n",
    "            h_index_by_year_df['h-index'].append(get_h_index(by_year.get(year,[])))\n",
    "\n",
    "            h_index_by_publication_year_df['Publication Year'].append(year)\n",
    "            h_index_by_publication_year_df['h-index'].append(get_h_index(by_publication_year.get(year, [])))\n",
    "\n",
    "            avg_citations_by_publication_year_df['Publication Year'].append(year)\n",
    "            avg_citations_by_publication_year_df['Avg Citations per Publication'].append(np.mean(by_publication_year.get(year, [0])))\n",
    "\n",
    "            for y in range(year, cur_year+1):\n",
    "                h_index_by_year_since_published_df['Publication Year'].append(year)\n",
    "                h_index_by_year_since_published_df['Year'].append(y)\n",
    "                h_index_by_year_since_published_df['h-index'].append(get_h_index(by_year_since_published.get(year,{}).get(y, [])))\n",
    "        \n",
    "        all_time_h_index = get_h_index(citations_list)\n",
    "        all_time_i10_index = len([i for i in citations_list if i>=10])\n",
    "        all_time_i20_index = len([i for i in citations_list if i>=20])\n",
    "        \n",
    "        with open(f'{profile_dir}/{dr_ntu_id}.json', 'r') as f:\n",
    "            profile = json.load(f)\n",
    "            profile['all_time_h_index'] = all_time_h_index\n",
    "            profile['all_time_i10_index'] = all_time_i10_index\n",
    "            profile['all_time_i20_index'] = all_time_i20_index\n",
    "            profile['h_index_by_year'] = h_index_by_year_df\n",
    "            profile['h_index_by_publication_year'] = h_index_by_publication_year_df\n",
    "            profile['avg_citations_by_publication_year'] = avg_citations_by_publication_year_df\n",
    "            profile['h_index_by_years_from_publication_year'] = h_index_by_year_since_published_df\n",
    "        \n",
    "        with open(f'{profile_dir}/{dr_ntu_id}.json', 'w') as f:\n",
    "            json.dump(profile, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scse_dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
